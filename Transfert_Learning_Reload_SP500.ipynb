{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Transfert_Learning_Vgg16forSP500.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/imiled/DL_Tools_For_Finance/blob/master/Transfert_Learning_Reload_SP500.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QGkfWRfEIyvy",
        "outputId": "815a33a0-08c7-4b66-a597-fd79ebdcf28f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SmqFDg5Dxk7m",
        "outputId": "d057f892-9bd6-4762-9f3e-87d6d38976cc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "%pwd"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J597ZlrUNtzb",
        "outputId": "fc915e90-7e44-4bd6-d936-fe1192855f30",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[0m\u001b[01;34mdrive\u001b[0m/  \u001b[01;34msample_data\u001b[0m/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cAHcIW__zC2D"
      },
      "source": [
        "Downloading the project from github"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dWRUvmdLI64s",
        "outputId": "23212022-f310-4d04-fc6f-abc56b8fe7b1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "!git clone https://github.com/imiled/DL_Tools_For_Finance.git\n",
        "#!git clone https://korakot:$password@github.com/korakot/myrepo\n",
        "\n",
        " "
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'DL_Tools_For_Finance'...\n",
            "remote: Enumerating objects: 254, done.\u001b[K\n",
            "remote: Counting objects: 100% (254/254), done.\u001b[K\n",
            "remote: Compressing objects: 100% (46/46), done.\u001b[K\n",
            "remote: Total 934 (delta 230), reused 204 (delta 204), pack-reused 680\u001b[K\n",
            "Receiving objects: 100% (934/934), 206.64 MiB | 22.61 MiB/s, done.\n",
            "Resolving deltas: 100% (516/516), done.\n",
            "Checking out files: 100% (61/61), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1LgIdJgy1U3_",
        "outputId": "c91d955e-0574-4a7b-f29d-2fdb87005bc6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 894
        }
      },
      "source": [
        "%cd /content/DL_Tools_For_Finance  \n",
        "!git init\n",
        "!git status\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ERROR:root:Internal Python error in the inspect module.\n",
            "Below is the traceback from this internal error.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-3-3fa0fa7b6dc4>\", line 1, in <module>\n",
            "    get_ipython().magic('cd /content/DL_Tools_For_Finance')\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2160, in magic\n",
            "    return self.run_line_magic(magic_name, magic_arg_s)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2081, in run_line_magic\n",
            "    result = fn(*args,**kwargs)\n",
            "  File \"<decorator-gen-91>\", line 2, in cd\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/magic.py\", line 188, in <lambda>\n",
            "    call = lambda f, *a, **k: f(*a, **k)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/magics/osm.py\", line 288, in cd\n",
            "    oldcwd = py3compat.getcwd()\n",
            "OSError: [Errno 107] Transport endpoint is not connected\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 1823, in showtraceback\n",
            "    stb = value._render_traceback_()\n",
            "AttributeError: 'OSError' object has no attribute '_render_traceback_'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/ultratb.py\", line 1132, in get_records\n",
            "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/ultratb.py\", line 313, in wrapped\n",
            "    return f(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/ultratb.py\", line 358, in _fixed_getinnerframes\n",
            "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
            "  File \"/usr/lib/python3.6/inspect.py\", line 1490, in getinnerframes\n",
            "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
            "  File \"/usr/lib/python3.6/inspect.py\", line 1448, in getframeinfo\n",
            "    filename = getsourcefile(frame) or getfile(frame)\n",
            "  File \"/usr/lib/python3.6/inspect.py\", line 696, in getsourcefile\n",
            "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
            "  File \"/usr/lib/python3.6/inspect.py\", line 725, in getmodule\n",
            "    file = getabsfile(object, _filename)\n",
            "  File \"/usr/lib/python3.6/inspect.py\", line 709, in getabsfile\n",
            "    return os.path.normcase(os.path.abspath(_filename))\n",
            "  File \"/usr/lib/python3.6/posixpath.py\", line 383, in abspath\n",
            "    cwd = os.getcwd()\n",
            "OSError: [Errno 107] Transport endpoint is not connected\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QDgwDAvbxhHX"
      },
      "source": [
        "!git log"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n8y-NAfI2Bpp"
      },
      "source": [
        "!ls > hello.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F1se_dxs2DyU",
        "outputId": "409c88d5-763e-4a5e-9488-5a01cf257a6a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "source": [
        "!cat hello.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "datas\n",
            "hello.txt\n",
            "ImageM\n",
            "LICENSE\n",
            "MainNotebook\n",
            "MANIFEST.in\n",
            "model\n",
            "Notebooks\n",
            "README.md\n",
            "requirements.txt\n",
            "setup.cfg\n",
            "setup.py\n",
            "step1_generate_dataset_IndexImage.py\n",
            "step2_loadingtrainingdatas_vgg_transfert_modelandtraining.py\n",
            "step3_evaluate_vggsp500_model.py\n",
            "step4_guess_future_marketstate_from_image.py\n",
            "tests\n",
            "TFM Imiled 2019-2020_Deep Learning application to Finance.docx\n",
            "trainer\n",
            "Transfert_Learning_Vgg16forSP500.ipynb\n",
            "versioneer.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4_dnJgtZMHHv",
        "outputId": "8b32395b-b120-4dd2-a534-14042eb32c74",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329
        }
      },
      "source": [
        "%cd /content/DL_Tools_For_Finance  \n",
        "!git add .\n",
        "!ls > hello.txt\n",
        "!git add hello.txt\n",
        "!git commit -m \"reorganisation of files\"\n",
        "!git push origin master          # push to github\n",
        "\n",
        "!git config user.email 'miledismael@gmail.com'\n",
        "!git config user.name 'imiled'\n",
        "\n",
        "username=input(\"what is you github username?\\n\")\n",
        "from getpass import getpass\n",
        "password = getpass('Password:')\n",
        "!git remote add origin https://$username:$password@github.com/$username/reponame.git\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/DL_Tools_For_Finance\n",
            "\n",
            "*** Please tell me who you are.\n",
            "\n",
            "Run\n",
            "\n",
            "  git config --global user.email \"you@example.com\"\n",
            "  git config --global user.name \"Your Name\"\n",
            "\n",
            "to set your account's default identity.\n",
            "Omit --global to set the identity only in this repository.\n",
            "\n",
            "fatal: unable to auto-detect email address (got 'root@4544a85cfe70.(none)')\n",
            "fatal: could not read Username for 'https://github.com': No such device or address\n",
            "what is you github username?\n",
            "imiled\n",
            "Password:··········\n",
            "fatal: remote origin already exists.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nr7TjeHXy80v"
      },
      "source": [
        "Saving dataset from drive to local"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3TiNDoAbMJx2",
        "outputId": "2461143e-2974-4951-d714-b6fd52a07124",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        }
      },
      "source": [
        "%cd /content/drive/My Drive/A_transfertTFMVggSP500\n",
        "%cp DatasetVggSP500.zip /content/DL_Tools_For_Finance/datas\n",
        "%cd /content/DL_Tools_For_Finance/datas\n",
        "!unzip DatasetVggSP500.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/A_transfertTFMVggSP500\n",
            "/content/DL_Tools_For_Finance/datas\n",
            "Archive:  DatasetVggSP500.zip\n",
            "replace X_test_image.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: A\n",
            "  inflating: X_test_image.csv        \n",
            "  inflating: X_train_image.csv       \n",
            "  inflating: Y_test_FutPredict_image.csv  \n",
            "  inflating: Y_test_StateClass_image.csv  \n",
            "  inflating: Y_train_FutPredict_image.csv  \n",
            "  inflating: Y_train_StateClass_image.csv  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mq_ZXp4zzQ7l"
      },
      "source": [
        "Saving latest model from drive to local"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iSxPytYKNjKH",
        "outputId": "be7754fe-5473-4898-a83b-df7e6c673fe3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        }
      },
      "source": [
        "%cd /content/drive/My Drive/A_transfertTFMVggSP500 \n",
        "%cp model/final_modelcategorical_crossentropy_adam_32.h5 /content/DL_Tools_For_Finance/model\n",
        "%cp vggforsp500.h5 /content/DL_Tools_For_Finance/model"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Errno 2] No such file or directory: '/content/drive/My Drive/A_transfertTFMVggSP500'\n",
            "/content/DL_Tools_For_Finance/datas\n",
            "cp: cannot stat 'model/final_modelcategorical_crossentropy_adam_32.h5': No such file or directory\n",
            "cp: cannot stat 'vggforsp500.h5': No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VehZVTphSljL"
      },
      "source": [
        "###Requirement instalation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EEpibDu_Ovpv",
        "outputId": "6419005f-7498-42fe-d00c-7a3a267ce5d7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "%cd /content/DL_Tools_For_Finance\n",
        "!pip3 install -r requirements.txt  "
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/DL_Tools_For_Finance\n",
            "Collecting tensorflow==1.15.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8e/64/7a19837dd54d3f53b1ce5ae346ab401dde9678e8f233220317000bfdb3e2/tensorflow-1.15.4-cp36-cp36m-manylinux2010_x86_64.whl (110.5MB)\n",
            "\u001b[K     |████████████████████████████████| 110.5MB 102kB/s \n",
            "\u001b[?25hCollecting Keras==2.3.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ad/fd/6bfe87920d7f4fd475acd28500a42482b6b84479832bdc0fe9e589a60ceb/Keras-2.3.1-py2.py3-none-any.whl (377kB)\n",
            "\u001b[K     |████████████████████████████████| 378kB 64.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 8)) (1.1.2)\n",
            "Collecting numpy>=1.19.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/63/97/af8a92864a04bfa48f1b5c9b1f8bf2ccb2847f24530026f26dd223de4ca0/numpy-1.19.2-cp36-cp36m-manylinux2010_x86_64.whl (14.5MB)\n",
            "\u001b[K     |████████████████████████████████| 14.5MB 335kB/s \n",
            "\u001b[?25hRequirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 10)) (0.22.2.post1)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 11)) (0.16.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 12)) (1.4.1)\n",
            "Requirement already satisfied: pandas_datareader in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 13)) (0.9.0)\n",
            "Collecting yfinance\n",
            "  Downloading https://files.pythonhosted.org/packages/7a/e8/b9d7104d3a4bf39924799067592d9e59119fcfc900a425a12e80a3123ec8/yfinance-0.1.55.tar.gz\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.4->-r requirements.txt (line 2)) (3.12.4)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.4->-r requirements.txt (line 2)) (0.35.1)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.4->-r requirements.txt (line 2)) (1.32.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.4->-r requirements.txt (line 2)) (0.10.0)\n",
            "Collecting tensorboard<1.16.0,>=1.15.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1e/e9/d3d747a97f7188f48aa5eda486907f3b345cd409f0a0850468ba867db246/tensorboard-1.15.0-py3-none-any.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 63.8MB/s \n",
            "\u001b[?25hCollecting tensorflow-estimator==1.15.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/de/62/2ee9cd74c9fa2fa450877847ba560b260f5d0fb70ee0595203082dafcc9d/tensorflow_estimator-1.15.1-py2.py3-none-any.whl (503kB)\n",
            "\u001b[K     |████████████████████████████████| 512kB 62.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.4->-r requirements.txt (line 2)) (0.8.1)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.4->-r requirements.txt (line 2)) (0.2.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.4->-r requirements.txt (line 2)) (1.1.2)\n",
            "Collecting gast==0.2.2\n",
            "  Downloading https://files.pythonhosted.org/packages/4e/35/11749bf99b2d4e3cceb4d55ca22590b0d7c2c62b9de38ac4a4a7f4687421/gast-0.2.2.tar.gz\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.4->-r requirements.txt (line 2)) (3.3.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.4->-r requirements.txt (line 2)) (1.12.1)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.4->-r requirements.txt (line 2)) (1.1.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.4->-r requirements.txt (line 2)) (1.15.0)\n",
            "Collecting keras-applications>=1.0.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl (50kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 6.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from Keras==2.3.1->-r requirements.txt (line 3)) (2.10.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from Keras==2.3.1->-r requirements.txt (line 3)) (3.13)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas->-r requirements.txt (line 8)) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->-r requirements.txt (line 8)) (2018.9)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->-r requirements.txt (line 10)) (0.16.0)\n",
            "Requirement already satisfied: pillow>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image->-r requirements.txt (line 11)) (7.0.0)\n",
            "Requirement already satisfied: PyWavelets>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image->-r requirements.txt (line 11)) (1.1.1)\n",
            "Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image->-r requirements.txt (line 11)) (2.4.1)\n",
            "Requirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image->-r requirements.txt (line 11)) (3.2.2)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image->-r requirements.txt (line 11)) (2.5)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.6/dist-packages (from pandas_datareader->-r requirements.txt (line 13)) (2.23.0)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.6/dist-packages (from pandas_datareader->-r requirements.txt (line 13)) (4.2.6)\n",
            "Requirement already satisfied: multitasking>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from yfinance->-r requirements.txt (line 16)) (0.0.9)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow==1.15.4->-r requirements.txt (line 2)) (50.3.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.4->-r requirements.txt (line 2)) (3.2.2)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.4->-r requirements.txt (line 2)) (1.0.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->-r requirements.txt (line 11)) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->-r requirements.txt (line 11)) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->-r requirements.txt (line 11)) (2.4.7)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx>=2.0->scikit-image->-r requirements.txt (line 11)) (4.4.2)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->pandas_datareader->-r requirements.txt (line 13)) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->pandas_datareader->-r requirements.txt (line 13)) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->pandas_datareader->-r requirements.txt (line 13)) (2020.6.20)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->pandas_datareader->-r requirements.txt (line 13)) (3.0.4)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.4->-r requirements.txt (line 2)) (2.0.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.4->-r requirements.txt (line 2)) (3.2.0)\n",
            "Building wheels for collected packages: yfinance, gast\n",
            "  Building wheel for yfinance (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for yfinance: filename=yfinance-0.1.55-py2.py3-none-any.whl size=22618 sha256=31d9b5bb2a5c9c1cc6b104340f1bb1755714f0e4a9806783f9813170bad798c4\n",
            "  Stored in directory: /root/.cache/pip/wheels/04/98/cc/2702a4242d60bdc14f48b4557c427ded1fe92aedf257d4565c\n",
            "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gast: filename=gast-0.2.2-cp36-none-any.whl size=7542 sha256=baad71e8cd125f01787dc753df1849d6f6aa603b20967b522c49c1fafa83e2eb\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd\n",
            "Successfully built yfinance gast\n",
            "\u001b[31mERROR: tensorflow 1.15.4 has requirement numpy<1.19.0,>=1.16.0, but you'll have numpy 1.19.2 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow-probability 0.11.0 has requirement gast>=0.3.2, but you'll have gast 0.2.2 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: yfinance 0.1.55 has requirement lxml>=4.5.1, but you'll have lxml 4.2.6 which is incompatible.\u001b[0m\n",
            "Installing collected packages: numpy, tensorboard, tensorflow-estimator, gast, keras-applications, tensorflow, Keras, yfinance\n",
            "  Found existing installation: numpy 1.18.5\n",
            "    Uninstalling numpy-1.18.5:\n",
            "      Successfully uninstalled numpy-1.18.5\n",
            "  Found existing installation: tensorboard 2.3.0\n",
            "    Uninstalling tensorboard-2.3.0:\n",
            "      Successfully uninstalled tensorboard-2.3.0\n",
            "  Found existing installation: tensorflow-estimator 2.3.0\n",
            "    Uninstalling tensorflow-estimator-2.3.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.3.0\n",
            "  Found existing installation: gast 0.3.3\n",
            "    Uninstalling gast-0.3.3:\n",
            "      Successfully uninstalled gast-0.3.3\n",
            "  Found existing installation: tensorflow 2.3.0\n",
            "    Uninstalling tensorflow-2.3.0:\n",
            "      Successfully uninstalled tensorflow-2.3.0\n",
            "  Found existing installation: Keras 2.4.3\n",
            "    Uninstalling Keras-2.4.3:\n",
            "      Successfully uninstalled Keras-2.4.3\n",
            "Successfully installed Keras-2.3.1 gast-0.2.2 keras-applications-1.0.8 numpy-1.19.2 tensorboard-1.15.0 tensorflow-1.15.4 tensorflow-estimator-1.15.1 yfinance-0.1.55\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dTwxPiI4zLz3"
      },
      "source": [
        "##Part with all steps through python command##"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_SEuIXmtTG26",
        "outputId": "35fef0a0-efce-4166-8dd0-77856a90d251",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "%cd /content/DL_Tools_For_Finance\n",
        "!python3 step1_generate_dataset_IndexImage.py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Errno 2] No such file or directory: '/content/DL_Tools_For_Finance'\n",
            "/content\n",
            "python3: can't open file 'step1_generate_dataset_IndexImage.py': [Errno 2] No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jKbZEi02Ti5r",
        "outputId": "fd6b1d56-610a-4156-cc40-48381fc4c2b7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "%cd /content/DL_Tools_For_Finance\n",
        "!python3 step2_loadingtrainingdatas_vgg_transfert_modelandtraining.py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/DL_Tools_For_Finance\n",
            "2020-08-31 14:00:07.536779: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-08-31 14:00:23.283485: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1\n",
            "2020-08-31 14:00:23.354418: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-08-31 14:00:23.355305: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \n",
            "pciBusID: 0000:00:04.0 name: Tesla T4 computeCapability: 7.5\n",
            "coreClock: 1.59GHz coreCount: 40 deviceMemorySize: 14.73GiB deviceMemoryBandwidth: 298.08GiB/s\n",
            "2020-08-31 14:00:23.355378: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-08-31 14:00:23.618262: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\n",
            "2020-08-31 14:00:23.772071: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10\n",
            "2020-08-31 14:00:23.799494: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10\n",
            "2020-08-31 14:00:24.091903: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10\n",
            "2020-08-31 14:00:24.121391: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10\n",
            "2020-08-31 14:00:24.631937: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7\n",
            "2020-08-31 14:00:24.632162: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-08-31 14:00:24.632916: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-08-31 14:00:24.633516: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\n",
            "2020-08-31 14:00:24.633867: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX512F\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2020-08-31 14:00:24.655218: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2000125000 Hz\n",
            "2020-08-31 14:00:24.655444: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x1b70f40 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
            "2020-08-31 14:00:24.655479: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
            "2020-08-31 14:00:24.794351: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-08-31 14:00:24.795101: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x1b70d80 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "2020-08-31 14:00:24.795132: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
            "2020-08-31 14:00:24.796328: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-08-31 14:00:24.796900: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \n",
            "pciBusID: 0000:00:04.0 name: Tesla T4 computeCapability: 7.5\n",
            "coreClock: 1.59GHz coreCount: 40 deviceMemorySize: 14.73GiB deviceMemoryBandwidth: 298.08GiB/s\n",
            "2020-08-31 14:00:24.796955: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-08-31 14:00:24.796995: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\n",
            "2020-08-31 14:00:24.797016: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10\n",
            "2020-08-31 14:00:24.797040: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10\n",
            "2020-08-31 14:00:24.797060: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10\n",
            "2020-08-31 14:00:24.797078: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10\n",
            "2020-08-31 14:00:24.797098: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7\n",
            "2020-08-31 14:00:24.797174: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-08-31 14:00:24.797845: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-08-31 14:00:24.798390: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\n",
            "2020-08-31 14:00:24.804305: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-08-31 14:00:28.654055: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2020-08-31 14:00:28.654111: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0 \n",
            "2020-08-31 14:00:28.654124: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N \n",
            "2020-08-31 14:00:28.661735: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-08-31 14:00:28.662361: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-08-31 14:00:28.662893: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2020-08-31 14:00:28.662938: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 13962 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "58892288/58889256 [==============================] - 1s 0us/step\n",
            "Model: \"vgg16\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 32, 32, 3)]       0         \n",
            "_________________________________________________________________\n",
            "block1_conv1 (Conv2D)        (None, 32, 32, 64)        1792      \n",
            "_________________________________________________________________\n",
            "block1_conv2 (Conv2D)        (None, 32, 32, 64)        36928     \n",
            "_________________________________________________________________\n",
            "block1_pool (MaxPooling2D)   (None, 16, 16, 64)        0         \n",
            "_________________________________________________________________\n",
            "block2_conv1 (Conv2D)        (None, 16, 16, 128)       73856     \n",
            "_________________________________________________________________\n",
            "block2_conv2 (Conv2D)        (None, 16, 16, 128)       147584    \n",
            "_________________________________________________________________\n",
            "block2_pool (MaxPooling2D)   (None, 8, 8, 128)         0         \n",
            "_________________________________________________________________\n",
            "block3_conv1 (Conv2D)        (None, 8, 8, 256)         295168    \n",
            "_________________________________________________________________\n",
            "block3_conv2 (Conv2D)        (None, 8, 8, 256)         590080    \n",
            "_________________________________________________________________\n",
            "block3_conv3 (Conv2D)        (None, 8, 8, 256)         590080    \n",
            "_________________________________________________________________\n",
            "block3_pool (MaxPooling2D)   (None, 4, 4, 256)         0         \n",
            "_________________________________________________________________\n",
            "block4_conv1 (Conv2D)        (None, 4, 4, 512)         1180160   \n",
            "_________________________________________________________________\n",
            "block4_conv2 (Conv2D)        (None, 4, 4, 512)         2359808   \n",
            "_________________________________________________________________\n",
            "block4_conv3 (Conv2D)        (None, 4, 4, 512)         2359808   \n",
            "_________________________________________________________________\n",
            "block4_pool (MaxPooling2D)   (None, 2, 2, 512)         0         \n",
            "_________________________________________________________________\n",
            "block5_conv1 (Conv2D)        (None, 2, 2, 512)         2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv2 (Conv2D)        (None, 2, 2, 512)         2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv3 (Conv2D)        (None, 2, 2, 512)         2359808   \n",
            "_________________________________________________________________\n",
            "block5_pool (MaxPooling2D)   (None, 1, 1, 512)         0         \n",
            "=================================================================\n",
            "Total params: 14,714,688\n",
            "Trainable params: 0\n",
            "Non-trainable params: 14,714,688\n",
            "_________________________________________________________________\n",
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
            "Epoch 1/50\n",
            "2020-08-31 14:00:30.921687: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\n",
            "2020-08-31 14:00:32.390571: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7\n",
            "149/149 [==============================] - ETA: 0s - loss: 2.0473 - accuracy: 0.3377\n",
            "Epoch 00001: loss improved from inf to 2.04734, saving model to model/best_modelcategorical_crossentropy_adam_Batch100_LR0.1_0.99.hdf5\n",
            "149/149 [==============================] - 3s 21ms/step - loss: 2.0473 - accuracy: 0.3377 - val_loss: 1.4949 - val_accuracy: 0.3399\n",
            "Epoch 2/50\n",
            "145/149 [============================>.] - ETA: 0s - loss: 1.5323 - accuracy: 0.3433\n",
            "Epoch 00002: loss improved from 2.04734 to 1.53141, saving model to model/best_modelcategorical_crossentropy_adam_Batch100_LR0.1_0.99.hdf5\n",
            "149/149 [==============================] - 3s 17ms/step - loss: 1.5314 - accuracy: 0.3436 - val_loss: 1.4906 - val_accuracy: 0.3399\n",
            "Epoch 3/50\n",
            "149/149 [==============================] - ETA: 0s - loss: 1.5308 - accuracy: 0.3436\n",
            "Epoch 00003: loss improved from 1.53141 to 1.53084, saving model to model/best_modelcategorical_crossentropy_adam_Batch100_LR0.1_0.99.hdf5\n",
            "149/149 [==============================] - 3s 17ms/step - loss: 1.5308 - accuracy: 0.3436 - val_loss: 1.4926 - val_accuracy: 0.3399\n",
            "Epoch 4/50\n",
            "149/149 [==============================] - ETA: 0s - loss: 1.5320 - accuracy: 0.3436\n",
            "Epoch 00004: loss did not improve from 1.53084\n",
            "149/149 [==============================] - 2s 16ms/step - loss: 1.5320 - accuracy: 0.3436 - val_loss: 1.4935 - val_accuracy: 0.3399\n",
            "Epoch 5/50\n",
            "145/149 [============================>.] - ETA: 0s - loss: 1.5321 - accuracy: 0.3389\n",
            "Epoch 00005: loss did not improve from 1.53084\n",
            "149/149 [==============================] - 2s 16ms/step - loss: 1.5314 - accuracy: 0.3394 - val_loss: 1.5005 - val_accuracy: 0.3399\n",
            "Epoch 6/50\n",
            "145/149 [============================>.] - ETA: 0s - loss: 1.5315 - accuracy: 0.3385\n",
            "Epoch 00006: loss did not improve from 1.53084\n",
            "149/149 [==============================] - 2s 16ms/step - loss: 1.5321 - accuracy: 0.3381 - val_loss: 1.5235 - val_accuracy: 0.3399\n",
            "Epoch 7/50\n",
            "149/149 [==============================] - ETA: 0s - loss: 1.5318 - accuracy: 0.3414\n",
            "Epoch 00007: loss did not improve from 1.53084\n",
            "149/149 [==============================] - 2s 16ms/step - loss: 1.5318 - accuracy: 0.3414 - val_loss: 1.5054 - val_accuracy: 0.3399\n",
            "Epoch 8/50\n",
            "149/149 [==============================] - ETA: 0s - loss: 1.5345 - accuracy: 0.3360\n",
            "Epoch 00008: loss did not improve from 1.53084\n",
            "149/149 [==============================] - 2s 17ms/step - loss: 1.5345 - accuracy: 0.3360 - val_loss: 1.4982 - val_accuracy: 0.3399\n",
            "Epoch 9/50\n",
            "145/149 [============================>.] - ETA: 0s - loss: 1.5337 - accuracy: 0.3428\n",
            "Epoch 00009: loss did not improve from 1.53084\n",
            "149/149 [==============================] - 2s 17ms/step - loss: 1.5329 - accuracy: 0.3436 - val_loss: 1.5099 - val_accuracy: 0.3399\n",
            "Epoch 10/50\n",
            "149/149 [==============================] - ETA: 0s - loss: 1.5336 - accuracy: 0.3358\n",
            "Epoch 00010: loss did not improve from 1.53084\n",
            "149/149 [==============================] - 2s 16ms/step - loss: 1.5336 - accuracy: 0.3358 - val_loss: 1.4912 - val_accuracy: 0.3399\n",
            "Epoch 11/50\n",
            "145/149 [============================>.] - ETA: 0s - loss: 1.5348 - accuracy: 0.3426\n",
            "Epoch 00011: loss did not improve from 1.53084\n",
            "149/149 [==============================] - 2s 16ms/step - loss: 1.5333 - accuracy: 0.3436 - val_loss: 1.4965 - val_accuracy: 0.3399\n",
            "Epoch 12/50\n",
            "145/149 [============================>.] - ETA: 0s - loss: 1.5330 - accuracy: 0.3417\n",
            "Epoch 00012: loss did not improve from 1.53084\n",
            "149/149 [==============================] - 2s 16ms/step - loss: 1.5326 - accuracy: 0.3422 - val_loss: 1.5040 - val_accuracy: 0.3399\n",
            "Epoch 13/50\n",
            "145/149 [============================>.] - ETA: 0s - loss: 1.5341 - accuracy: 0.3383\n",
            "Epoch 00013: loss did not improve from 1.53084\n",
            "149/149 [==============================] - 2s 16ms/step - loss: 1.5341 - accuracy: 0.3381 - val_loss: 1.4915 - val_accuracy: 0.3399\n",
            "Epoch 14/50\n",
            "145/149 [============================>.] - ETA: 0s - loss: 1.5342 - accuracy: 0.3352\n",
            "Epoch 00014: loss did not improve from 1.53084\n",
            "149/149 [==============================] - 2s 16ms/step - loss: 1.5330 - accuracy: 0.3366 - val_loss: 1.5078 - val_accuracy: 0.3399\n",
            "Epoch 15/50\n",
            "149/149 [==============================] - ETA: 0s - loss: 1.5326 - accuracy: 0.3412\n",
            "Epoch 00015: loss did not improve from 1.53084\n",
            "149/149 [==============================] - 2s 16ms/step - loss: 1.5326 - accuracy: 0.3412 - val_loss: 1.5212 - val_accuracy: 0.3399\n",
            "Epoch 16/50\n",
            "149/149 [==============================] - ETA: 0s - loss: 1.5344 - accuracy: 0.3402\n",
            "Epoch 00016: loss did not improve from 1.53084\n",
            "149/149 [==============================] - 2s 16ms/step - loss: 1.5344 - accuracy: 0.3402 - val_loss: 1.5016 - val_accuracy: 0.3399\n",
            "Epoch 17/50\n",
            "149/149 [==============================] - ETA: 0s - loss: 1.5320 - accuracy: 0.3436\n",
            "Epoch 00017: loss did not improve from 1.53084\n",
            "149/149 [==============================] - 2s 17ms/step - loss: 1.5320 - accuracy: 0.3436 - val_loss: 1.4908 - val_accuracy: 0.3399\n",
            "Epoch 18/50\n",
            "145/149 [============================>.] - ETA: 0s - loss: 1.5335 - accuracy: 0.3357\n",
            "Epoch 00018: loss did not improve from 1.53084\n",
            "149/149 [==============================] - 2s 16ms/step - loss: 1.5332 - accuracy: 0.3360 - val_loss: 1.5041 - val_accuracy: 0.3399\n",
            "Epoch 19/50\n",
            "149/149 [==============================] - ETA: 0s - loss: 1.5333 - accuracy: 0.3388\n",
            "Epoch 00019: loss did not improve from 1.53084\n",
            "149/149 [==============================] - 2s 16ms/step - loss: 1.5333 - accuracy: 0.3388 - val_loss: 1.4987 - val_accuracy: 0.3399\n",
            "Epoch 20/50\n",
            "145/149 [============================>.] - ETA: 0s - loss: 1.5306 - accuracy: 0.3443\n",
            "Epoch 00020: loss improved from 1.53084 to 1.53068, saving model to model/best_modelcategorical_crossentropy_adam_Batch100_LR0.1_0.99.hdf5\n",
            "149/149 [==============================] - 3s 17ms/step - loss: 1.5307 - accuracy: 0.3436 - val_loss: 1.4947 - val_accuracy: 0.3399\n",
            "Epoch 21/50\n",
            "149/149 [==============================] - ETA: 0s - loss: 1.5315 - accuracy: 0.3436\n",
            "Epoch 00021: loss did not improve from 1.53068\n",
            "149/149 [==============================] - 3s 17ms/step - loss: 1.5315 - accuracy: 0.3436 - val_loss: 1.4899 - val_accuracy: 0.3399\n",
            "Epoch 22/50\n",
            "145/149 [============================>.] - ETA: 0s - loss: 1.5312 - accuracy: 0.3430\n",
            "Epoch 00022: loss did not improve from 1.53068\n",
            "149/149 [==============================] - 2s 17ms/step - loss: 1.5323 - accuracy: 0.3421 - val_loss: 1.5026 - val_accuracy: 0.3399\n",
            "Epoch 23/50\n",
            "145/149 [============================>.] - ETA: 0s - loss: 1.5311 - accuracy: 0.3438\n",
            "Epoch 00023: loss did not improve from 1.53068\n",
            "149/149 [==============================] - 2s 17ms/step - loss: 1.5313 - accuracy: 0.3436 - val_loss: 1.5046 - val_accuracy: 0.3399\n",
            "Epoch 24/50\n",
            "149/149 [==============================] - ETA: 0s - loss: 1.5319 - accuracy: 0.3413\n",
            "Epoch 00024: loss did not improve from 1.53068\n",
            "149/149 [==============================] - 2s 17ms/step - loss: 1.5319 - accuracy: 0.3413 - val_loss: 1.5038 - val_accuracy: 0.3399\n",
            "Epoch 25/50\n",
            "145/149 [============================>.] - ETA: 0s - loss: 1.5322 - accuracy: 0.3435\n",
            "Epoch 00025: loss did not improve from 1.53068\n",
            "149/149 [==============================] - 2s 17ms/step - loss: 1.5322 - accuracy: 0.3436 - val_loss: 1.5049 - val_accuracy: 0.3399\n",
            "Epoch 26/50\n",
            "149/149 [==============================] - ETA: 0s - loss: 1.5319 - accuracy: 0.3436\n",
            "Epoch 00026: loss did not improve from 1.53068\n",
            "149/149 [==============================] - 2s 17ms/step - loss: 1.5319 - accuracy: 0.3436 - val_loss: 1.4944 - val_accuracy: 0.3399\n",
            "Epoch 27/50\n",
            "149/149 [==============================] - ETA: 0s - loss: 1.5332 - accuracy: 0.3436\n",
            "Epoch 00027: loss did not improve from 1.53068\n",
            "149/149 [==============================] - 2s 17ms/step - loss: 1.5332 - accuracy: 0.3436 - val_loss: 1.5045 - val_accuracy: 0.3399\n",
            "Epoch 28/50\n",
            "145/149 [============================>.] - ETA: 0s - loss: 1.5328 - accuracy: 0.3399\n",
            "Epoch 00028: loss did not improve from 1.53068\n",
            "149/149 [==============================] - 2s 17ms/step - loss: 1.5326 - accuracy: 0.3402 - val_loss: 1.4924 - val_accuracy: 0.3399\n",
            "Epoch 29/50\n",
            "149/149 [==============================] - ETA: 0s - loss: 1.5317 - accuracy: 0.3436\n",
            "Epoch 00029: loss did not improve from 1.53068\n",
            "149/149 [==============================] - 2s 17ms/step - loss: 1.5317 - accuracy: 0.3436 - val_loss: 1.4979 - val_accuracy: 0.3399\n",
            "Epoch 30/50\n",
            "145/149 [============================>.] - ETA: 0s - loss: 1.5314 - accuracy: 0.3441\n",
            "Epoch 00030: loss did not improve from 1.53068\n",
            "149/149 [==============================] - 2s 17ms/step - loss: 1.5317 - accuracy: 0.3436 - val_loss: 1.4978 - val_accuracy: 0.3399\n",
            "Epoch 31/50\n",
            "149/149 [==============================] - ETA: 0s - loss: 1.5328 - accuracy: 0.3383\n",
            "Epoch 00031: loss did not improve from 1.53068\n",
            "149/149 [==============================] - 2s 17ms/step - loss: 1.5328 - accuracy: 0.3383 - val_loss: 1.5141 - val_accuracy: 0.3399\n",
            "Epoch 32/50\n",
            "149/149 [==============================] - ETA: 0s - loss: 1.5315 - accuracy: 0.3410\n",
            "Epoch 00032: loss did not improve from 1.53068\n",
            "149/149 [==============================] - 2s 17ms/step - loss: 1.5315 - accuracy: 0.3410 - val_loss: 1.4968 - val_accuracy: 0.3399\n",
            "Epoch 33/50\n",
            "149/149 [==============================] - ETA: 0s - loss: 1.5342 - accuracy: 0.3372\n",
            "Epoch 00033: loss did not improve from 1.53068\n",
            "149/149 [==============================] - 2s 17ms/step - loss: 1.5342 - accuracy: 0.3372 - val_loss: 1.4964 - val_accuracy: 0.3399\n",
            "Epoch 34/50\n",
            "149/149 [==============================] - ETA: 0s - loss: 1.5337 - accuracy: 0.3420\n",
            "Epoch 00034: loss did not improve from 1.53068\n",
            "149/149 [==============================] - 3s 17ms/step - loss: 1.5337 - accuracy: 0.3420 - val_loss: 1.4939 - val_accuracy: 0.3399\n",
            "Epoch 35/50\n",
            "149/149 [==============================] - ETA: 0s - loss: 1.5328 - accuracy: 0.3436\n",
            "Epoch 00035: loss did not improve from 1.53068\n",
            "149/149 [==============================] - 2s 17ms/step - loss: 1.5328 - accuracy: 0.3436 - val_loss: 1.4953 - val_accuracy: 0.3399\n",
            "Epoch 36/50\n",
            "145/149 [============================>.] - ETA: 0s - loss: 1.5336 - accuracy: 0.3441\n",
            "Epoch 00036: loss did not improve from 1.53068\n",
            "149/149 [==============================] - 3s 17ms/step - loss: 1.5341 - accuracy: 0.3436 - val_loss: 1.5010 - val_accuracy: 0.3399\n",
            "Epoch 37/50\n",
            "149/149 [==============================] - ETA: 0s - loss: 1.5328 - accuracy: 0.3399\n",
            "Epoch 00037: loss did not improve from 1.53068\n",
            "149/149 [==============================] - 3s 17ms/step - loss: 1.5328 - accuracy: 0.3399 - val_loss: 1.4952 - val_accuracy: 0.3399\n",
            "Epoch 38/50\n",
            "149/149 [==============================] - ETA: 0s - loss: 1.5319 - accuracy: 0.3415\n",
            "Epoch 00038: loss did not improve from 1.53068\n",
            "149/149 [==============================] - 2s 17ms/step - loss: 1.5319 - accuracy: 0.3415 - val_loss: 1.5012 - val_accuracy: 0.3399\n",
            "Epoch 39/50\n",
            "149/149 [==============================] - ETA: 0s - loss: 1.5335 - accuracy: 0.3417\n",
            "Epoch 00039: loss did not improve from 1.53068\n",
            "149/149 [==============================] - 3s 17ms/step - loss: 1.5335 - accuracy: 0.3417 - val_loss: 1.4931 - val_accuracy: 0.3399\n",
            "Epoch 40/50\n",
            "149/149 [==============================] - ETA: 0s - loss: 1.5328 - accuracy: 0.3427\n",
            "Epoch 00040: loss did not improve from 1.53068\n",
            "149/149 [==============================] - 3s 17ms/step - loss: 1.5328 - accuracy: 0.3427 - val_loss: 1.5152 - val_accuracy: 0.3399\n",
            "Epoch 41/50\n",
            "149/149 [==============================] - ETA: 0s - loss: 1.5332 - accuracy: 0.3414\n",
            "Epoch 00041: loss did not improve from 1.53068\n",
            "149/149 [==============================] - 3s 17ms/step - loss: 1.5332 - accuracy: 0.3414 - val_loss: 1.4931 - val_accuracy: 0.3399\n",
            "Epoch 42/50\n",
            "149/149 [==============================] - ETA: 0s - loss: 1.5312 - accuracy: 0.3436\n",
            "Epoch 00042: loss did not improve from 1.53068\n",
            "149/149 [==============================] - 3s 17ms/step - loss: 1.5312 - accuracy: 0.3436 - val_loss: 1.5019 - val_accuracy: 0.3399\n",
            "Epoch 43/50\n",
            "149/149 [==============================] - ETA: 0s - loss: 1.5320 - accuracy: 0.3397\n",
            "Epoch 00043: loss did not improve from 1.53068\n",
            "149/149 [==============================] - 3s 17ms/step - loss: 1.5320 - accuracy: 0.3397 - val_loss: 1.5109 - val_accuracy: 0.3399\n",
            "Epoch 44/50\n",
            "149/149 [==============================] - ETA: 0s - loss: 1.5329 - accuracy: 0.3436\n",
            "Epoch 00044: loss did not improve from 1.53068\n",
            "149/149 [==============================] - 3s 17ms/step - loss: 1.5329 - accuracy: 0.3436 - val_loss: 1.5056 - val_accuracy: 0.3399\n",
            "Epoch 45/50\n",
            "149/149 [==============================] - ETA: 0s - loss: 1.5321 - accuracy: 0.3436\n",
            "Epoch 00045: loss did not improve from 1.53068\n",
            "149/149 [==============================] - 3s 17ms/step - loss: 1.5321 - accuracy: 0.3436 - val_loss: 1.5010 - val_accuracy: 0.3399\n",
            "Epoch 46/50\n",
            "149/149 [==============================] - ETA: 0s - loss: 1.5316 - accuracy: 0.3409\n",
            "Epoch 00046: loss did not improve from 1.53068\n",
            "149/149 [==============================] - 3s 17ms/step - loss: 1.5316 - accuracy: 0.3409 - val_loss: 1.4912 - val_accuracy: 0.3399\n",
            "Epoch 47/50\n",
            "149/149 [==============================] - ETA: 0s - loss: 1.5313 - accuracy: 0.3436\n",
            "Epoch 00047: loss did not improve from 1.53068\n",
            "149/149 [==============================] - 3s 17ms/step - loss: 1.5313 - accuracy: 0.3436 - val_loss: 1.4899 - val_accuracy: 0.3399\n",
            "Epoch 48/50\n",
            "145/149 [============================>.] - ETA: 0s - loss: 1.5329 - accuracy: 0.3438\n",
            "Epoch 00048: loss did not improve from 1.53068\n",
            "149/149 [==============================] - 3s 17ms/step - loss: 1.5331 - accuracy: 0.3436 - val_loss: 1.5025 - val_accuracy: 0.3399\n",
            "Epoch 49/50\n",
            "145/149 [============================>.] - ETA: 0s - loss: 1.5322 - accuracy: 0.3412\n",
            "Epoch 00049: loss did not improve from 1.53068\n",
            "149/149 [==============================] - 3s 17ms/step - loss: 1.5326 - accuracy: 0.3408 - val_loss: 1.5109 - val_accuracy: 0.3399\n",
            "Epoch 50/50\n",
            "149/149 [==============================] - ETA: 0s - loss: 1.5326 - accuracy: 0.3436\n",
            "Epoch 00050: loss did not improve from 1.53068\n",
            "149/149 [==============================] - 3s 17ms/step - loss: 1.5326 - accuracy: 0.3436 - val_loss: 1.4915 - val_accuracy: 0.3399\n",
            "Traceback (most recent call last):\n",
            "  File \"step2_loadingtrainingdatas_vgg_transfert_modelandtraining.py\", line 160, in <module>\n",
            "    transfer_model.save(\"model/\"+datetime.now().strftime(\"%Y%m%d-%H%M%S\")+\"vggforsp500.h5\")\n",
            "NameError: name 'datetime' is not defined\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CjSc-KxlVc0L",
        "outputId": "b639276e-8b87-4388-99ea-a75aba35a86f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "%cd /content/DL_Tools_For_Finance\n",
        "!python3 step3_evaluate_vggsp500_model.py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/DL_Tools_For_Finance\n",
            "2020-08-31 14:02:47.233647: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-08-31 14:02:51.669914: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1\n",
            "2020-08-31 14:02:51.704030: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-08-31 14:02:51.704632: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \n",
            "pciBusID: 0000:00:04.0 name: Tesla T4 computeCapability: 7.5\n",
            "coreClock: 1.59GHz coreCount: 40 deviceMemorySize: 14.73GiB deviceMemoryBandwidth: 298.08GiB/s\n",
            "2020-08-31 14:02:51.704688: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-08-31 14:02:51.706171: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\n",
            "2020-08-31 14:02:51.707708: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10\n",
            "2020-08-31 14:02:51.708035: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10\n",
            "2020-08-31 14:02:51.709428: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10\n",
            "2020-08-31 14:02:51.710185: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10\n",
            "2020-08-31 14:02:51.713048: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7\n",
            "2020-08-31 14:02:51.713172: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-08-31 14:02:51.713766: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-08-31 14:02:51.714261: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\n",
            "2020-08-31 14:02:51.714718: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX512F\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2020-08-31 14:02:51.719389: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2000125000 Hz\n",
            "2020-08-31 14:02:51.719605: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x28f6bc0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
            "2020-08-31 14:02:51.719633: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
            "2020-08-31 14:02:51.807725: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-08-31 14:02:51.808365: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x28f6a00 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "2020-08-31 14:02:51.808401: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
            "2020-08-31 14:02:51.808620: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-08-31 14:02:51.809133: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \n",
            "pciBusID: 0000:00:04.0 name: Tesla T4 computeCapability: 7.5\n",
            "coreClock: 1.59GHz coreCount: 40 deviceMemorySize: 14.73GiB deviceMemoryBandwidth: 298.08GiB/s\n",
            "2020-08-31 14:02:51.809188: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-08-31 14:02:51.809222: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\n",
            "2020-08-31 14:02:51.809244: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10\n",
            "2020-08-31 14:02:51.809264: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10\n",
            "2020-08-31 14:02:51.809287: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10\n",
            "2020-08-31 14:02:51.809306: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10\n",
            "2020-08-31 14:02:51.809326: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7\n",
            "2020-08-31 14:02:51.809405: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-08-31 14:02:51.810022: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-08-31 14:02:51.810533: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\n",
            "2020-08-31 14:02:51.810597: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-08-31 14:02:52.339278: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2020-08-31 14:02:52.339335: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0 \n",
            "2020-08-31 14:02:52.339348: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N \n",
            "2020-08-31 14:02:52.339561: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-08-31 14:02:52.340218: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-08-31 14:02:52.340755: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2020-08-31 14:02:52.340796: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 13962 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)\n",
            "2020-08-31 14:02:52.849852: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\n",
            "2020-08-31 14:02:53.190922: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7\n",
            "146/146 [==============================] - 1s 9ms/step - loss: 2.0681 - accuracy: 0.2512\n",
            "Confusion Matrix\n",
            "Accuracy on the Test Images:  0.25123897194862366\n",
            "             SS  ...  Error\n",
            "SS     0.152104  ...    0.0\n",
            "SN     0.175889  ...    0.0\n",
            "N      0.157669  ...    0.0\n",
            "NB     0.157421  ...    0.0\n",
            "BB     0.150538  ...    0.0\n",
            "Error  0.000000  ...    0.0\n",
            "\n",
            "[6 rows x 6 columns]\n",
            "classification report\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          SS       0.26      0.47      0.33      1209\n",
            "          SN       0.00      0.00      0.00        11\n",
            "           N       0.36      0.26      0.30      1630\n",
            "          NB       0.14      0.10      0.12       667\n",
            "          BB       0.52      0.02      0.05       506\n",
            "       Error       0.13      0.15      0.14       618\n",
            "\n",
            "    accuracy                           0.25      4641\n",
            "   macro avg       0.23      0.17      0.16      4641\n",
            "weighted avg       0.29      0.25      0.23      4641\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MfSMBNcGxMF2",
        "outputId": "c3008cb3-6bfe-4259-ce16-c9adcf367400",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 988
        }
      },
      "source": [
        "%cd /content/DL_Tools_For_Finance\n",
        "!python3 step4_guess_future_marketstate_from_image.py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/DL_Tools_For_Finance\n",
            "2020-08-31 14:03:08.385690: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "After resizing: (32, 32, 3)\n",
            "2020-08-31 14:03:09.535186: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1\n",
            "2020-08-31 14:03:09.574599: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-08-31 14:03:09.575181: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \n",
            "pciBusID: 0000:00:04.0 name: Tesla T4 computeCapability: 7.5\n",
            "coreClock: 1.59GHz coreCount: 40 deviceMemorySize: 14.73GiB deviceMemoryBandwidth: 298.08GiB/s\n",
            "2020-08-31 14:03:09.575244: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-08-31 14:03:09.576673: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\n",
            "2020-08-31 14:03:09.585531: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10\n",
            "2020-08-31 14:03:09.585899: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10\n",
            "2020-08-31 14:03:09.591997: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10\n",
            "2020-08-31 14:03:09.592918: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10\n",
            "2020-08-31 14:03:09.602832: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7\n",
            "2020-08-31 14:03:09.602955: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-08-31 14:03:09.603582: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-08-31 14:03:09.604090: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\n",
            "2020-08-31 14:03:09.604397: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX512F\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2020-08-31 14:03:09.610479: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2000125000 Hz\n",
            "2020-08-31 14:03:09.610705: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x1354bc0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
            "2020-08-31 14:03:09.610739: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
            "2020-08-31 14:03:09.722558: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-08-31 14:03:09.723230: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x1354d80 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "2020-08-31 14:03:09.723264: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
            "2020-08-31 14:03:09.723464: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-08-31 14:03:09.724073: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \n",
            "pciBusID: 0000:00:04.0 name: Tesla T4 computeCapability: 7.5\n",
            "coreClock: 1.59GHz coreCount: 40 deviceMemorySize: 14.73GiB deviceMemoryBandwidth: 298.08GiB/s\n",
            "2020-08-31 14:03:09.724132: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-08-31 14:03:09.724172: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\n",
            "2020-08-31 14:03:09.724199: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10\n",
            "2020-08-31 14:03:09.724224: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10\n",
            "2020-08-31 14:03:09.724253: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10\n",
            "2020-08-31 14:03:09.724275: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10\n",
            "2020-08-31 14:03:09.724305: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7\n",
            "2020-08-31 14:03:09.724386: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-08-31 14:03:09.725011: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-08-31 14:03:09.725535: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\n",
            "2020-08-31 14:03:09.725602: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-08-31 14:03:10.322458: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2020-08-31 14:03:10.322529: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0 \n",
            "2020-08-31 14:03:10.322543: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N \n",
            "2020-08-31 14:03:10.322768: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-08-31 14:03:10.323702: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-08-31 14:03:10.324428: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2020-08-31 14:03:10.324484: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 13962 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)\n",
            "2020-08-31 14:03:10.764422: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\n",
            "2020-08-31 14:03:11.077927: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7\n",
            "for  ImageM/image1.PNG the best result is  N\n",
            "                          SS  ...     Error\n",
            "ImageM/image1.PNG   0.209848  ...  0.000418\n",
            "ImageM/image1.PNG1  0.209848  ...  0.000418\n",
            "\n",
            "[2 rows x 6 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1n0vJA_lL-Xv"
      },
      "source": [
        "##Step 1 : Generate Dataset of the Image and the Future maket state"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7jbsgi1Qa6Rg"
      },
      "source": [
        "In this part we are generating the training and testing dataset.\n",
        "First we download the historical prices of the sp500 from 1927 to 31 July 2020 and built the image of 15 days historical graph also we get the 5 days future price evolution of the sp500. \n",
        "From the future price evolution, we calculate a future state which can be splitted in 6 classes :\n",
        "\n",
        "Sell-Sell | Sell- Neutral | Neutral | Neutral -Buy | Buy -Buy (and the Error class)\n",
        "\n",
        "The objective is to get the following files which represent a dataframe in the data/ repertory:\n",
        "\n",
        "X_train_image.csv , X_test_image.csv a 3072 column time serie dataframe  of the image (32 x 32 x3) of the sp500 closing price \n",
        "\n",
        "Y_test_StateClass.csv, Y_train_StateClass.csv a 1 column time serie dataframe of the future state value betwwen -1 to 4\n",
        "\n",
        "We generate also the following files but we won´t use it in this project - more fore RNN & price prediction - Y_test_FutPredict.csv Y_train_FutPredict.csv\n",
        "\n",
        "the testing and training time serie dataset are shuffled by the date of reference with a split number of 0.8\n",
        "\n",
        "Please note that: \n",
        "1. We can increase the dataset taking into account the evolution very liquid stocks or other indices as long as we have very high the liquidity and number of participants \n",
        "2. The calculation of the dataset can take more than 6 hours of calulation as the code is not optimized so far, we can quickly implement parallel computing and rapid image setup instead of using matplotlib library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_7hN1p8cJY_3"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import bs4 as bs\n",
        "import requests\n",
        "import yfinance as yf\n",
        "import datetime\n",
        "import io\n",
        "import cv2\n",
        "import skimage\n",
        "import datetime\n",
        "import os.path as path\n",
        "from PIL import Image\n",
        "from pandas_datareader import data as pdr\n",
        "from skimage import measure\n",
        "from skimage.measure import block_reduce\n",
        "from datetime import datetime\n",
        "from tempfile import mkdtemp\n",
        "\n",
        "'''\n",
        "Functions to be used for data generation \n",
        "'''\n",
        "\n",
        "def get_img_from_fig(fig, dpi=180):\n",
        "   # get_img_from_fig is function which returns an image as numpy array from figure\n",
        "    buf = io.BytesIO()\n",
        "    fig.savefig(buf, format=\"png\", dpi=dpi)\n",
        "    buf.seek(0)\n",
        "    img_arr = np.frombuffer(buf.getvalue(), dtype=np.uint8)\n",
        "    buf.close()\n",
        "    img = cv2.imdecode(img_arr, 1)\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    return img\n",
        "\n",
        "def build_image(stockindex, idate=10, pastlag=10, futlag=3,nb_dates=1000):\n",
        "  # Build image from a table stockindex price list \n",
        "  #return a (32,32,3) np.array representing the image in color\n",
        "  #ising idate as a starting point\n",
        "  #paslag futlag number of days to consider for translate\n",
        "  sp500close=stockindex\n",
        "  nb_days=nb_dates\n",
        "\n",
        "  x_datas=[]\n",
        "  x_datas=np.zeros((32,32,3))\n",
        "  i=idate\n",
        "  \n",
        "  fig=plt.figure()\n",
        "  ax=fig.add_subplot(111)\n",
        "  ax.plot(sp500close[(i-pastlag):i])\n",
        "  plot_img_np = get_img_from_fig(fig)\n",
        "  x_tmp= skimage.measure.block_reduce(plot_img_np[90:620,140:970], (18,28,1), np.mean)\n",
        "  (x_datas[1:-1])[:,1:-1][:]=x_tmp\n",
        "  fig.clear()\n",
        "  plt.close(fig)\n",
        "    \n",
        "  x_datas=x_datas/255\n",
        "  return x_datas\n",
        "\n",
        "  \n",
        "'''\n",
        "MAIN FUNCTION OF CLASSIFICATION \n",
        "build y state y fut \n",
        "and x  \n",
        "'''\n",
        "def class_shortterm_returnfut(x, yfut, indexforpast,tpastlag):\n",
        "  '''\n",
        "  #this function is use to classifiy the future state based on the position of future value with the past range \n",
        "  #Put the value from the 2 boxes (max min) or (open close) on the time range  and check if it is within\n",
        "  #go down go up or exit the box\n",
        "  #the fucntion return 5 state depending on the future value position on the boxes and one state for error cases\n",
        "  '''\n",
        "\n",
        "  xpast_min=np.min(x[(indexforpast-tpastlag):indexforpast])\n",
        "  xpast_max=np.max(x[(indexforpast-tpastlag):indexforpast])\n",
        "  x_open=x[int(indexforpast-tpastlag)]\n",
        "  x_close=x[indexforpast]\n",
        "  \n",
        "  if (yfut < xpast_min ): return 0\n",
        "  elif  (yfut < min(x_open,x_close)): return 1\n",
        "  elif  (yfut < max(x_open,x_close)): return 2\n",
        "  elif  (yfut < xpast_max): return 3\n",
        "  elif  (yfut > xpast_max): return 4\n",
        "  else  : return -1\n",
        "\n",
        "def main_class_shortterm_returnfut(iterable):\n",
        "  return class_shortterm_returnfut(sp500close, iterable, pastlag,futlag)\n",
        "\n",
        "def normalise_df_image(xdf):\n",
        "  #normalisation to 0,1 range of the equity index\n",
        "  df_tmp=xdf\n",
        "  maxval=np.max(df_tmp)\n",
        "  df_tmp=df_tmp/maxval\n",
        "  return df_tmp, maxval\n",
        "\n",
        "def build_image(stockindex, idate=10, pastlag=10, futlag=3):\n",
        "  #another version of returning image from a data frame index\n",
        "  #using the pastlag as range for the graph\n",
        "  #ising idate as a starting point\n",
        "  #return a (32,32,3) np array\n",
        "\n",
        "  #number of days to consider for translate\n",
        "  sp500close=stockindex\n",
        "  x_datas=[]\n",
        "  x_datas=np.zeros((32,32,3))\n",
        "  i=idate\n",
        "  \n",
        "  fig=plt.figure()\n",
        "  ax=fig.add_subplot(111)\n",
        "  ax.plot(sp500close[(i-pastlag):i])\n",
        "  plot_img_np = get_img_from_fig(fig)\n",
        "  x_tmp= skimage.measure.block_reduce(plot_img_np[90:620,140:970], (18,28,1), np.mean)\n",
        "  (x_datas[1:-1])[:,1:-1][:]=x_tmp\n",
        "  fig.clear()\n",
        "  plt.close(fig)\n",
        "    \n",
        "  x_datas=x_datas/255\n",
        "  return x_datas\n",
        "\n",
        "def build_image_df(xdf, past_step,fut_step) :\n",
        "  '''\n",
        "  returning a dictionary of time series dataframes to be used in setup_input_NN_image so a to generate \n",
        "  Input X Result Y_StateClass, Y_FutPredict\n",
        "  pastlag as range for the graph\n",
        "  fut _step the future value lag in time to predict or to check the financial state of the market \n",
        "  #times series to get information from the stock index value\n",
        "  'stock_value':the time serie of the index normalised on the whole period\n",
        "  'moving_average':  time serie of the rolling moving average value of the index for past step image\n",
        "  \"max\": time serie of the rolling max  value of the index for past step image\n",
        "  \"min\": time serie of the rolling  min value of the index for past step image\n",
        "  'volatility':  time serie of the rolling  vol value of the index for past step image\n",
        "          \n",
        "  'df_x_image': is a time series of flattened (1, ) calculed from images (32, 32, 3) list \n",
        "  #I had to flatten it because panda does not create table with this format\n",
        "  'market_state': future markket state to be predicted time lag is futlag\n",
        "  'future_value': future value of stock price to predict  time lag is futlag\n",
        "  'future_volatility':  time serie of the future volatility of the index time lag is futlag\n",
        "  '''\n",
        "\n",
        "  df_stockvaluecorrected=xdf\n",
        "  df_stockvaluecorrected, _ = normalise_df_image(df_stockvaluecorrected)\n",
        "  df_pctchge = df_stockvaluecorrected.pct_change(periods=past_step)\n",
        "  df_movave = df_stockvaluecorrected.rolling(window=past_step).mean()\n",
        "  df_volaty = np.sqrt(252)*df_pctchge.rolling(window=past_step).std()\n",
        "  df_max =df_stockvaluecorrected.rolling(window=past_step).max()\n",
        "  df_min =df_stockvaluecorrected.rolling(window=past_step).min()\n",
        "  df_Fut_value =df_stockvaluecorrected.shift(periods=-fut_step)\n",
        "  df_Fut_value.name='future_value'\n",
        "  df_Fut_volaty =df_volaty.shift(periods=-fut_step)\n",
        "  \n",
        "  df_market_state=pd.DataFrame(index=df_stockvaluecorrected.index,columns=['market_state'],dtype=np.float64)\n",
        "  \n",
        "  tmpimage=np.zeros((255,255))\n",
        "  flatten_image=np.reshape(tmpimage,(1,-1))\n",
        "  colname_d_x_image_flattened = ['Image Col'+str(j) for j in range(flatten_image.shape[1])]\n",
        "\n",
        "  #write frile in drive instead of RAMmemory\n",
        "  filename = path.join(mkdtemp(), 'np_x_image.dat')\n",
        "  np_x_image=np.memmap(filename,  dtype='float32', mode='w+', shape=(len(df_stockvaluecorrected.index),flatten_image.shape[1]))\n",
        "  \n",
        "  for i in range(len(df_stockvaluecorrected.index)):\n",
        "        yfut=df_Fut_value.iloc[i]\n",
        "        df_market_state.iloc[i]=class_shortterm_returnfut(df_stockvaluecorrected,yfut, i,tpastlag=past_step)\n",
        "        print(\"loop 1 market state :\", \"step \",i,\"market state fut\", df_market_state.iloc[i],\" future value\",df_Fut_value.iloc[i] )\n",
        "  df_market_state.index=df_Fut_value.index\n",
        "\n",
        "  fig=plt.figure()\n",
        "  \n",
        "  '''\n",
        "  for i in range(len(df_stockvaluecorrected.index)):\n",
        "        try:\n",
        "          tmpimage=build_image_optimfig(fig, df_stockvaluecorrected,i,pastlag=past_step,futlag=fut_step)\n",
        "          np_x_image[i,:]=np.reshape(tmpimage,(1,-1))\n",
        "          print(\"loop 2 image :\", \"step \",i,\"market state fut\", df_market_state.iloc[i],\" future value\",df_Fut_value.iloc[i] )\n",
        "        except:\n",
        "           print(\"error at index\", i)\n",
        "  '''           \n",
        "  \n",
        "  def build_image_optimfig_simplified(i_index):\n",
        "    return build_image_optimfig(fig, df_stockvaluecorrected,i_index,pastlag=past_step,futlag=fut_step)\n",
        "\n",
        "  def quick_build_image_from_index(indexstart, index_end, np_x_image):\n",
        "        if (indexstart==index_end):\n",
        "            tmpimage=build_image_optimfig_simplified(indexstart)\n",
        "            np_x_image[indexstart,:]=np.reshape(tmpimage,(1,-1))\n",
        "            print(\"loop 2 image :\", \"step \",indexstart)\n",
        "        else :\n",
        "            i_split=indexstart+(index_end-indexstart)//2\n",
        "            quick_build_image_from_index(indexstart, i_split,np_x_image)\n",
        "            quick_build_image_from_index(i_split+1, index_end,np_x_image)\n",
        "\n",
        "  quick_build_image_from_index(0, len(df_stockvaluecorrected.index)-1, np_x_image)\n",
        "\n",
        "  df_x_image=pd.DataFrame(data=np_x_image,columns=colname_d_x_image_flattened, index=df_stockvaluecorrected.index)\n",
        "  fig.clear\n",
        "  plt.close(fig)\n",
        "\n",
        "\n",
        "  df_data= {\n",
        "          'stock_value': df_stockvaluecorrected, \n",
        "          'moving_average': df_movave, \n",
        "          \"max\": df_max, \n",
        "          \"min\": df_max,\n",
        "          'volatility': df_volaty,\n",
        "          'future_volatility': df_Fut_volaty,\n",
        "          \n",
        "          'df_x_image':df_x_image,\n",
        "          'market_state':df_market_state,\n",
        "          'future_value': df_Fut_value,\n",
        "\n",
        "          }\n",
        "\n",
        "  return df_data\n",
        "\n",
        "\n",
        "\n",
        "def build_image_optimfig(fig, stockindex, idate=10, pastlag=10, futlag=3):\n",
        "  '''\n",
        "  #version of returning image from a data frame index\n",
        "  #using the pastlag as range for the graph\n",
        "  #ising idate as a starting point\n",
        "  #return a (32,32,3) np array\n",
        "  #this one is optimisng the use of ram \n",
        "  '''\n",
        "\n",
        "  #number of days to consider for translate\n",
        "  sp500close=stockindex\n",
        "  x_datas=[]\n",
        "  x_datas=np.zeros((255,255,3))\n",
        "  i=idate\n",
        "  \n",
        "  plt.plot(sp500close[(i-pastlag):i])\n",
        "  plot_img_np = get_img_from_fig(fig)\n",
        "  #x_tmp= skimage.measure.block_reduce(plot_img_np[90:620,140:970], (18,28,1), np.mean)\n",
        "  x_tmp= skimage.measure.block_reduce(plot_img_np[90:620,140:970], (2,3,1), np.mean)\n",
        "  (x_datas[:])[:,:][:]=(x_tmp[5:-5])[:,11:-11][:]\n",
        "    \n",
        "  x_datas=x_datas[:,:,0]/255\n",
        "  return x_datas     \n",
        "\n",
        "def setup_input_NN_image(xdf, past_step=25,fut_step=5, split=0.8):\n",
        "  '''\n",
        "  this function the time serie of the index price \n",
        "  and generate the random dataset with split value from the whole time serie\n",
        "  X is a time serie of the flattened 32, 32 ,3 image list\n",
        "  Y_StateClass is a time serie of future state to predict with a classification made with class_shortterm_returnfut\n",
        "  Y_FutPredict is the time serie of stocke index shifted in time to be predicted\n",
        "  we randomize the dates and retun 2 set of dataframes\n",
        "  '''\n",
        "  xdf_data=build_image_df(xdf,past_step,fut_step)\n",
        "  \n",
        "  tmp_data=pd.concat([xdf_data['market_state'],xdf_data['future_value'],xdf_data['df_x_image']],axis=1)\n",
        "  tmp_data=tmp_data.dropna()\n",
        "\n",
        "  Y_StateClass= tmp_data['market_state']\n",
        "  Y_FutPredict= tmp_data['future_value']  \n",
        "  X=tmp_data.drop(columns=['market_state','future_value'])\n",
        "\n",
        "  nb_dates=len(Y_StateClass.index)\n",
        "  rng = np.random.default_rng()\n",
        "  list_shuffle = np.arange(nb_dates)\n",
        "  rng.shuffle(list_shuffle)\n",
        "  split_index=int(split*nb_dates)\n",
        "    \n",
        "  train_split=list_shuffle[:split_index]\n",
        "  test_split=list_shuffle[(split_index+1):]\n",
        "\n",
        "  X_train=(X.iloc[train_split])\n",
        "  Y_train_StateClass=(Y_StateClass.iloc[train_split])\n",
        "  Y_train_FutPredict=(Y_FutPredict.iloc[train_split])\n",
        "\n",
        "  X_test=(X.iloc[test_split])\n",
        "  Y_test_StateClass=(Y_StateClass.iloc[test_split])\n",
        "  Y_test_FutPredict=(Y_FutPredict.iloc[test_split])\n",
        "\n",
        "  return (X_train, Y_train_StateClass, Y_train_FutPredict), (X_test, Y_test_StateClass, Y_test_FutPredict)\n",
        "\n",
        "def change_X_df__nparray_image(df_X_train_image_flattened ):\n",
        "  '''\n",
        "  setup_input_NN_image returns a dataframe of flaten image for x train and xtest\n",
        "  then this function will change each date into a nparray list of images with 32, 32, 3 size \n",
        "  '''\n",
        "  X_train_image=df_X_train_image_flattened\n",
        "  nb_train=len(X_train_image.index)\n",
        "  \n",
        "  x_train=np.zeros((nb_train,255,255,1))\n",
        "  for i in range(nb_train):\n",
        "    tmp=np.array(X_train_image.iloc[i])\n",
        "    tmp=tmp.reshape(255,255,1)\n",
        "    x_train[i]=tmp\n",
        "  return x_train\n",
        "  \n",
        "def setup_input_NN_image(xdf, past_step=25,fut_step=5, split=0.8, is_shuffle=False):\n",
        "  '''\n",
        "  this function the time serie of the index price \n",
        "  and generate the random dataset with split value from the whole time serie\n",
        "  X is a time serie of the flattened 32, 32 ,3 image list\n",
        "  Y_StateClass is a time serie of future state to predict with a classification made with class_shortterm_returnfut\n",
        "  Y_FutPredict is the time serie of stocke index shifted in time to be predicted\n",
        "  we randomize the dates and retun 2 set of dataframes\n",
        "  '''\n",
        "  xdf_data=build_image_df(xdf,past_step,fut_step)\n",
        "  \n",
        "  tmp_data=pd.concat([xdf_data['market_state'],xdf_data['future_value'],xdf_data['df_x_image']],axis=1)\n",
        "  tmp_data=tmp_data.dropna()\n",
        "\n",
        "  Y_StateClass= tmp_data['market_state']\n",
        "  Y_FutPredict= tmp_data['future_value']  \n",
        "  X=tmp_data.drop(columns=['market_state','future_value'])\n",
        "\n",
        "  nb_dates=len(Y_StateClass.index)\n",
        "  split_index=int(split*nb_dates)\n",
        "  list_shuffle = np.arange(nb_dates)\n",
        "  rng = np.random.default_rng()\n",
        "\n",
        "  if (is_shuffle==True) : rng.shuffle(list_shuffle)\n",
        "  train_split=list_shuffle[:split_index]\n",
        "  test_split=list_shuffle[(split_index+1):]\t\t\n",
        "\n",
        "  X_train=(X.iloc[train_split])\n",
        "  Y_train_StateClass=(Y_StateClass.iloc[train_split])\n",
        "  Y_train_FutPredict=(Y_FutPredict.iloc[train_split])\n",
        "\n",
        "  X_test=(X.iloc[test_split])\n",
        "  Y_test_StateClass=(Y_StateClass.iloc[test_split])\n",
        "  Y_test_FutPredict=(Y_FutPredict.iloc[test_split])\n",
        "\n",
        "  return (X_train, Y_train_StateClass, Y_train_FutPredict), (X_test, Y_test_StateClass, Y_test_FutPredict)\n",
        "\n",
        "def change_X_df__nparray_image(df_X_train_image_flattened ):\n",
        "  '''\n",
        "  setup_input_NN_image returns a dataframe of flaten image for x train and xtest\n",
        "  then this function will change each date into a nparray list of images with 32, 32, 3 size \n",
        "  '''\n",
        "  X_train_image=df_X_train_image_flattened\n",
        "  nb_train=len(X_train_image.index)\n",
        "  \n",
        "  x_train=np.zeros((nb_train,255,255,1))\n",
        "  for i in range(nb_train):\n",
        "    tmp=np.array(X_train_image.iloc[i])\n",
        "    tmp=tmp.reshape(255,255,1)\n",
        "    x_train[i]=tmp\n",
        "  return x_train\n",
        "\n",
        "def build_image_optimfig(fig, stockindex, idate=10, pastlag=10, futlag=3):\n",
        "  '''\n",
        "  #version of returning image from a data frame index\n",
        "  #using the pastlag as range for the graph\n",
        "  #ising idate as a starting point\n",
        "  #return a (32,32,3) np array\n",
        "  #this one is optimisng the use of ram \n",
        "  '''\n",
        "\n",
        "  #number of days to consider for translate\n",
        "  sp500close=stockindex\n",
        "  x_datas=[]\n",
        "  x_datas=np.zeros((255,255,3))\n",
        "  i=idate\n",
        "  \n",
        "  plt.plot(sp500close[(i-pastlag):i])\n",
        "  plot_img_np = get_img_from_fig(fig)\n",
        "  #x_tmp= skimage.measure.block_reduce(plot_img_np[90:620,140:970], (18,28,1), np.mean)\n",
        "  x_tmp= skimage.measure.block_reduce(plot_img_np[90:620,140:970], (2,3,1), np.mean)\n",
        "  (x_datas[:])[:,:][:]=(x_tmp[5:-5])[:,11:-11][:]\n",
        "    \n",
        "  x_datas=x_datas[:,:,0]/255\n",
        "  return x_datas     \n",
        "\n",
        "def splitted_NN(index, nb_split):\n",
        "\t\n",
        "\tdef launch_splitted(i_index, i_last, indexstock):\n",
        "\t\tif i_index==i_last:\n",
        "\t\t\ttestsp500=indexstock[i_index:]\n",
        "\t\telse:\n",
        "\t\t\ttestsp500=indexstock[i_index:i_last]\n",
        "\t\t\t_ , (X_test_image, Y_test_StateClass_image, Y_test_FutPredict_image) = setup_input_NN_image(testsp500, split=0)\n",
        "\t\treturn X_test_image, Y_test_StateClass_image, Y_test_FutPredict_image\n",
        "\t\t\n",
        "\ty_StateClass_image=pd.DataFrame()\n",
        "\tx_image=pd.DataFrame()\n",
        "\ty_futurepredict_image=pd.DataFrame()\n",
        "\t\n",
        "\tnb_dates=len(index)\n",
        "\tnb_block=nb_dates//nb_split\n",
        "\t#First parts\n",
        "  #https://stackoverflow.com/questions/31326928/using-pandas-to-concatenate-csv-files-in-directory-recursively\n",
        "  '''\n",
        "  import pandas,os\n",
        "\n",
        "  df = None\n",
        "  dfList=[]\n",
        "  for filename in [directory+x for x in os.listdir(path)]:\n",
        "      dfList.append(pd.read_csv(filename))\n",
        "  df=pandas.concat(dfList)\n",
        "  df.to_csv('out.csv', mode='w')\n",
        "\n",
        "  '''\n",
        "\n",
        "  for i in range(nb_block):\n",
        "\t\tX_image, Y_StateClass_image, Y_FutPredict_image =launch_splitted(0+i*nb_block,(i+1)*nb_block,index)\n",
        "\t\t\n",
        "    y_StateClass_image=pd.concat([y_StateClass_image,Y_StateClass_image],axis=1)\n",
        "\t\tx_image=pd.concat([x_image,X_image],axis=1)\n",
        "\t\ty_futurepredict_image=pd.concat([y_futurepredict_image,Y_FutPredict_image],axis=1)\n",
        "\n",
        "\n",
        "\t#Final part\n",
        "\tX_image, Y_StateClass_image, Y_FutPredict_image =launch_splitted(0+23*1000,(23)*1000,index)\n",
        "\ty_StateClass_image=pd.concat([y_StateClass_image,Y_StateClass_image],axis=1)\n",
        "\tx_image=pd.concat([x_image,X_image],axis=1)\n",
        "\ty_futurepredict_image=pd.concat([y_futurepredict_image,Y_FutPredict_image],axis=1)\n",
        "\t\t\n",
        "\treturn x_image, y_StateClass_image, y_futurepredict_image\n",
        "\t\t\n",
        "def main():\n",
        "\t'''\n",
        "\tCOMMAND NOW FOR DOWNLOADING HISTORICAL DATAS FOR SP500\n",
        "\t'''\n",
        "\n",
        "\t#Recuperation from yahoo of sp500 large history\n",
        "\tstart = datetime(1920,1,1)\n",
        "\tend = datetime(2020,7,31)\n",
        "\tyf.pdr_override() # <== that's all it takes :-)\n",
        "\tsp500 = pdr.get_data_yahoo('^GSPC', start,end)\n",
        "\ttestsp500=(sp500['Close'])[15000:]\n",
        "\t#generate the dataset it can take 6 - 8 hours\n",
        "\t#Need to be optimzed with more time\n",
        "\t\n",
        "\tx_image, y_StateClass_image, y_futurepredict_image=splitted_NN(index=testsp500, nb_split=20)\n",
        "\t\t\n",
        "\t#group by y state the x_image \n",
        "\t#count the min of the of each state \n",
        "\t#construct a directory for each block like cat, dog etc\n",
        "\n",
        "\tnon_monotonic_index =pd.Index(list(y_StateClass_image))\n",
        "\n",
        "\tdef localize_index_from_state(non_monotonic_index, state=0):\n",
        "\t  state_loc=non_monotonic_index.get_loc(state)\n",
        "\t  return [i for i in range(0,state_loc.size) if state_loc[i]]\n",
        "\n",
        "\ttry : \n",
        "\t  state_error_loc=localize_index_from_state(non_monotonic_index,-1) \n",
        "\t  y_StateClass_image_error =y_StateClass_image.iloc[state_error_loc]\n",
        "\t  x_image_State_is_error =x_image.iloc[state_error_loc]\n",
        "\t  y_futpredict_image_is_error =y_futurepredict_image.iloc[state_error_loc]\n",
        "\t  print(\"dataset class -1 size is :\",y_StateClass_image_error.size, \"and for x \", x_image_State_is_error.index.size)\n",
        "\t  print_data_class(state=-1)\n",
        "      \n",
        "\texcept :\n",
        "\t  print(\"No value for error state\")\n",
        "\n",
        "\tstate_zero_loc=localize_index_from_state(non_monotonic_index, 0)\n",
        "\tstate_one_loc=localize_index_from_state(non_monotonic_index, 1)\n",
        "\tstate_two_loc=localize_index_from_state(non_monotonic_index, 2)\n",
        "\tstate_three_loc=localize_index_from_state(non_monotonic_index, 3)\n",
        "\tstate_four_loc=localize_index_from_state(non_monotonic_index, 4)\n",
        "\n",
        "\t#Build up class for the dataset\n",
        "\ty_StateClass_image_0 =y_StateClass_image.iloc[state_zero_loc]\n",
        "\ty_StateClass_image_1 =y_StateClass_image.iloc[state_one_loc]\n",
        "\ty_StateClass_image_2 =y_StateClass_image.iloc[state_two_loc]\n",
        "\ty_StateClass_image_3 =y_StateClass_image.iloc[state_three_loc]\n",
        "\ty_StateClass_image_4 =y_StateClass_image.iloc[state_four_loc]\n",
        "\n",
        "\tx_image_State_is_0 =x_image.iloc[state_zero_loc]\n",
        "\tx_image_State_is_1 =x_image.iloc[state_one_loc]\n",
        "\tx_image_State_is_2 =x_image.iloc[state_two_loc]\n",
        "\tx_image_State_is_3 =x_image.iloc[state_three_loc]\n",
        "\tx_image_State_is_4 =x_image.iloc[state_four_loc]\n",
        "\n",
        "\ty_futpredict_image_0 =y_futurepredict_image.iloc[state_zero_loc]\n",
        "\ty_futpredict_image_1 =y_futurepredict_image.iloc[state_one_loc]\n",
        "\ty_futpredict_image_2 =y_futurepredict_image.iloc[state_two_loc]\n",
        "\ty_futpredict_image_3 =y_futurepredict_image.iloc[state_three_loc]\n",
        "\ty_futpredict_image_4 =y_futurepredict_image.iloc[state_four_loc]\n",
        "\n",
        "\t#print size of each dataset\n",
        "\tprint(\"dataset class 0 size is :\",y_StateClass_image_0.size, \"and for x \", x_image_State_is_0.index.size)\n",
        "\tprint(\"dataset class 1 size is :\",y_StateClass_image_1.size, \"and for x \", x_image_State_is_1.index.size)\n",
        "\tprint(\"dataset class 2 size is :\",y_StateClass_image_2.size, \"and for x \", x_image_State_is_2.index.size)\n",
        "\tprint(\"dataset class 3 size is :\",y_StateClass_image_3.size, \"and for x \", x_image_State_is_3.index.size)\n",
        "\tprint(\"dataset class 4 size is :\",y_StateClass_image_4.size, \"and for x \", x_image_State_is_4.index.size)\n",
        "\n",
        "\t#write dataset for each set  in corresponding folder\n",
        "\tdef print_data_class(state=0,write_path='datas/state_is_') :\n",
        "\t  state_zero_loc=localize_index_from_state(non_monotonic_index, state)\n",
        "\t  y_StateClass_image_0 =y_StateClass_image.iloc[state_zero_loc]\n",
        "\t  x_image_State_is_0 =x_image.iloc[state_zero_loc]\n",
        "\t  y_futpredict_image_0 =y_futurepredict_image.iloc[state_zero_loc]\n",
        "\t  y_StateClass_image_0.to_csv(write_path+str(state)+'/y_stateclass.csv')\n",
        "\t  x_image_State_is_0.to_csv(write_path+str(state)+'/x_image.csv')\n",
        "\t  y_futpredict_image_0.to_csv(write_path+str(state)+'/y_future.csv')\n",
        "\n",
        "\t\n",
        "\tprint_data_class(state=0)\n",
        "\tprint_data_class(state=1)\n",
        "\tprint_data_class(state=2)\n",
        "\tprint_data_class(state=3)\n",
        "\tprint_data_class(state=4)\n",
        "\n",
        "\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iA_9dsFM1_0d",
        "outputId": "c6d9daad-35af-4822-a6ff-c721a9aa8c95",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "%cd /content/drive/My Drive/A_transfertTFM\n",
        "\n",
        "main()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "loop 2 image : step  357\n",
            "loop 2 image : step  358\n",
            "loop 2 image : step  359\n",
            "loop 2 image : step  360\n",
            "loop 2 image : step  361\n",
            "loop 2 image : step  362\n",
            "loop 2 image : step  363\n",
            "loop 2 image : step  364\n",
            "loop 2 image : step  365\n",
            "loop 2 image : step  366\n",
            "loop 2 image : step  367\n",
            "loop 2 image : step  368\n",
            "loop 2 image : step  369\n",
            "loop 2 image : step  370\n",
            "loop 2 image : step  371\n",
            "loop 2 image : step  372\n",
            "loop 2 image : step  373\n",
            "loop 2 image : step  374\n",
            "loop 2 image : step  375\n",
            "loop 2 image : step  376\n",
            "loop 2 image : step  377\n",
            "loop 2 image : step  378\n",
            "loop 2 image : step  379\n",
            "loop 2 image : step  380\n",
            "loop 2 image : step  381\n",
            "loop 2 image : step  382\n",
            "loop 2 image : step  383\n",
            "loop 2 image : step  384\n",
            "loop 2 image : step  385\n",
            "loop 2 image : step  386\n",
            "loop 2 image : step  387\n",
            "loop 2 image : step  388\n",
            "loop 2 image : step  389\n",
            "loop 2 image : step  390\n",
            "loop 2 image : step  391\n",
            "loop 2 image : step  392\n",
            "loop 2 image : step  393\n",
            "loop 2 image : step  394\n",
            "loop 2 image : step  395\n",
            "loop 2 image : step  396\n",
            "loop 2 image : step  397\n",
            "loop 2 image : step  398\n",
            "loop 2 image : step  399\n",
            "loop 2 image : step  400\n",
            "loop 2 image : step  401\n",
            "loop 2 image : step  402\n",
            "loop 2 image : step  403\n",
            "loop 2 image : step  404\n",
            "loop 2 image : step  405\n",
            "loop 2 image : step  406\n",
            "loop 2 image : step  407\n",
            "loop 2 image : step  408\n",
            "loop 2 image : step  409\n",
            "loop 2 image : step  410\n",
            "loop 2 image : step  411\n",
            "loop 1 market state : step  0 market state fut market_state    1.0\n",
            "Name: 1994-05-05 00:00:00, dtype: float64  future value 0.7137801770293436\n",
            "loop 1 market state : step  1 market state fut market_state    1.0\n",
            "Name: 1994-05-06 00:00:00, dtype: float64  future value 0.7144075228880608\n",
            "loop 1 market state : step  2 market state fut market_state    2.0\n",
            "Name: 1994-05-09 00:00:00, dtype: float64  future value 0.714970465165664\n",
            "loop 1 market state : step  3 market state fut market_state    2.0\n",
            "Name: 1994-05-10 00:00:00, dtype: float64  future value 0.7228200443186961\n",
            "loop 1 market state : step  4 market state fut market_state    2.0\n",
            "Name: 1994-05-11 00:00:00, dtype: float64  future value 0.7297688569218486\n",
            "loop 1 market state : step  5 market state fut market_state    2.0\n",
            "Name: 1994-05-12 00:00:00, dtype: float64  future value 0.734256637864063\n",
            "loop 1 market state : step  6 market state fut market_state    2.0\n",
            "Name: 1994-05-13 00:00:00, dtype: float64  future value 0.731747352605385\n",
            "loop 1 market state : step  7 market state fut market_state    2.0\n",
            "Name: 1994-05-16 00:00:00, dtype: float64  future value 0.7289806984627277\n",
            "loop 1 market state : step  8 market state fut market_state    2.0\n",
            "Name: 1994-05-17 00:00:00, dtype: float64  future value 0.7315703900216076\n",
            "loop 1 market state : step  9 market state fut market_state    2.0\n",
            "Name: 1994-05-18 00:00:00, dtype: float64  future value 0.7340314216825455\n",
            "loop 1 market state : step  10 market state fut market_state    2.0\n",
            "Name: 1994-05-19 00:00:00, dtype: float64  future value 0.7351895571164043\n",
            "loop 1 market state : step  11 market state fut market_state    2.0\n",
            "Name: 1994-05-20 00:00:00, dtype: float64  future value 0.7356238394960656\n",
            "loop 1 market state : step  12 market state fut market_state    2.0\n",
            "Name: 1994-05-23 00:00:00, dtype: float64  future value 0.7342887905665247\n",
            "loop 1 market state : step  13 market state fut market_state    2.0\n",
            "Name: 1994-05-24 00:00:00, dtype: float64  future value 0.7361064245615623\n",
            "loop 1 market state : step  14 market state fut market_state    2.0\n",
            "Name: 1994-05-25 00:00:00, dtype: float64  future value 0.736138577264024\n",
            "loop 1 market state : step  15 market state fut market_state    2.0\n",
            "Name: 1994-05-26 00:00:00, dtype: float64  future value 0.7401277213335586\n",
            "loop 1 market state : step  16 market state fut market_state    2.0\n",
            "Name: 1994-05-27 00:00:00, dtype: float64  future value 0.7381170729475605\n",
            "loop 1 market state : step  17 market state fut market_state    2.0\n",
            "Name: 1994-05-31 00:00:00, dtype: float64  future value 0.7370393438139035\n",
            "loop 1 market state : step  18 market state fut market_state    1.0\n",
            "Name: 1994-06-01 00:00:00, dtype: float64  future value 0.7351895571164043\n",
            "loop 1 market state : step  19 market state fut market_state    2.0\n",
            "Name: 1994-06-02 00:00:00, dtype: float64  future value 0.736476352448205\n",
            "loop 1 market state : step  20 market state fut market_state    1.0\n",
            "Name: 1994-06-03 00:00:00, dtype: float64  future value 0.7377792977633795\n",
            "loop 1 market state : step  21 market state fut market_state    2.0\n",
            "Name: 1994-06-06 00:00:00, dtype: float64  future value 0.7384709490270199\n",
            "loop 1 market state : step  22 market state fut market_state    2.0\n",
            "Name: 1994-06-07 00:00:00, dtype: float64  future value 0.7437307875330768\n",
            "loop 1 market state : step  23 market state fut market_state    2.0\n",
            "Name: 1994-06-08 00:00:00, dtype: float64  future value 0.7408997788974009\n",
            "loop 1 market state : step  24 market state fut market_state    2.0\n",
            "Name: 1994-06-09 00:00:00, dtype: float64  future value 0.7430230353741578\n",
            "loop 1 market state : step  25 market state fut market_state    2.0\n",
            "Name: 1994-06-10 00:00:00, dtype: float64  future value 0.73742542168392\n",
            "loop 1 market state : step  26 market state fut market_state    2.0\n",
            "Name: 1994-06-13 00:00:00, dtype: float64  future value 0.7326481191552646\n",
            "loop 1 market state : step  27 market state fut market_state    2.0\n",
            "Name: 1994-06-14 00:00:00, dtype: float64  future value 0.7259888281385529\n",
            "loop 1 market state : step  28 market state fut market_state    2.0\n",
            "Name: 1994-06-15 00:00:00, dtype: float64  future value 0.7288037358789503\n",
            "loop 1 market state : step  29 market state fut market_state    2.0\n",
            "Name: 1994-06-16 00:00:00, dtype: float64  future value 0.7232382748911741\n",
            "loop 1 market state : step  30 market state fut market_state    0.0\n",
            "Name: 1994-06-17 00:00:00, dtype: float64  future value 0.7122520646207469\n",
            "loop 1 market state : step  31 market state fut market_state    2.0\n",
            "Name: 1994-06-20 00:00:00, dtype: float64  future value 0.7195064997056186\n",
            "loop 1 market state : step  32 market state fut market_state    2.0\n",
            "Name: 1994-06-21 00:00:00, dtype: float64  future value 0.717511952214899\n",
            "loop 1 market state : step  33 market state fut market_state    0.0\n",
            "Name: 1994-06-22 00:00:00, dtype: float64  future value 0.7200212374735772\n",
            "loop 1 market state : step  34 market state fut market_state    0.0\n",
            "Name: 1994-06-23 00:00:00, dtype: float64  future value 0.7146165890862045\n",
            "loop 1 market state : step  35 market state fut market_state    0.0\n",
            "Name: 1994-06-24 00:00:00, dtype: float64  future value 0.7177210675011381\n",
            "loop 1 market state : step  36 market state fut market_state    1.0\n",
            "Name: 1994-06-27 00:00:00, dtype: float64  future value 0.7179944881923005\n",
            "loop 1 market state : step  37 market state fut market_state    2.0\n",
            "Name: 1994-06-28 00:00:00, dtype: float64  future value 0.7176084594103793\n",
            "loop 1 market state : step  38 market state fut market_state    2.0\n",
            "Name: 1994-06-29 00:00:00, dtype: float64  future value 0.721227626505176\n",
            "loop 1 market state : step  39 market state fut market_state    2.0\n",
            "Name: 1994-06-30 00:00:00, dtype: float64  future value 0.7231095659051369\n",
            "loop 1 market state : step  40 market state fut market_state    2.0\n",
            "Name: 1994-07-01 00:00:00, dtype: float64  future value 0.7207128887372176\n",
            "loop 1 market state : step  41 market state fut market_state    2.0\n",
            "Name: 1994-07-05 00:00:00, dtype: float64  future value 0.7205359752415356\n",
            "loop 1 market state : step  42 market state fut market_state    2.0\n",
            "Name: 1994-07-06 00:00:00, dtype: float64  future value 0.7217906178708745\n",
            "loop 1 market state : step  43 market state fut market_state    2.0\n",
            "Name: 1994-07-07 00:00:00, dtype: float64  future value 0.7293184736469087\n",
            "loop 1 market state : step  44 market state fut market_state    2.0\n",
            "Name: 1994-07-08 00:00:00, dtype: float64  future value 0.7305248626785076\n",
            "loop 1 market state : step  45 market state fut market_state    2.0\n",
            "Name: 1994-07-11 00:00:00, dtype: float64  future value 0.7322298885827865\n",
            "loop 1 market state : step  46 market state fut market_state    2.0\n",
            "Name: 1994-07-12 00:00:00, dtype: float64  future value 0.7300422776130109\n",
            "loop 1 market state : step  47 market state fut market_state    2.0\n",
            "Name: 1994-07-13 00:00:00, dtype: float64  future value 0.7264070587110311\n",
            "loop 1 market state : step  48 market state fut market_state    1.0\n",
            "Name: 1994-07-14 00:00:00, dtype: float64  future value 0.7280316292270127\n",
            "loop 1 market state : step  49 market state fut market_state    1.0\n",
            "Name: 1994-07-15 00:00:00, dtype: float64  future value 0.728835888581412\n",
            "loop 1 market state : step  50 market state fut market_state    1.0\n",
            "Name: 1994-07-18 00:00:00, dtype: float64  future value 0.730669623471728\n",
            "loop 1 market state : step  51 market state fut market_state    1.0\n",
            "Name: 1994-07-19 00:00:00, dtype: float64  future value 0.7292380182586116\n",
            "loop 1 market state : step  52 market state fut market_state    2.0\n",
            "Name: 1994-07-20 00:00:00, dtype: float64  future value 0.7279673238220894\n",
            "loop 1 market state : step  53 market state fut market_state    2.0\n",
            "Name: 1994-07-21 00:00:00, dtype: float64  future value 0.7306535225764496\n",
            "loop 1 market state : step  54 market state fut market_state    2.0\n",
            "Name: 1994-07-22 00:00:00, dtype: float64  future value 0.7371197992022006\n",
            "loop 1 market state : step  55 market state fut market_state    4.0\n",
            "Name: 1994-07-25 00:00:00, dtype: float64  future value 0.7415432256513965\n",
            "loop 1 market state : step  56 market state fut market_state    4.0\n",
            "Name: 1994-07-26 00:00:00, dtype: float64  future value 0.7408193725971991\n",
            "loop 1 market state : step  57 market state fut market_state    4.0\n",
            "Name: 1994-07-27 00:00:00, dtype: float64  future value 0.7422509778103156\n",
            "loop 1 market state : step  58 market state fut market_state    4.0\n",
            "Name: 1994-07-28 00:00:00, dtype: float64  future value 0.7373449662956229\n",
            "loop 1 market state : step  59 market state fut market_state    2.0\n",
            "Name: 1994-07-29 00:00:00, dtype: float64  future value 0.7352378107141444\n",
            "loop 1 market state : step  60 market state fut market_state    2.0\n",
            "Name: 1994-08-01 00:00:00, dtype: float64  future value 0.7365246551340404\n",
            "loop 1 market state : step  61 market state fut market_state    2.0\n",
            "Name: 1994-08-02 00:00:00, dtype: float64  future value 0.7365729087317806\n",
            "loop 1 market state : step  62 market state fut market_state    2.0\n",
            "Name: 1994-08-03 00:00:00, dtype: float64  future value 0.740401142024721\n",
            "loop 1 market state : step  63 market state fut market_state    3.0\n",
            "Name: 1994-08-04 00:00:00, dtype: float64  future value 0.7381170729475605\n",
            "loop 1 market state : step  64 market state fut market_state    4.0\n",
            "Name: 1994-08-05 00:00:00, dtype: float64  future value 0.7430391362694363\n",
            "loop 1 market state : step  65 market state fut market_state    3.0\n",
            "Name: 1994-08-08 00:00:00, dtype: float64  future value 0.7418971017308561\n",
            "loop 1 market state : step  66 market state fut market_state    4.0\n",
            "Name: 1994-08-09 00:00:00, dtype: float64  future value 0.7479773004865906\n",
            "loop 1 market state : step  67 market state fut market_state    4.0\n",
            "Name: 1994-08-10 00:00:00, dtype: float64  future value 0.7482346693705698\n",
            "loop 1 market state : step  68 market state fut market_state    4.0\n",
            "Name: 1994-08-11 00:00:00, dtype: float64  future value 0.7450176319529728\n",
            "loop 1 market state : step  69 market state fut market_state    4.0\n",
            "Name: 1994-08-12 00:00:00, dtype: float64  future value 0.7458379431145552\n",
            "loop 1 market state : step  70 market state fut market_state    4.0\n",
            "Name: 1994-08-15 00:00:00, dtype: float64  future value 0.743650381232875\n",
            "loop 1 market state : step  71 market state fut market_state    2.0\n",
            "Name: 1994-08-16 00:00:00, dtype: float64  future value 0.7471730411321914\n",
            "loop 1 market state : step  72 market state fut market_state    4.0\n",
            "Name: 1994-08-17 00:00:00, dtype: float64  future value 0.7544435280242464\n",
            "loop 1 market state : step  73 market state fut market_state    4.0\n",
            "Name: 1994-08-18 00:00:00, dtype: float64  future value 0.7529154156156497\n",
            "loop 1 market state : step  74 market state fut market_state    4.0\n",
            "Name: 1994-08-19 00:00:00, dtype: float64  future value 0.762116144593501\n",
            "loop 1 market state : step  75 market state fut market_state    4.0\n",
            "Name: 1994-08-22 00:00:00, dtype: float64  future value 0.7633868881181185\n",
            "loop 1 market state : step  76 market state fut market_state    4.0\n",
            "Name: 1994-08-23 00:00:00, dtype: float64  future value 0.7657675134788546\n",
            "loop 1 market state : step  77 market state fut market_state    4.0\n",
            "Name: 1994-08-24 00:00:00, dtype: float64  future value 0.7648345451384181\n",
            "loop 1 market state : step  78 market state fut market_state    4.0\n",
            "Name: 1994-08-25 00:00:00, dtype: float64  future value 0.761102819040958\n",
            "loop 1 market state : step  79 market state fut market_state    2.0\n",
            "Name: 1994-08-26 00:00:00, dtype: float64  future value 0.7575962109488248\n",
            "loop 1 market state : step  80 market state fut market_state    2.0\n",
            "Name: 1994-08-29 00:00:00, dtype: float64  future value 0.7589956143713842\n",
            "loop 1 market state : step  81 market state fut market_state    2.0\n",
            "Name: 1994-08-30 00:00:00, dtype: float64  future value 0.7575962109488248\n",
            "loop 1 market state : step  82 market state fut market_state    2.0\n",
            "Name: 1994-08-31 00:00:00, dtype: float64  future value 0.7610545654432179\n",
            "loop 1 market state : step  83 market state fut market_state    2.0\n",
            "Name: 1994-09-01 00:00:00, dtype: float64  future value 0.7530762773041486\n",
            "loop 1 market state : step  84 market state fut market_state    2.0\n",
            "Name: 1994-09-02 00:00:00, dtype: float64  future value 0.7499074934842916\n",
            "loop 1 market state : step  85 market state fut market_state    2.0\n",
            "Name: 1994-09-06 00:00:00, dtype: float64  future value 0.751998597258587\n",
            "loop 1 market state : step  86 market state fut market_state    2.0\n",
            "Name: 1994-09-07 00:00:00, dtype: float64  future value 0.7540735510495084\n",
            "loop 1 market state : step  87 market state fut market_state    3.0\n",
            "Name: 1994-09-08 00:00:00, dtype: float64  future value 0.763740764197578\n",
            "loop 1 market state : step  88 market state fut market_state    3.0\n",
            "Name: 1994-09-09 00:00:00, dtype: float64  future value 0.7579179343258227\n",
            "loop 1 market state : step  89 market state fut market_state    3.0\n",
            "Name: 1994-09-12 00:00:00, dtype: float64  future value 0.7573710438554025\n",
            "loop 1 market state : step  90 market state fut market_state    2.0\n",
            "Name: 1994-09-13 00:00:00, dtype: float64  future value 0.7453232053465968\n",
            "loop 1 market state : step  91 market state fut market_state    2.0\n",
            "Name: 1994-09-14 00:00:00, dtype: float64  future value 0.7422670296174987\n",
            "loop 1 market state : step  92 market state fut market_state    2.0\n",
            "Name: 1994-09-15 00:00:00, dtype: float64  future value 0.7419614071357793\n",
            "loop 1 market state : step  93 market state fut market_state    2.0\n",
            "Name: 1994-09-16 00:00:00, dtype: float64  future value 0.739387816472178\n",
            "loop 1 market state : step  94 market state fut market_state    0.0\n",
            "Name: 1994-09-19 00:00:00, dtype: float64  future value 0.7412376031696772\n",
            "loop 1 market state : step  95 market state fut market_state    2.0\n",
            "Name: 1994-09-20 00:00:00, dtype: float64  future value 0.7432160497651183\n",
            "loop 1 market state : step  96 market state fut market_state    2.0\n",
            "Name: 1994-09-21 00:00:00, dtype: float64  future value 0.7477038307073329\n",
            "loop 1 market state : step  97 market state fut market_state    2.0\n",
            "Name: 1994-09-22 00:00:00, dtype: float64  future value 0.7435216722468377\n",
            "loop 1 market state : step  98 market state fut market_state    2.0\n",
            "Name: 1994-09-23 00:00:00, dtype: float64  future value 0.7442776780034969\n",
            "loop 1 market state : step  99 market state fut market_state    2.0\n",
            "Name: 1994-09-26 00:00:00, dtype: float64  future value 0.7427174128924384\n",
            "loop 1 market state : step  100 market state fut market_state    0.0\n",
            "Name: 1994-09-27 00:00:00, dtype: float64  future value 0.7312165139421481\n",
            "loop 1 market state : step  101 market state fut market_state    0.0\n",
            "Name: 1994-09-28 00:00:00, dtype: float64  future value 0.7294953871425908\n",
            "loop 1 market state : step  102 market state fut market_state    0.0\n",
            "Name: 1994-09-29 00:00:00, dtype: float64  future value 0.727629499549813\n",
            "loop 1 market state : step  103 market state fut market_state    0.0\n",
            "Name: 1994-09-30 00:00:00, dtype: float64  future value 0.7320368741918258\n",
            "loop 1 market state : step  104 market state fut market_state    0.0\n",
            "Name: 1994-10-03 00:00:00, dtype: float64  future value 0.7383744418315397\n",
            "loop 1 market state : step  105 market state fut market_state    2.0\n",
            "Name: 1994-10-04 00:00:00, dtype: float64  future value 0.7492319431159297\n",
            "loop 1 market state : step  106 market state fut market_state    2.0\n",
            "Name: 1994-10-05 00:00:00, dtype: float64  future value 0.7487172053479713\n",
            "loop 1 market state : step  107 market state fut market_state    2.0\n",
            "Name: 1994-10-06 00:00:00, dtype: float64  future value 0.7524489805335267\n",
            "loop 1 market state : step  108 market state fut market_state    2.0\n",
            "Name: 1994-10-07 00:00:00, dtype: float64  future value 0.7545561361150052\n",
            "loop 1 market state : step  109 market state fut market_state    2.0\n",
            "Name: 1994-10-10 00:00:00, dtype: float64  future value 0.7543309199334876\n",
            "loop 1 market state : step  110 market state fut market_state    2.0\n",
            "Name: 1994-10-11 00:00:00, dtype: float64  future value 0.7522398652472877\n",
            "loop 1 market state : step  111 market state fut market_state    2.0\n",
            "Name: 1994-10-12 00:00:00, dtype: float64  future value 0.7564541764102445\n",
            "loop 1 market state : step  112 market state fut market_state    1.0\n",
            "Name: 1994-10-13 00:00:00, dtype: float64  future value 0.7509369690202085\n",
            "loop 1 market state : step  113 market state fut market_state    1.0\n",
            "Name: 1994-10-14 00:00:00, dtype: float64  future value 0.7477842860956301\n",
            "loop 1 market state : step  114 market state fut market_state    1.0\n",
            "Name: 1994-10-17 00:00:00, dtype: float64  future value 0.7412536549768604\n",
            "loop 1 market state : step  115 market state fut market_state    1.0\n",
            "Name: 1994-10-18 00:00:00, dtype: float64  future value 0.7423796377082574\n",
            "loop 1 market state : step  116 market state fut market_state    1.0\n",
            "Name: 1994-10-19 00:00:00, dtype: float64  future value 0.7441329172102764\n",
            "loop 1 market state : step  117 market state fut market_state    1.0\n",
            "Name: 1994-10-20 00:00:00, dtype: float64  future value 0.7493284503114099\n",
            "loop 1 market state : step  118 market state fut market_state    4.0\n",
            "Name: 1994-10-21 00:00:00, dtype: float64  future value 0.7620678909957609\n",
            "loop 1 market state : step  119 market state fut market_state    4.0\n",
            "Name: 1994-10-24 00:00:00, dtype: float64  future value 0.7597838219186004\n",
            "loop 1 market state : step  120 market state fut market_state    3.0\n",
            "Name: 1994-10-25 00:00:00, dtype: float64  future value 0.753462355174165\n",
            "loop 1 market state : step  121 market state fut market_state    3.0\n",
            "Name: 1994-10-26 00:00:00, dtype: float64  future value 0.7503900785497885\n",
            "loop 1 market state : step  122 market state fut market_state    3.0\n",
            "Name: 1994-10-27 00:00:00, dtype: float64  future value 0.7526419949244872\n",
            "loop 1 market state : step  123 market state fut market_state    2.0\n",
            "Name: 1994-10-28 00:00:00, dtype: float64  future value 0.7435860267398564\n",
            "loop 1 market state : step  124 market state fut market_state    2.0\n",
            "Name: 1994-10-31 00:00:00, dtype: float64  future value 0.7448567702644739\n",
            "loop 1 market state : step  125 market state fut market_state    2.0\n",
            "Name: 1994-11-01 00:00:00, dtype: float64  future value 0.7490067269344122\n",
            "loop 1 market state : step  126 market state fut market_state    2.0\n",
            "Name: 1994-11-02 00:00:00, dtype: float64  future value 0.7486045972572125\n",
            "loop 1 market state : step  127 market state fut market_state    2.0\n",
            "Name: 1994-11-03 00:00:00, dtype: float64  future value 0.7469478249506738\n",
            "loop 1 market state : step  128 market state fut market_state    2.0\n",
            "Name: 1994-11-04 00:00:00, dtype: float64  future value 0.7436986348306152\n",
            "loop 1 market state : step  129 market state fut market_state    3.0\n",
            "Name: 1994-11-07 00:00:00, dtype: float64  future value 0.7496340727931293\n",
            "loop 1 market state : step  130 market state fut market_state    2.0\n",
            "Name: 1994-11-08 00:00:00, dtype: float64  future value 0.7480094531890523\n",
            "loop 1 market state : step  131 market state fut market_state    3.0\n",
            "Name: 1994-11-09 00:00:00, dtype: float64  future value 0.748958473336672\n",
            "loop 1 market state : step  132 market state fut market_state    2.0\n",
            "Name: 1994-11-10 00:00:00, dtype: float64  future value 0.7456610296188732\n",
            "loop 1 market state : step  133 market state fut market_state    2.0\n",
            "Name: 1994-11-11 00:00:00, dtype: float64  future value 0.7422831305127772\n",
            "loop 1 market state : step  134 market state fut market_state    0.0\n",
            "Name: 1994-11-14 00:00:00, dtype: float64  future value 0.7371841046071239\n",
            "loop 1 market state : step  135 market state fut market_state    0.0\n",
            "Name: 1994-11-15 00:00:00, dtype: float64  future value 0.7239781797525547\n",
            "loop 1 market state : step  136 market state fut market_state    0.0\n",
            "Name: 1994-11-16 00:00:00, dtype: float64  future value 0.7237208108685755\n",
            "loop 1 market state : step  137 market state fut market_state    0.0\n",
            "Name: 1994-11-17 00:00:00, dtype: float64  future value 0.7275169405471497\n",
            "loop 1 market state : step  138 market state fut market_state    0.0\n",
            "Name: 1994-11-18 00:00:00, dtype: float64  future value 0.7305248626785076\n",
            "loop 1 market state : step  139 market state fut market_state    0.0\n",
            "Name: 1994-11-21 00:00:00, dtype: float64  future value 0.7321494822825847\n",
            "loop 1 market state : step  140 market state fut market_state    0.0\n",
            "Name: 1994-11-22 00:00:00, dtype: float64  future value 0.7297688569218486\n",
            "loop 1 market state : step  141 market state fut market_state    0.0\n",
            "Name: 1994-11-23 00:00:00, dtype: float64  future value 0.7220962403525939\n",
            "loop 1 market state : step  142 market state fut market_state    2.0\n",
            "Name: 1994-11-25 00:00:00, dtype: float64  future value 0.7291415110631313\n",
            "loop 1 market state : step  143 market state fut market_state    1.0\n",
            "Name: 1994-11-28 00:00:00, dtype: float64  future value 0.7291737128536884\n",
            "loop 1 market state : step  144 market state fut market_state    1.0\n",
            "Name: 1994-11-29 00:00:00, dtype: float64  future value 0.728835888581412\n",
            "loop 1 market state : step  145 market state fut market_state    1.0\n",
            "Name: 1994-11-30 00:00:00, dtype: float64  future value 0.7258119146428709\n",
            "loop 1 market state : step  146 market state fut market_state    0.0\n",
            "Name: 1994-12-01 00:00:00, dtype: float64  future value 0.7165146784695392\n",
            "loop 1 market state : step  147 market state fut market_state    0.0\n",
            "Name: 1994-12-02 00:00:00, dtype: float64  future value 0.7189435083399202\n",
            "loop 1 market state : step  148 market state fut market_state    1.0\n",
            "Name: 1994-12-05 00:00:00, dtype: float64  future value 0.7229809060071949\n",
            "loop 1 market state : step  149 market state fut market_state    1.0\n",
            "Name: 1994-12-06 00:00:00, dtype: float64  future value 0.724074686948035\n",
            "loop 1 market state : step  150 market state fut market_state    2.0\n",
            "Name: 1994-12-07 00:00:00, dtype: float64  future value 0.7318277589055868\n",
            "loop 1 market state : step  151 market state fut market_state    2.0\n",
            "Name: 1994-12-08 00:00:00, dtype: float64  future value 0.732422902973747\n",
            "loop 1 market state : step  152 market state fut market_state    2.0\n",
            "Name: 1994-12-09 00:00:00, dtype: float64  future value 0.7379883639615232\n",
            "loop 1 market state : step  153 market state fut market_state    2.0\n",
            "Name: 1994-12-12 00:00:00, dtype: float64  future value 0.7365568078365021\n",
            "loop 1 market state : step  154 market state fut market_state    2.0\n",
            "Name: 1994-12-13 00:00:00, dtype: float64  future value 0.7352539116094229\n",
            "loop 1 market state : step  155 market state fut market_state    2.0\n",
            "Name: 1994-12-14 00:00:00, dtype: float64  future value 0.7392912601886024\n",
            "loop 1 market state : step  156 market state fut market_state    2.0\n",
            "Name: 1994-12-15 00:00:00, dtype: float64  future value 0.7394038682793611\n",
            "loop 1 market state : step  157 market state fut market_state    2.0\n",
            "Name: 1994-12-16 00:00:00, dtype: float64  future value 0.7396451362680618\n",
            "loop 1 market state : step  158 market state fut market_state    3.0\n",
            "Name: 1994-12-19 00:00:00, dtype: float64  future value 0.7438916492215757\n",
            "loop 1 market state : step  159 market state fut market_state    2.0\n",
            "Name: 1994-12-20 00:00:00, dtype: float64  future value 0.7413019085746005\n",
            "loop 1 market state : step  160 market state fut market_state    2.0\n",
            "Name: 1994-12-21 00:00:00, dtype: float64  future value 0.7418005945353757\n",
            "loop 1 market state : step  161 market state fut market_state    1.0\n",
            "Name: 1994-12-22 00:00:00, dtype: float64  future value 0.7387443697181822\n",
            "loop 1 market state : step  162 market state fut market_state    1.0\n",
            "Name: 1994-12-23 00:00:00, dtype: float64  future value 0.738487000834203\n",
            "loop 1 market state : step  163 market state fut market_state    1.0\n",
            "Name: 1994-12-27 00:00:00, dtype: float64  future value 0.7410606405858998\n",
            "loop 1 market state : step  164 market state fut market_state    2.0\n",
            "Name: 1994-12-28 00:00:00, dtype: float64  future value 0.7404654965177396\n",
            "loop 1 market state : step  165 market state fut market_state    2.0\n",
            "Name: 1994-12-29 00:00:00, dtype: float64  future value 0.7410123869881596\n",
            "loop 1 market state : step  166 market state fut market_state    3.0\n",
            "Name: 1994-12-30 00:00:00, dtype: float64  future value 0.7412536549768604\n",
            "loop 1 market state : step  167 market state fut market_state    3.0\n",
            "Name: 1995-01-03 00:00:00, dtype: float64  future value 0.7426209056969582\n",
            "loop 1 market state : step  168 market state fut market_state    3.0\n",
            "Name: 1995-01-04 00:00:00, dtype: float64  future value 0.7425887529944966\n",
            "loop 1 market state : step  169 market state fut market_state    3.0\n",
            "Name: 1995-01-05 00:00:00, dtype: float64  future value 0.7425566002920349\n",
            "loop 1 market state : step  170 market state fut market_state    4.0\n",
            "Name: 1995-01-06 00:00:00, dtype: float64  future value 0.7495214647023706\n",
            "loop 1 market state : step  171 market state fut market_state    4.0\n",
            "Name: 1995-01-09 00:00:00, dtype: float64  future value 0.7550065193899449\n",
            "loop 1 market state : step  172 market state fut market_state    4.0\n",
            "Name: 1995-01-10 00:00:00, dtype: float64  future value 0.7560841994355065\n",
            "loop 1 market state : step  173 market state fut market_state    4.0\n",
            "Name: 1995-01-11 00:00:00, dtype: float64  future value 0.7555373089650865\n",
            "loop 1 market state : step  174 market state fut market_state    4.0\n",
            "Name: 1995-01-12 00:00:00, dtype: float64  future value 0.7510978307087074\n",
            "loop 1 market state : step  175 market state fut market_state    2.0\n",
            "Name: 1995-01-13 00:00:00, dtype: float64  future value 0.7476073235118527\n",
            "loop 1 market state : step  176 market state fut market_state    2.0\n",
            "Name: 1995-01-16 00:00:00, dtype: float64  future value 0.7492801967136699\n",
            "loop 1 market state : step  177 market state fut market_state    2.0\n",
            "Name: 1995-01-17 00:00:00, dtype: float64  future value 0.7493445021185932\n",
            "loop 1 market state : step  178 market state fut market_state    2.0\n",
            "Name: 1995-01-18 00:00:00, dtype: float64  future value 0.7518859891678282\n",
            "loop 1 market state : step  179 market state fut market_state    3.0\n",
            "Name: 1995-01-19 00:00:00, dtype: float64  future value 0.7533014934856661\n",
            "loop 1 market state : step  180 market state fut market_state    4.0\n",
            "Name: 1995-01-20 00:00:00, dtype: float64  future value 0.7566311389940219\n",
            "loop 1 market state : step  181 market state fut market_state    3.0\n",
            "Name: 1995-01-23 00:00:00, dtype: float64  future value 0.7536071159673855\n",
            "loop 1 market state : step  182 market state fut market_state    4.0\n",
            "Name: 1995-01-24 00:00:00, dtype: float64  future value 0.7566793925917621\n",
            "loop 1 market state : step  183 market state fut market_state    4.0\n",
            "Name: 1995-01-25 00:00:00, dtype: float64  future value 0.7566471908012051\n",
            "loop 1 market state : step  184 market state fut market_state    4.0\n",
            "Name: 1995-01-26 00:00:00, dtype: float64  future value 0.7604915740775193\n",
            "loop 1 market state : step  185 market state fut market_state    4.0\n",
            "Name: 1995-01-27 00:00:00, dtype: float64  future value 0.7699174701487929\n",
            "loop 1 market state : step  186 market state fut market_state    4.0\n",
            "Name: 1995-01-30 00:00:00, dtype: float64  future value 0.773922715113606\n",
            "loop 1 market state : step  187 market state fut market_state    4.0\n",
            "Name: 1995-01-31 00:00:00, dtype: float64  future value 0.7733918764503691\n",
            "loop 1 market state : step  188 market state fut market_state    4.0\n",
            "Name: 1995-02-01 00:00:00, dtype: float64  future value 0.7740031214138078\n",
            "loop 1 market state : step  189 market state fut market_state    4.0\n",
            "Name: 1995-02-02 00:00:00, dtype: float64  future value 0.7723946027050093\n",
            "loop 1 market state : step  190 market state fut market_state    4.0\n",
            "Name: 1995-02-03 00:00:00, dtype: float64  future value 0.7744374037934691\n",
            "loop 1 market state : step  191 market state fut market_state    4.0\n",
            "Name: 1995-02-06 00:00:00, dtype: float64  future value 0.7747430262751884\n",
            "loop 1 market state : step  192 market state fut market_state    4.0\n",
            "Name: 1995-02-07 00:00:00, dtype: float64  future value 0.776190683295488\n",
            "loop 1 market state : step  193 market state fut market_state    4.0\n",
            "Name: 1995-02-08 00:00:00, dtype: float64  future value 0.779391668905902\n",
            "loop 1 market state : step  194 market state fut market_state    4.0\n",
            "Name: 1995-02-09 00:00:00, dtype: float64  future value 0.7804854498467421\n",
            "loop 1 market state : step  195 market state fut market_state    4.0\n",
            "Name: 1995-02-10 00:00:00, dtype: float64  future value 0.7752577640431468\n",
            "loop 1 market state : step  196 market state fut market_state    4.0\n",
            "Name: 1995-02-13 00:00:00, dtype: float64  future value 0.7764641530747458\n",
            "loop 1 market state : step  197 market state fut market_state    4.0\n",
            "Name: 1995-02-14 00:00:00, dtype: float64  future value 0.7802441818580413\n",
            "loop 1 market state : step  198 market state fut market_state    4.0\n",
            "Name: 1995-02-15 00:00:00, dtype: float64  future value 0.7832038503916592\n",
            "loop 1 market state : step  199 market state fut market_state    4.0\n",
            "Name: 1995-02-16 00:00:00, dtype: float64  future value 0.7851340433893602\n",
            "loop 1 market state : step  200 market state fut market_state    3.0\n",
            "Name: 1995-02-17 00:00:00, dtype: float64  future value 0.7782174325767647\n",
            "loop 1 market state : step  201 market state fut market_state    4.0\n",
            "Name: 1995-02-21 00:00:00, dtype: float64  future value 0.7839759570435968\n",
            "loop 1 market state : step  202 market state fut market_state    4.0\n",
            "Name: 1995-02-22 00:00:00, dtype: float64  future value 0.7811771011103825\n",
            "loop 1 market state : step  203 market state fut market_state    2.0\n",
            "Name: 1995-02-23 00:00:00, dtype: float64  future value 0.7803406890535216\n",
            "loop 1 market state : step  204 market state fut market_state    2.0\n",
            "Name: 1995-02-24 00:00:00, dtype: float64  future value 0.7808071732237399\n",
            "loop 1 market state : step  205 market state fut market_state    3.0\n",
            "Name: 1995-02-27 00:00:00, dtype: float64  future value 0.7811449484079209\n",
            "loop 1 market state : step  206 market state fut market_state    2.0\n",
            "Name: 1995-02-28 00:00:00, dtype: float64  future value 0.7754990320318476\n",
            "loop 1 market state : step  207 market state fut market_state    2.0\n",
            "Name: 1995-03-01 00:00:00, dtype: float64  future value 0.777139752531203\n",
            "loop 1 market state : step  208 market state fut market_state    2.0\n",
            "Name: 1995-03-02 00:00:00, dtype: float64  future value 0.7771719052336648\n",
            "loop 1 market state : step  209 market state fut market_state    4.0\n",
            "Name: 1995-03-03 00:00:00, dtype: float64  future value 0.7874825160476346\n",
            "loop 1 market state : step  210 market state fut market_state    4.0\n",
            "Name: 1995-03-06 00:00:00, dtype: float64  future value 0.788254573611477\n",
            "loop 1 market state : step  211 market state fut market_state    4.0\n",
            "Name: 1995-03-07 00:00:00, dtype: float64  future value 0.7928228099419886\n",
            "loop 1 market state : step  212 market state fut market_state    4.0\n",
            "Name: 1995-03-08 00:00:00, dtype: float64  future value 0.7911981903379116\n",
            "loop 1 market state : step  213 market state fut market_state    4.0\n",
            "Name: 1995-03-09 00:00:00, dtype: float64  future value 0.7968762594164466\n",
            "loop 1 market state : step  214 market state fut market_state    4.0\n",
            "Name: 1995-03-10 00:00:00, dtype: float64  future value 0.7970531729121286\n",
            "loop 1 market state : step  215 market state fut market_state    4.0\n",
            "Name: 1995-03-13 00:00:00, dtype: float64  future value 0.7980504957455838\n",
            "loop 1 market state : step  216 market state fut market_state    4.0\n",
            "Name: 1995-03-14 00:00:00, dtype: float64  future value 0.7963293689460266\n",
            "loop 1 market state : step  217 market state fut market_state    4.0\n",
            "Name: 1995-03-15 00:00:00, dtype: float64  future value 0.7972944899889247\n",
            "loop 1 market state : step  218 market state fut market_state    4.0\n",
            "Name: 1995-03-16 00:00:00, dtype: float64  future value 0.7977448732638646\n",
            "loop 1 market state : step  219 market state fut market_state    4.0\n",
            "Name: 1995-03-17 00:00:00, dtype: float64  future value 0.8058196195103188\n",
            "loop 1 market state : step  220 market state fut market_state    4.0\n",
            "Name: 1995-03-20 00:00:00, dtype: float64  future value 0.8094066339026538\n",
            "loop 1 market state : step  221 market state fut market_state    4.0\n",
            "Name: 1995-03-21 00:00:00, dtype: float64  future value 0.8105325675459555\n",
            "loop 1 market state : step  222 market state fut market_state    4.0\n",
            "Name: 1995-03-22 00:00:00, dtype: float64  future value 0.8092779249166165\n",
            "loop 1 market state : step  223 market state fut market_state    4.0\n",
            "Name: 1995-03-23 00:00:00, dtype: float64  future value 0.807830267896317\n",
            "loop 1 market state : step  224 market state fut market_state    2.0\n",
            "Name: 1995-03-24 00:00:00, dtype: float64  future value 0.8054013889378406\n",
            "loop 1 market state : step  225 market state fut market_state    2.0\n",
            "Name: 1995-03-27 00:00:00, dtype: float64  future value 0.8072351238281568\n",
            "loop 1 market state : step  226 market state fut market_state    4.0\n",
            "Name: 1995-03-28 00:00:00, dtype: float64  future value 0.8126879767251741\n",
            "loop 1 market state : step  227 market state fut market_state    4.0\n",
            "Name: 1995-03-29 00:00:00, dtype: float64  future value 0.813218815388411\n",
            "loop 1 market state : step  228 market state fut market_state    4.0\n",
            "Name: 1995-03-30 00:00:00, dtype: float64  future value 0.8140391265499934\n",
            "loop 1 market state : step  229 market state fut market_state    4.0\n",
            "Name: 1995-03-31 00:00:00, dtype: float64  future value 0.8145860661085088\n",
            "loop 1 market state : step  230 market state fut market_state    4.0\n",
            "Name: 1995-04-03 00:00:00, dtype: float64  future value 0.8155350862561285\n",
            "loop 1 market state : step  231 market state fut market_state    4.0\n",
            "Name: 1995-04-04 00:00:00, dtype: float64  future value 0.8131544608953923\n",
            "loop 1 market state : step  232 market state fut market_state    4.0\n",
            "Name: 1995-04-05 00:00:00, dtype: float64  future value 0.8157924551401078\n",
            "loop 1 market state : step  233 market state fut market_state    4.0\n",
            "Name: 1995-04-06 00:00:00, dtype: float64  future value 0.8191059997531851\n",
            "loop 1 market state : step  234 market state fut market_state    2.0\n",
            "Name: 1995-04-07 00:00:00, dtype: float64  future value 0.8141195819382906\n",
            "loop 1 market state : step  235 market state fut market_state    2.0\n",
            "Name: 1995-04-10 00:00:00, dtype: float64  future value 0.8128970920114131\n",
            "loop 1 market state : step  236 market state fut market_state    2.0\n",
            "Name: 1995-04-11 00:00:00, dtype: float64  future value 0.8121732880453111\n",
            "loop 1 market state : step  237 market state fut market_state    2.0\n",
            "Name: 1995-04-12 00:00:00, dtype: float64  future value 0.8127684321134713\n",
            "loop 1 market state : step  238 market state fut market_state    2.0\n",
            "Name: 1995-04-13 00:00:00, dtype: float64  future value 0.8179156625287693\n",
            "loop 1 market state : step  239 market state fut market_state    4.0\n",
            "Name: 1995-04-17 00:00:00, dtype: float64  future value 0.8249931841179591\n",
            "loop 1 market state : step  240 market state fut market_state    4.0\n",
            "Name: 1995-04-18 00:00:00, dtype: float64  future value 0.8237223915052462\n",
            "loop 1 market state : step  241 market state fut market_state    4.0\n",
            "Name: 1995-04-19 00:00:00, dtype: float64  future value 0.8246231580551258\n",
            "loop 1 market state : step  242 market state fut market_state    4.0\n",
            "Name: 1995-04-20 00:00:00, dtype: float64  future value 0.8260547632682422\n",
            "loop 1 market state : step  243 market state fut market_state    4.0\n",
            "Name: 1995-04-21 00:00:00, dtype: float64  future value 0.8279206999491153\n",
            "loop 1 market state : step  244 market state fut market_state    4.0\n",
            "Name: 1995-04-24 00:00:00, dtype: float64  future value 0.8271968468949178\n",
            "loop 1 market state : step  245 market state fut market_state    4.0\n",
            "Name: 1995-04-25 00:00:00, dtype: float64  future value 0.8281619188497207\n",
            "loop 1 market state : step  246 market state fut market_state    4.0\n",
            "Name: 1995-04-26 00:00:00, dtype: float64  future value 0.837201786139073\n",
            "loop 1 market state : step  247 market state fut market_state    4.0\n",
            "Name: 1995-04-27 00:00:00, dtype: float64  future value 0.8372982933345533\n",
            "loop 1 market state : step  248 market state fut market_state    4.0\n",
            "Name: 1995-04-28 00:00:00, dtype: float64  future value 0.8366227429661913\n",
            "loop 1 market state : step  249 market state fut market_state    4.0\n",
            "Name: 1995-05-01 00:00:00, dtype: float64  future value 0.8427994980055016\n",
            "loop 1 market state : step  250 market state fut market_state    4.0\n",
            "Name: 1995-05-02 00:00:00, dtype: float64  future value 0.8421560512515059\n",
            "loop 1 market state : step  251 market state fut market_state    4.0\n",
            "Name: 1995-05-03 00:00:00, dtype: float64  future value 0.8434428465833066\n",
            "loop 1 market state : step  252 market state fut market_state    4.0\n",
            "Name: 1995-05-04 00:00:00, dtype: float64  future value 0.8434589474785851\n",
            "loop 1 market state : step  253 market state fut market_state    4.0\n",
            "Name: 1995-05-05 00:00:00, dtype: float64  future value 0.8453569877738244\n",
            "loop 1 market state : step  254 market state fut market_state    4.0\n",
            "Name: 1995-05-08 00:00:00, dtype: float64  future value 0.8488796476731408\n",
            "loop 1 market state : step  255 market state fut market_state    4.0\n",
            "Name: 1995-05-09 00:00:00, dtype: float64  future value 0.8496035007273383\n",
            "loop 1 market state : step  256 market state fut market_state    4.0\n",
            "Name: 1995-05-10 00:00:00, dtype: float64  future value 0.8478019676275792\n",
            "loop 1 market state : step  257 market state fut market_state    2.0\n",
            "Name: 1995-05-11 00:00:00, dtype: float64  future value 0.8357541782068688\n",
            "loop 1 market state : step  258 market state fut market_state    2.0\n",
            "Name: 1995-05-12 00:00:00, dtype: float64  future value 0.8351268323481517\n",
            "loop 1 market state : step  259 market state fut market_state    2.0\n",
            "Name: 1995-05-15 00:00:00, dtype: float64  future value 0.8423008611328217\n",
            "loop 1 market state : step  260 market state fut market_state    4.0\n",
            "Name: 1995-05-16 00:00:00, dtype: float64  future value 0.850246947481334\n",
            "loop 1 market state : step  261 market state fut market_state    4.0\n",
            "Name: 1995-05-17 00:00:00, dtype: float64  future value 0.8502790510957002\n",
            "loop 1 market state : step  262 market state fut market_state    4.0\n",
            "Name: 1995-05-18 00:00:00, dtype: float64  future value 0.850246947481334\n",
            "loop 1 market state : step  263 market state fut market_state    3.0\n",
            "Name: 1995-05-19 00:00:00, dtype: float64  future value 0.8423008611328217\n",
            "loop 1 market state : step  264 market state fut market_state    2.0\n",
            "Name: 1995-05-22 00:00:00, dtype: float64  future value 0.8421882530420629\n",
            "loop 1 market state : step  265 market state fut market_state    4.0\n",
            "Name: 1995-05-23 00:00:00, dtype: float64  future value 0.8579839185436072\n",
            "loop 1 market state : step  266 market state fut market_state    4.0\n",
            "Name: 1995-05-24 00:00:00, dtype: float64  future value 0.8581286302487323\n",
            "loop 1 market state : step  267 market state fut market_state    4.0\n",
            "Name: 1995-05-25 00:00:00, dtype: float64  future value 0.8565523133304908\n",
            "loop 1 market state : step  268 market state fut market_state    4.0\n",
            "Name: 1995-05-26 00:00:00, dtype: float64  future value 0.8615225811620114\n",
            "loop 1 market state : step  269 market state fut market_state    4.0\n",
            "Name: 1995-05-30 00:00:00, dtype: float64  future value 0.8614421748618096\n",
            "loop 1 market state : step  270 market state fut market_state    2.0\n",
            "Name: 1995-05-31 00:00:00, dtype: float64  future value 0.8575495870758506\n",
            "loop 1 market state : step  271 market state fut market_state    2.0\n",
            "Name: 1995-06-01 00:00:00, dtype: float64  future value 0.8562948953584162\n",
            "loop 1 market state : step  272 market state fut market_state    2.0\n",
            "Name: 1995-06-02 00:00:00, dtype: float64  future value 0.8492013710501387\n",
            "loop 1 market state : step  273 market state fut market_state    2.0\n",
            "Name: 1995-06-05 00:00:00, dtype: float64  future value 0.853930419981054\n",
            "loop 1 market state : step  274 market state fut market_state    4.0\n",
            "Name: 1995-06-06 00:00:00, dtype: float64  future value 0.862246434216209\n",
            "loop 1 market state : step  275 market state fut market_state    4.0\n",
            "Name: 1995-06-07 00:00:00, dtype: float64  future value 0.8629219845845709\n",
            "loop 1 market state : step  276 market state fut market_state    4.0\n",
            "Name: 1995-06-08 00:00:00, dtype: float64  future value 0.8639675610157662\n",
            "loop 1 market state : step  277 market state fut market_state    4.0\n",
            "Name: 1995-06-09 00:00:00, dtype: float64  future value 0.8683266820600388\n",
            "loop 1 market state : step  278 market state fut market_state    4.0\n",
            "Name: 1995-06-12 00:00:00, dtype: float64  future value 0.876996523286558\n",
            "loop 1 market state : step  279 market state fut market_state    4.0\n",
            "Name: 1995-06-13 00:00:00, dtype: float64  future value 0.8766104945046368\n",
            "loop 1 market state : step  280 market state fut market_state    4.0\n",
            "Name: 1995-06-14 00:00:00, dtype: float64  future value 0.8750019757958383\n",
            "loop 1 market state : step  281 market state fut market_state    4.0\n",
            "Name: 1995-06-15 00:00:00, dtype: float64  future value 0.8864064166387438\n",
            "loop 1 market state : step  282 market state fut market_state    4.0\n",
            "Name: 1995-06-16 00:00:00, dtype: float64  future value 0.8842188547570635\n",
            "loop 1 market state : step  283 market state fut market_state    2.0\n",
            "Name: 1995-06-19 00:00:00, dtype: float64  future value 0.8752432928726344\n",
            "loop 1 market state : step  284 market state fut market_state    2.0\n",
            "Name: 1995-06-20 00:00:00, dtype: float64  future value 0.8725087914324388\n",
            "loop 1 market state : step  285 market state fut market_state    3.0\n",
            "Name: 1995-06-21 00:00:00, dtype: float64  future value 0.8762083648274371\n",
            "loop 1 market state : step  286 market state fut market_state    2.0\n",
            "Name: 1995-06-22 00:00:00, dtype: float64  future value 0.8748250623001562\n",
            "loop 1 market state : step  287 market state fut market_state    2.0\n",
            "Name: 1995-06-23 00:00:00, dtype: float64  future value 0.8762405666179942\n",
            "loop 1 market state : step  288 market state fut market_state    3.0\n",
            "Name: 1995-06-26 00:00:00, dtype: float64  future value 0.8800045435941066\n",
            "loop 1 market state : step  289 market state fut market_state    3.0\n",
            "Name: 1995-06-27 00:00:00, dtype: float64  future value 0.8802779642852689\n",
            "loop 1 market state : step  290 market state fut market_state    4.0\n",
            "Name: 1995-06-28 00:00:00, dtype: float64  future value 0.891103263779102\n",
            "loop 1 market state : step  291 market state fut market_state    4.0\n",
            "Name: 1995-06-29 00:00:00, dtype: float64  future value 0.8949315461601377\n",
            "loop 1 market state : step  292 market state fut market_state    4.0\n",
            "Name: 1995-06-30 00:00:00, dtype: float64  future value 0.8962505432824954\n",
            "loop 1 market state : step  293 market state fut market_state    4.0\n",
            "Name: 1995-07-03 00:00:00, dtype: float64  future value 0.8923740563918149\n",
            "loop 1 market state : step  294 market state fut market_state    4.0\n",
            "Name: 1995-07-05 00:00:00, dtype: float64  future value 0.9022020821402881\n",
            "loop 1 market state : step  295 market state fut market_state    4.0\n",
            "Name: 1995-07-06 00:00:00, dtype: float64  future value 0.9023789956359701\n",
            "loop 1 market state : step  296 market state fut market_state    4.0\n",
            "Name: 1995-07-07 00:00:00, dtype: float64  future value 0.9005935634314896\n",
            "loop 1 market state : step  297 market state fut market_state    4.0\n",
            "Name: 1995-07-10 00:00:00, dtype: float64  future value 0.9051456006905321\n",
            "loop 1 market state : step  298 market state fut market_state    4.0\n",
            "Name: 1995-07-11 00:00:00, dtype: float64  future value 0.8982933934590506\n",
            "loop 1 market state : step  299 market state fut market_state    2.0\n",
            "Name: 1995-07-12 00:00:00, dtype: float64  future value 0.8862616067574279\n",
            "loop 1 market state : step  300 market state fut market_state    2.0\n",
            "Name: 1995-07-13 00:00:00, dtype: float64  future value 0.8903794107249046\n",
            "loop 1 market state : step  301 market state fut market_state    2.0\n",
            "Name: 1995-07-14 00:00:00, dtype: float64  future value 0.8905081197109418\n",
            "loop 1 market state : step  302 market state fut market_state    2.0\n",
            "Name: 1995-07-17 00:00:00, dtype: float64  future value 0.8953497767326158\n",
            "loop 1 market state : step  303 market state fut market_state    3.0\n",
            "Name: 1995-07-18 00:00:00, dtype: float64  future value 0.9025398082363737\n",
            "loop 1 market state : step  304 market state fut market_state    3.0\n",
            "Name: 1995-07-19 00:00:00, dtype: float64  future value 0.9033601684860515\n",
            "loop 1 market state : step  305 market state fut market_state    4.0\n",
            "Name: 1995-07-20 00:00:00, dtype: float64  future value 0.9091668974625283\n",
            "loop 1 market state : step  306 market state fut market_state    4.0\n",
            "Name: 1995-07-21 00:00:00, dtype: float64  future value 0.9054834249628084\n",
            "loop 1 market state : step  307 market state fut market_state    3.0\n",
            "Name: 1995-07-24 00:00:00, dtype: float64  future value 0.9040840215402489\n",
            "loop 1 market state : step  308 market state fut market_state    2.0\n",
            "Name: 1995-07-25 00:00:00, dtype: float64  future value 0.9001914337542899\n",
            "loop 1 market state : step  309 market state fut market_state    2.0\n",
            "Name: 1995-07-26 00:00:00, dtype: float64  future value 0.8988402348413752\n",
            "loop 1 market state : step  310 market state fut market_state    2.0\n",
            "Name: 1995-07-27 00:00:00, dtype: float64  future value 0.8987598285411734\n",
            "loop 1 market state : step  311 market state fut market_state    2.0\n",
            "Name: 1995-07-28 00:00:00, dtype: float64  future value 0.8990654510228928\n",
            "loop 1 market state : step  312 market state fut market_state    2.0\n",
            "Name: 1995-07-31 00:00:00, dtype: float64  future value 0.9008187796130072\n",
            "loop 1 market state : step  313 market state fut market_state    3.0\n",
            "Name: 1995-08-01 00:00:00, dtype: float64  future value 0.9013978227858889\n",
            "loop 1 market state : step  314 market state fut market_state    3.0\n",
            "Name: 1995-08-02 00:00:00, dtype: float64  future value 0.9003040418450488\n",
            "loop 1 market state : step  315 market state fut market_state    2.0\n",
            "Name: 1995-08-03 00:00:00, dtype: float64  future value 0.8966687738549736\n",
            "loop 1 market state : step  316 market state fut market_state    2.0\n",
            "Name: 1995-08-04 00:00:00, dtype: float64  future value 0.892904796878861\n",
            "loop 1 market state : step  317 market state fut market_state    2.0\n",
            "Name: 1995-08-07 00:00:00, dtype: float64  future value 0.9003522463546935\n",
            "loop 1 market state : step  318 market state fut market_state    2.0\n",
            "Name: 1995-08-08 00:00:00, dtype: float64  future value 0.8984703069547326\n",
            "loop 1 market state : step  319 market state fut market_state    3.0\n",
            "Name: 1995-08-09 00:00:00, dtype: float64  future value 0.9007221742413362\n",
            "loop 1 market state : step  320 market state fut market_state    3.0\n",
            "Name: 1995-08-10 00:00:00, dtype: float64  future value 0.8992262636232964\n",
            "loop 1 market state : step  321 market state fut market_state    3.0\n",
            "Name: 1995-08-11 00:00:00, dtype: float64  future value 0.8994997824906494\n",
            "loop 1 market state : step  322 market state fut market_state    2.0\n",
            "Name: 1995-08-14 00:00:00, dtype: float64  future value 0.8977303530052566\n",
            "loop 1 market state : step  323 market state fut market_state    3.0\n",
            "Name: 1995-08-15 00:00:00, dtype: float64  future value 0.8999984193633294\n",
            "loop 1 market state : step  324 market state fut market_state    1.0\n",
            "Name: 1995-08-16 00:00:00, dtype: float64  future value 0.8961701369822936\n",
            "loop 1 market state : step  325 market state fut market_state    1.0\n",
            "Name: 1995-08-17 00:00:00, dtype: float64  future value 0.896684874750252\n",
            "loop 1 market state : step  326 market state fut market_state    3.0\n",
            "Name: 1995-08-18 00:00:00, dtype: float64  future value 0.9009312895275752\n",
            "loop 1 market state : step  327 market state fut market_state    2.0\n",
            "Name: 1995-08-21 00:00:00, dtype: float64  future value 0.8992423645185749\n",
            "loop 1 market state : step  328 market state fut market_state    3.0\n",
            "Name: 1995-08-22 00:00:00, dtype: float64  future value 0.9007704769271716\n",
            "loop 1 market state : step  329 market state fut market_state    3.0\n",
            "Name: 1995-08-23 00:00:00, dtype: float64  future value 0.9022502866499329\n",
            "loop 1 market state : step  330 market state fut market_state    3.0\n",
            "Name: 1995-08-24 00:00:00, dtype: float64  future value 0.9037944999538081\n",
            "loop 1 market state : step  331 market state fut market_state    3.0\n",
            "Name: 1995-08-25 00:00:00, dtype: float64  future value 0.9069472319664819\n",
            "loop 1 market state : step  332 market state fut market_state    4.0\n",
            "Name: 1995-08-28 00:00:00, dtype: float64  future value 0.9155205659975206\n",
            "loop 1 market state : step  333 market state fut market_state    4.0\n",
            "Name: 1995-08-29 00:00:00, dtype: float64  future value 0.9171290847063192\n",
            "loop 1 market state : step  334 market state fut market_state    4.0\n",
            "Name: 1995-08-30 00:00:00, dtype: float64  future value 0.9173220990972798\n",
            "loop 1 market state : step  335 market state fut market_state    4.0\n",
            "Name: 1995-08-31 00:00:00, dtype: float64  future value 0.921166482373594\n",
            "loop 1 market state : step  336 market state fut market_state    4.0\n",
            "Name: 1995-09-01 00:00:00, dtype: float64  future value 0.9231449289690351\n",
            "loop 1 market state : step  337 market state fut market_state    4.0\n",
            "Name: 1995-09-05 00:00:00, dtype: float64  future value 0.9273271365176258\n",
            "loop 1 market state : step  338 market state fut market_state    4.0\n",
            "Name: 1995-09-06 00:00:00, dtype: float64  future value 0.9309624045077008\n",
            "loop 1 market state : step  339 market state fut market_state    4.0\n",
            "Name: 1995-09-07 00:00:00, dtype: float64  future value 0.9387475800796189\n",
            "loop 1 market state : step  340 market state fut market_state    4.0\n",
            "Name: 1995-09-08 00:00:00, dtype: float64  future value 0.9383293495071408\n",
            "loop 1 market state : step  341 market state fut market_state    4.0\n",
            "Name: 1995-09-11 00:00:00, dtype: float64  future value 0.9373964793428949\n",
            "loop 1 market state : step  342 market state fut market_state    4.0\n",
            "Name: 1995-09-12 00:00:00, dtype: float64  future value 0.9396966493153339\n",
            "loop 1 market state : step  343 market state fut market_state    4.0\n",
            "Name: 1995-09-13 00:00:00, dtype: float64  future value 0.943830554178089\n",
            "loop 1 market state : step  344 market state fut market_state    2.0\n",
            "Name: 1995-09-14 00:00:00, dtype: float64  future value 0.9377664072295376\n",
            "loop 1 market state : step  345 market state fut market_state    2.0\n",
            "Name: 1995-09-15 00:00:00, dtype: float64  future value 0.9357235570529825\n",
            "loop 1 market state : step  346 market state fut market_state    2.0\n",
            "Name: 1995-09-18 00:00:00, dtype: float64  future value 0.9358522660390197\n",
            "loop 1 market state : step  347 market state fut market_state    2.0\n",
            "Name: 1995-09-19 00:00:00, dtype: float64  future value 0.9352088192850241\n",
            "loop 1 market state : step  348 market state fut market_state    2.0\n",
            "Name: 1995-09-20 00:00:00, dtype: float64  future value 0.9346136752168639\n",
            "loop 1 market state : step  349 market state fut market_state    3.0\n",
            "Name: 1995-09-21 00:00:00, dtype: float64  future value 0.9423828480696941\n",
            "loop 1 market state : step  350 market state fut market_state    3.0\n",
            "Name: 1995-09-22 00:00:00, dtype: float64  future value 0.9400343754114197\n",
            "loop 1 market state : step  351 market state fut market_state    2.0\n",
            "Name: 1995-09-25 00:00:00, dtype: float64  future value 0.9357074561577039\n",
            "loop 1 market state : step  352 market state fut market_state    3.0\n",
            "Name: 1995-09-26 00:00:00, dtype: float64  future value 0.9367048280792545\n",
            "loop 1 market state : step  353 market state fut market_state    3.0\n",
            "Name: 1995-09-27 00:00:00, dtype: float64  future value 0.9353053264805044\n",
            "loop 1 market state : step  354 market state fut market_state    2.0\n",
            "Name: 1995-09-28 00:00:00, dtype: float64  future value 0.9371712631613774\n",
            "loop 1 market state : step  355 market state fut market_state    2.0\n",
            "Name: 1995-09-29 00:00:00, dtype: float64  future value 0.9369460469798598\n",
            "loop 1 market state : step  356 market state fut market_state    2.0\n",
            "Name: 1995-10-02 00:00:00, dtype: float64  future value 0.9303189577537052\n",
            "loop 1 market state : step  357 market state fut market_state    2.0\n",
            "Name: 1995-10-03 00:00:00, dtype: float64  future value 0.9289517561217028\n",
            "loop 1 market state : step  358 market state fut market_state    2.0\n",
            "Name: 1995-10-04 00:00:00, dtype: float64  future value 0.9320722863438194\n",
            "loop 1 market state : step  359 market state fut market_state    3.0\n",
            "Name: 1995-10-05 00:00:00, dtype: float64  future value 0.9379272198299412\n",
            "loop 1 market state : step  360 market state fut market_state    3.0\n",
            "Name: 1995-10-06 00:00:00, dtype: float64  future value 0.9401791852927354\n",
            "loop 1 market state : step  361 market state fut market_state    3.0\n",
            "Name: 1995-10-09 00:00:00, dtype: float64  future value 0.9378147099153731\n",
            "loop 1 market state : step  362 market state fut market_state    4.0\n",
            "Name: 1995-10-10 00:00:00, dtype: float64  future value 0.9438466550733675\n",
            "loop 1 market state : step  363 market state fut market_state    4.0\n",
            "Name: 1995-10-11 00:00:00, dtype: float64  future value 0.9449082342236507\n",
            "loop 1 market state : step  364 market state fut market_state    4.0\n",
            "Name: 1995-10-12 00:00:00, dtype: float64  future value 0.9500716146223226\n",
            "loop 1 market state : step  365 market state fut market_state    4.0\n",
            "Name: 1995-10-13 00:00:00, dtype: float64  future value 0.9449404360142076\n",
            "loop 1 market state : step  366 market state fut market_state    3.0\n",
            "Name: 1995-10-16 00:00:00, dtype: float64  future value 0.941079951842615\n",
            "loop 1 market state : step  367 market state fut market_state    2.0\n",
            "Name: 1995-10-17 00:00:00, dtype: float64  future value 0.9434605281152557\n",
            "loop 1 market state : step  368 market state fut market_state    2.0\n",
            "Name: 1995-10-18 00:00:00, dtype: float64  future value 0.9369138451893029\n",
            "loop 1 market state : step  369 market state fut market_state    0.0\n",
            "Name: 1995-10-19 00:00:00, dtype: float64  future value 0.9276648626137114\n",
            "loop 1 market state : step  370 market state fut market_state    1.0\n",
            "Name: 1995-10-20 00:00:00, dtype: float64  future value 0.9324583151257406\n",
            "loop 1 market state : step  371 market state fut market_state    2.0\n",
            "Name: 1995-10-23 00:00:00, dtype: float64  future value 0.9381685369067372\n",
            "loop 1 market state : step  372 market state fut market_state    1.0\n",
            "Name: 1995-10-24 00:00:00, dtype: float64  future value 0.9353536291663398\n",
            "loop 1 market state : step  373 market state fut market_state    2.0\n",
            "Name: 1995-10-25 00:00:00, dtype: float64  future value 0.9397287529297003\n",
            "loop 1 market state : step  374 market state fut market_state    3.0\n",
            "Name: 1995-10-26 00:00:00, dtype: float64  future value 0.9485756058280921\n",
            "loop 1 market state : step  375 market state fut market_state    3.0\n",
            "Name: 1995-10-27 00:00:00, dtype: float64  future value 0.9499429056362853\n",
            "loop 1 market state : step  376 market state fut market_state    3.0\n",
            "Name: 1995-10-30 00:00:00, dtype: float64  future value 0.9465489547230062\n",
            "loop 1 market state : step  377 market state fut market_state    3.0\n",
            "Name: 1995-10-31 00:00:00, dtype: float64  future value 0.9431067011238916\n",
            "loop 1 market state : step  378 market state fut market_state    4.0\n",
            "Name: 1995-11-01 00:00:00, dtype: float64  future value 0.9517766405266014\n",
            "loop 1 market state : step  379 market state fut market_state    4.0\n",
            "Name: 1995-11-02 00:00:00, dtype: float64  future value 0.9542698248900009\n",
            "loop 1 market state : step  380 market state fut market_state    4.0\n",
            "Name: 1995-11-03 00:00:00, dtype: float64  future value 0.9534011619544877\n",
            "loop 1 market state : step  381 market state fut market_state    4.0\n",
            "Name: 1995-11-06 00:00:00, dtype: float64  future value 0.9527256115861257\n",
            "loop 1 market state : step  382 market state fut market_state    3.0\n",
            "Name: 1995-11-07 00:00:00, dtype: float64  future value 0.9478839545644516\n",
            "loop 1 market state : step  383 market state fut market_state    4.0\n",
            "Name: 1995-11-08 00:00:00, dtype: float64  future value 0.9553958076213981\n",
            "loop 1 market state : step  384 market state fut market_state    4.0\n",
            "Name: 1995-11-09 00:00:00, dtype: float64  future value 0.9608326087112323\n",
            "loop 1 market state : step  385 market state fut market_state    4.0\n",
            "Name: 1995-11-10 00:00:00, dtype: float64  future value 0.9652238333698713\n",
            "loop 1 market state : step  386 market state fut market_state    4.0\n",
            "Name: 1995-11-13 00:00:00, dtype: float64  future value 0.9600443520759209\n",
            "loop 1 market state : step  387 market state fut market_state    4.0\n",
            "Name: 1995-11-14 00:00:00, dtype: float64  future value 0.9654972540610336\n",
            "loop 1 market state : step  388 market state fut market_state    4.0\n",
            "Name: 1995-11-15 00:00:00, dtype: float64  future value 0.9625376346155111\n",
            "loop 1 market state : step  389 market state fut market_state    4.0\n",
            "Name: 1995-11-16 00:00:00, dtype: float64  future value 0.965062922593277\n",
            "loop 1 market state : step  390 market state fut market_state    4.0\n",
            "Name: 1995-11-17 00:00:00, dtype: float64  future value 0.9672344817558693\n",
            "loop 1 market state : step  391 market state fut market_state    4.0\n",
            "Name: 1995-11-20 00:00:00, dtype: float64  future value 0.9754861905861011\n",
            "loop 1 market state : step  392 market state fut market_state    4.0\n",
            "Name: 1995-11-21 00:00:00, dtype: float64  future value 0.9774003317766189\n",
            "loop 1 market state : step  393 market state fut market_state    4.0\n",
            "Name: 1995-11-22 00:00:00, dtype: float64  future value 0.9737489628912652\n",
            "loop 1 market state : step  394 market state fut market_state    4.0\n",
            "Name: 1995-11-24 00:00:00, dtype: float64  future value 0.9763386544501451\n",
            "loop 1 market state : step  395 market state fut market_state    4.0\n",
            "Name: 1995-11-27 00:00:00, dtype: float64  future value 0.9871157494343333\n",
            "loop 1 market state : step  396 market state fut market_state    4.0\n",
            "Name: 1995-11-28 00:00:00, dtype: float64  future value 0.9935498242695274\n",
            "loop 1 market state : step  397 market state fut market_state    4.0\n",
            "Name: 1995-11-29 00:00:00, dtype: float64  future value 0.9975711210415237\n",
            "loop 1 market state : step  398 market state fut market_state    4.0\n",
            "Name: 1995-11-30 00:00:00, dtype: float64  future value 0.9911209453110511\n",
            "loop 1 market state : step  399 market state fut market_state    4.0\n",
            "Name: 1995-12-01 00:00:00, dtype: float64  future value 0.9932281008925296\n",
            "loop 1 market state : step  400 market state fut market_state    4.0\n",
            "Name: 1995-12-04 00:00:00, dtype: float64  future value 0.9965095418912406\n",
            "loop 1 market state : step  401 market state fut market_state    4.0\n",
            "Name: 1995-12-05 00:00:00, dtype: float64  future value 0.9953192537549203\n",
            "loop 1 market state : step  402 market state fut market_state    4.0\n",
            "Name: 1995-12-06 00:00:00, dtype: float64  future value 1.0\n",
            "loop 1 market state : step  403 market state fut market_state    3.0\n",
            "Name: 1995-12-07 00:00:00, dtype: float64  future value 0.99232733434265\n",
            "loop 1 market state : step  404 market state fut market_state    2.0\n",
            "Name: 1995-12-08 00:00:00, dtype: float64  future value 0.9913944641784042\n",
            "loop 1 market state : step  405 market state fut market_state    2.0\n",
            "Name: 1995-12-11 00:00:00, dtype: float64  future value 0.9760652337589828\n",
            "loop 1 market state : step  406 market state fut market_state    2.0\n",
            "Name: 1995-12-12 00:00:00, dtype: float64  future value 0.984300841693936\n",
            "loop 1 market state : step  407 market state fut market_state   -1.0\n",
            "Name: 1995-12-13 00:00:00, dtype: float64  future value nan\n",
            "loop 1 market state : step  408 market state fut market_state   -1.0\n",
            "Name: 1995-12-14 00:00:00, dtype: float64  future value nan\n",
            "loop 1 market state : step  409 market state fut market_state   -1.0\n",
            "Name: 1995-12-15 00:00:00, dtype: float64  future value nan\n",
            "loop 1 market state : step  410 market state fut market_state   -1.0\n",
            "Name: 1995-12-18 00:00:00, dtype: float64  future value nan\n",
            "loop 1 market state : step  411 market state fut market_state   -1.0\n",
            "Name: 1995-12-19 00:00:00, dtype: float64  future value nan\n",
            "loop 2 image : step  0\n",
            "loop 2 image : step  1\n",
            "loop 2 image : step  2\n",
            "loop 2 image : step  3\n",
            "loop 2 image : step  4\n",
            "loop 2 image : step  5\n",
            "loop 2 image : step  6\n",
            "loop 2 image : step  7\n",
            "loop 2 image : step  8\n",
            "loop 2 image : step  9\n",
            "loop 2 image : step  10\n",
            "loop 2 image : step  11\n",
            "loop 2 image : step  12\n",
            "loop 2 image : step  13\n",
            "loop 2 image : step  14\n",
            "loop 2 image : step  15\n",
            "loop 2 image : step  16\n",
            "loop 2 image : step  17\n",
            "loop 2 image : step  18\n",
            "loop 2 image : step  19\n",
            "loop 2 image : step  20\n",
            "loop 2 image : step  21\n",
            "loop 2 image : step  22\n",
            "loop 2 image : step  23\n",
            "loop 2 image : step  24\n",
            "loop 2 image : step  25\n",
            "loop 2 image : step  26\n",
            "loop 2 image : step  27\n",
            "loop 2 image : step  28\n",
            "loop 2 image : step  29\n",
            "loop 2 image : step  30\n",
            "loop 2 image : step  31\n",
            "loop 2 image : step  32\n",
            "loop 2 image : step  33\n",
            "loop 2 image : step  34\n",
            "loop 2 image : step  35\n",
            "loop 2 image : step  36\n",
            "loop 2 image : step  37\n",
            "loop 2 image : step  38\n",
            "loop 2 image : step  39\n",
            "loop 2 image : step  40\n",
            "loop 2 image : step  41\n",
            "loop 2 image : step  42\n",
            "loop 2 image : step  43\n",
            "loop 2 image : step  44\n",
            "loop 2 image : step  45\n",
            "loop 2 image : step  46\n",
            "loop 2 image : step  47\n",
            "loop 2 image : step  48\n",
            "loop 2 image : step  49\n",
            "loop 2 image : step  50\n",
            "loop 2 image : step  51\n",
            "loop 2 image : step  52\n",
            "loop 2 image : step  53\n",
            "loop 2 image : step  54\n",
            "loop 2 image : step  55\n",
            "loop 2 image : step  56\n",
            "loop 2 image : step  57\n",
            "loop 2 image : step  58\n",
            "loop 2 image : step  59\n",
            "loop 2 image : step  60\n",
            "loop 2 image : step  61\n",
            "loop 2 image : step  62\n",
            "loop 2 image : step  63\n",
            "loop 2 image : step  64\n",
            "loop 2 image : step  65\n",
            "loop 2 image : step  66\n",
            "loop 2 image : step  67\n",
            "loop 2 image : step  68\n",
            "loop 2 image : step  69\n",
            "loop 2 image : step  70\n",
            "loop 2 image : step  71\n",
            "loop 2 image : step  72\n",
            "loop 2 image : step  73\n",
            "loop 2 image : step  74\n",
            "loop 2 image : step  75\n",
            "loop 2 image : step  76\n",
            "loop 2 image : step  77\n",
            "loop 2 image : step  78\n",
            "loop 2 image : step  79\n",
            "loop 2 image : step  80\n",
            "loop 2 image : step  81\n",
            "loop 2 image : step  82\n",
            "loop 2 image : step  83\n",
            "loop 2 image : step  84\n",
            "loop 2 image : step  85\n",
            "loop 2 image : step  86\n",
            "loop 2 image : step  87\n",
            "loop 2 image : step  88\n",
            "loop 2 image : step  89\n",
            "loop 2 image : step  90\n",
            "loop 2 image : step  91\n",
            "loop 2 image : step  92\n",
            "loop 2 image : step  93\n",
            "loop 2 image : step  94\n",
            "loop 2 image : step  95\n",
            "loop 2 image : step  96\n",
            "loop 2 image : step  97\n",
            "loop 2 image : step  98\n",
            "loop 2 image : step  99\n",
            "loop 2 image : step  100\n",
            "loop 2 image : step  101\n",
            "loop 2 image : step  102\n",
            "loop 2 image : step  103\n",
            "loop 2 image : step  104\n",
            "loop 2 image : step  105\n",
            "loop 2 image : step  106\n",
            "loop 2 image : step  107\n",
            "loop 2 image : step  108\n",
            "loop 2 image : step  109\n",
            "loop 2 image : step  110\n",
            "loop 2 image : step  111\n",
            "loop 2 image : step  112\n",
            "loop 2 image : step  113\n",
            "loop 2 image : step  114\n",
            "loop 2 image : step  115\n",
            "loop 2 image : step  116\n",
            "loop 2 image : step  117\n",
            "loop 2 image : step  118\n",
            "loop 2 image : step  119\n",
            "loop 2 image : step  120\n",
            "loop 2 image : step  121\n",
            "loop 2 image : step  122\n",
            "loop 2 image : step  123\n",
            "loop 2 image : step  124\n",
            "loop 2 image : step  125\n",
            "loop 2 image : step  126\n",
            "loop 2 image : step  127\n",
            "loop 2 image : step  128\n",
            "loop 2 image : step  129\n",
            "loop 2 image : step  130\n",
            "loop 2 image : step  131\n",
            "loop 2 image : step  132\n",
            "loop 2 image : step  133\n",
            "loop 2 image : step  134\n",
            "loop 2 image : step  135\n",
            "loop 2 image : step  136\n",
            "loop 2 image : step  137\n",
            "loop 2 image : step  138\n",
            "loop 2 image : step  139\n",
            "loop 2 image : step  140\n",
            "loop 2 image : step  141\n",
            "loop 2 image : step  142\n",
            "loop 2 image : step  143\n",
            "loop 2 image : step  144\n",
            "loop 2 image : step  145\n",
            "loop 2 image : step  146\n",
            "loop 2 image : step  147\n",
            "loop 2 image : step  148\n",
            "loop 2 image : step  149\n",
            "loop 2 image : step  150\n",
            "loop 2 image : step  151\n",
            "loop 2 image : step  152\n",
            "loop 2 image : step  153\n",
            "loop 2 image : step  154\n",
            "loop 2 image : step  155\n",
            "loop 2 image : step  156\n",
            "loop 2 image : step  157\n",
            "loop 2 image : step  158\n",
            "loop 2 image : step  159\n",
            "loop 2 image : step  160\n",
            "loop 2 image : step  161\n",
            "loop 2 image : step  162\n",
            "loop 2 image : step  163\n",
            "loop 2 image : step  164\n",
            "loop 2 image : step  165\n",
            "loop 2 image : step  166\n",
            "loop 2 image : step  167\n",
            "loop 2 image : step  168\n",
            "loop 2 image : step  169\n",
            "loop 2 image : step  170\n",
            "loop 2 image : step  171\n",
            "loop 2 image : step  172\n",
            "loop 2 image : step  173\n",
            "loop 2 image : step  174\n",
            "loop 2 image : step  175\n",
            "loop 2 image : step  176\n",
            "loop 2 image : step  177\n",
            "loop 2 image : step  178\n",
            "loop 2 image : step  179\n",
            "loop 2 image : step  180\n",
            "loop 2 image : step  181\n",
            "loop 2 image : step  182\n",
            "loop 2 image : step  183\n",
            "loop 2 image : step  184\n",
            "loop 2 image : step  185\n",
            "loop 2 image : step  186\n",
            "loop 2 image : step  187\n",
            "loop 2 image : step  188\n",
            "loop 2 image : step  189\n",
            "loop 2 image : step  190\n",
            "loop 2 image : step  191\n",
            "loop 2 image : step  192\n",
            "loop 2 image : step  193\n",
            "loop 2 image : step  194\n",
            "loop 2 image : step  195\n",
            "loop 2 image : step  196\n",
            "loop 2 image : step  197\n",
            "loop 2 image : step  198\n",
            "loop 2 image : step  199\n",
            "loop 2 image : step  200\n",
            "loop 2 image : step  201\n",
            "loop 2 image : step  202\n",
            "loop 2 image : step  203\n",
            "loop 2 image : step  204\n",
            "loop 2 image : step  205\n",
            "loop 2 image : step  206\n",
            "loop 2 image : step  207\n",
            "loop 2 image : step  208\n",
            "loop 2 image : step  209\n",
            "loop 2 image : step  210\n",
            "loop 2 image : step  211\n",
            "loop 2 image : step  212\n",
            "loop 2 image : step  213\n",
            "loop 2 image : step  214\n",
            "loop 2 image : step  215\n",
            "loop 2 image : step  216\n",
            "loop 2 image : step  217\n",
            "loop 2 image : step  218\n",
            "loop 2 image : step  219\n",
            "loop 2 image : step  220\n",
            "loop 2 image : step  221\n",
            "loop 2 image : step  222\n",
            "loop 2 image : step  223\n",
            "loop 2 image : step  224\n",
            "loop 2 image : step  225\n",
            "loop 2 image : step  226\n",
            "loop 2 image : step  227\n",
            "loop 2 image : step  228\n",
            "loop 2 image : step  229\n",
            "loop 2 image : step  230\n",
            "loop 2 image : step  231\n",
            "loop 2 image : step  232\n",
            "loop 2 image : step  233\n",
            "loop 2 image : step  234\n",
            "loop 2 image : step  235\n",
            "loop 2 image : step  236\n",
            "loop 2 image : step  237\n",
            "loop 2 image : step  238\n",
            "loop 2 image : step  239\n",
            "loop 2 image : step  240\n",
            "loop 2 image : step  241\n",
            "loop 2 image : step  242\n",
            "loop 2 image : step  243\n",
            "loop 2 image : step  244\n",
            "loop 2 image : step  245\n",
            "loop 2 image : step  246\n",
            "loop 2 image : step  247\n",
            "loop 2 image : step  248\n",
            "loop 2 image : step  249\n",
            "loop 2 image : step  250\n",
            "loop 2 image : step  251\n",
            "loop 2 image : step  252\n",
            "loop 2 image : step  253\n",
            "loop 2 image : step  254\n",
            "loop 2 image : step  255\n",
            "loop 2 image : step  256\n",
            "loop 2 image : step  257\n",
            "loop 2 image : step  258\n",
            "loop 2 image : step  259\n",
            "loop 2 image : step  260\n",
            "loop 2 image : step  261\n",
            "loop 2 image : step  262\n",
            "loop 2 image : step  263\n",
            "loop 2 image : step  264\n",
            "loop 2 image : step  265\n",
            "loop 2 image : step  266\n",
            "loop 2 image : step  267\n",
            "loop 2 image : step  268\n",
            "loop 2 image : step  269\n",
            "loop 2 image : step  270\n",
            "loop 2 image : step  271\n",
            "loop 2 image : step  272\n",
            "loop 2 image : step  273\n",
            "loop 2 image : step  274\n",
            "loop 2 image : step  275\n",
            "loop 2 image : step  276\n",
            "loop 2 image : step  277\n",
            "loop 2 image : step  278\n",
            "loop 2 image : step  279\n",
            "loop 2 image : step  280\n",
            "loop 2 image : step  281\n",
            "loop 2 image : step  282\n",
            "loop 2 image : step  283\n",
            "loop 2 image : step  284\n",
            "loop 2 image : step  285\n",
            "loop 2 image : step  286\n",
            "loop 2 image : step  287\n",
            "loop 2 image : step  288\n",
            "loop 2 image : step  289\n",
            "loop 2 image : step  290\n",
            "loop 2 image : step  291\n",
            "loop 2 image : step  292\n",
            "loop 2 image : step  293\n",
            "loop 2 image : step  294\n",
            "loop 2 image : step  295\n",
            "loop 2 image : step  296\n",
            "loop 2 image : step  297\n",
            "loop 2 image : step  298\n",
            "loop 2 image : step  299\n",
            "loop 2 image : step  300\n",
            "loop 2 image : step  301\n",
            "loop 2 image : step  302\n",
            "loop 2 image : step  303\n",
            "loop 2 image : step  304\n",
            "loop 2 image : step  305\n",
            "loop 2 image : step  306\n",
            "loop 2 image : step  307\n",
            "loop 2 image : step  308\n",
            "loop 2 image : step  309\n",
            "loop 2 image : step  310\n",
            "loop 2 image : step  311\n",
            "loop 2 image : step  312\n",
            "loop 2 image : step  313\n",
            "loop 2 image : step  314\n",
            "loop 2 image : step  315\n",
            "loop 2 image : step  316\n",
            "loop 2 image : step  317\n",
            "loop 2 image : step  318\n",
            "loop 2 image : step  319\n",
            "loop 2 image : step  320\n",
            "loop 2 image : step  321\n",
            "loop 2 image : step  322\n",
            "loop 2 image : step  323\n",
            "loop 2 image : step  324\n",
            "loop 2 image : step  325\n",
            "loop 2 image : step  326\n",
            "loop 2 image : step  327\n",
            "loop 2 image : step  328\n",
            "loop 2 image : step  329\n",
            "loop 2 image : step  330\n",
            "loop 2 image : step  331\n",
            "loop 2 image : step  332\n",
            "loop 2 image : step  333\n",
            "loop 2 image : step  334\n",
            "loop 2 image : step  335\n",
            "loop 2 image : step  336\n",
            "loop 2 image : step  337\n",
            "loop 2 image : step  338\n",
            "loop 2 image : step  339\n",
            "loop 2 image : step  340\n",
            "loop 2 image : step  341\n",
            "loop 2 image : step  342\n",
            "loop 2 image : step  343\n",
            "loop 2 image : step  344\n",
            "loop 2 image : step  345\n",
            "loop 2 image : step  346\n",
            "loop 2 image : step  347\n",
            "loop 2 image : step  348\n",
            "loop 2 image : step  349\n",
            "loop 2 image : step  350\n",
            "loop 2 image : step  351\n",
            "loop 2 image : step  352\n",
            "loop 2 image : step  353\n",
            "loop 2 image : step  354\n",
            "loop 2 image : step  355\n",
            "loop 2 image : step  356\n",
            "loop 2 image : step  357\n",
            "loop 2 image : step  358\n",
            "loop 2 image : step  359\n",
            "loop 2 image : step  360\n",
            "loop 2 image : step  361\n",
            "loop 2 image : step  362\n",
            "loop 2 image : step  363\n",
            "loop 2 image : step  364\n",
            "loop 2 image : step  365\n",
            "loop 2 image : step  366\n",
            "loop 2 image : step  367\n",
            "loop 2 image : step  368\n",
            "loop 2 image : step  369\n",
            "loop 2 image : step  370\n",
            "loop 2 image : step  371\n",
            "loop 2 image : step  372\n",
            "loop 2 image : step  373\n",
            "loop 2 image : step  374\n",
            "loop 2 image : step  375\n",
            "loop 2 image : step  376\n",
            "loop 2 image : step  377\n",
            "loop 2 image : step  378\n",
            "loop 2 image : step  379\n",
            "loop 2 image : step  380\n",
            "loop 2 image : step  381\n",
            "loop 2 image : step  382\n",
            "loop 2 image : step  383\n",
            "loop 2 image : step  384\n",
            "loop 2 image : step  385\n",
            "loop 2 image : step  386\n",
            "loop 2 image : step  387\n",
            "loop 2 image : step  388\n",
            "loop 2 image : step  389\n",
            "loop 2 image : step  390\n",
            "loop 2 image : step  391\n",
            "loop 2 image : step  392\n",
            "loop 2 image : step  393\n",
            "loop 2 image : step  394\n",
            "loop 2 image : step  395\n",
            "loop 2 image : step  396\n",
            "loop 2 image : step  397\n",
            "loop 2 image : step  398\n",
            "loop 2 image : step  399\n",
            "loop 2 image : step  400\n",
            "loop 2 image : step  401\n",
            "loop 2 image : step  402\n",
            "loop 2 image : step  403\n",
            "loop 2 image : step  404\n",
            "loop 2 image : step  405\n",
            "loop 2 image : step  406\n",
            "loop 2 image : step  407\n",
            "loop 2 image : step  408\n",
            "loop 2 image : step  409\n",
            "loop 2 image : step  410\n",
            "loop 2 image : step  411\n",
            "loop 1 market state : step  0 market state fut market_state    2.0\n",
            "Name: 1995-12-20 00:00:00, dtype: float64  future value 0.6394951583153377\n",
            "loop 1 market state : step  1 market state fut market_state    2.0\n",
            "Name: 1995-12-21 00:00:00, dtype: float64  future value 0.641379944162544\n",
            "loop 1 market state : step  2 market state fut market_state    2.0\n",
            "Name: 1995-12-22 00:00:00, dtype: float64  future value 0.6463782653017058\n",
            "loop 1 market state : step  3 market state fut market_state    2.0\n",
            "Name: 1995-12-26 00:00:00, dtype: float64  future value 0.6469926718026313\n",
            "loop 1 market state : step  4 market state fut market_state    2.0\n",
            "Name: 1995-12-27 00:00:00, dtype: float64  future value 0.6432231001082187\n",
            "loop 1 market state : step  5 market state fut market_state    2.0\n",
            "Name: 1995-12-28 00:00:00, dtype: float64  future value 0.6421922039206723\n",
            "loop 1 market state : step  6 market state fut market_state    2.0\n",
            "Name: 1995-12-29 00:00:00, dtype: float64  future value 0.6440145131370304\n",
            "loop 1 market state : step  7 market state fut market_state    1.0\n",
            "Name: 1996-01-02 00:00:00, dtype: float64  future value 0.6346322138025305\n",
            "loop 1 market state : step  8 market state fut market_state    1.0\n",
            "Name: 1996-01-03 00:00:00, dtype: float64  future value 0.6232089052651529\n",
            "loop 1 market state : step  9 market state fut market_state    1.0\n",
            "Name: 1996-01-04 00:00:00, dtype: float64  future value 0.6275928834604909\n",
            "loop 1 market state : step  10 market state fut market_state    1.0\n",
            "Name: 1996-01-05 00:00:00, dtype: float64  future value 0.6266765171699827\n",
            "loop 1 market state : step  11 market state fut market_state    1.0\n",
            "Name: 1996-01-08 00:00:00, dtype: float64  future value 0.6246043014302318\n",
            "loop 1 market state : step  12 market state fut market_state    1.0\n",
            "Name: 1996-01-09 00:00:00, dtype: float64  future value 0.6335804708856676\n",
            "loop 1 market state : step  13 market state fut market_state    2.0\n",
            "Name: 1996-01-10 00:00:00, dtype: float64  future value 0.6314249317857518\n",
            "loop 1 market state : step  14 market state fut market_state    2.0\n",
            "Name: 1996-01-11 00:00:00, dtype: float64  future value 0.6333721942638063\n",
            "loop 1 market state : step  15 market state fut market_state    2.0\n",
            "Name: 1996-01-12 00:00:00, dtype: float64  future value 0.6371105594213456\n",
            "loop 1 market state : step  16 market state fut market_state    2.0\n",
            "Name: 1996-01-15 00:00:00, dtype: float64  future value 0.6387454387451591\n",
            "loop 1 market state : step  17 market state fut market_state    2.0\n",
            "Name: 1996-01-16 00:00:00, dtype: float64  future value 0.638110185514917\n",
            "loop 1 market state : step  18 market state fut market_state    2.0\n",
            "Name: 1996-01-17 00:00:00, dtype: float64  future value 0.6455764924653374\n",
            "loop 1 market state : step  19 market state fut market_state    2.0\n",
            "Name: 1996-01-18 00:00:00, dtype: float64  future value 0.64252543380423\n",
            "loop 1 market state : step  20 market state fut market_state    2.0\n",
            "Name: 1996-01-19 00:00:00, dtype: float64  future value 0.6473050549568724\n",
            "loop 1 market state : step  21 market state fut market_state    2.0\n",
            "Name: 1996-01-22 00:00:00, dtype: float64  future value 0.6500124603697638\n",
            "loop 1 market state : step  22 market state fut market_state    2.0\n",
            "Name: 1996-01-23 00:00:00, dtype: float64  future value 0.6561875412445866\n",
            "loop 1 market state : step  23 market state fut market_state    2.0\n",
            "Name: 1996-01-24 00:00:00, dtype: float64  future value 0.6623000819314596\n",
            "loop 1 market state : step  24 market state fut market_state    2.0\n",
            "Name: 1996-01-25 00:00:00, dtype: float64  future value 0.664840904181123\n",
            "loop 1 market state : step  25 market state fut market_state    4.0\n",
            "Name: 1996-01-26 00:00:00, dtype: float64  future value 0.662112652038915\n",
            "loop 1 market state : step  26 market state fut market_state    4.0\n",
            "Name: 1996-01-29 00:00:00, dtype: float64  future value 0.667933592743762\n",
            "loop 1 market state : step  27 market state fut market_state    4.0\n",
            "Name: 1996-01-30 00:00:00, dtype: float64  future value 0.6730360839724053\n",
            "loop 1 market state : step  28 market state fut market_state    4.0\n",
            "Name: 1996-01-31 00:00:00, dtype: float64  future value 0.6767848089375014\n",
            "loop 1 market state : step  29 market state fut market_state    4.0\n",
            "Name: 1996-02-01 00:00:00, dtype: float64  future value 0.6831785262417421\n",
            "loop 1 market state : step  30 market state fut market_state    4.0\n",
            "Name: 1996-02-02 00:00:00, dtype: float64  future value 0.6834909093959832\n",
            "loop 1 market state : step  31 market state fut market_state    4.0\n",
            "Name: 1996-02-05 00:00:00, dtype: float64  future value 0.6887808305171711\n",
            "loop 1 market state : step  32 market state fut market_state    4.0\n",
            "Name: 1996-02-06 00:00:00, dtype: float64  future value 0.6878019875958147\n",
            "loop 1 market state : step  33 market state fut market_state    4.0\n",
            "Name: 1996-02-07 00:00:00, dtype: float64  future value 0.682668289830298\n",
            "loop 1 market state : step  34 market state fut market_state    2.0\n",
            "Name: 1996-02-08 00:00:00, dtype: float64  future value 0.6782322583687701\n",
            "loop 1 market state : step  35 market state fut market_state    2.0\n",
            "Name: 1996-02-09 00:00:00, dtype: float64  future value 0.674754223099282\n",
            "loop 1 market state : step  36 market state fut market_state    2.0\n",
            "Name: 1996-02-12 00:00:00, dtype: float64  future value 0.6671213965427352\n",
            "loop 1 market state : step  37 market state fut market_state    2.0\n",
            "Name: 1996-02-13 00:00:00, dtype: float64  future value 0.6748791763609784\n",
            "loop 1 market state : step  38 market state fut market_state    3.0\n",
            "Name: 1996-02-14 00:00:00, dtype: float64  future value 0.6860837849118364\n",
            "loop 1 market state : step  39 market state fut market_state    3.0\n",
            "Name: 1996-02-15 00:00:00, dtype: float64  future value 0.6863129082630143\n",
            "loop 1 market state : step  40 market state fut market_state    3.0\n",
            "Name: 1996-02-16 00:00:00, dtype: float64  future value 0.6773367388075785\n",
            "loop 1 market state : step  41 market state fut market_state    3.0\n",
            "Name: 1996-02-20 00:00:00, dtype: float64  future value 0.6739836567997868\n",
            "loop 1 market state : step  42 market state fut market_state    2.0\n",
            "Name: 1996-02-21 00:00:00, dtype: float64  future value 0.6713907812839335\n",
            "loop 1 market state : step  43 market state fut market_state    2.0\n",
            "Name: 1996-02-22 00:00:00, dtype: float64  future value 0.6668922731915573\n",
            "loop 1 market state : step  44 market state fut market_state    2.0\n",
            "Name: 1996-02-23 00:00:00, dtype: float64  future value 0.6709950747695277\n",
            "loop 1 market state : step  45 market state fut market_state    3.0\n",
            "Name: 1996-02-26 00:00:00, dtype: float64  future value 0.6777011752280095\n",
            "loop 1 market state : step  46 market state fut market_state    3.0\n",
            "Name: 1996-02-27 00:00:00, dtype: float64  future value 0.682886926259716\n",
            "loop 1 market state : step  47 market state fut market_state    3.0\n",
            "Name: 1996-02-28 00:00:00, dtype: float64  future value 0.6789403480374171\n",
            "loop 1 market state : step  48 market state fut market_state    3.0\n",
            "Name: 1996-02-29 00:00:00, dtype: float64  future value 0.6806585507213954\n",
            "loop 1 market state : step  49 market state fut market_state    2.0\n",
            "Name: 1996-03-01 00:00:00, dtype: float64  future value 0.6596759363216315\n",
            "loop 1 market state : step  50 market state fut market_state    2.0\n",
            "Name: 1996-03-04 00:00:00, dtype: float64  future value 0.6664653601402781\n",
            "loop 1 market state : step  51 market state fut market_state    2.0\n",
            "Name: 1996-03-05 00:00:00, dtype: float64  future value 0.6634143014791708\n",
            "loop 1 market state : step  52 market state fut market_state    2.0\n",
            "Name: 1996-03-06 00:00:00, dtype: float64  future value 0.6649345873488445\n",
            "loop 1 market state : step  53 market state fut market_state    2.0\n",
            "Name: 1996-03-07 00:00:00, dtype: float64  future value 0.6673504563368114\n",
            "loop 1 market state : step  54 market state fut market_state    3.0\n",
            "Name: 1996-03-08 00:00:00, dtype: float64  future value 0.667933592743762\n",
            "loop 1 market state : step  55 market state fut market_state    3.0\n",
            "Name: 1996-03-11 00:00:00, dtype: float64  future value 0.6796172311691907\n",
            "loop 1 market state : step  56 market state fut market_state    3.0\n",
            "Name: 1996-03-12 00:00:00, dtype: float64  future value 0.6786175415185177\n",
            "loop 1 market state : step  57 market state fut market_state    3.0\n",
            "Name: 1996-03-13 00:00:00, dtype: float64  future value 0.6768368622036912\n",
            "loop 1 market state : step  58 market state fut market_state    2.0\n",
            "Name: 1996-03-14 00:00:00, dtype: float64  future value 0.6760142426380061\n",
            "loop 1 market state : step  59 market state fut market_state    2.0\n",
            "Name: 1996-03-15 00:00:00, dtype: float64  future value 0.6775033219708065\n",
            "loop 1 market state : step  60 market state fut market_state    1.0\n",
            "Name: 1996-03-18 00:00:00, dtype: float64  future value 0.6768993388345395\n",
            "loop 1 market state : step  61 market state fut market_state    2.0\n",
            "Name: 1996-03-19 00:00:00, dtype: float64  future value 0.6799503974956468\n",
            "loop 1 market state : step  62 market state fut market_state    1.0\n",
            "Name: 1996-03-20 00:00:00, dtype: float64  future value 0.6757226426559801\n",
            "loop 1 market state : step  63 market state fut market_state    1.0\n",
            "Name: 1996-03-21 00:00:00, dtype: float64  future value 0.675753912749955\n",
            "loop 1 market state : step  64 market state fut market_state    1.0\n",
            "Name: 1996-03-22 00:00:00, dtype: float64  future value 0.672171770948087\n",
            "loop 1 market state : step  65 market state fut market_state    3.0\n",
            "Name: 1996-03-25 00:00:00, dtype: float64  future value 0.6807418105244586\n",
            "loop 1 market state : step  66 market state fut market_state    3.0\n",
            "Name: 1996-03-26 00:00:00, dtype: float64  future value 0.6823350599467404\n",
            "loop 1 market state : step  67 market state fut market_state    3.0\n",
            "Name: 1996-03-27 00:00:00, dtype: float64  future value 0.6829806729845391\n",
            "loop 1 market state : step  68 market state fut market_state    2.0\n",
            "Name: 1996-03-28 00:00:00, dtype: float64  future value 0.6829598262552226\n",
            "loop 1 market state : step  69 market state fut market_state    1.0\n",
            "Name: 1996-03-29 00:00:00, dtype: float64  future value 0.6708596981431729\n",
            "loop 1 market state : step  70 market state fut market_state    1.0\n",
            "Name: 1996-04-01 00:00:00, dtype: float64  future value 0.6687250057725738\n",
            "loop 1 market state : step  71 market state fut market_state    1.0\n",
            "Name: 1996-04-02 00:00:00, dtype: float64  future value 0.6596759363216315\n",
            "loop 1 market state : step  72 market state fut market_state    0.0\n",
            "Name: 1996-04-03 00:00:00, dtype: float64  future value 0.6572600673336646\n",
            "loop 1 market state : step  73 market state fut market_state    1.0\n",
            "Name: 1996-04-04 00:00:00, dtype: float64  future value 0.6630185949647649\n",
            "loop 1 market state : step  74 market state fut market_state    1.0\n",
            "Name: 1996-04-08 00:00:00, dtype: float64  future value 0.6690373889268149\n",
            "loop 1 market state : step  75 market state fut market_state    2.0\n",
            "Name: 1996-04-09 00:00:00, dtype: float64  future value 0.6716511111719847\n",
            "loop 1 market state : step  76 market state fut market_state    2.0\n",
            "Name: 1996-04-10 00:00:00, dtype: float64  future value 0.6681210226363066\n",
            "loop 1 market state : step  77 market state fut market_state    2.0\n",
            "Name: 1996-04-11 00:00:00, dtype: float64  future value 0.6702036617407159\n",
            "loop 1 market state : step  78 market state fut market_state    2.0\n",
            "Name: 1996-04-12 00:00:00, dtype: float64  future value 0.6717240111674913\n",
            "loop 1 market state : step  79 market state fut market_state    3.0\n",
            "Name: 1996-04-15 00:00:00, dtype: float64  future value 0.6746605399315604\n",
            "loop 1 market state : step  80 market state fut market_state    3.0\n",
            "Name: 1996-04-16 00:00:00, dtype: float64  future value 0.6785030116214796\n",
            "loop 1 market state : step  81 market state fut market_state    3.0\n",
            "Name: 1996-04-17 00:00:00, dtype: float64  future value 0.6770347154608942\n",
            "loop 1 market state : step  82 market state fut market_state    3.0\n",
            "Name: 1996-04-18 00:00:00, dtype: float64  future value 0.679846290963267\n",
            "loop 1 market state : step  83 market state fut market_state    3.0\n",
            "Name: 1996-04-19 00:00:00, dtype: float64  future value 0.6804606974641924\n",
            "loop 1 market state : step  84 market state fut market_state    3.0\n",
            "Name: 1996-04-22 00:00:00, dtype: float64  future value 0.6811895703050543\n",
            "loop 1 market state : step  85 market state fut market_state    3.0\n",
            "Name: 1996-04-23 00:00:00, dtype: float64  future value 0.6811999936697127\n",
            "loop 1 market state : step  86 market state fut market_state    3.0\n",
            "Name: 1996-04-24 00:00:00, dtype: float64  future value 0.6816269702780935\n",
            "loop 1 market state : step  87 market state fut market_state    1.0\n",
            "Name: 1996-04-25 00:00:00, dtype: float64  future value 0.6699641785819813\n",
            "loop 1 market state : step  88 market state fut market_state    1.0\n",
            "Name: 1996-04-26 00:00:00, dtype: float64  future value 0.6681418693656233\n",
            "loop 1 market state : step  89 market state fut market_state    1.0\n",
            "Name: 1996-04-29 00:00:00, dtype: float64  future value 0.6672879797059632\n",
            "loop 1 market state : step  90 market state fut market_state    1.0\n",
            "Name: 1996-04-30 00:00:00, dtype: float64  future value 0.6646326275592618\n",
            "loop 1 market state : step  91 market state fut market_state    1.0\n",
            "Name: 1996-05-01 00:00:00, dtype: float64  future value 0.6714116280132502\n",
            "loop 1 market state : step  92 market state fut market_state    2.0\n",
            "Name: 1996-05-02 00:00:00, dtype: float64  future value 0.6721092943172388\n",
            "loop 1 market state : step  93 market state fut market_state    3.0\n",
            "Name: 1996-05-03 00:00:00, dtype: float64  future value 0.6790340947622402\n",
            "loop 1 market state : step  94 market state fut market_state    4.0\n",
            "Name: 1996-05-06 00:00:00, dtype: float64  future value 0.6888433071480193\n",
            "loop 1 market state : step  95 market state fut market_state    4.0\n",
            "Name: 1996-05-07 00:00:00, dtype: float64  future value 0.6931022685245594\n",
            "loop 1 market state : step  96 market state fut market_state    4.0\n",
            "Name: 1996-05-08 00:00:00, dtype: float64  future value 0.6929148386320148\n",
            "loop 1 market state : step  97 market state fut market_state    4.0\n",
            "Name: 1996-05-09 00:00:00, dtype: float64  future value 0.692321278860406\n",
            "loop 1 market state : step  98 market state fut market_state    4.0\n",
            "Name: 1996-05-10 00:00:00, dtype: float64  future value 0.6965490337000726\n",
            "loop 1 market state : step  99 market state fut market_state    4.0\n",
            "Name: 1996-05-13 00:00:00, dtype: float64  future value 0.7009642819893855\n",
            "loop 1 market state : step  100 market state fut market_state    4.0\n",
            "Name: 1996-05-14 00:00:00, dtype: float64  future value 0.7005581521103214\n",
            "loop 1 market state : step  101 market state fut market_state    4.0\n",
            "Name: 1996-05-15 00:00:00, dtype: float64  future value 0.706451992810675\n",
            "loop 1 market state : step  102 market state fut market_state    4.0\n",
            "Name: 1996-05-16 00:00:00, dtype: float64  future value 0.7039320172903282\n",
            "loop 1 market state : step  103 market state fut market_state    4.0\n",
            "Name: 1996-05-17 00:00:00, dtype: float64  future value 0.706545739535498\n",
            "loop 1 market state : step  104 market state fut market_state    2.0\n",
            "Name: 1996-05-20 00:00:00, dtype: float64  future value 0.7000062222402442\n",
            "loop 1 market state : step  105 market state fut market_state    2.0\n",
            "Name: 1996-05-21 00:00:00, dtype: float64  future value 0.6955285608771846\n",
            "loop 1 market state : step  106 market state fut market_state    2.0\n",
            "Name: 1996-05-22 00:00:00, dtype: float64  future value 0.6994543559272686\n",
            "loop 1 market state : step  107 market state fut market_state    2.0\n",
            "Name: 1996-05-23 00:00:00, dtype: float64  future value 0.6967677336865922\n",
            "loop 1 market state : step  108 market state fut market_state    2.0\n",
            "Name: 1996-05-24 00:00:00, dtype: float64  future value 0.6952682309891335\n",
            "loop 1 market state : step  109 market state fut market_state    3.0\n",
            "Name: 1996-05-28 00:00:00, dtype: float64  future value 0.7003498754884602\n",
            "loop 1 market state : step  110 market state fut market_state    3.0\n",
            "Name: 1996-05-29 00:00:00, dtype: float64  future value 0.7064728395399915\n",
            "loop 1 market state : step  111 market state fut market_state    3.0\n",
            "Name: 1996-05-30 00:00:00, dtype: float64  future value 0.7008393287276892\n",
            "loop 1 market state : step  112 market state fut market_state    3.0\n",
            "Name: 1996-05-31 00:00:00, dtype: float64  future value 0.7011308651526136\n",
            "loop 1 market state : step  113 market state fut market_state    3.0\n",
            "Name: 1996-06-03 00:00:00, dtype: float64  future value 0.6999333222447377\n",
            "loop 1 market state : step  114 market state fut market_state    2.0\n",
            "Name: 1996-06-04 00:00:00, dtype: float64  future value 0.6986941494353301\n",
            "loop 1 market state : step  115 market state fut market_state    2.0\n",
            "Name: 1996-06-05 00:00:00, dtype: float64  future value 0.6966844103264274\n",
            "loop 1 market state : step  116 market state fut market_state    2.0\n",
            "Name: 1996-06-06 00:00:00, dtype: float64  future value 0.6955181375125263\n",
            "loop 1 market state : step  117 market state fut market_state    2.0\n",
            "Name: 1996-06-07 00:00:00, dtype: float64  future value 0.6933625984126105\n",
            "loop 1 market state : step  118 market state fut market_state    2.0\n",
            "Name: 1996-06-10 00:00:00, dtype: float64  future value 0.6926440853793053\n",
            "loop 1 market state : step  119 market state fut market_state    2.0\n",
            "Name: 1996-06-11 00:00:00, dtype: float64  future value 0.6894160201903116\n",
            "loop 1 market state : step  120 market state fut market_state    2.0\n",
            "Name: 1996-06-12 00:00:00, dtype: float64  future value 0.6893119136579318\n",
            "loop 1 market state : step  121 market state fut market_state    2.0\n",
            "Name: 1996-06-13 00:00:00, dtype: float64  future value 0.6894576500918432\n",
            "loop 1 market state : step  122 market state fut market_state    3.0\n",
            "Name: 1996-06-14 00:00:00, dtype: float64  future value 0.6943935581572585\n",
            "loop 1 market state : step  123 market state fut market_state    3.0\n",
            "Name: 1996-06-17 00:00:00, dtype: float64  future value 0.6964865570692245\n",
            "loop 1 market state : step  124 market state fut market_state    3.0\n",
            "Name: 1996-06-18 00:00:00, dtype: float64  future value 0.6961012739194768\n",
            "loop 1 market state : step  125 market state fut market_state    2.0\n",
            "Name: 1996-06-19 00:00:00, dtype: float64  future value 0.6918423125429368\n",
            "loop 1 market state : step  126 market state fut market_state    3.0\n",
            "Name: 1996-06-20 00:00:00, dtype: float64  future value 0.6961741739149834\n",
            "loop 1 market state : step  127 market state fut market_state    3.0\n",
            "Name: 1996-06-21 00:00:00, dtype: float64  future value 0.6983401363795574\n",
            "loop 1 market state : step  128 market state fut market_state    3.0\n",
            "Name: 1996-06-24 00:00:00, dtype: float64  future value 0.7038070640286317\n",
            "loop 1 market state : step  129 market state fut market_state    3.0\n",
            "Name: 1996-06-25 00:00:00, dtype: float64  future value 0.7014432483068547\n",
            "loop 1 market state : step  130 market state fut market_state    2.0\n",
            "Name: 1996-06-26 00:00:00, dtype: float64  future value 0.7001832923252321\n",
            "loop 1 market state : step  131 market state fut market_state    0.0\n",
            "Name: 1996-06-27 00:00:00, dtype: float64  future value 0.6846051289436943\n",
            "loop 1 market state : step  132 market state fut market_state    0.0\n",
            "Name: 1996-06-28 00:00:00, dtype: float64  future value 0.679502637715051\n",
            "loop 1 market state : step  133 market state fut market_state    0.0\n",
            "Name: 1996-07-01 00:00:00, dtype: float64  future value 0.6818039768059798\n",
            "loop 1 market state : step  134 market state fut market_state    0.0\n",
            "Name: 1996-07-02 00:00:00, dtype: float64  future value 0.6831681028770838\n",
            "loop 1 market state : step  135 market state fut market_state    0.0\n",
            "Name: 1996-07-03 00:00:00, dtype: float64  future value 0.6723487774759733\n",
            "loop 1 market state : step  136 market state fut market_state    0.0\n",
            "Name: 1996-07-05 00:00:00, dtype: float64  future value 0.6728902839813923\n",
            "loop 1 market state : step  137 market state fut market_state    0.0\n",
            "Name: 1996-07-08 00:00:00, dtype: float64  future value 0.655823041267054\n",
            "loop 1 market state : step  138 market state fut market_state    0.0\n",
            "Name: 1996-07-09 00:00:00, dtype: float64  future value 0.6543339619342536\n",
            "loop 1 market state : step  139 market state fut market_state    0.0\n",
            "Name: 1996-07-10 00:00:00, dtype: float64  future value 0.6602694960932403\n",
            "loop 1 market state : step  140 market state fut market_state    0.0\n",
            "Name: 1996-07-11 00:00:00, dtype: float64  future value 0.6701516084745259\n",
            "loop 1 market state : step  141 market state fut market_state    0.0\n",
            "Name: 1996-07-12 00:00:00, dtype: float64  future value 0.6651220172413892\n",
            "loop 1 market state : step  142 market state fut market_state    0.0\n",
            "Name: 1996-07-15 00:00:00, dtype: float64  future value 0.6599571129389993\n",
            "loop 1 market state : step  143 market state fut market_state    0.0\n",
            "Name: 1996-07-16 00:00:00, dtype: float64  future value 0.6527719826059467\n",
            "loop 1 market state : step  144 market state fut market_state    0.0\n",
            "Name: 1996-07-17 00:00:00, dtype: float64  future value 0.6525429228118704\n",
            "loop 1 market state : step  145 market state fut market_state    1.0\n",
            "Name: 1996-07-18 00:00:00, dtype: float64  future value 0.6572496439690062\n",
            "loop 1 market state : step  146 market state fut market_state    1.0\n",
            "Name: 1996-07-19 00:00:00, dtype: float64  future value 0.6621751286697632\n",
            "loop 1 market state : step  147 market state fut market_state    1.0\n",
            "Name: 1996-07-22 00:00:00, dtype: float64  future value 0.6569788907162968\n",
            "loop 1 market state : step  148 market state fut market_state    2.0\n",
            "Name: 1996-07-23 00:00:00, dtype: float64  future value 0.6615086689026479\n",
            "loop 1 market state : step  149 market state fut market_state    2.0\n",
            "Name: 1996-07-24 00:00:00, dtype: float64  future value 0.6663924601447716\n",
            "loop 1 market state : step  150 market state fut market_state    2.0\n",
            "Name: 1996-07-25 00:00:00, dtype: float64  future value 0.6768785556623245\n",
            "loop 1 market state : step  151 market state fut market_state    3.0\n",
            "Name: 1996-07-26 00:00:00, dtype: float64  future value 0.6898637799709074\n",
            "loop 1 market state : step  152 market state fut market_state    2.0\n",
            "Name: 1996-07-29 00:00:00, dtype: float64  future value 0.6875103876137887\n",
            "loop 1 market state : step  153 market state fut market_state    2.0\n",
            "Name: 1996-07-30 00:00:00, dtype: float64  future value 0.6897492500738692\n",
            "loop 1 market state : step  154 market state fut market_state    2.0\n",
            "Name: 1996-07-31 00:00:00, dtype: float64  future value 0.6916027658271007\n",
            "loop 1 market state : step  155 market state fut market_state    2.0\n",
            "Name: 1996-08-01 00:00:00, dtype: float64  future value 0.6899679500603888\n",
            "loop 1 market state : step  156 market state fut market_state    1.0\n",
            "Name: 1996-08-02 00:00:00, dtype: float64  future value 0.6894576500918432\n",
            "loop 1 market state : step  157 market state fut market_state    2.0\n",
            "Name: 1996-08-05 00:00:00, dtype: float64  future value 0.6932793386095473\n",
            "loop 1 market state : step  158 market state fut market_state    1.0\n",
            "Name: 1996-08-06 00:00:00, dtype: float64  future value 0.6874791810769153\n",
            "loop 1 market state : step  159 market state fut market_state    1.0\n",
            "Name: 1996-08-07 00:00:00, dtype: float64  future value 0.6894055968256533\n",
            "loop 1 market state : step  160 market state fut market_state    1.0\n",
            "Name: 1996-08-08 00:00:00, dtype: float64  future value 0.6896451435414894\n",
            "loop 1 market state : step  161 market state fut market_state    4.0\n",
            "Name: 1996-08-09 00:00:00, dtype: float64  future value 0.6926962022025968\n",
            "loop 1 market state : step  162 market state fut market_state    4.0\n",
            "Name: 1996-08-12 00:00:00, dtype: float64  future value 0.694122804904549\n",
            "loop 1 market state : step  163 market state fut market_state    3.0\n",
            "Name: 1996-08-13 00:00:00, dtype: float64  future value 0.6931960152493825\n",
            "loop 1 market state : step  164 market state fut market_state    3.0\n",
            "Name: 1996-08-14 00:00:00, dtype: float64  future value 0.6925504022115837\n",
            "loop 1 market state : step  165 market state fut market_state    4.0\n",
            "Name: 1996-08-15 00:00:00, dtype: float64  future value 0.6983921896457473\n",
            "loop 1 market state : step  166 market state fut market_state    4.0\n",
            "Name: 1996-08-16 00:00:00, dtype: float64  future value 0.6945914114144613\n",
            "loop 1 market state : step  167 market state fut market_state    2.0\n",
            "Name: 1996-08-19 00:00:00, dtype: float64  future value 0.6913112294021762\n",
            "loop 1 market state : step  168 market state fut market_state    3.0\n",
            "Name: 1996-08-20 00:00:00, dtype: float64  future value 0.6939353750120043\n",
            "loop 1 market state : step  169 market state fut market_state    2.0\n",
            "Name: 1996-08-21 00:00:00, dtype: float64  future value 0.6922796489588743\n",
            "loop 1 market state : step  170 market state fut market_state    2.0\n",
            "Name: 1996-08-22 00:00:00, dtype: float64  future value 0.6845634990421627\n",
            "loop 1 market state : step  171 market state fut market_state    2.0\n",
            "Name: 1996-08-23 00:00:00, dtype: float64  future value 0.6789299246727588\n",
            "loop 1 market state : step  172 market state fut market_state    2.0\n",
            "Name: 1996-08-26 00:00:00, dtype: float64  future value 0.681772706712005\n",
            "loop 1 market state : step  173 market state fut market_state    2.0\n",
            "Name: 1996-08-27 00:00:00, dtype: float64  future value 0.6826994963671714\n",
            "loop 1 market state : step  174 market state fut market_state    2.0\n",
            "Name: 1996-08-28 00:00:00, dtype: float64  future value 0.6762745725260573\n",
            "loop 1 market state : step  175 market state fut market_state    2.0\n",
            "Name: 1996-08-29 00:00:00, dtype: float64  future value 0.682772396362678\n",
            "loop 1 market state : step  176 market state fut market_state    3.0\n",
            "Name: 1996-08-30 00:00:00, dtype: float64  future value 0.6911862761404798\n",
            "loop 1 market state : step  177 market state fut market_state    3.0\n",
            "Name: 1996-09-03 00:00:00, dtype: float64  future value 0.6912383294066696\n",
            "loop 1 market state : step  178 market state fut market_state    3.0\n",
            "Name: 1996-09-04 00:00:00, dtype: float64  future value 0.6948517413025126\n",
            "loop 1 market state : step  179 market state fut market_state    4.0\n",
            "Name: 1996-09-05 00:00:00, dtype: float64  future value 0.6988816428849763\n",
            "loop 1 market state : step  180 market state fut market_state    4.0\n",
            "Name: 1996-09-06 00:00:00, dtype: float64  future value 0.7086595851767806\n",
            "loop 1 market state : step  181 market state fut market_state    4.0\n",
            "Name: 1996-09-09 00:00:00, dtype: float64  future value 0.7122417269786485\n",
            "loop 1 market state : step  182 market state fut market_state    4.0\n",
            "Name: 1996-09-10 00:00:00, dtype: float64  future value 0.7111587775249123\n",
            "loop 1 market state : step  183 market state fut market_state    4.0\n",
            "Name: 1996-09-11 00:00:00, dtype: float64  future value 0.7096280047334786\n",
            "loop 1 market state : step  184 market state fut market_state    4.0\n",
            "Name: 1996-09-12 00:00:00, dtype: float64  future value 0.7112212541557605\n",
            "loop 1 market state : step  185 market state fut market_state    4.0\n",
            "Name: 1996-09-13 00:00:00, dtype: float64  future value 0.7154178024585539\n",
            "loop 1 market state : step  186 market state fut market_state    4.0\n",
            "Name: 1996-09-16 00:00:00, dtype: float64  future value 0.7148450258591601\n",
            "loop 1 market state : step  187 market state fut market_state    4.0\n",
            "Name: 1996-09-17 00:00:00, dtype: float64  future value 0.7139390829333102\n",
            "loop 1 market state : step  188 market state fut market_state    4.0\n",
            "Name: 1996-09-18 00:00:00, dtype: float64  future value 0.7141682062844881\n",
            "loop 1 market state : step  189 market state fut market_state    4.0\n",
            "Name: 1996-09-19 00:00:00, dtype: float64  future value 0.7141994128213613\n",
            "loop 1 market state : step  190 market state fut market_state    2.0\n",
            "Name: 1996-09-20 00:00:00, dtype: float64  future value 0.7145430660695774\n",
            "loop 1 market state : step  191 market state fut market_state    4.0\n",
            "Name: 1996-09-23 00:00:00, dtype: float64  future value 0.715730185612795\n",
            "loop 1 market state : step  192 market state fut market_state    4.0\n",
            "Name: 1996-09-24 00:00:00, dtype: float64  future value 0.7175524948291531\n",
            "loop 1 market state : step  193 market state fut market_state    4.0\n",
            "Name: 1996-09-25 00:00:00, dtype: float64  future value 0.7226861925946697\n",
            "loop 1 market state : step  194 market state fut market_state    4.0\n",
            "Name: 1996-09-26 00:00:00, dtype: float64  future value 0.7214053898837306\n",
            "loop 1 market state : step  195 market state fut market_state    4.0\n",
            "Name: 1996-09-27 00:00:00, dtype: float64  future value 0.7304440359700145\n",
            "loop 1 market state : step  196 market state fut market_state    4.0\n",
            "Name: 1996-09-30 00:00:00, dtype: float64  future value 0.7324017218127273\n",
            "loop 1 market state : step  197 market state fut market_state    4.0\n",
            "Name: 1996-10-01 00:00:00, dtype: float64  future value 0.7295901463103546\n",
            "loop 1 market state : step  198 market state fut market_state    4.0\n",
            "Name: 1996-10-02 00:00:00, dtype: float64  future value 0.7255289746339159\n",
            "loop 1 market state : step  199 market state fut market_state    4.0\n",
            "Name: 1996-10-03 00:00:00, dtype: float64  future value 0.7233109589031519\n",
            "loop 1 market state : step  200 market state fut market_state    2.0\n",
            "Name: 1996-10-04 00:00:00, dtype: float64  future value 0.7296109294825696\n",
            "loop 1 market state : step  201 market state fut market_state    4.0\n",
            "Name: 1996-10-07 00:00:00, dtype: float64  future value 0.732609934877487\n",
            "loop 1 market state : step  202 market state fut market_state    3.0\n",
            "Name: 1996-10-08 00:00:00, dtype: float64  future value 0.7315998854192572\n",
            "loop 1 market state : step  203 market state fut market_state    4.0\n",
            "Name: 1996-10-09 00:00:00, dtype: float64  future value 0.7335158778033369\n",
            "loop 1 market state : step  204 market state fut market_state    4.0\n",
            "Name: 1996-10-10 00:00:00, dtype: float64  future value 0.7362025000440133\n",
            "loop 1 market state : step  205 market state fut market_state    4.0\n",
            "Name: 1996-10-11 00:00:00, dtype: float64  future value 0.7401907717249454\n",
            "loop 1 market state : step  206 market state fut market_state    4.0\n",
            "Name: 1996-10-14 00:00:00, dtype: float64  future value 0.7391806587096141\n",
            "loop 1 market state : step  207 market state fut market_state    4.0\n",
            "Name: 1996-10-15 00:00:00, dtype: float64  future value 0.7357651636280758\n",
            "loop 1 market state : step  208 market state fut market_state    4.0\n",
            "Name: 1996-10-16 00:00:00, dtype: float64  future value 0.7364941000260393\n",
            "loop 1 market state : step  209 market state fut market_state    2.0\n",
            "Name: 1996-10-17 00:00:00, dtype: float64  future value 0.7313082854372313\n",
            "loop 1 market state : step  210 market state fut market_state    2.0\n",
            "Name: 1996-10-18 00:00:00, dtype: float64  future value 0.729881682735279\n",
            "loop 1 market state : step  211 market state fut market_state    2.0\n",
            "Name: 1996-10-21 00:00:00, dtype: float64  future value 0.7260704811393348\n",
            "loop 1 market state : step  212 market state fut market_state    2.0\n",
            "Name: 1996-10-22 00:00:00, dtype: float64  future value 0.7304856658715462\n",
            "loop 1 market state : step  213 market state fut market_state    2.0\n",
            "Name: 1996-10-23 00:00:00, dtype: float64  future value 0.729860899563064\n",
            "loop 1 market state : step  214 market state fut market_state    3.0\n",
            "Name: 1996-10-24 00:00:00, dtype: float64  future value 0.7344114609216301\n",
            "loop 1 market state : step  215 market state fut market_state    3.0\n",
            "Name: 1996-10-25 00:00:00, dtype: float64  future value 0.7328494815933232\n",
            "loop 1 market state : step  216 market state fut market_state    3.0\n",
            "Name: 1996-10-28 00:00:00, dtype: float64  future value 0.7359317467913038\n",
            "loop 1 market state : step  217 market state fut market_state    4.0\n",
            "Name: 1996-10-29 00:00:00, dtype: float64  future value 0.743647960265117\n",
            "loop 1 market state : step  218 market state fut market_state    4.0\n",
            "Name: 1996-10-30 00:00:00, dtype: float64  future value 0.7545297622970757\n",
            "loop 1 market state : step  219 market state fut market_state    4.0\n",
            "Name: 1996-10-31 00:00:00, dtype: float64  future value 0.7577161975845378\n",
            "loop 1 market state : step  220 market state fut market_state    4.0\n",
            "Name: 1996-11-01 00:00:00, dtype: float64  future value 0.761017162769038\n",
            "loop 1 market state : step  221 market state fut market_state    4.0\n",
            "Name: 1996-11-04 00:00:00, dtype: float64  future value 0.7621105355874326\n",
            "loop 1 market state : step  222 market state fut market_state    4.0\n",
            "Name: 1996-11-05 00:00:00, dtype: float64  future value 0.7597050899641239\n",
            "loop 1 market state : step  223 market state fut market_state    4.0\n",
            "Name: 1996-11-06 00:00:00, dtype: float64  future value 0.7613399692879373\n",
            "loop 1 market state : step  224 market state fut market_state    4.0\n",
            "Name: 1996-11-07 00:00:00, dtype: float64  future value 0.7662862371609094\n",
            "loop 1 market state : step  225 market state fut market_state    4.0\n",
            "Name: 1996-11-08 00:00:00, dtype: float64  future value 0.7680981230126092\n",
            "loop 1 market state : step  226 market state fut market_state    4.0\n",
            "Name: 1996-11-11 00:00:00, dtype: float64  future value 0.767473356704127\n",
            "loop 1 market state : step  227 market state fut market_state    4.0\n",
            "Name: 1996-11-12 00:00:00, dtype: float64  future value 0.7728256908990616\n",
            "loop 1 market state : step  228 market state fut market_state    4.0\n",
            "Name: 1996-11-13 00:00:00, dtype: float64  future value 0.7746896935740529\n",
            "loop 1 market state : step  229 market state fut market_state    4.0\n",
            "Name: 1996-11-14 00:00:00, dtype: float64  future value 0.773440097399987\n",
            "loop 1 market state : step  230 market state fut market_state    4.0\n",
            "Name: 1996-11-15 00:00:00, dtype: float64  future value 0.7796671679838982\n",
            "loop 1 market state : step  231 market state fut market_state    4.0\n",
            "Name: 1996-11-18 00:00:00, dtype: float64  future value 0.7883101711128778\n",
            "loop 1 market state : step  232 market state fut market_state    4.0\n",
            "Name: 1996-11-19 00:00:00, dtype: float64  future value 0.7871959515651668\n",
            "loop 1 market state : step  233 market state fut market_state    4.0\n",
            "Name: 1996-11-20 00:00:00, dtype: float64  future value 0.7861962619144937\n",
            "loop 1 market state : step  234 market state fut market_state    4.0\n",
            "Name: 1996-11-21 00:00:00, dtype: float64  future value 0.7882997477482195\n",
            "loop 1 market state : step  235 market state fut market_state    4.0\n",
            "Name: 1996-11-22 00:00:00, dtype: float64  future value 0.7878207178736488\n",
            "loop 1 market state : step  236 market state fut market_state    2.0\n",
            "Name: 1996-11-25 00:00:00, dtype: float64  future value 0.7791986250310874\n",
            "loop 1 market state : step  237 market state fut market_state    2.0\n",
            "Name: 1996-11-26 00:00:00, dtype: float64  future value 0.7758871729248272\n",
            "loop 1 market state : step  238 market state fut market_state    2.0\n",
            "Name: 1996-11-27 00:00:00, dtype: float64  future value 0.7751374533546487\n",
            "loop 1 market state : step  239 market state fut market_state    2.0\n",
            "Name: 1996-11-29 00:00:00, dtype: float64  future value 0.7701599153877018\n",
            "loop 1 market state : step  240 market state fut market_state    2.0\n",
            "Name: 1996-12-02 00:00:00, dtype: float64  future value 0.7807397576300777\n",
            "loop 1 market state : step  241 market state fut market_state    2.0\n",
            "Name: 1996-12-03 00:00:00, dtype: float64  future value 0.7784279951744906\n",
            "loop 1 market state : step  242 market state fut market_state    2.0\n",
            "Name: 1996-12-04 00:00:00, dtype: float64  future value 0.7713366115662612\n",
            "loop 1 market state : step  243 market state fut market_state    2.0\n",
            "Name: 1996-12-05 00:00:00, dtype: float64  future value 0.7594343367114145\n",
            "loop 1 market state : step  244 market state fut market_state    2.0\n",
            "Name: 1996-12-06 00:00:00, dtype: float64  future value 0.7587470937720842\n",
            "loop 1 market state : step  245 market state fut market_state    2.0\n",
            "Name: 1996-12-09 00:00:00, dtype: float64  future value 0.7507705504102198\n",
            "loop 1 market state : step  246 market state fut market_state    2.0\n",
            "Name: 1996-12-10 00:00:00, dtype: float64  future value 0.7560396248020911\n",
            "loop 1 market state : step  247 market state fut market_state    2.0\n",
            "Name: 1996-12-11 00:00:00, dtype: float64  future value 0.7617668823392166\n",
            "loop 1 market state : step  248 market state fut market_state    3.0\n",
            "Name: 1996-12-12 00:00:00, dtype: float64  future value 0.7765744794212592\n",
            "loop 1 market state : step  249 market state fut market_state    3.0\n",
            "Name: 1996-12-13 00:00:00, dtype: float64  future value 0.7798129679749112\n",
            "loop 1 market state : step  250 market state fut market_state    3.0\n",
            "Name: 1996-12-16 00:00:00, dtype: float64  future value 0.7777823821366919\n",
            "loop 1 market state : step  251 market state fut market_state    3.0\n",
            "Name: 1996-12-17 00:00:00, dtype: float64  future value 0.7820622537996501\n",
            "loop 1 market state : step  252 market state fut market_state    3.0\n",
            "Name: 1996-12-18 00:00:00, dtype: float64  future value 0.7870501515741537\n",
            "loop 1 market state : step  253 market state fut market_state    3.0\n",
            "Name: 1996-12-19 00:00:00, dtype: float64  future value 0.7880602010323834\n",
            "loop 1 market state : step  254 market state fut market_state    3.0\n",
            "Name: 1996-12-20 00:00:00, dtype: float64  future value 0.7849987190066178\n",
            "loop 1 market state : step  255 market state fut market_state    2.0\n",
            "Name: 1996-12-23 00:00:00, dtype: float64  future value 0.7713470349309195\n",
            "loop 1 market state : step  256 market state fut market_state    1.0\n",
            "Name: 1996-12-24 00:00:00, dtype: float64  future value 0.7674629333394687\n",
            "loop 1 market state : step  257 market state fut market_state    2.0\n",
            "Name: 1996-12-26 00:00:00, dtype: float64  future value 0.7789382951430363\n",
            "loop 1 market state : step  258 market state fut market_state    2.0\n",
            "Name: 1996-12-27 00:00:00, dtype: float64  future value 0.7785425886286303\n",
            "loop 1 market state : step  259 market state fut market_state    2.0\n",
            "Name: 1996-12-30 00:00:00, dtype: float64  future value 0.784353105968819\n",
            "loop 1 market state : step  260 market state fut market_state    2.0\n",
            "Name: 1996-12-31 00:00:00, dtype: float64  future value 0.7793339381003406\n",
            "loop 1 market state : step  261 market state fut market_state    2.0\n",
            "Name: 1997-01-02 00:00:00, dtype: float64  future value 0.7860400385588224\n",
            "loop 1 market state : step  262 market state fut market_state    4.0\n",
            "Name: 1997-01-03 00:00:00, dtype: float64  future value 0.7908821998994146\n",
            "loop 1 market state : step  263 market state fut market_state    4.0\n",
            "Name: 1997-01-06 00:00:00, dtype: float64  future value 0.7908926232640728\n",
            "loop 1 market state : step  264 market state fut market_state    4.0\n",
            "Name: 1997-01-07 00:00:00, dtype: float64  future value 0.8006289356543455\n",
            "loop 1 market state : step  265 market state fut market_state    4.0\n",
            "Name: 1997-01-08 00:00:00, dtype: float64  future value 0.7989003731628105\n",
            "loop 1 market state : step  266 market state fut market_state    4.0\n",
            "Name: 1997-01-09 00:00:00, dtype: float64  future value 0.8015557253095119\n",
            "loop 1 market state : step  267 market state fut market_state    4.0\n",
            "Name: 1997-01-10 00:00:00, dtype: float64  future value 0.8082409790386772\n",
            "loop 1 market state : step  268 market state fut market_state    4.0\n",
            "Name: 1997-01-13 00:00:00, dtype: float64  future value 0.8087929089087544\n",
            "loop 1 market state : step  269 market state fut market_state    4.0\n",
            "Name: 1997-01-14 00:00:00, dtype: float64  future value 0.8150616093941973\n",
            "loop 1 market state : step  270 market state fut market_state    4.0\n",
            "Name: 1997-01-15 00:00:00, dtype: float64  future value 0.8187166511915717\n",
            "loop 1 market state : step  271 market state fut market_state    4.0\n",
            "Name: 1997-01-16 00:00:00, dtype: float64  future value 0.809688428469946\n",
            "loop 1 market state : step  272 market state fut market_state    2.0\n",
            "Name: 1997-01-17 00:00:00, dtype: float64  future value 0.802357561702982\n",
            "loop 1 market state : step  273 market state fut market_state    2.0\n",
            "Name: 1997-01-20 00:00:00, dtype: float64  future value 0.7966303041658566\n",
            "loop 1 market state : step  274 market state fut market_state    2.0\n",
            "Name: 1997-01-21 00:00:00, dtype: float64  future value 0.7966303041658566\n",
            "loop 1 market state : step  275 market state fut market_state    2.0\n",
            "Name: 1997-01-22 00:00:00, dtype: float64  future value 0.8044193540780746\n",
            "loop 1 market state : step  276 market state fut market_state    3.0\n",
            "Name: 1997-01-23 00:00:00, dtype: float64  future value 0.8165715354563142\n",
            "loop 1 market state : step  277 market state fut market_state    3.0\n",
            "Name: 1997-01-24 00:00:00, dtype: float64  future value 0.8186437511960653\n",
            "loop 1 market state : step  278 market state fut market_state    4.0\n",
            "Name: 1997-01-27 00:00:00, dtype: float64  future value 0.819237310967674\n",
            "loop 1 market state : step  279 market state fut market_state    4.0\n",
            "Name: 1997-01-28 00:00:00, dtype: float64  future value 0.8218718799421605\n",
            "loop 1 market state : step  280 market state fut market_state    3.0\n",
            "Name: 1997-01-29 00:00:00, dtype: float64  future value 0.8104382115972262\n",
            "loop 1 market state : step  281 market state fut market_state    2.0\n",
            "Name: 1997-01-30 00:00:00, dtype: float64  future value 0.8123854740752807\n",
            "loop 1 market state : step  282 market state fut market_state    4.0\n",
            "Name: 1997-01-31 00:00:00, dtype: float64  future value 0.8221842630964016\n",
            "loop 1 market state : step  283 market state fut market_state    2.0\n",
            "Name: 1997-02-03 00:00:00, dtype: float64  future value 0.8178836082612283\n",
            "loop 1 market state : step  284 market state fut market_state    4.0\n",
            "Name: 1997-02-04 00:00:00, dtype: float64  future value 0.8222155331903764\n",
            "loop 1 market state : step  285 market state fut market_state    4.0\n",
            "Name: 1997-02-05 00:00:00, dtype: float64  future value 0.8359401172615812\n",
            "loop 1 market state : step  286 market state fut market_state    4.0\n",
            "Name: 1997-02-06 00:00:00, dtype: float64  future value 0.8453640464976129\n",
            "loop 1 market state : step  287 market state fut market_state    4.0\n",
            "Name: 1997-02-07 00:00:00, dtype: float64  future value 0.8418860112281247\n",
            "loop 1 market state : step  288 market state fut market_state    4.0\n",
            "Name: 1997-02-10 00:00:00, dtype: float64  future value 0.8500187143885588\n",
            "loop 1 market state : step  289 market state fut market_state    4.0\n",
            "Name: 1997-02-11 00:00:00, dtype: float64  future value 0.8460617128016015\n",
            "loop 1 market state : step  290 market state fut market_state    4.0\n",
            "Name: 1997-02-12 00:00:00, dtype: float64  future value 0.8359713237984546\n",
            "loop 1 market state : step  291 market state fut market_state    2.0\n",
            "Name: 1997-02-13 00:00:00, dtype: float64  future value 0.8348987977093766\n",
            "loop 1 market state : step  292 market state fut market_state    3.0\n",
            "Name: 1997-02-14 00:00:00, dtype: float64  future value 0.8437604372677743\n",
            "loop 1 market state : step  293 market state fut market_state    2.0\n",
            "Name: 1997-02-18 00:00:00, dtype: float64  future value 0.8455827464841323\n",
            "loop 1 market state : step  294 market state fut market_state    2.0\n",
            "Name: 1997-02-19 00:00:00, dtype: float64  future value 0.838970329193372\n",
            "loop 1 market state : step  295 market state fut market_state    2.0\n",
            "Name: 1997-02-20 00:00:00, dtype: float64  future value 0.8279219439981853\n",
            "loop 1 market state : step  296 market state fut market_state    2.0\n",
            "Name: 1997-02-21 00:00:00, dtype: float64  future value 0.8234963359013157\n",
            "loop 1 market state : step  297 market state fut market_state    2.0\n",
            "Name: 1997-02-24 00:00:00, dtype: float64  future value 0.8281718505215782\n",
            "loop 1 market state : step  298 market state fut market_state    2.0\n",
            "Name: 1997-02-25 00:00:00, dtype: float64  future value 0.8236317125276704\n",
            "loop 1 market state : step  299 market state fut market_state    2.0\n",
            "Name: 1997-02-26 00:00:00, dtype: float64  future value 0.8351278575034529\n",
            "loop 1 market state : step  300 market state fut market_state    3.0\n",
            "Name: 1997-02-27 00:00:00, dtype: float64  future value 0.8315561390662433\n",
            "loop 1 market state : step  301 market state fut market_state    3.0\n",
            "Name: 1997-02-28 00:00:00, dtype: float64  future value 0.8382309694307502\n",
            "loop 1 market state : step  302 market state fut market_state    3.0\n",
            "Name: 1997-03-03 00:00:00, dtype: float64  future value 0.8472696790741358\n",
            "loop 1 market state : step  303 market state fut market_state    3.0\n",
            "Name: 1997-03-04 00:00:00, dtype: float64  future value 0.8448642334508271\n",
            "loop 1 market state : step  304 market state fut market_state    3.0\n",
            "Name: 1997-03-05 00:00:00, dtype: float64  future value 0.8374916732252299\n",
            "loop 1 market state : step  305 market state fut market_state    2.0\n",
            "Name: 1997-03-06 00:00:00, dtype: float64  future value 0.8221842630964016\n",
            "loop 1 market state : step  306 market state fut market_state    2.0\n",
            "Name: 1997-03-07 00:00:00, dtype: float64  future value 0.8259434114261559\n",
            "loop 1 market state : step  307 market state fut market_state    2.0\n",
            "Name: 1997-03-10 00:00:00, dtype: float64  future value 0.8285884037653006\n",
            "loop 1 market state : step  308 market state fut market_state    2.0\n",
            "Name: 1997-03-11 00:00:00, dtype: float64  future value 0.8222883696287814\n",
            "loop 1 market state : step  309 market state fut market_state    1.0\n",
            "Name: 1997-03-12 00:00:00, dtype: float64  future value 0.8182376848741026\n",
            "loop 1 market state : step  310 market state fut market_state    2.0\n",
            "Name: 1997-03-13 00:00:00, dtype: float64  future value 0.8149887729557923\n",
            "loop 1 market state : step  311 market state fut market_state    2.0\n",
            "Name: 1997-03-14 00:00:00, dtype: float64  future value 0.8164986354608077\n",
            "loop 1 market state : step  312 market state fut market_state    2.0\n",
            "Name: 1997-03-17 00:00:00, dtype: float64  future value 0.8235692358968222\n",
            "loop 1 market state : step  313 market state fut market_state    2.0\n",
            "Name: 1997-03-18 00:00:00, dtype: float64  future value 0.8216740266849576\n",
            "loop 1 market state : step  314 market state fut market_state    3.0\n",
            "Name: 1997-03-19 00:00:00, dtype: float64  future value 0.823163106017758\n",
            "loop 1 market state : step  315 market state fut market_state    0.0\n",
            "Name: 1997-03-20 00:00:00, dtype: float64  future value 0.8058563801446852\n",
            "loop 1 market state : step  316 market state fut market_state    0.0\n",
            "Name: 1997-03-21 00:00:00, dtype: float64  future value 0.7884038542805993\n",
            "loop 1 market state : step  317 market state fut market_state    0.0\n",
            "Name: 1997-03-24 00:00:00, dtype: float64  future value 0.7910279998904276\n",
            "loop 1 market state : step  318 market state fut market_state    0.0\n",
            "Name: 1997-03-25 00:00:00, dtype: float64  future value 0.7811041940505087\n",
            "loop 1 market state : step  319 market state fut market_state    0.0\n",
            "Name: 1997-03-26 00:00:00, dtype: float64  future value 0.7813228940370283\n",
            "loop 1 market state : step  320 market state fut market_state    0.0\n",
            "Name: 1997-03-27 00:00:00, dtype: float64  future value 0.7892161140387277\n",
            "loop 1 market state : step  321 market state fut market_state    0.0\n",
            "Name: 1997-03-31 00:00:00, dtype: float64  future value 0.7936208754062808\n",
            "loop 1 market state : step  322 market state fut market_state    2.0\n",
            "Name: 1997-04-01 00:00:00, dtype: float64  future value 0.797775730250441\n",
            "loop 1 market state : step  323 market state fut market_state    2.0\n",
            "Name: 1997-04-02 00:00:00, dtype: float64  future value 0.792027625983999\n",
            "loop 1 market state : step  324 market state fut market_state    2.0\n",
            "Name: 1997-04-03 00:00:00, dtype: float64  future value 0.7896742971839819\n",
            "loop 1 market state : step  325 market state fut market_state    0.0\n",
            "Name: 1997-04-04 00:00:00, dtype: float64  future value 0.768129393106584\n",
            "loop 1 market state : step  326 market state fut market_state    0.0\n",
            "Name: 1997-04-07 00:00:00, dtype: float64  future value 0.7744605702228751\n",
            "loop 1 market state : step  327 market state fut market_state    1.0\n",
            "Name: 1997-04-08 00:00:00, dtype: float64  future value 0.7859046619324677\n",
            "loop 1 market state : step  328 market state fut market_state    2.0\n",
            "Name: 1997-04-09 00:00:00, dtype: float64  future value 0.7950787482022079\n",
            "loop 1 market state : step  329 market state fut market_state    2.0\n",
            "Name: 1997-04-10 00:00:00, dtype: float64  future value 0.7932460156211916\n",
            "loop 1 market state : step  330 market state fut market_state    2.0\n",
            "Name: 1997-04-11 00:00:00, dtype: float64  future value 0.7980048536016189\n",
            "loop 1 market state : step  331 market state fut market_state    2.0\n",
            "Name: 1997-04-14 00:00:00, dtype: float64  future value 0.7917881428252644\n",
            "loop 1 market state : step  332 market state fut market_state    2.0\n",
            "Name: 1997-04-15 00:00:00, dtype: float64  future value 0.806616523079522\n",
            "loop 1 market state : step  333 market state fut market_state    2.0\n",
            "Name: 1997-04-16 00:00:00, dtype: float64  future value 0.8056064736212923\n",
            "loop 1 market state : step  334 market state fut market_state    2.0\n",
            "Name: 1997-04-17 00:00:00, dtype: float64  future value 0.8030448046423124\n",
            "loop 1 market state : step  335 market state fut market_state    1.0\n",
            "Name: 1997-04-18 00:00:00, dtype: float64  future value 0.7969947405862876\n",
            "loop 1 market state : step  336 market state fut market_state    2.0\n",
            "Name: 1997-04-21 00:00:00, dtype: float64  future value 0.8048983839526453\n",
            "loop 1 market state : step  337 market state fut market_state    2.0\n",
            "Name: 1997-04-22 00:00:00, dtype: float64  future value 0.8268597777166641\n",
            "loop 1 market state : step  338 market state fut market_state    4.0\n",
            "Name: 1997-04-23 00:00:00, dtype: float64  future value 0.8344510379287808\n",
            "loop 1 market state : step  339 market state fut market_state    4.0\n",
            "Name: 1997-04-24 00:00:00, dtype: float64  future value 0.8315249325293699\n",
            "loop 1 market state : step  340 market state fut market_state    4.0\n",
            "Name: 1997-04-25 00:00:00, dtype: float64  future value 0.8465615258483872\n",
            "loop 1 market state : step  341 market state fut market_state    4.0\n",
            "Name: 1997-04-28 00:00:00, dtype: float64  future value 0.8645971881194235\n",
            "loop 1 market state : step  342 market state fut market_state    4.0\n",
            "Name: 1997-04-29 00:00:00, dtype: float64  future value 0.8619626827020387\n",
            "loop 1 market state : step  343 market state fut market_state    4.0\n",
            "Name: 1997-04-30 00:00:00, dtype: float64  future value 0.8493210480845701\n",
            "loop 1 market state : step  344 market state fut market_state    4.0\n",
            "Name: 1997-05-01 00:00:00, dtype: float64  future value 0.8541527860605039\n",
            "loop 1 market state : step  345 market state fut market_state    4.0\n",
            "Name: 1997-05-02 00:00:00, dtype: float64  future value 0.8588595707747414\n",
            "loop 1 market state : step  346 market state fut market_state    4.0\n",
            "Name: 1997-05-05 00:00:00, dtype: float64  future value 0.8722717081346035\n",
            "loop 1 market state : step  347 market state fut market_state    4.0\n",
            "Name: 1997-05-06 00:00:00, dtype: float64  future value 0.8675545636128094\n",
            "loop 1 market state : step  348 market state fut market_state    4.0\n",
            "Name: 1997-05-07 00:00:00, dtype: float64  future value 0.8705847755446001\n",
            "loop 1 market state : step  349 market state fut market_state    4.0\n",
            "Name: 1997-05-08 00:00:00, dtype: float64  future value 0.8766661096945999\n",
            "loop 1 market state : step  350 market state fut market_state    3.0\n",
            "Name: 1997-05-09 00:00:00, dtype: float64  future value 0.8640348984417896\n",
            "loop 1 market state : step  351 market state fut market_state    2.0\n",
            "Name: 1997-05-12 00:00:00, dtype: float64  future value 0.8677003636038224\n",
            "loop 1 market state : step  352 market state fut market_state    4.0\n",
            "Name: 1997-05-13 00:00:00, dtype: float64  future value 0.876436986343422\n",
            "loop 1 market state : step  353 market state fut market_state    4.0\n",
            "Name: 1997-05-14 00:00:00, dtype: float64  future value 0.8740315407201134\n",
            "loop 1 market state : step  354 market state fut market_state    2.0\n",
            "Name: 1997-05-15 00:00:00, dtype: float64  future value 0.8701890690301942\n",
            "loop 1 market state : step  355 market state fut market_state    4.0\n",
            "Name: 1997-05-16 00:00:00, dtype: float64  future value 0.8820289308112943\n",
            "loop 1 market state : step  356 market state fut market_state    4.0\n",
            "Name: 1997-05-19 00:00:00, dtype: float64  future value 0.8848196595843505\n",
            "loop 1 market state : step  357 market state fut market_state    4.0\n",
            "Name: 1997-05-20 00:00:00, dtype: float64  future value 0.882216360703839\n",
            "loop 1 market state : step  358 market state fut market_state    4.0\n",
            "Name: 1997-05-21 00:00:00, dtype: float64  future value 0.8789570254208704\n",
            "loop 1 market state : step  359 market state fut market_state    4.0\n",
            "Name: 1997-05-22 00:00:00, dtype: float64  future value 0.8833305802515501\n",
            "loop 1 market state : step  360 market state fut market_state    2.0\n",
            "Name: 1997-05-23 00:00:00, dtype: float64  future value 0.8813312009502041\n",
            "loop 1 market state : step  361 market state fut market_state    2.0\n",
            "Name: 1997-05-27 00:00:00, dtype: float64  future value 0.8804148346596959\n",
            "loop 1 market state : step  362 market state fut market_state    2.0\n",
            "Name: 1997-05-28 00:00:00, dtype: float64  future value 0.8748229537489252\n",
            "loop 1 market state : step  363 market state fut market_state    2.0\n",
            "Name: 1997-05-29 00:00:00, dtype: float64  future value 0.8782801422890967\n",
            "loop 1 market state : step  364 market state fut market_state    4.0\n",
            "Name: 1997-05-30 00:00:00, dtype: float64  future value 0.8934625991562286\n",
            "loop 1 market state : step  365 market state fut market_state    4.0\n",
            "Name: 1997-06-02 00:00:00, dtype: float64  future value 0.8985650268277704\n",
            "loop 1 market state : step  366 market state fut market_state    4.0\n",
            "Name: 1997-06-03 00:00:00, dtype: float64  future value 0.9010225892743705\n",
            "loop 1 market state : step  367 market state fut market_state    4.0\n",
            "Name: 1997-06-04 00:00:00, dtype: float64  future value 0.9055002506374301\n",
            "loop 1 market state : step  368 market state fut market_state    4.0\n",
            "Name: 1997-06-05 00:00:00, dtype: float64  future value 0.9199641944712568\n",
            "loop 1 market state : step  369 market state fut market_state    4.0\n",
            "Name: 1997-06-06 00:00:00, dtype: float64  future value 0.9301795367361001\n",
            "loop 1 market state : step  370 market state fut market_state    4.0\n",
            "Name: 1997-06-09 00:00:00, dtype: float64  future value 0.9308355731385571\n",
            "loop 1 market state : step  371 market state fut market_state    4.0\n",
            "Name: 1997-06-10 00:00:00, dtype: float64  future value 0.9313770160868744\n",
            "loop 1 market state : step  372 market state fut market_state    4.0\n",
            "Name: 1997-06-11 00:00:00, dtype: float64  future value 0.925795558540762\n",
            "loop 1 market state : step  373 market state fut market_state    4.0\n",
            "Name: 1997-06-12 00:00:00, dtype: float64  future value 0.9350945345150972\n",
            "loop 1 market state : step  374 market state fut market_state    4.0\n",
            "Name: 1997-06-13 00:00:00, dtype: float64  future value 0.935833894277719\n",
            "loop 1 market state : step  375 market state fut market_state    2.0\n",
            "Name: 1997-06-16 00:00:00, dtype: float64  future value 0.9149241798734616\n",
            "loop 1 market state : step  376 market state fut market_state    4.0\n",
            "Name: 1997-06-17 00:00:00, dtype: float64  future value 0.9333763953882205\n",
            "loop 1 market state : step  377 market state fut market_state    2.0\n",
            "Name: 1997-06-18 00:00:00, dtype: float64  future value 0.9257226585452555\n",
            "loop 1 market state : step  378 market state fut market_state    2.0\n",
            "Name: 1997-06-19 00:00:00, dtype: float64  future value 0.9201932542653329\n",
            "loop 1 market state : step  379 market state fut market_state    2.0\n",
            "Name: 1997-06-20 00:00:00, dtype: float64  future value 0.9239628259597455\n",
            "loop 1 market state : step  380 market state fut market_state    3.0\n",
            "Name: 1997-06-23 00:00:00, dtype: float64  future value 0.9217136036921083\n",
            "loop 1 market state : step  381 market state fut market_state    2.0\n",
            "Name: 1997-06-24 00:00:00, dtype: float64  future value 0.9278469911082979\n",
            "loop 1 market state : step  382 market state fut market_state    4.0\n",
            "Name: 1997-06-25 00:00:00, dtype: float64  future value 0.9413841452869581\n",
            "loop 1 market state : step  383 market state fut market_state    4.0\n",
            "Name: 1997-06-26 00:00:00, dtype: float64  future value 0.9548067060114785\n",
            "loop 1 market state : step  384 market state fut market_state    4.0\n",
            "Name: 1997-06-27 00:00:00, dtype: float64  future value 0.9498917082324815\n",
            "loop 1 market state : step  385 market state fut market_state    4.0\n",
            "Name: 1997-06-30 00:00:00, dtype: float64  future value 0.9567123385880014\n",
            "loop 1 market state : step  386 market state fut market_state    4.0\n",
            "Name: 1997-07-01 00:00:00, dtype: float64  future value 0.945039123527231\n",
            "loop 1 market state : step  387 market state fut market_state    4.0\n",
            "Name: 1997-07-02 00:00:00, dtype: float64  future value 0.9515370109209532\n",
            "loop 1 market state : step  388 market state fut market_state    2.0\n",
            "Name: 1997-07-03 00:00:00, dtype: float64  future value 0.9545567994880857\n",
            "loop 1 market state : step  389 market state fut market_state    4.0\n",
            "Name: 1997-07-07 00:00:00, dtype: float64  future value 0.9563270554382539\n",
            "loop 1 market state : step  390 market state fut market_state    4.0\n",
            "Name: 1997-07-08 00:00:00, dtype: float64  future value 0.9640119988180922\n",
            "loop 1 market state : step  391 market state fut market_state    4.0\n",
            "Name: 1997-07-09 00:00:00, dtype: float64  future value 0.9752895073644567\n",
            "loop 1 market state : step  392 market state fut market_state    4.0\n",
            "Name: 1997-07-10 00:00:00, dtype: float64  future value 0.9701036927756486\n",
            "loop 1 market state : step  393 market state fut market_state    2.0\n",
            "Name: 1997-07-11 00:00:00, dtype: float64  future value 0.9531197734214751\n",
            "loop 1 market state : step  394 market state fut market_state    2.0\n",
            "Name: 1997-07-14 00:00:00, dtype: float64  future value 0.9506622745319766\n",
            "loop 1 market state : step  395 market state fut market_state    4.0\n",
            "Name: 1997-07-15 00:00:00, dtype: float64  future value 0.9725716150298054\n",
            "loop 1 market state : step  396 market state fut market_state    2.0\n",
            "Name: 1997-07-16 00:00:00, dtype: float64  future value 0.9752582372704818\n",
            "loop 1 market state : step  397 market state fut market_state    4.0\n",
            "Name: 1997-07-17 00:00:00, dtype: float64  future value 0.9791527622265909\n",
            "loop 1 market state : step  398 market state fut market_state    4.0\n",
            "Name: 1997-07-18 00:00:00, dtype: float64  future value 0.9775803595336257\n",
            "loop 1 market state : step  399 market state fut market_state    3.0\n",
            "Name: 1997-07-21 00:00:00, dtype: float64  future value 0.9751437073734437\n",
            "loop 1 market state : step  400 market state fut market_state    4.0\n",
            "Name: 1997-07-22 00:00:00, dtype: float64  future value 0.9812249779663418\n",
            "loop 1 market state : step  401 market state fut market_state    4.0\n",
            "Name: 1997-07-23 00:00:00, dtype: float64  future value 0.9916381734883881\n",
            "loop 1 market state : step  402 market state fut market_state    4.0\n",
            "Name: 1997-07-24 00:00:00, dtype: float64  future value 0.9937416593221139\n",
            "loop 1 market state : step  403 market state fut market_state    4.0\n",
            "Name: 1997-07-25 00:00:00, dtype: float64  future value 0.9862754159287952\n",
            "loop 1 market state : step  404 market state fut market_state    4.0\n",
            "Name: 1997-07-28 00:00:00, dtype: float64  future value 0.9895659577486371\n",
            "loop 1 market state : step  405 market state fut market_state    4.0\n",
            "Name: 1997-07-29 00:00:00, dtype: float64  future value 0.9917214968485529\n",
            "loop 1 market state : step  406 market state fut market_state    4.0\n",
            "Name: 1997-07-30 00:00:00, dtype: float64  future value 1.0\n",
            "loop 1 market state : step  407 market state fut market_state   -1.0\n",
            "Name: 1997-07-31 00:00:00, dtype: float64  future value nan\n",
            "loop 1 market state : step  408 market state fut market_state   -1.0\n",
            "Name: 1997-08-01 00:00:00, dtype: float64  future value nan\n",
            "loop 1 market state : step  409 market state fut market_state   -1.0\n",
            "Name: 1997-08-04 00:00:00, dtype: float64  future value nan\n",
            "loop 1 market state : step  410 market state fut market_state   -1.0\n",
            "Name: 1997-08-05 00:00:00, dtype: float64  future value nan\n",
            "loop 1 market state : step  411 market state fut market_state   -1.0\n",
            "Name: 1997-08-06 00:00:00, dtype: float64  future value nan\n",
            "loop 2 image : step  0\n",
            "loop 2 image : step  1\n",
            "loop 2 image : step  2\n",
            "loop 2 image : step  3\n",
            "loop 2 image : step  4\n",
            "loop 2 image : step  5\n",
            "loop 2 image : step  6\n",
            "loop 2 image : step  7\n",
            "loop 2 image : step  8\n",
            "loop 2 image : step  9\n",
            "loop 2 image : step  10\n",
            "loop 2 image : step  11\n",
            "loop 2 image : step  12\n",
            "loop 2 image : step  13\n",
            "loop 2 image : step  14\n",
            "loop 2 image : step  15\n",
            "loop 2 image : step  16\n",
            "loop 2 image : step  17\n",
            "loop 2 image : step  18\n",
            "loop 2 image : step  19\n",
            "loop 2 image : step  20\n",
            "loop 2 image : step  21\n",
            "loop 2 image : step  22\n",
            "loop 2 image : step  23\n",
            "loop 2 image : step  24\n",
            "loop 2 image : step  25\n",
            "loop 2 image : step  26\n",
            "loop 2 image : step  27\n",
            "loop 2 image : step  28\n",
            "loop 2 image : step  29\n",
            "loop 2 image : step  30\n",
            "loop 2 image : step  31\n",
            "loop 2 image : step  32\n",
            "loop 2 image : step  33\n",
            "loop 2 image : step  34\n",
            "loop 2 image : step  35\n",
            "loop 2 image : step  36\n",
            "loop 2 image : step  37\n",
            "loop 2 image : step  38\n",
            "loop 2 image : step  39\n",
            "loop 2 image : step  40\n",
            "loop 2 image : step  41\n",
            "loop 2 image : step  42\n",
            "loop 2 image : step  43\n",
            "loop 2 image : step  44\n",
            "loop 2 image : step  45\n",
            "loop 2 image : step  46\n",
            "loop 2 image : step  47\n",
            "loop 2 image : step  48\n",
            "loop 2 image : step  49\n",
            "loop 2 image : step  50\n",
            "loop 2 image : step  51\n",
            "loop 2 image : step  52\n",
            "loop 2 image : step  53\n",
            "loop 2 image : step  54\n",
            "loop 2 image : step  55\n",
            "loop 2 image : step  56\n",
            "loop 2 image : step  57\n",
            "loop 2 image : step  58\n",
            "loop 2 image : step  59\n",
            "loop 2 image : step  60\n",
            "loop 2 image : step  61\n",
            "loop 2 image : step  62\n",
            "loop 2 image : step  63\n",
            "loop 2 image : step  64\n",
            "loop 2 image : step  65\n",
            "loop 2 image : step  66\n",
            "loop 2 image : step  67\n",
            "loop 2 image : step  68\n",
            "loop 2 image : step  69\n",
            "loop 2 image : step  70\n",
            "loop 2 image : step  71\n",
            "loop 2 image : step  72\n",
            "loop 2 image : step  73\n",
            "loop 2 image : step  74\n",
            "loop 2 image : step  75\n",
            "loop 2 image : step  76\n",
            "loop 2 image : step  77\n",
            "loop 2 image : step  78\n",
            "loop 2 image : step  79\n",
            "loop 2 image : step  80\n",
            "loop 2 image : step  81\n",
            "loop 2 image : step  82\n",
            "loop 2 image : step  83\n",
            "loop 2 image : step  84\n",
            "loop 2 image : step  85\n",
            "loop 2 image : step  86\n",
            "loop 2 image : step  87\n",
            "loop 2 image : step  88\n",
            "loop 2 image : step  89\n",
            "loop 2 image : step  90\n",
            "loop 2 image : step  91\n",
            "loop 2 image : step  92\n",
            "loop 2 image : step  93\n",
            "loop 2 image : step  94\n",
            "loop 2 image : step  95\n",
            "loop 2 image : step  96\n",
            "loop 2 image : step  97\n",
            "loop 2 image : step  98\n",
            "loop 2 image : step  99\n",
            "loop 2 image : step  100\n",
            "loop 2 image : step  101\n",
            "loop 2 image : step  102\n",
            "loop 2 image : step  103\n",
            "loop 2 image : step  104\n",
            "loop 2 image : step  105\n",
            "loop 2 image : step  106\n",
            "loop 2 image : step  107\n",
            "loop 2 image : step  108\n",
            "loop 2 image : step  109\n",
            "loop 2 image : step  110\n",
            "loop 2 image : step  111\n",
            "loop 2 image : step  112\n",
            "loop 2 image : step  113\n",
            "loop 2 image : step  114\n",
            "loop 2 image : step  115\n",
            "loop 2 image : step  116\n",
            "loop 2 image : step  117\n",
            "loop 2 image : step  118\n",
            "loop 2 image : step  119\n",
            "loop 2 image : step  120\n",
            "loop 2 image : step  121\n",
            "loop 2 image : step  122\n",
            "loop 2 image : step  123\n",
            "loop 2 image : step  124\n",
            "loop 2 image : step  125\n",
            "loop 2 image : step  126\n",
            "loop 2 image : step  127\n",
            "loop 2 image : step  128\n",
            "loop 2 image : step  129\n",
            "loop 2 image : step  130\n",
            "loop 2 image : step  131\n",
            "loop 2 image : step  132\n",
            "loop 2 image : step  133\n",
            "loop 2 image : step  134\n",
            "loop 2 image : step  135\n",
            "loop 2 image : step  136\n",
            "loop 2 image : step  137\n",
            "loop 2 image : step  138\n",
            "loop 2 image : step  139\n",
            "loop 2 image : step  140\n",
            "loop 2 image : step  141\n",
            "loop 2 image : step  142\n",
            "loop 2 image : step  143\n",
            "loop 2 image : step  144\n",
            "loop 2 image : step  145\n",
            "loop 2 image : step  146\n",
            "loop 2 image : step  147\n",
            "loop 2 image : step  148\n",
            "loop 2 image : step  149\n",
            "loop 2 image : step  150\n",
            "loop 2 image : step  151\n",
            "loop 2 image : step  152\n",
            "loop 2 image : step  153\n",
            "loop 2 image : step  154\n",
            "loop 2 image : step  155\n",
            "loop 2 image : step  156\n",
            "loop 2 image : step  157\n",
            "loop 2 image : step  158\n",
            "loop 2 image : step  159\n",
            "loop 2 image : step  160\n",
            "loop 2 image : step  161\n",
            "loop 2 image : step  162\n",
            "loop 2 image : step  163\n",
            "loop 2 image : step  164\n",
            "loop 2 image : step  165\n",
            "loop 2 image : step  166\n",
            "loop 2 image : step  167\n",
            "loop 2 image : step  168\n",
            "loop 2 image : step  169\n",
            "loop 2 image : step  170\n",
            "loop 2 image : step  171\n",
            "loop 2 image : step  172\n",
            "loop 2 image : step  173\n",
            "loop 2 image : step  174\n",
            "loop 2 image : step  175\n",
            "loop 2 image : step  176\n",
            "loop 2 image : step  177\n",
            "loop 2 image : step  178\n",
            "loop 2 image : step  179\n",
            "loop 2 image : step  180\n",
            "loop 2 image : step  181\n",
            "loop 2 image : step  182\n",
            "loop 2 image : step  183\n",
            "loop 2 image : step  184\n",
            "loop 2 image : step  185\n",
            "loop 2 image : step  186\n",
            "loop 2 image : step  187\n",
            "loop 2 image : step  188\n",
            "loop 2 image : step  189\n",
            "loop 2 image : step  190\n",
            "loop 2 image : step  191\n",
            "loop 2 image : step  192\n",
            "loop 2 image : step  193\n",
            "loop 2 image : step  194\n",
            "loop 2 image : step  195\n",
            "loop 2 image : step  196\n",
            "loop 2 image : step  197\n",
            "loop 2 image : step  198\n",
            "loop 2 image : step  199\n",
            "loop 2 image : step  200\n",
            "loop 2 image : step  201\n",
            "loop 2 image : step  202\n",
            "loop 2 image : step  203\n",
            "loop 2 image : step  204\n",
            "loop 2 image : step  205\n",
            "loop 2 image : step  206\n",
            "loop 2 image : step  207\n",
            "loop 2 image : step  208\n",
            "loop 2 image : step  209\n",
            "loop 2 image : step  210\n",
            "loop 2 image : step  211\n",
            "loop 2 image : step  212\n",
            "loop 2 image : step  213\n",
            "loop 2 image : step  214\n",
            "loop 2 image : step  215\n",
            "loop 2 image : step  216\n",
            "loop 2 image : step  217\n",
            "loop 2 image : step  218\n",
            "loop 2 image : step  219\n",
            "loop 2 image : step  220\n",
            "loop 2 image : step  221\n",
            "loop 2 image : step  222\n",
            "loop 2 image : step  223\n",
            "loop 2 image : step  224\n",
            "loop 2 image : step  225\n",
            "loop 2 image : step  226\n",
            "loop 2 image : step  227\n",
            "loop 2 image : step  228\n",
            "loop 2 image : step  229\n",
            "loop 2 image : step  230\n",
            "loop 2 image : step  231\n",
            "loop 2 image : step  232\n",
            "loop 2 image : step  233\n",
            "loop 2 image : step  234\n",
            "loop 2 image : step  235\n",
            "loop 2 image : step  236\n",
            "loop 2 image : step  237\n",
            "loop 2 image : step  238\n",
            "loop 2 image : step  239\n",
            "loop 2 image : step  240\n",
            "loop 2 image : step  241\n",
            "loop 2 image : step  242\n",
            "loop 2 image : step  243\n",
            "loop 2 image : step  244\n",
            "loop 2 image : step  245\n",
            "loop 2 image : step  246\n",
            "loop 2 image : step  247\n",
            "loop 2 image : step  248\n",
            "loop 2 image : step  249\n",
            "loop 2 image : step  250\n",
            "loop 2 image : step  251\n",
            "loop 2 image : step  252\n",
            "loop 2 image : step  253\n",
            "loop 2 image : step  254\n",
            "loop 2 image : step  255\n",
            "loop 2 image : step  256\n",
            "loop 2 image : step  257\n",
            "loop 2 image : step  258\n",
            "loop 2 image : step  259\n",
            "loop 2 image : step  260\n",
            "loop 2 image : step  261\n",
            "loop 2 image : step  262\n",
            "loop 2 image : step  263\n",
            "loop 2 image : step  264\n",
            "loop 2 image : step  265\n",
            "loop 2 image : step  266\n",
            "loop 2 image : step  267\n",
            "loop 2 image : step  268\n",
            "loop 2 image : step  269\n",
            "loop 2 image : step  270\n",
            "loop 2 image : step  271\n",
            "loop 2 image : step  272\n",
            "loop 2 image : step  273\n",
            "loop 2 image : step  274\n",
            "loop 2 image : step  275\n",
            "loop 2 image : step  276\n",
            "loop 2 image : step  277\n",
            "loop 2 image : step  278\n",
            "loop 2 image : step  279\n",
            "loop 2 image : step  280\n",
            "loop 2 image : step  281\n",
            "loop 2 image : step  282\n",
            "loop 2 image : step  283\n",
            "loop 2 image : step  284\n",
            "loop 2 image : step  285\n",
            "loop 2 image : step  286\n",
            "loop 2 image : step  287\n",
            "loop 2 image : step  288\n",
            "loop 2 image : step  289\n",
            "loop 2 image : step  290\n",
            "loop 2 image : step  291\n",
            "loop 2 image : step  292\n",
            "loop 2 image : step  293\n",
            "loop 2 image : step  294\n",
            "loop 2 image : step  295\n",
            "loop 2 image : step  296\n",
            "loop 2 image : step  297\n",
            "loop 2 image : step  298\n",
            "loop 2 image : step  299\n",
            "loop 2 image : step  300\n",
            "loop 2 image : step  301\n",
            "loop 2 image : step  302\n",
            "loop 2 image : step  303\n",
            "loop 2 image : step  304\n",
            "loop 2 image : step  305\n",
            "loop 2 image : step  306\n",
            "loop 2 image : step  307\n",
            "loop 2 image : step  308\n",
            "loop 2 image : step  309\n",
            "loop 2 image : step  310\n",
            "loop 2 image : step  311\n",
            "loop 2 image : step  312\n",
            "loop 2 image : step  313\n",
            "loop 2 image : step  314\n",
            "loop 2 image : step  315\n",
            "loop 2 image : step  316\n",
            "loop 2 image : step  317\n",
            "loop 2 image : step  318\n",
            "loop 2 image : step  319\n",
            "loop 2 image : step  320\n",
            "loop 2 image : step  321\n",
            "loop 2 image : step  322\n",
            "loop 2 image : step  323\n",
            "loop 2 image : step  324\n",
            "loop 2 image : step  325\n",
            "loop 2 image : step  326\n",
            "loop 2 image : step  327\n",
            "loop 2 image : step  328\n",
            "loop 2 image : step  329\n",
            "loop 2 image : step  330\n",
            "loop 2 image : step  331\n",
            "loop 2 image : step  332\n",
            "loop 2 image : step  333\n",
            "loop 2 image : step  334\n",
            "loop 2 image : step  335\n",
            "loop 2 image : step  336\n",
            "loop 2 image : step  337\n",
            "loop 2 image : step  338\n",
            "loop 2 image : step  339\n",
            "loop 2 image : step  340\n",
            "loop 2 image : step  341\n",
            "loop 2 image : step  342\n",
            "loop 2 image : step  343\n",
            "loop 2 image : step  344\n",
            "loop 2 image : step  345\n",
            "loop 2 image : step  346\n",
            "loop 2 image : step  347\n",
            "loop 2 image : step  348\n",
            "loop 2 image : step  349\n",
            "loop 2 image : step  350\n",
            "loop 2 image : step  351\n",
            "loop 2 image : step  352\n",
            "loop 2 image : step  353\n",
            "loop 2 image : step  354\n",
            "loop 2 image : step  355\n",
            "loop 2 image : step  356\n",
            "loop 2 image : step  357\n",
            "loop 2 image : step  358\n",
            "loop 2 image : step  359\n",
            "loop 2 image : step  360\n",
            "loop 2 image : step  361\n",
            "loop 2 image : step  362\n",
            "loop 2 image : step  363\n",
            "loop 2 image : step  364\n",
            "loop 2 image : step  365\n",
            "loop 2 image : step  366\n",
            "loop 2 image : step  367\n",
            "loop 2 image : step  368\n",
            "loop 2 image : step  369\n",
            "loop 2 image : step  370\n",
            "loop 2 image : step  371\n",
            "loop 2 image : step  372\n",
            "loop 2 image : step  373\n",
            "loop 2 image : step  374\n",
            "loop 2 image : step  375\n",
            "loop 2 image : step  376\n",
            "loop 2 image : step  377\n",
            "loop 2 image : step  378\n",
            "loop 2 image : step  379\n",
            "loop 2 image : step  380\n",
            "loop 2 image : step  381\n",
            "loop 2 image : step  382\n",
            "loop 2 image : step  383\n",
            "loop 2 image : step  384\n",
            "loop 2 image : step  385\n",
            "loop 2 image : step  386\n",
            "loop 2 image : step  387\n",
            "loop 2 image : step  388\n",
            "loop 2 image : step  389\n",
            "loop 2 image : step  390\n",
            "loop 2 image : step  391\n",
            "loop 2 image : step  392\n",
            "loop 2 image : step  393\n",
            "loop 2 image : step  394\n",
            "loop 2 image : step  395\n",
            "loop 2 image : step  396\n",
            "loop 2 image : step  397\n",
            "loop 2 image : step  398\n",
            "loop 2 image : step  399\n",
            "loop 2 image : step  400\n",
            "loop 2 image : step  401\n",
            "loop 2 image : step  402\n",
            "loop 2 image : step  403\n",
            "loop 2 image : step  404\n",
            "loop 2 image : step  405\n",
            "loop 2 image : step  406\n",
            "loop 2 image : step  407\n",
            "loop 2 image : step  408\n",
            "loop 2 image : step  409\n",
            "loop 2 image : step  410\n",
            "loop 2 image : step  411\n",
            "loop 1 market state : step  0 market state fut market_state    1.0\n",
            "Name: 1997-08-07 00:00:00, dtype: float64  future value 0.7024191904853123\n",
            "loop 1 market state : step  1 market state fut market_state    1.0\n",
            "Name: 1997-08-08 00:00:00, dtype: float64  future value 0.6842200935394854\n",
            "loop 1 market state : step  2 market state fut market_state    1.0\n",
            "Name: 1997-08-11 00:00:00, dtype: float64  future value 0.6930917598207466\n",
            "loop 1 market state : step  3 market state fut market_state    1.0\n",
            "Name: 1997-08-12 00:00:00, dtype: float64  future value 0.7033610386402523\n",
            "loop 1 market state : step  4 market state fut market_state    2.0\n",
            "Name: 1997-08-13 00:00:00, dtype: float64  future value 0.7134935556928221\n",
            "loop 1 market state : step  5 market state fut market_state    2.0\n",
            "Name: 1997-08-14 00:00:00, dtype: float64  future value 0.7026318434429176\n",
            "loop 1 market state : step  6 market state fut market_state    2.0\n",
            "Name: 1997-08-15 00:00:00, dtype: float64  future value 0.7014848989974946\n",
            "loop 1 market state : step  7 market state fut market_state    2.0\n",
            "Name: 1997-08-18 00:00:00, dtype: float64  future value 0.6989175792926181\n",
            "loop 1 market state : step  8 market state fut market_state    1.0\n",
            "Name: 1997-08-19 00:00:00, dtype: float64  future value 0.6934943493746696\n",
            "loop 1 market state : step  9 market state fut market_state    1.0\n",
            "Name: 1997-08-20 00:00:00, dtype: float64  future value 0.694010845254478\n",
            "loop 1 market state : step  10 market state fut market_state    1.0\n",
            "Name: 1997-08-21 00:00:00, dtype: float64  future value 0.6863924267174821\n",
            "loop 1 market state : step  11 market state fut market_state    1.0\n",
            "Name: 1997-08-22 00:00:00, dtype: float64  future value 0.6832022614740341\n",
            "loop 1 market state : step  12 market state fut market_state    2.0\n",
            "Name: 1997-08-25 00:00:00, dtype: float64  future value 0.7045535548880139\n",
            "loop 1 market state : step  13 market state fut market_state    2.0\n",
            "Name: 1997-08-26 00:00:00, dtype: float64  future value 0.7047662078456193\n",
            "loop 1 market state : step  14 market state fut market_state    2.0\n",
            "Name: 1997-08-27 00:00:00, dtype: float64  future value 0.7070524937094224\n",
            "loop 1 market state : step  15 market state fut market_state    2.0\n",
            "Name: 1997-08-28 00:00:00, dtype: float64  future value 0.7056700872252641\n",
            "loop 1 market state : step  16 market state fut market_state    2.0\n",
            "Name: 1997-08-29 00:00:00, dtype: float64  future value 0.7073031618022437\n",
            "loop 1 market state : step  17 market state fut market_state    2.0\n",
            "Name: 1997-09-02 00:00:00, dtype: float64  future value 0.7091412863097856\n",
            "loop 1 market state : step  18 market state fut market_state    1.0\n",
            "Name: 1997-09-03 00:00:00, dtype: float64  future value 0.6980593180752325\n",
            "loop 1 market state : step  19 market state fut market_state    1.0\n",
            "Name: 1997-09-04 00:00:00, dtype: float64  future value 0.6931677437312579\n",
            "loop 1 market state : step  20 market state fut market_state    1.0\n",
            "Name: 1997-09-05 00:00:00, dtype: float64  future value 0.7017659328385679\n",
            "loop 1 market state : step  21 market state fut market_state    1.0\n",
            "Name: 1997-09-08 00:00:00, dtype: float64  future value 0.6986213857573792\n",
            "loop 1 market state : step  22 market state fut market_state    2.0\n",
            "Name: 1997-09-09 00:00:00, dtype: float64  future value 0.7182712237109115\n",
            "loop 1 market state : step  23 market state fut market_state    2.0\n",
            "Name: 1997-09-10 00:00:00, dtype: float64  future value 0.7162659716881817\n",
            "loop 1 market state : step  24 market state fut market_state    2.0\n",
            "Name: 1997-09-11 00:00:00, dtype: float64  future value 0.7195244714551767\n",
            "loop 1 market state : step  25 market state fut market_state    2.0\n",
            "Name: 1997-09-12 00:00:00, dtype: float64  future value 0.7219702818071245\n",
            "loop 1 market state : step  26 market state fut market_state    4.0\n",
            "Name: 1997-09-15 00:00:00, dtype: float64  future value 0.7257073086786329\n",
            "loop 1 market state : step  27 market state fut market_state    4.0\n",
            "Name: 1997-09-16 00:00:00, dtype: float64  future value 0.7230488453690797\n",
            "loop 1 market state : step  28 market state fut market_state    3.0\n",
            "Name: 1997-09-17 00:00:00, dtype: float64  future value 0.7173901070524752\n",
            "loop 1 market state : step  29 market state fut market_state    2.0\n",
            "Name: 1997-09-18 00:00:00, dtype: float64  future value 0.7123997860767806\n",
            "loop 1 market state : step  30 market state fut market_state    2.0\n",
            "Name: 1997-09-19 00:00:00, dtype: float64  future value 0.7179521747346219\n",
            "loop 1 market state : step  31 market state fut market_state    2.0\n",
            "Name: 1997-09-22 00:00:00, dtype: float64  future value 0.7241198522639125\n",
            "loop 1 market state : step  32 market state fut market_state    2.0\n",
            "Name: 1997-09-23 00:00:00, dtype: float64  future value 0.7195169147880545\n",
            "loop 1 market state : step  33 market state fut market_state    3.0\n",
            "Name: 1997-09-24 00:00:00, dtype: float64  future value 0.7256921026245464\n",
            "loop 1 market state : step  34 market state fut market_state    4.0\n",
            "Name: 1997-09-25 00:00:00, dtype: float64  future value 0.7295279224876956\n",
            "loop 1 market state : step  35 market state fut market_state    4.0\n",
            "Name: 1997-09-26 00:00:00, dtype: float64  future value 0.732999121572217\n",
            "loop 1 market state : step  36 market state fut market_state    4.0\n",
            "Name: 1997-09-29 00:00:00, dtype: float64  future value 0.7388173380170453\n",
            "loop 1 market state : step  37 market state fut market_state    4.0\n",
            "Name: 1997-09-30 00:00:00, dtype: float64  future value 0.7467395531163231\n",
            "loop 1 market state : step  38 market state fut market_state    4.0\n",
            "Name: 1997-10-01 00:00:00, dtype: float64  future value 0.7396908516484383\n",
            "loop 1 market state : step  39 market state fut market_state    4.0\n",
            "Name: 1997-10-02 00:00:00, dtype: float64  future value 0.7372450412964904\n",
            "loop 1 market state : step  40 market state fut market_state    4.0\n",
            "Name: 1997-10-03 00:00:00, dtype: float64  future value 0.7344802283281741\n",
            "loop 1 market state : step  41 market state fut market_state    2.0\n",
            "Name: 1997-10-06 00:00:00, dtype: float64  future value 0.7353309328784374\n",
            "loop 1 market state : step  42 market state fut market_state    2.0\n",
            "Name: 1997-10-07 00:00:00, dtype: float64  future value 0.7369868165365467\n",
            "loop 1 market state : step  43 market state fut market_state    2.0\n",
            "Name: 1997-10-08 00:00:00, dtype: float64  future value 0.7335231741191476\n",
            "loop 1 market state : step  44 market state fut market_state    2.0\n",
            "Name: 1997-10-09 00:00:00, dtype: float64  future value 0.7255705932716178\n",
            "loop 1 market state : step  45 market state fut market_state    2.0\n",
            "Name: 1997-10-10 00:00:00, dtype: float64  future value 0.7171470419866969\n",
            "loop 1 market state : step  46 market state fut market_state    2.0\n",
            "Name: 1997-10-13 00:00:00, dtype: float64  future value 0.725844024085648\n",
            "loop 1 market state : step  47 market state fut market_state    3.0\n",
            "Name: 1997-10-14 00:00:00, dtype: float64  future value 0.7385059384277199\n",
            "loop 1 market state : step  48 market state fut market_state    3.0\n",
            "Name: 1997-10-15 00:00:00, dtype: float64  future value 0.7356271727735972\n",
            "loop 1 market state : step  49 market state fut market_state    2.0\n",
            "Name: 1997-10-16 00:00:00, dtype: float64  future value 0.7221069972141396\n",
            "loop 1 market state : step  50 market state fut market_state    2.0\n",
            "Name: 1997-10-17 00:00:00, dtype: float64  future value 0.7152329799285649\n",
            "loop 1 market state : step  51 market state fut market_state    0.0\n",
            "Name: 1997-10-20 00:00:00, dtype: float64  future value 0.6661273462524216\n",
            "loop 1 market state : step  52 market state fut market_state    0.0\n",
            "Name: 1997-10-21 00:00:00, dtype: float64  future value 0.7002012391450563\n",
            "loop 1 market state : step  53 market state fut market_state    0.0\n",
            "Name: 1997-10-22 00:00:00, dtype: float64  future value 0.6981580183470315\n",
            "loop 1 market state : step  54 market state fut market_state    0.0\n",
            "Name: 1997-10-23 00:00:00, dtype: float64  future value 0.6864000297445253\n",
            "loop 1 market state : step  55 market state fut market_state    0.0\n",
            "Name: 1997-10-24 00:00:00, dtype: float64  future value 0.6947096283436398\n",
            "loop 1 market state : step  56 market state fut market_state    2.0\n",
            "Name: 1997-10-27 00:00:00, dtype: float64  future value 0.7132201248787919\n",
            "loop 1 market state : step  57 market state fut market_state    2.0\n",
            "Name: 1997-10-28 00:00:00, dtype: float64  future value 0.714564562587655\n",
            "loop 1 market state : step  58 market state fut market_state    2.0\n",
            "Name: 1997-10-29 00:00:00, dtype: float64  future value 0.7160836844788283\n",
            "loop 1 market state : step  59 market state fut market_state    3.0\n",
            "Name: 1997-10-30 00:00:00, dtype: float64  future value 0.7124909760413782\n",
            "loop 1 market state : step  60 market state fut market_state    2.0\n",
            "Name: 1997-10-31 00:00:00, dtype: float64  future value 0.7045003800586324\n",
            "loop 1 market state : step  61 market state fut market_state    1.0\n",
            "Name: 1997-11-03 00:00:00, dtype: float64  future value 0.699654377516996\n",
            "loop 1 market state : step  62 market state fut market_state    1.0\n",
            "Name: 1997-11-04 00:00:00, dtype: float64  future value 0.701667232566769\n",
            "loop 1 market state : step  63 market state fut market_state    1.0\n",
            "Name: 1997-11-05 00:00:00, dtype: float64  future value 0.6881318509532249\n",
            "loop 1 market state : step  64 market state fut market_state    1.0\n",
            "Name: 1997-11-06 00:00:00, dtype: float64  future value 0.6962591159830649\n",
            "loop 1 market state : step  65 market state fut market_state    2.0\n",
            "Name: 1997-11-07 00:00:00, dtype: float64  future value 0.7051383852913693\n",
            "loop 1 market state : step  66 market state fut market_state    2.0\n",
            "Name: 1997-11-10 00:00:00, dtype: float64  future value 0.7186965759860431\n",
            "loop 1 market state : step  67 market state fut market_state    2.0\n",
            "Name: 1997-11-11 00:00:00, dtype: float64  future value 0.7126428511425588\n",
            "loop 1 market state : step  68 market state fut market_state    2.0\n",
            "Name: 1997-11-12 00:00:00, dtype: float64  future value 0.7174736939900297\n",
            "loop 1 market state : step  69 market state fut market_state    2.0\n",
            "Name: 1997-11-13 00:00:00, dtype: float64  future value 0.7284037407634811\n",
            "loop 1 market state : step  70 market state fut market_state    2.0\n",
            "Name: 1997-11-14 00:00:00, dtype: float64  future value 0.7315255714833822\n",
            "loop 1 market state : step  71 market state fut market_state    2.0\n",
            "Name: 1997-11-17 00:00:00, dtype: float64  future value 0.7190535473777067\n",
            "loop 1 market state : step  72 market state fut market_state    2.0\n",
            "Name: 1997-11-18 00:00:00, dtype: float64  future value 0.7222057438458596\n",
            "loop 1 market state : step  73 market state fut market_state    2.0\n",
            "Name: 1997-11-19 00:00:00, dtype: float64  future value 0.7228285893844311\n",
            "loop 1 market state : step  74 market state fut market_state    2.0\n",
            "Name: 1997-11-20 00:00:00, dtype: float64  future value 0.7256845459574242\n",
            "loop 1 market state : step  75 market state fut market_state    4.0\n",
            "Name: 1997-11-21 00:00:00, dtype: float64  future value 0.7403972377646433\n",
            "loop 1 market state : step  76 market state fut market_state    3.0\n",
            "Name: 1997-11-24 00:00:00, dtype: float64  future value 0.7380501740444154\n",
            "loop 1 market state : step  77 market state fut market_state    4.0\n",
            "Name: 1997-11-25 00:00:00, dtype: float64  future value 0.7419163596558165\n",
            "loop 1 market state : step  78 market state fut market_state    4.0\n",
            "Name: 1997-11-26 00:00:00, dtype: float64  future value 0.7391287376063705\n",
            "loop 1 market state : step  79 market state fut market_state    4.0\n",
            "Name: 1997-11-28 00:00:00, dtype: float64  future value 0.7472484459690883\n",
            "loop 1 market state : step  80 market state fut market_state    4.0\n",
            "Name: 1997-12-01 00:00:00, dtype: float64  future value 0.7461698824071332\n",
            "loop 1 market state : step  81 market state fut market_state    4.0\n",
            "Name: 1997-12-02 00:00:00, dtype: float64  future value 0.7411644017372732\n",
            "loop 1 market state : step  82 market state fut market_state    2.0\n",
            "Name: 1997-12-03 00:00:00, dtype: float64  future value 0.7366145927308757\n",
            "loop 1 market state : step  83 market state fut market_state    2.0\n",
            "Name: 1997-12-04 00:00:00, dtype: float64  future value 0.7253351312328827\n",
            "loop 1 market state : step  84 market state fut market_state    2.0\n",
            "Name: 1997-12-05 00:00:00, dtype: float64  future value 0.7241578210392077\n",
            "loop 1 market state : step  85 market state fut market_state    2.0\n",
            "Name: 1997-12-08 00:00:00, dtype: float64  future value 0.7317534304950739\n",
            "loop 1 market state : step  86 market state fut market_state    2.0\n",
            "Name: 1997-12-09 00:00:00, dtype: float64  future value 0.7352853610760991\n",
            "loop 1 market state : step  87 market state fut market_state    2.0\n",
            "Name: 1997-12-10 00:00:00, dtype: float64  future value 0.7333864587121325\n",
            "loop 1 market state : step  88 market state fut market_state    3.0\n",
            "Name: 1997-12-11 00:00:00, dtype: float64  future value 0.7256085620469129\n",
            "loop 1 market state : step  89 market state fut market_state    2.0\n",
            "Name: 1997-12-12 00:00:00, dtype: float64  future value 0.7191371343152612\n",
            "loop 1 market state : step  90 market state fut market_state    2.0\n",
            "Name: 1997-12-15 00:00:00, dtype: float64  future value 0.7243932830779427\n",
            "loop 1 market state : step  91 market state fut market_state    2.0\n",
            "Name: 1997-12-16 00:00:00, dtype: float64  future value 0.7133264745375552\n",
            "loop 1 market state : step  92 market state fut market_state    2.0\n",
            "Name: 1997-12-17 00:00:00, dtype: float64  future value 0.7084425032206237\n",
            "loop 1 market state : step  93 market state fut market_state    2.0\n",
            "Name: 1997-12-18 00:00:00, dtype: float64  future value 0.7112984597936167\n",
            "loop 1 market state : step  94 market state fut market_state    3.0\n",
            "Name: 1997-12-19 00:00:00, dtype: float64  future value 0.7241274089310348\n",
            "loop 1 market state : step  95 market state fut market_state    3.0\n",
            "Name: 1997-12-22 00:00:00, dtype: float64  future value 0.7374121688116784\n",
            "loop 1 market state : step  96 market state fut market_state    3.0\n",
            "Name: 1997-12-23 00:00:00, dtype: float64  future value 0.7371007228624321\n",
            "loop 1 market state : step  97 market state fut market_state    3.0\n",
            "Name: 1997-12-24 00:00:00, dtype: float64  future value 0.7406022876952054\n",
            "loop 1 market state : step  98 market state fut market_state    3.0\n",
            "Name: 1997-12-26 00:00:00, dtype: float64  future value 0.7421442186675083\n",
            "loop 1 market state : step  99 market state fut market_state    3.0\n",
            "Name: 1997-12-29 00:00:00, dtype: float64  future value 0.7341764317658921\n",
            "loop 1 market state : step  100 market state fut market_state    2.0\n",
            "Name: 1997-12-30 00:00:00, dtype: float64  future value 0.7322167515455007\n",
            "loop 1 market state : step  101 market state fut market_state    2.0\n",
            "Name: 1997-12-31 00:00:00, dtype: float64  future value 0.7261782327561028\n",
            "loop 1 market state : step  102 market state fut market_state    0.0\n",
            "Name: 1998-01-02 00:00:00, dtype: float64  future value 0.7046370954656473\n",
            "loop 1 market state : step  103 market state fut market_state    1.0\n",
            "Name: 1998-01-05 00:00:00, dtype: float64  future value 0.7133872523939799\n",
            "loop 1 market state : step  104 market state fut market_state    1.0\n",
            "Name: 1998-01-06 00:00:00, dtype: float64  future value 0.723193163803138\n",
            "loop 1 market state : step  105 market state fut market_state    1.0\n",
            "Name: 1998-01-07 00:00:00, dtype: float64  future value 0.7276138140696427\n",
            "loop 1 market state : step  106 market state fut market_state    1.0\n",
            "Name: 1998-01-08 00:00:00, dtype: float64  future value 0.7221373629623915\n",
            "loop 1 market state : step  107 market state fut market_state    2.0\n",
            "Name: 1998-01-09 00:00:00, dtype: float64  future value 0.7303254522085774\n",
            "loop 1 market state : step  108 market state fut market_state    3.0\n",
            "Name: 1998-01-12 00:00:00, dtype: float64  future value 0.743306322807097\n",
            "loop 1 market state : step  109 market state fut market_state    2.0\n",
            "Name: 1998-01-13 00:00:00, dtype: float64  future value 0.7373893597305488\n",
            "loop 1 market state : step  110 market state fut market_state    2.0\n",
            "Name: 1998-01-14 00:00:00, dtype: float64  future value 0.731487556348166\n",
            "loop 1 market state : step  111 market state fut market_state    2.0\n",
            "Name: 1998-01-15 00:00:00, dtype: float64  future value 0.7273479862826557\n",
            "loop 1 market state : step  112 market state fut market_state    1.0\n",
            "Name: 1998-01-16 00:00:00, dtype: float64  future value 0.7268618561510992\n",
            "loop 1 market state : step  113 market state fut market_state    2.0\n",
            "Name: 1998-01-20 00:00:00, dtype: float64  future value 0.7360297623275203\n",
            "loop 1 market state : step  114 market state fut market_state    3.0\n",
            "Name: 1998-01-21 00:00:00, dtype: float64  future value 0.7424404585626682\n",
            "loop 1 market state : step  115 market state fut market_state    4.0\n",
            "Name: 1998-01-22 00:00:00, dtype: float64  future value 0.7485397088485698\n",
            "loop 1 market state : step  116 market state fut market_state    4.0\n",
            "Name: 1998-01-23 00:00:00, dtype: float64  future value 0.7445824259924129\n",
            "loop 1 market state : step  117 market state fut market_state    4.0\n",
            "Name: 1998-01-26 00:00:00, dtype: float64  future value 0.7605256028226887\n",
            "loop 1 market state : step  118 market state fut market_state    4.0\n",
            "Name: 1998-01-27 00:00:00, dtype: float64  future value 0.7641183112601387\n",
            "loop 1 market state : step  119 market state fut market_state    4.0\n",
            "Name: 1998-01-28 00:00:00, dtype: float64  future value 0.764801934655135\n",
            "loop 1 market state : step  120 market state fut market_state    4.0\n",
            "Name: 1998-01-29 00:00:00, dtype: float64  future value 0.762249774644424\n",
            "loop 1 market state : step  121 market state fut market_state    4.0\n",
            "Name: 1998-01-30 00:00:00, dtype: float64  future value 0.7690250916581998\n",
            "loop 1 market state : step  122 market state fut market_state    4.0\n",
            "Name: 1998-02-02 00:00:00, dtype: float64  future value 0.7677186227246319\n",
            "loop 1 market state : step  123 market state fut market_state    4.0\n",
            "Name: 1998-02-03 00:00:00, dtype: float64  future value 0.774000206579808\n",
            "loop 1 market state : step  124 market state fut market_state    4.0\n",
            "Name: 1998-02-04 00:00:00, dtype: float64  future value 0.7747597675253945\n",
            "loop 1 market state : step  125 market state fut market_state    4.0\n",
            "Name: 1998-02-05 00:00:00, dtype: float64  future value 0.777896757939461\n",
            "loop 1 market state : step  126 market state fut market_state    4.0\n",
            "Name: 1998-02-06 00:00:00, dtype: float64  future value 0.7748205453818194\n",
            "loop 1 market state : step  127 market state fut market_state    4.0\n",
            "Name: 1998-02-09 00:00:00, dtype: float64  future value 0.7768485601257578\n",
            "loop 1 market state : step  128 market state fut market_state    4.0\n",
            "Name: 1998-02-10 00:00:00, dtype: float64  future value 0.7839276273418946\n",
            "loop 1 market state : step  129 market state fut market_state    4.0\n",
            "Name: 1998-02-11 00:00:00, dtype: float64  future value 0.7810413513805706\n",
            "loop 1 market state : step  130 market state fut market_state    4.0\n",
            "Name: 1998-02-12 00:00:00, dtype: float64  future value 0.7855454958647877\n",
            "loop 1 market state : step  131 market state fut market_state    4.0\n",
            "Name: 1998-02-13 00:00:00, dtype: float64  future value 0.7885306111776736\n",
            "loop 1 market state : step  132 market state fut market_state    4.0\n",
            "Name: 1998-02-17 00:00:00, dtype: float64  future value 0.7827731725892701\n",
            "loop 1 market state : step  133 market state fut market_state    4.0\n",
            "Name: 1998-02-18 00:00:00, dtype: float64  future value 0.7921461286962533\n",
            "loop 1 market state : step  134 market state fut market_state    4.0\n",
            "Name: 1998-02-19 00:00:00, dtype: float64  future value 0.7965288101874628\n",
            "loop 1 market state : step  135 market state fut market_state    4.0\n",
            "Name: 1998-02-20 00:00:00, dtype: float64  future value 0.797037656680307\n",
            "loop 1 market state : step  136 market state fut market_state    4.0\n",
            "Name: 1998-02-23 00:00:00, dtype: float64  future value 0.7957919656031639\n",
            "loop 1 market state : step  137 market state fut market_state    4.0\n",
            "Name: 1998-02-24 00:00:00, dtype: float64  future value 0.7990733208112096\n",
            "loop 1 market state : step  138 market state fut market_state    4.0\n",
            "Name: 1998-02-25 00:00:00, dtype: float64  future value 0.7955109317620905\n",
            "loop 1 market state : step  139 market state fut market_state    2.0\n",
            "Name: 1998-02-26 00:00:00, dtype: float64  future value 0.7861835938173668\n",
            "loop 1 market state : step  140 market state fut market_state    4.0\n",
            "Name: 1998-02-27 00:00:00, dtype: float64  future value 0.8018608501408137\n",
            "loop 1 market state : step  141 market state fut market_state    4.0\n",
            "Name: 1998-03-02 00:00:00, dtype: float64  future value 0.7992936231557791\n",
            "loop 1 market state : step  142 market state fut market_state    4.0\n",
            "Name: 1998-03-03 00:00:00, dtype: float64  future value 0.8083627363405592\n",
            "loop 1 market state : step  143 market state fut market_state    4.0\n",
            "Name: 1998-03-04 00:00:00, dtype: float64  future value 0.8115680612781727\n",
            "loop 1 market state : step  144 market state fut market_state    4.0\n",
            "Name: 1998-03-05 00:00:00, dtype: float64  future value 0.8126694802811785\n",
            "loop 1 market state : step  145 market state fut market_state    4.0\n",
            "Name: 1998-03-06 00:00:00, dtype: float64  future value 0.8116744109369358\n",
            "loop 1 market state : step  146 market state fut market_state    4.0\n",
            "Name: 1998-03-09 00:00:00, dtype: float64  future value 0.819771356578445\n",
            "loop 1 market state : step  147 market state fut market_state    4.0\n",
            "Name: 1998-03-10 00:00:00, dtype: float64  future value 0.8206675865711257\n",
            "loop 1 market state : step  148 market state fut market_state    4.0\n",
            "Name: 1998-03-11 00:00:00, dtype: float64  future value 0.8245186124883613\n",
            "loop 1 market state : step  149 market state fut market_state    4.0\n",
            "Name: 1998-03-12 00:00:00, dtype: float64  future value 0.8277239374259748\n",
            "loop 1 market state : step  150 market state fut market_state    4.0\n",
            "Name: 1998-03-13 00:00:00, dtype: float64  future value 0.8348790349125439\n",
            "loop 1 market state : step  151 market state fut market_state    4.0\n",
            "Name: 1998-03-16 00:00:00, dtype: float64  future value 0.8321370310253572\n",
            "loop 1 market state : step  152 market state fut market_state    4.0\n",
            "Name: 1998-03-17 00:00:00, dtype: float64  future value 0.8398085780318136\n",
            "loop 1 market state : step  153 market state fut market_state    4.0\n",
            "Name: 1998-03-18 00:00:00, dtype: float64  future value 0.8369830335669936\n",
            "loop 1 market state : step  154 market state fut market_state    4.0\n",
            "Name: 1998-03-19 00:00:00, dtype: float64  future value 0.836124725989687\n",
            "loop 1 market state : step  155 market state fut market_state    2.0\n",
            "Name: 1998-03-20 00:00:00, dtype: float64  future value 0.8320533977278818\n",
            "loop 1 market state : step  156 market state fut market_state    2.0\n",
            "Name: 1998-03-23 00:00:00, dtype: float64  future value 0.8306558315495581\n",
            "loop 1 market state : step  157 market state fut market_state    2.0\n",
            "Name: 1998-03-24 00:00:00, dtype: float64  future value 0.8368462718000574\n",
            "loop 1 market state : step  158 market state fut market_state    4.0\n",
            "Name: 1998-03-25 00:00:00, dtype: float64  future value 0.8417074803957802\n",
            "loop 1 market state : step  159 market state fut market_state    4.0\n",
            "Name: 1998-03-26 00:00:00, dtype: float64  future value 0.8507158620840565\n",
            "loop 1 market state : step  160 market state fut market_state    4.0\n",
            "Name: 1998-03-27 00:00:00, dtype: float64  future value 0.8527590365221603\n",
            "loop 1 market state : step  161 market state fut market_state    4.0\n",
            "Name: 1998-03-30 00:00:00, dtype: float64  future value 0.8517564568707164\n",
            "loop 1 market state : step  162 market state fut market_state    4.0\n",
            "Name: 1998-03-31 00:00:00, dtype: float64  future value 0.8427708842635698\n",
            "loop 1 market state : step  163 market state fut market_state    2.0\n",
            "Name: 1998-04-01 00:00:00, dtype: float64  future value 0.8367703342494672\n",
            "loop 1 market state : step  164 market state fut market_state    2.0\n",
            "Name: 1998-04-02 00:00:00, dtype: float64  future value 0.8436215888138332\n",
            "loop 1 market state : step  165 market state fut market_state    2.0\n",
            "Name: 1998-04-03 00:00:00, dtype: float64  future value 0.842877141202491\n",
            "loop 1 market state : step  166 market state fut market_state    2.0\n",
            "Name: 1998-04-06 00:00:00, dtype: float64  future value 0.8474801250382701\n",
            "loop 1 market state : step  167 market state fut market_state    3.0\n",
            "Name: 1998-04-07 00:00:00, dtype: float64  future value 0.8501917168172839\n",
            "loop 1 market state : step  168 market state fut market_state    3.0\n",
            "Name: 1998-04-08 00:00:00, dtype: float64  future value 0.8417226864498667\n",
            "loop 1 market state : step  169 market state fut market_state    4.0\n",
            "Name: 1998-04-09 00:00:00, dtype: float64  future value 0.8527742425762468\n",
            "loop 1 market state : step  170 market state fut market_state    4.0\n",
            "Name: 1998-04-13 00:00:00, dtype: float64  future value 0.8534806750523728\n",
            "loop 1 market state : step  171 market state fut market_state    4.0\n",
            "Name: 1998-04-14 00:00:00, dtype: float64  future value 0.8557745639432192\n",
            "loop 1 market state : step  172 market state fut market_state    4.0\n",
            "Name: 1998-04-15 00:00:00, dtype: float64  future value 0.8587140610938456\n",
            "loop 1 market state : step  173 market state fut market_state    3.0\n",
            "Name: 1998-04-16 00:00:00, dtype: float64  future value 0.8503892100807238\n",
            "loop 1 market state : step  174 market state fut market_state    2.0\n",
            "Name: 1998-04-17 00:00:00, dtype: float64  future value 0.8415175901593835\n",
            "loop 1 market state : step  175 market state fut market_state    2.0\n",
            "Name: 1998-04-20 00:00:00, dtype: float64  future value 0.8252933794880344\n",
            "loop 1 market state : step  176 market state fut market_state    2.0\n",
            "Name: 1998-04-21 00:00:00, dtype: float64  future value 0.8242071665391151\n",
            "loop 1 market state : step  177 market state fut market_state    2.0\n",
            "Name: 1998-04-22 00:00:00, dtype: float64  future value 0.8314305985492312\n",
            "loop 1 market state : step  178 market state fut market_state    2.0\n",
            "Name: 1998-04-23 00:00:00, dtype: float64  future value 0.8444418812559237\n",
            "loop 1 market state : step  179 market state fut market_state    3.0\n",
            "Name: 1998-04-24 00:00:00, dtype: float64  future value 0.8514678200025999\n",
            "loop 1 market state : step  180 market state fut market_state    3.0\n",
            "Name: 1998-04-27 00:00:00, dtype: float64  future value 0.8522805094176471\n",
            "loop 1 market state : step  181 market state fut market_state    3.0\n",
            "Name: 1998-04-28 00:00:00, dtype: float64  future value 0.8472902348018735\n",
            "loop 1 market state : step  182 market state fut market_state    2.0\n",
            "Name: 1998-04-29 00:00:00, dtype: float64  future value 0.8392541133767102\n",
            "loop 1 market state : step  183 market state fut market_state    1.0\n",
            "Name: 1998-04-30 00:00:00, dtype: float64  future value 0.8318255850761109\n",
            "loop 1 market state : step  184 market state fut market_state    2.0\n",
            "Name: 1998-05-01 00:00:00, dtype: float64  future value 0.841699877368737\n",
            "loop 1 market state : step  185 market state fut market_state    2.0\n",
            "Name: 1998-05-04 00:00:00, dtype: float64  future value 0.8405605359503571\n",
            "loop 1 market state : step  186 market state fut market_state    3.0\n",
            "Name: 1998-05-05 00:00:00, dtype: float64  future value 0.847510537146443\n",
            "loop 1 market state : step  187 market state fut market_state    3.0\n",
            "Name: 1998-05-06 00:00:00, dtype: float64  future value 0.8498423484526635\n",
            "loop 1 market state : step  188 market state fut market_state    3.0\n",
            "Name: 1998-05-07 00:00:00, dtype: float64  future value 0.8487106100613268\n",
            "loop 1 market state : step  189 market state fut market_state    2.0\n",
            "Name: 1998-05-08 00:00:00, dtype: float64  future value 0.8421479923650773\n",
            "loop 1 market state : step  190 market state fut market_state    1.0\n",
            "Name: 1998-05-11 00:00:00, dtype: float64  future value 0.8399376440518646\n",
            "loop 1 market state : step  191 market state fut market_state    1.0\n",
            "Name: 1998-05-12 00:00:00, dtype: float64  future value 0.8427480751824402\n",
            "loop 1 market state : step  192 market state fut market_state    3.0\n",
            "Name: 1998-05-13 00:00:00, dtype: float64  future value 0.8499943162736859\n",
            "loop 1 market state : step  193 market state fut market_state    2.0\n",
            "Name: 1998-05-14 00:00:00, dtype: float64  future value 0.84663702351505\n",
            "loop 1 market state : step  194 market state fut market_state    2.0\n",
            "Name: 1998-05-15 00:00:00, dtype: float64  future value 0.8434696209928106\n",
            "loop 1 market state : step  195 market state fut market_state    1.0\n",
            "Name: 1998-05-18 00:00:00, dtype: float64  future value 0.8309748805258476\n",
            "loop 1 market state : step  196 market state fut market_state    1.0\n",
            "Name: 1998-05-19 00:00:00, dtype: float64  future value 0.8296152367628982\n",
            "loop 1 market state : step  197 market state fut market_state    1.0\n",
            "Name: 1998-05-20 00:00:00, dtype: float64  future value 0.8336940753319045\n",
            "loop 1 market state : step  198 market state fut market_state    1.0\n",
            "Name: 1998-05-21 00:00:00, dtype: float64  future value 0.8285442298680652\n",
            "loop 1 market state : step  199 market state fut market_state    1.0\n",
            "Name: 1998-05-22 00:00:00, dtype: float64  future value 0.8286657855809149\n",
            "loop 1 market state : step  200 market state fut market_state    1.0\n",
            "Name: 1998-05-26 00:00:00, dtype: float64  future value 0.8303671946814415\n",
            "loop 1 market state : step  201 market state fut market_state    0.0\n",
            "Name: 1998-05-27 00:00:00, dtype: float64  future value 0.8223994077798252\n",
            "loop 1 market state : step  202 market state fut market_state    1.0\n",
            "Name: 1998-05-28 00:00:00, dtype: float64  future value 0.8315900766774549\n",
            "loop 1 market state : step  203 market state fut market_state    2.0\n",
            "Name: 1998-05-29 00:00:00, dtype: float64  future value 0.8460445437247304\n",
            "loop 1 market state : step  204 market state fut market_state    3.0\n",
            "Name: 1998-06-01 00:00:00, dtype: float64  future value 0.8474573159571405\n",
            "loop 1 market state : step  205 market state fut market_state    3.0\n",
            "Name: 1998-06-02 00:00:00, dtype: float64  future value 0.8495005831150862\n",
            "loop 1 market state : step  206 market state fut market_state    3.0\n",
            "Name: 1998-06-03 00:00:00, dtype: float64  future value 0.8448444708098466\n",
            "loop 1 market state : step  207 market state fut market_state    1.0\n",
            "Name: 1998-06-04 00:00:00, dtype: float64  future value 0.8314001864410583\n",
            "loop 1 market state : step  208 market state fut market_state    1.0\n",
            "Name: 1998-06-05 00:00:00, dtype: float64  future value 0.8346359234868447\n",
            "loop 1 market state : step  209 market state fut market_state    0.0\n",
            "Name: 1998-06-08 00:00:00, dtype: float64  future value 0.8180547414238318\n",
            "loop 1 market state : step  210 market state fut market_state    1.0\n",
            "Name: 1998-06-09 00:00:00, dtype: float64  future value 0.8260908628489951\n",
            "loop 1 market state : step  211 market state fut market_state    1.0\n",
            "Name: 1998-06-10 00:00:00, dtype: float64  future value 0.8409175073420206\n",
            "loop 1 market state : step  212 market state fut market_state    3.0\n",
            "Name: 1998-06-11 00:00:00, dtype: float64  future value 0.8403554396598739\n",
            "loop 1 market state : step  213 market state fut market_state    3.0\n",
            "Name: 1998-06-12 00:00:00, dtype: float64  future value 0.8360107733038805\n",
            "loop 1 market state : step  214 market state fut market_state    2.0\n",
            "Name: 1998-06-15 00:00:00, dtype: float64  future value 0.8379552011102644\n",
            "loop 1 market state : step  215 market state fut market_state    4.0\n",
            "Name: 1998-06-16 00:00:00, dtype: float64  future value 0.8503208755571767\n",
            "loop 1 market state : step  216 market state fut market_state    4.0\n",
            "Name: 1998-06-17 00:00:00, dtype: float64  future value 0.8604914077449626\n",
            "loop 1 market state : step  217 market state fut market_state    4.0\n",
            "Name: 1998-06-18 00:00:00, dtype: float64  future value 0.8577570068848192\n",
            "loop 1 market state : step  218 market state fut market_state    4.0\n",
            "Name: 1998-06-19 00:00:00, dtype: float64  future value 0.8607344264508198\n",
            "loop 1 market state : step  219 market state fut market_state    4.0\n",
            "Name: 1998-06-22 00:00:00, dtype: float64  future value 0.8647525335233225\n",
            "loop 1 market state : step  220 market state fut market_state    4.0\n",
            "Name: 1998-06-23 00:00:00, dtype: float64  future value 0.8612205565823763\n",
            "loop 1 market state : step  221 market state fut market_state    4.0\n",
            "Name: 1998-06-24 00:00:00, dtype: float64  future value 0.8724013641684912\n",
            "loop 1 market state : step  222 market state fut market_state    4.0\n",
            "Name: 1998-06-25 00:00:00, dtype: float64  future value 0.8707758926185548\n",
            "loop 1 market state : step  223 market state fut market_state    4.0\n",
            "Name: 1998-06-26 00:00:00, dtype: float64  future value 0.8790626357766186\n",
            "loop 1 market state : step  224 market state fut market_state    4.0\n",
            "Name: 1998-06-29 00:00:00, dtype: float64  future value 0.8770346673926012\n",
            "loop 1 market state : step  225 market state fut market_state    4.0\n",
            "Name: 1998-06-30 00:00:00, dtype: float64  future value 0.8859366994221143\n",
            "loop 1 market state : step  226 market state fut market_state    4.0\n",
            "Name: 1998-07-01 00:00:00, dtype: float64  future value 0.8799969736243575\n",
            "loop 1 market state : step  227 market state fut market_state    4.0\n",
            "Name: 1998-07-02 00:00:00, dtype: float64  future value 0.8843795623957249\n",
            "loop 1 market state : step  228 market state fut market_state    4.0\n",
            "Name: 1998-07-06 00:00:00, dtype: float64  future value 0.8850327736825484\n",
            "loop 1 market state : step  229 market state fut market_state    4.0\n",
            "Name: 1998-07-07 00:00:00, dtype: float64  future value 0.8944437449247477\n",
            "loop 1 market state : step  230 market state fut market_state    4.0\n",
            "Name: 1998-07-08 00:00:00, dtype: float64  future value 0.89233983899014\n",
            "loop 1 market state : step  231 market state fut market_state    4.0\n",
            "Name: 1998-07-09 00:00:00, dtype: float64  future value 0.8993125565475136\n",
            "loop 1 market state : step  232 market state fut market_state    4.0\n",
            "Name: 1998-07-10 00:00:00, dtype: float64  future value 0.90140895217492\n",
            "loop 1 market state : step  233 market state fut market_state    4.0\n",
            "Name: 1998-07-13 00:00:00, dtype: float64  future value 0.8993960971251471\n",
            "loop 1 market state : step  234 market state fut market_state    2.0\n",
            "Name: 1998-07-14 00:00:00, dtype: float64  future value 0.8849416300778716\n",
            "loop 1 market state : step  235 market state fut market_state    2.0\n",
            "Name: 1998-07-15 00:00:00, dtype: float64  future value 0.8841896721593283\n",
            "loop 1 market state : step  236 market state fut market_state    2.0\n",
            "Name: 1998-07-16 00:00:00, dtype: float64  future value 0.865709587732349\n",
            "loop 1 market state : step  237 market state fut market_state    2.0\n",
            "Name: 1998-07-17 00:00:00, dtype: float64  future value 0.8665071638131517\n",
            "loop 1 market state : step  238 market state fut market_state    2.0\n",
            "Name: 1998-07-20 00:00:00, dtype: float64  future value 0.871421500878335\n",
            "loop 1 market state : step  239 market state fut market_state    2.0\n",
            "Name: 1998-07-21 00:00:00, dtype: float64  future value 0.8584861557222329\n",
            "loop 1 market state : step  240 market state fut market_state    2.0\n",
            "Name: 1998-07-22 00:00:00, dtype: float64  future value 0.8546655419131701\n",
            "loop 1 market state : step  241 market state fut market_state    3.0\n",
            "Name: 1998-07-23 00:00:00, dtype: float64  future value 0.8681401456702894\n",
            "loop 1 market state : step  242 market state fut market_state    2.0\n",
            "Name: 1998-07-24 00:00:00, dtype: float64  future value 0.8512171982696994\n",
            "loop 1 market state : step  243 market state fut market_state    2.0\n",
            "Name: 1998-07-27 00:00:00, dtype: float64  future value 0.8449659338028542\n",
            "loop 1 market state : step  244 market state fut market_state    0.0\n",
            "Name: 1998-07-28 00:00:00, dtype: float64  future value 0.8143404772735322\n",
            "loop 1 market state : step  245 market state fut market_state    0.0\n",
            "Name: 1998-07-29 00:00:00, dtype: float64  future value 0.8214120341824678\n",
            "loop 1 market state : step  246 market state fut market_state    0.0\n",
            "Name: 1998-07-30 00:00:00, dtype: float64  future value 0.8276403968483413\n",
            "loop 1 market state : step  247 market state fut market_state    0.0\n",
            "Name: 1998-07-31 00:00:00, dtype: float64  future value 0.8275036350814052\n",
            "loop 1 market state : step  248 market state fut market_state    0.0\n",
            "Name: 1998-08-03 00:00:00, dtype: float64  future value 0.8227108537290715\n",
            "loop 1 market state : step  249 market state fut market_state    0.0\n",
            "Name: 1998-08-04 00:00:00, dtype: float64  future value 0.8119554447780092\n",
            "loop 1 market state : step  250 market state fut market_state    2.0\n",
            "Name: 1998-08-05 00:00:00, dtype: float64  future value 0.823531146171162\n",
            "loop 1 market state : step  251 market state fut market_state    1.0\n",
            "Name: 1998-08-06 00:00:00, dtype: float64  future value 0.8164596819820683\n",
            "loop 1 market state : step  252 market state fut market_state    0.0\n",
            "Name: 1998-08-07 00:00:00, dtype: float64  future value 0.8072233949221793\n",
            "loop 1 market state : step  253 market state fut market_state    2.0\n",
            "Name: 1998-08-10 00:00:00, dtype: float64  future value 0.8231134432829945\n",
            "loop 1 market state : step  254 market state fut market_state    2.0\n",
            "Name: 1998-08-11 00:00:00, dtype: float64  future value 0.836428476192048\n",
            "loop 1 market state : step  255 market state fut market_state    2.0\n",
            "Name: 1998-08-12 00:00:00, dtype: float64  future value 0.834043536416367\n",
            "loop 1 market state : step  256 market state fut market_state    2.0\n",
            "Name: 1998-08-13 00:00:00, dtype: float64  future value 0.8291367096583848\n",
            "loop 1 market state : step  257 market state fut market_state    2.0\n",
            "Name: 1998-08-14 00:00:00, dtype: float64  future value 0.8212676693884885\n",
            "loop 1 market state : step  258 market state fut market_state    2.0\n",
            "Name: 1998-08-17 00:00:00, dtype: float64  future value 0.8265086584570046\n",
            "loop 1 market state : step  259 market state fut market_state    1.0\n",
            "Name: 1998-08-18 00:00:00, dtype: float64  future value 0.8300861608403681\n",
            "loop 1 market state : step  260 market state fut market_state    1.0\n",
            "Name: 1998-08-19 00:00:00, dtype: float64  future value 0.8235083370900322\n",
            "loop 1 market state : step  261 market state fut market_state    0.0\n",
            "Name: 1998-08-20 00:00:00, dtype: float64  future value 0.7919106202975973\n",
            "loop 1 market state : step  262 market state fut market_state    0.0\n",
            "Name: 1998-08-21 00:00:00, dtype: float64  future value 0.7801754407762208\n",
            "loop 1 market state : step  263 market state fut market_state    0.0\n",
            "Name: 1998-08-24 00:00:00, dtype: float64  future value 0.7271125242439207\n",
            "loop 1 market state : step  264 market state fut market_state    0.0\n",
            "Name: 1998-08-25 00:00:00, dtype: float64  future value 0.7552010731765392\n",
            "loop 1 market state : step  265 market state fut market_state    0.0\n",
            "Name: 1998-08-26 00:00:00, dtype: float64  future value 0.7523299105494596\n",
            "loop 1 market state : step  266 market state fut market_state    0.0\n",
            "Name: 1998-08-27 00:00:00, dtype: float64  future value 0.7460863418294997\n",
            "loop 1 market state : step  267 market state fut market_state    0.0\n",
            "Name: 1998-08-28 00:00:00, dtype: float64  future value 0.7397288204237333\n",
            "loop 1 market state : step  268 market state fut market_state    0.0\n",
            "Name: 1998-08-31 00:00:00, dtype: float64  future value 0.7773802620596526\n",
            "loop 1 market state : step  269 market state fut market_state    2.0\n",
            "Name: 1998-09-01 00:00:00, dtype: float64  future value 0.7642702327212402\n",
            "loop 1 market state : step  270 market state fut market_state    1.0\n",
            "Name: 1998-09-02 00:00:00, dtype: float64  future value 0.7445140451089449\n",
            "loop 1 market state : step  271 market state fut market_state    2.0\n",
            "Name: 1998-09-03 00:00:00, dtype: float64  future value 0.7664425658992369\n",
            "loop 1 market state : step  272 market state fut market_state    2.0\n",
            "Name: 1998-09-04 00:00:00, dtype: float64  future value 0.7821350746366912\n",
            "loop 1 market state : step  273 market state fut market_state    2.0\n",
            "Name: 1998-09-08 00:00:00, dtype: float64  future value 0.7881812428130532\n",
            "loop 1 market state : step  274 market state fut market_state    2.0\n",
            "Name: 1998-09-09 00:00:00, dtype: float64  future value 0.7941057625567237\n",
            "loop 1 market state : step  275 market state fut market_state    2.0\n",
            "Name: 1998-09-10 00:00:00, dtype: float64  future value 0.7738938569210448\n",
            "loop 1 market state : step  276 market state fut market_state    2.0\n",
            "Name: 1998-09-11 00:00:00, dtype: float64  future value 0.7748205453818194\n",
            "loop 1 market state : step  277 market state fut market_state    1.0\n",
            "Name: 1998-09-14 00:00:00, dtype: float64  future value 0.7777068677030643\n",
            "loop 1 market state : step  278 market state fut market_state    1.0\n",
            "Name: 1998-09-15 00:00:00, dtype: float64  future value 0.7820667401131441\n",
            "loop 1 market state : step  279 market state fut market_state    2.0\n",
            "Name: 1998-09-16 00:00:00, dtype: float64  future value 0.8097603025188829\n",
            "loop 1 market state : step  280 market state fut market_state    2.0\n",
            "Name: 1998-09-17 00:00:00, dtype: float64  future value 0.7920093669293172\n",
            "loop 1 market state : step  281 market state fut market_state    2.0\n",
            "Name: 1998-09-18 00:00:00, dtype: float64  future value 0.7935512979016202\n",
            "loop 1 market state : step  282 market state fut market_state    2.0\n",
            "Name: 1998-09-21 00:00:00, dtype: float64  future value 0.7965439235217073\n",
            "loop 1 market state : step  283 market state fut market_state    2.0\n",
            "Name: 1998-09-22 00:00:00, dtype: float64  future value 0.7967946379744497\n",
            "loop 1 market state : step  284 market state fut market_state    1.0\n",
            "Name: 1998-09-23 00:00:00, dtype: float64  future value 0.7724810846886347\n",
            "loop 1 market state : step  285 market state fut market_state    1.0\n",
            "Name: 1998-09-24 00:00:00, dtype: float64  future value 0.7492233322435661\n",
            "loop 1 market state : step  286 market state fut market_state    1.0\n",
            "Name: 1998-09-25 00:00:00, dtype: float64  future value 0.7615357855011757\n",
            "loop 1 market state : step  287 market state fut market_state    1.0\n",
            "Name: 1998-09-28 00:00:00, dtype: float64  future value 0.7508715665147112\n",
            "loop 1 market state : step  288 market state fut market_state    1.0\n",
            "Name: 1998-09-29 00:00:00, dtype: float64  future value 0.7478561318134944\n",
            "loop 1 market state : step  289 market state fut market_state    1.0\n",
            "Name: 1998-09-30 00:00:00, dtype: float64  future value 0.7372906130988288\n",
            "loop 1 market state : step  290 market state fut market_state    1.0\n",
            "Name: 1998-10-01 00:00:00, dtype: float64  future value 0.7287531554880226\n",
            "loop 1 market state : step  291 market state fut market_state    1.0\n",
            "Name: 1998-10-02 00:00:00, dtype: float64  future value 0.7477042103523929\n",
            "loop 1 market state : step  292 market state fut market_state    2.0\n",
            "Name: 1998-10-05 00:00:00, dtype: float64  future value 0.7578215677107971\n",
            "loop 1 market state : step  293 market state fut market_state    3.0\n",
            "Name: 1998-10-06 00:00:00, dtype: float64  future value 0.7556112193975844\n",
            "loop 1 market state : step  294 market state fut market_state    3.0\n",
            "Name: 1998-10-07 00:00:00, dtype: float64  future value 0.763761339868475\n",
            "loop 1 market state : step  295 market state fut market_state    3.0\n",
            "Name: 1998-10-08 00:00:00, dtype: float64  future value 0.7956324874749402\n",
            "loop 1 market state : step  296 market state fut market_state    3.0\n",
            "Name: 1998-10-09 00:00:00, dtype: float64  future value 0.8024154075157591\n",
            "loop 1 market state : step  297 market state fut market_state    3.0\n",
            "Name: 1998-10-12 00:00:00, dtype: float64  future value 0.8069499641081491\n",
            "loop 1 market state : step  298 market state fut market_state    3.0\n",
            "Name: 1998-10-13 00:00:00, dtype: float64  future value 0.808119717634702\n",
            "loop 1 market state : step  299 market state fut market_state    4.0\n",
            "Name: 1998-10-14 00:00:00, dtype: float64  future value 0.8126694802811785\n",
            "loop 1 market state : step  300 market state fut market_state    4.0\n",
            "Name: 1998-10-15 00:00:00, dtype: float64  future value 0.8191712737610821\n",
            "loop 1 market state : step  301 market state fut market_state    4.0\n",
            "Name: 1998-10-16 00:00:00, dtype: float64  future value 0.8132391509903685\n",
            "loop 1 market state : step  302 market state fut market_state    4.0\n",
            "Name: 1998-10-19 00:00:00, dtype: float64  future value 0.8144923523747127\n",
            "loop 1 market state : step  303 market state fut market_state    3.0\n",
            "Name: 1998-10-20 00:00:00, dtype: float64  future value 0.8091906318096929\n",
            "loop 1 market state : step  304 market state fut market_state    2.0\n",
            "Name: 1998-10-21 00:00:00, dtype: float64  future value 0.8112794244100561\n",
            "loop 1 market state : step  305 market state fut market_state    4.0\n",
            "Name: 1998-10-22 00:00:00, dtype: float64  future value 0.8248300584376076\n",
            "loop 1 market state : step  306 market state fut market_state    4.0\n",
            "Name: 1998-10-23 00:00:00, dtype: float64  future value 0.8345068574667938\n",
            "loop 1 market state : step  307 market state fut market_state    4.0\n",
            "Name: 1998-10-26 00:00:00, dtype: float64  future value 0.8443279285701173\n",
            "loop 1 market state : step  308 market state fut market_state    4.0\n",
            "Name: 1998-10-27 00:00:00, dtype: float64  future value 0.843750654833884\n",
            "loop 1 market state : step  309 market state fut market_state    4.0\n",
            "Name: 1998-10-28 00:00:00, dtype: float64  future value 0.8496980763785261\n",
            "loop 1 market state : step  310 market state fut market_state    4.0\n",
            "Name: 1998-10-29 00:00:00, dtype: float64  future value 0.8612281596094196\n",
            "loop 1 market state : step  311 market state fut market_state    4.0\n",
            "Name: 1998-10-30 00:00:00, dtype: float64  future value 0.8666666419413754\n",
            "loop 1 market state : step  312 market state fut market_state    4.0\n",
            "Name: 1998-11-02 00:00:00, dtype: float64  future value 0.85845574361406\n",
            "loop 1 market state : step  313 market state fut market_state    4.0\n",
            "Name: 1998-11-03 00:00:00, dtype: float64  future value 0.8569822398851461\n",
            "loop 1 market state : step  314 market state fut market_state    4.0\n",
            "Name: 1998-11-04 00:00:00, dtype: float64  future value 0.8514450109214702\n",
            "loop 1 market state : step  315 market state fut market_state    2.0\n",
            "Name: 1998-11-05 00:00:00, dtype: float64  future value 0.848953628767184\n",
            "loop 1 market state : step  316 market state fut market_state    2.0\n",
            "Name: 1998-11-06 00:00:00, dtype: float64  future value 0.8550529254130066\n",
            "loop 1 market state : step  317 market state fut market_state    3.0\n",
            "Name: 1998-11-09 00:00:00, dtype: float64  future value 0.8627624875546792\n",
            "loop 1 market state : step  318 market state fut market_state    3.0\n",
            "Name: 1998-11-10 00:00:00, dtype: float64  future value 0.8653829357290163\n",
            "loop 1 market state : step  319 market state fut market_state    4.0\n",
            "Name: 1998-11-11 00:00:00, dtype: float64  future value 0.8693022961697989\n",
            "loop 1 market state : step  320 market state fut market_state    4.0\n",
            "Name: 1998-11-12 00:00:00, dtype: float64  future value 0.8754775303662119\n",
            "loop 1 market state : step  321 market state fut market_state    4.0\n",
            "Name: 1998-11-13 00:00:00, dtype: float64  future value 0.8837871753252473\n",
            "loop 1 market state : step  322 market state fut market_state    4.0\n",
            "Name: 1998-11-16 00:00:00, dtype: float64  future value 0.9025178814851271\n",
            "loop 1 market state : step  323 market state fut market_state    4.0\n",
            "Name: 1998-11-17 00:00:00, dtype: float64  future value 0.898552995601927\n",
            "loop 1 market state : step  324 market state fut market_state    4.0\n",
            "Name: 1998-11-18 00:00:00, dtype: float64  future value 0.9015000957795968\n",
            "loop 1 market state : step  325 market state fut market_state    4.0\n",
            "Name: 1998-11-19 00:00:00, dtype: float64  future value 0.9056472688721503\n",
            "loop 1 market state : step  326 market state fut market_state    4.0\n",
            "Name: 1998-11-20 00:00:00, dtype: float64  future value 0.8838479068217511\n",
            "loop 1 market state : step  327 market state fut market_state    2.0\n",
            "Name: 1998-11-23 00:00:00, dtype: float64  future value 0.8926968103818036\n",
            "loop 1 market state : step  328 market state fut market_state    2.0\n",
            "Name: 1998-11-24 00:00:00, dtype: float64  future value 0.8896357575183275\n",
            "loop 1 market state : step  329 market state fut market_state    2.0\n",
            "Name: 1998-11-25 00:00:00, dtype: float64  future value 0.873601437083375\n",
            "loop 1 market state : step  330 market state fut market_state    2.0\n",
            "Name: 1998-11-27 00:00:00, dtype: float64  future value 0.8938057396920106\n",
            "loop 1 market state : step  331 market state fut market_state    3.0\n",
            "Name: 1998-11-30 00:00:00, dtype: float64  future value 0.9021304979852905\n",
            "loop 1 market state : step  332 market state fut market_state    3.0\n",
            "Name: 1998-12-01 00:00:00, dtype: float64  future value 0.8973301136059136\n",
            "loop 1 market state : step  333 market state fut market_state    3.0\n",
            "Name: 1998-12-02 00:00:00, dtype: float64  future value 0.8989327760747203\n",
            "loop 1 market state : step  334 market state fut market_state    3.0\n",
            "Name: 1998-12-03 00:00:00, dtype: float64  future value 0.8849037076624976\n",
            "loop 1 market state : step  335 market state fut market_state    2.0\n",
            "Name: 1998-12-04 00:00:00, dtype: float64  future value 0.8859974309186182\n",
            "loop 1 market state : step  336 market state fut market_state    2.0\n",
            "Name: 1998-12-07 00:00:00, dtype: float64  future value 0.8668109140155128\n",
            "loop 1 market state : step  337 market state fut market_state    2.0\n",
            "Name: 1998-12-08 00:00:00, dtype: float64  future value 0.883240220977345\n",
            "loop 1 market state : step  338 market state fut market_state    2.0\n",
            "Name: 1998-12-09 00:00:00, dtype: float64  future value 0.8825642006093919\n",
            "loop 1 market state : step  339 market state fut market_state    3.0\n",
            "Name: 1998-12-10 00:00:00, dtype: float64  future value 0.896266709738124\n",
            "loop 1 market state : step  340 market state fut market_state    3.0\n",
            "Name: 1998-12-11 00:00:00, dtype: float64  future value 0.902381212438033\n",
            "loop 1 market state : step  341 market state fut market_state    4.0\n",
            "Name: 1998-12-14 00:00:00, dtype: float64  future value 0.913630261827853\n",
            "loop 1 market state : step  342 market state fut market_state    4.0\n",
            "Name: 1998-12-15 00:00:00, dtype: float64  future value 0.9141847264829566\n",
            "loop 1 market state : step  343 market state fut market_state    4.0\n",
            "Name: 1998-12-16 00:00:00, dtype: float64  future value 0.9331510337613343\n",
            "loop 1 market state : step  344 market state fut market_state    4.0\n",
            "Name: 1998-12-17 00:00:00, dtype: float64  future value 0.931426815579678\n",
            "loop 1 market state : step  345 market state fut market_state    4.0\n",
            "Name: 1998-12-18 00:00:00, dtype: float64  future value 0.9308343357893584\n",
            "loop 1 market state : step  346 market state fut market_state    4.0\n",
            "Name: 1998-12-21 00:00:00, dtype: float64  future value 0.9432304223444434\n",
            "loop 1 market state : step  347 market state fut market_state    4.0\n",
            "Name: 1998-12-22 00:00:00, dtype: float64  future value 0.935725956493254\n",
            "loop 1 market state : step  348 market state fut market_state    4.0\n",
            "Name: 1998-12-23 00:00:00, dtype: float64  future value 0.9336750863082649\n",
            "loop 1 market state : step  349 market state fut market_state    3.0\n",
            "Name: 1998-12-24 00:00:00, dtype: float64  future value 0.9328167787309584\n",
            "loop 1 market state : step  350 market state fut market_state    4.0\n",
            "Name: 1998-12-28 00:00:00, dtype: float64  future value 0.9454862961000736\n",
            "loop 1 market state : step  351 market state fut market_state    4.0\n",
            "Name: 1998-12-29 00:00:00, dtype: float64  future value 0.966419747546123\n",
            "loop 1 market state : step  352 market state fut market_state    4.0\n",
            "Name: 1998-12-30 00:00:00, dtype: float64  future value 0.964437304604523\n",
            "loop 1 market state : step  353 market state fut market_state    4.0\n",
            "Name: 1998-12-31 00:00:00, dtype: float64  future value 0.9685085401464862\n",
            "loop 1 market state : step  354 market state fut market_state    4.0\n",
            "Name: 1999-01-04 00:00:00, dtype: float64  future value 0.9599938916168097\n",
            "loop 1 market state : step  355 market state fut market_state    2.0\n",
            "Name: 1999-01-05 00:00:00, dtype: float64  future value 0.9414833950816575\n",
            "loop 1 market state : step  356 market state fut market_state    2.0\n",
            "Name: 1999-01-06 00:00:00, dtype: float64  future value 0.9376020497760909\n",
            "loop 1 market state : step  357 market state fut market_state    2.0\n",
            "Name: 1999-01-07 00:00:00, dtype: float64  future value 0.9207321381251194\n",
            "loop 1 market state : step  358 market state fut market_state    2.0\n",
            "Name: 1999-01-08 00:00:00, dtype: float64  future value 0.9443317486276073\n",
            "loop 1 market state : step  359 market state fut market_state    2.0\n",
            "Name: 1999-01-11 00:00:00, dtype: float64  future value 0.9509703038744469\n",
            "loop 1 market state : step  360 market state fut market_state    3.0\n",
            "Name: 1999-01-12 00:00:00, dtype: float64  future value 0.9544794717342634\n",
            "loop 1 market state : step  361 market state fut market_state    3.0\n",
            "Name: 1999-01-13 00:00:00, dtype: float64  future value 0.938179323512324\n",
            "loop 1 market state : step  362 market state fut market_state    3.0\n",
            "Name: 1999-01-14 00:00:00, dtype: float64  future value 0.9306064304177455\n",
            "loop 1 market state : step  363 market state fut market_state    2.0\n",
            "Name: 1999-01-15 00:00:00, dtype: float64  future value 0.9372830007998013\n",
            "loop 1 market state : step  364 market state fut market_state    3.0\n",
            "Name: 1999-01-19 00:00:00, dtype: float64  future value 0.951205812273103\n",
            "loop 1 market state : step  365 market state fut market_state    2.0\n",
            "Name: 1999-01-20 00:00:00, dtype: float64  future value 0.9442634141040602\n",
            "loop 1 market state : step  366 market state fut market_state    3.0\n",
            "Name: 1999-01-21 00:00:00, dtype: float64  future value 0.9611256300081463\n",
            "loop 1 market state : step  367 market state fut market_state    4.0\n",
            "Name: 1999-01-22 00:00:00, dtype: float64  future value 0.9719645795368421\n",
            "loop 1 market state : step  368 market state fut market_state    3.0\n",
            "Name: 1999-01-25 00:00:00, dtype: float64  future value 0.966921083731766\n",
            "loop 1 market state : step  369 market state fut market_state    3.0\n",
            "Name: 1999-01-26 00:00:00, dtype: float64  future value 0.9585583103032699\n",
            "loop 1 market state : step  370 market state fut market_state    3.0\n",
            "Name: 1999-01-27 00:00:00, dtype: float64  future value 0.9662146512556399\n",
            "loop 1 market state : step  371 market state fut market_state    2.0\n",
            "Name: 1999-01-28 00:00:00, dtype: float64  future value 0.9483042375378505\n",
            "loop 1 market state : step  372 market state fut market_state    2.0\n",
            "Name: 1999-01-29 00:00:00, dtype: float64  future value 0.941399854504024\n",
            "loop 1 market state : step  373 market state fut market_state    2.0\n",
            "Name: 1999-02-01 00:00:00, dtype: float64  future value 0.9447191321274437\n",
            "loop 1 market state : step  374 market state fut market_state    1.0\n",
            "Name: 1999-02-02 00:00:00, dtype: float64  future value 0.9237324594920918\n",
            "loop 1 market state : step  375 market state fut market_state    1.0\n",
            "Name: 1999-02-03 00:00:00, dtype: float64  future value 0.9293608320604444\n",
            "loop 1 market state : step  376 market state fut market_state    3.0\n",
            "Name: 1999-02-04 00:00:00, dtype: float64  future value 0.9525198378737931\n",
            "loop 1 market state : step  377 market state fut market_state    1.0\n",
            "Name: 1999-02-05 00:00:00, dtype: float64  future value 0.9343587097032613\n",
            "loop 1 market state : step  378 market state fut market_state    2.0\n",
            "Name: 1999-02-08 00:00:00, dtype: float64  future value 0.9432759477868609\n",
            "loop 1 market state : step  379 market state fut market_state    2.0\n",
            "Name: 1999-02-09 00:00:00, dtype: float64  future value 0.9297254064791513\n",
            "loop 1 market state : step  380 market state fut market_state    2.0\n",
            "Name: 1999-02-10 00:00:00, dtype: float64  future value 0.939789589008174\n",
            "loop 1 market state : step  381 market state fut market_state    1.0\n",
            "Name: 1999-02-11 00:00:00, dtype: float64  future value 0.9412630927370879\n",
            "loop 1 market state : step  382 market state fut market_state    3.0\n",
            "Name: 1999-02-12 00:00:00, dtype: float64  future value 0.9662678724449425\n",
            "loop 1 market state : step  383 market state fut market_state    2.0\n",
            "Name: 1999-02-16 00:00:00, dtype: float64  future value 0.9655387236075288\n",
            "loop 1 market state : step  384 market state fut market_state    2.0\n",
            "Name: 1999-02-17 00:00:00, dtype: float64  future value 0.9520413107692799\n",
            "loop 1 market state : step  385 market state fut market_state    3.0\n",
            "Name: 1999-02-18 00:00:00, dtype: float64  future value 0.945668583309427\n",
            "loop 1 market state : step  386 market state fut market_state    2.0\n",
            "Name: 1999-02-19 00:00:00, dtype: float64  future value 0.9405870723691347\n",
            "loop 1 market state : step  387 market state fut market_state    2.0\n",
            "Name: 1999-02-22 00:00:00, dtype: float64  future value 0.9389388844579106\n",
            "loop 1 market state : step  388 market state fut market_state    1.0\n",
            "Name: 1999-02-23 00:00:00, dtype: float64  future value 0.9308419388164015\n",
            "loop 1 market state : step  389 market state fut market_state    1.0\n",
            "Name: 1999-02-24 00:00:00, dtype: float64  future value 0.9325129358087553\n",
            "loop 1 market state : step  390 market state fut market_state    2.0\n",
            "Name: 1999-02-25 00:00:00, dtype: float64  future value 0.9468990683324837\n",
            "loop 1 market state : step  391 market state fut market_state    3.0\n",
            "Name: 1999-02-26 00:00:00, dtype: float64  future value 0.9687971770146028\n",
            "loop 1 market state : step  392 market state fut market_state    4.0\n",
            "Name: 1999-03-01 00:00:00, dtype: float64  future value 0.9743115968971491\n",
            "loop 1 market state : step  393 market state fut market_state    4.0\n",
            "Name: 1999-03-02 00:00:00, dtype: float64  future value 0.9721164546380227\n",
            "loop 1 market state : step  394 market state fut market_state    4.0\n",
            "Name: 1999-03-03 00:00:00, dtype: float64  future value 0.977433381257129\n",
            "loop 1 market state : step  395 market state fut market_state    4.0\n",
            "Name: 1999-03-04 00:00:00, dtype: float64  future value 0.9856670886655742\n",
            "loop 1 market state : step  396 market state fut market_state    4.0\n",
            "Name: 1999-03-05 00:00:00, dtype: float64  future value 0.9833199785854253\n",
            "loop 1 market state : step  397 market state fut market_state    4.0\n",
            "Name: 1999-03-08 00:00:00, dtype: float64  future value 0.9929436491451509\n",
            "loop 1 market state : step  398 market state fut market_state    4.0\n",
            "Name: 1999-03-09 00:00:00, dtype: float64  future value 0.9922752318042409\n",
            "loop 1 market state : step  399 market state fut market_state    4.0\n",
            "Name: 1999-03-10 00:00:00, dtype: float64  future value 0.9857733456044954\n",
            "loop 1 market state : step  400 market state fut market_state    4.0\n",
            "Name: 1999-03-11 00:00:00, dtype: float64  future value 1.0\n",
            "loop 1 market state : step  401 market state fut market_state    4.0\n",
            "Name: 1999-03-12 00:00:00, dtype: float64  future value 0.9868899706615876\n",
            "loop 1 market state : step  402 market state fut market_state    2.0\n",
            "Name: 1999-03-15 00:00:00, dtype: float64  future value 0.985158149452888\n",
            "loop 1 market state : step  403 market state fut market_state    2.0\n",
            "Name: 1999-03-16 00:00:00, dtype: float64  future value 0.9586722629890763\n",
            "loop 1 market state : step  404 market state fut market_state    2.0\n",
            "Name: 1999-03-17 00:00:00, dtype: float64  future value 0.9635713940001732\n",
            "loop 1 market state : step  405 market state fut market_state    2.0\n",
            "Name: 1999-03-18 00:00:00, dtype: float64  future value 0.9798260167796953\n",
            "loop 1 market state : step  406 market state fut market_state    2.0\n",
            "Name: 1999-03-19 00:00:00, dtype: float64  future value 0.9743648180864516\n",
            "loop 1 market state : step  407 market state fut market_state   -1.0\n",
            "Name: 1999-03-22 00:00:00, dtype: float64  future value nan\n",
            "loop 1 market state : step  408 market state fut market_state   -1.0\n",
            "Name: 1999-03-23 00:00:00, dtype: float64  future value nan\n",
            "loop 1 market state : step  409 market state fut market_state   -1.0\n",
            "Name: 1999-03-24 00:00:00, dtype: float64  future value nan\n",
            "loop 1 market state : step  410 market state fut market_state   -1.0\n",
            "Name: 1999-03-25 00:00:00, dtype: float64  future value nan\n",
            "loop 1 market state : step  411 market state fut market_state   -1.0\n",
            "Name: 1999-03-26 00:00:00, dtype: float64  future value nan\n",
            "loop 2 image : step  0\n",
            "loop 2 image : step  1\n",
            "loop 2 image : step  2\n",
            "loop 2 image : step  3\n",
            "loop 2 image : step  4\n",
            "loop 2 image : step  5\n",
            "loop 2 image : step  6\n",
            "loop 2 image : step  7\n",
            "loop 2 image : step  8\n",
            "loop 2 image : step  9\n",
            "loop 2 image : step  10\n",
            "loop 2 image : step  11\n",
            "loop 2 image : step  12\n",
            "loop 2 image : step  13\n",
            "loop 2 image : step  14\n",
            "loop 2 image : step  15\n",
            "loop 2 image : step  16\n",
            "loop 2 image : step  17\n",
            "loop 2 image : step  18\n",
            "loop 2 image : step  19\n",
            "loop 2 image : step  20\n",
            "loop 2 image : step  21\n",
            "loop 2 image : step  22\n",
            "loop 2 image : step  23\n",
            "loop 2 image : step  24\n",
            "loop 2 image : step  25\n",
            "loop 2 image : step  26\n",
            "loop 2 image : step  27\n",
            "loop 2 image : step  28\n",
            "loop 2 image : step  29\n",
            "loop 2 image : step  30\n",
            "loop 2 image : step  31\n",
            "loop 2 image : step  32\n",
            "loop 2 image : step  33\n",
            "loop 2 image : step  34\n",
            "loop 2 image : step  35\n",
            "loop 2 image : step  36\n",
            "loop 2 image : step  37\n",
            "loop 2 image : step  38\n",
            "loop 2 image : step  39\n",
            "loop 2 image : step  40\n",
            "loop 2 image : step  41\n",
            "loop 2 image : step  42\n",
            "loop 2 image : step  43\n",
            "loop 2 image : step  44\n",
            "loop 2 image : step  45\n",
            "loop 2 image : step  46\n",
            "loop 2 image : step  47\n",
            "loop 2 image : step  48\n",
            "loop 2 image : step  49\n",
            "loop 2 image : step  50\n",
            "loop 2 image : step  51\n",
            "loop 2 image : step  52\n",
            "loop 2 image : step  53\n",
            "loop 2 image : step  54\n",
            "loop 2 image : step  55\n",
            "loop 2 image : step  56\n",
            "loop 2 image : step  57\n",
            "loop 2 image : step  58\n",
            "loop 2 image : step  59\n",
            "loop 2 image : step  60\n",
            "loop 2 image : step  61\n",
            "loop 2 image : step  62\n",
            "loop 2 image : step  63\n",
            "loop 2 image : step  64\n",
            "loop 2 image : step  65\n",
            "loop 2 image : step  66\n",
            "loop 2 image : step  67\n",
            "loop 2 image : step  68\n",
            "loop 2 image : step  69\n",
            "loop 2 image : step  70\n",
            "loop 2 image : step  71\n",
            "loop 2 image : step  72\n",
            "loop 2 image : step  73\n",
            "loop 2 image : step  74\n",
            "loop 2 image : step  75\n",
            "loop 2 image : step  76\n",
            "loop 2 image : step  77\n",
            "loop 2 image : step  78\n",
            "loop 2 image : step  79\n",
            "loop 2 image : step  80\n",
            "loop 2 image : step  81\n",
            "loop 2 image : step  82\n",
            "loop 2 image : step  83\n",
            "loop 2 image : step  84\n",
            "loop 2 image : step  85\n",
            "loop 2 image : step  86\n",
            "loop 2 image : step  87\n",
            "loop 2 image : step  88\n",
            "loop 2 image : step  89\n",
            "loop 2 image : step  90\n",
            "loop 2 image : step  91\n",
            "loop 2 image : step  92\n",
            "loop 2 image : step  93\n",
            "loop 2 image : step  94\n",
            "loop 2 image : step  95\n",
            "loop 2 image : step  96\n",
            "loop 2 image : step  97\n",
            "loop 2 image : step  98\n",
            "loop 2 image : step  99\n",
            "loop 2 image : step  100\n",
            "loop 2 image : step  101\n",
            "loop 2 image : step  102\n",
            "loop 2 image : step  103\n",
            "loop 2 image : step  104\n",
            "loop 2 image : step  105\n",
            "loop 2 image : step  106\n",
            "loop 2 image : step  107\n",
            "loop 2 image : step  108\n",
            "loop 2 image : step  109\n",
            "loop 2 image : step  110\n",
            "loop 2 image : step  111\n",
            "loop 2 image : step  112\n",
            "loop 2 image : step  113\n",
            "loop 2 image : step  114\n",
            "loop 2 image : step  115\n",
            "loop 2 image : step  116\n",
            "loop 2 image : step  117\n",
            "loop 2 image : step  118\n",
            "loop 2 image : step  119\n",
            "loop 2 image : step  120\n",
            "loop 2 image : step  121\n",
            "loop 2 image : step  122\n",
            "loop 2 image : step  123\n",
            "loop 2 image : step  124\n",
            "loop 2 image : step  125\n",
            "loop 2 image : step  126\n",
            "loop 2 image : step  127\n",
            "loop 2 image : step  128\n",
            "loop 2 image : step  129\n",
            "loop 2 image : step  130\n",
            "loop 2 image : step  131\n",
            "loop 2 image : step  132\n",
            "loop 2 image : step  133\n",
            "loop 2 image : step  134\n",
            "loop 2 image : step  135\n",
            "loop 2 image : step  136\n",
            "loop 2 image : step  137\n",
            "loop 2 image : step  138\n",
            "loop 2 image : step  139\n",
            "loop 2 image : step  140\n",
            "loop 2 image : step  141\n",
            "loop 2 image : step  142\n",
            "loop 2 image : step  143\n",
            "loop 2 image : step  144\n",
            "loop 2 image : step  145\n",
            "loop 2 image : step  146\n",
            "loop 2 image : step  147\n",
            "loop 2 image : step  148\n",
            "loop 2 image : step  149\n",
            "loop 2 image : step  150\n",
            "loop 2 image : step  151\n",
            "loop 2 image : step  152\n",
            "loop 2 image : step  153\n",
            "loop 2 image : step  154\n",
            "loop 2 image : step  155\n",
            "loop 2 image : step  156\n",
            "loop 2 image : step  157\n",
            "loop 2 image : step  158\n",
            "loop 2 image : step  159\n",
            "loop 2 image : step  160\n",
            "loop 2 image : step  161\n",
            "loop 2 image : step  162\n",
            "loop 2 image : step  163\n",
            "loop 2 image : step  164\n",
            "loop 2 image : step  165\n",
            "loop 2 image : step  166\n",
            "loop 2 image : step  167\n",
            "loop 2 image : step  168\n",
            "loop 2 image : step  169\n",
            "loop 2 image : step  170\n",
            "loop 2 image : step  171\n",
            "loop 2 image : step  172\n",
            "loop 2 image : step  173\n",
            "loop 2 image : step  174\n",
            "loop 2 image : step  175\n",
            "loop 2 image : step  176\n",
            "loop 2 image : step  177\n",
            "loop 2 image : step  178\n",
            "loop 2 image : step  179\n",
            "loop 2 image : step  180\n",
            "loop 2 image : step  181\n",
            "loop 2 image : step  182\n",
            "loop 2 image : step  183\n",
            "loop 2 image : step  184\n",
            "loop 2 image : step  185\n",
            "loop 2 image : step  186\n",
            "loop 2 image : step  187\n",
            "loop 2 image : step  188\n",
            "loop 2 image : step  189\n",
            "loop 2 image : step  190\n",
            "loop 2 image : step  191\n",
            "loop 2 image : step  192\n",
            "loop 2 image : step  193\n",
            "loop 2 image : step  194\n",
            "loop 2 image : step  195\n",
            "loop 2 image : step  196\n",
            "loop 2 image : step  197\n",
            "loop 2 image : step  198\n",
            "loop 2 image : step  199\n",
            "loop 2 image : step  200\n",
            "loop 2 image : step  201\n",
            "loop 2 image : step  202\n",
            "loop 2 image : step  203\n",
            "loop 2 image : step  204\n",
            "loop 2 image : step  205\n",
            "loop 2 image : step  206\n",
            "loop 2 image : step  207\n",
            "loop 2 image : step  208\n",
            "loop 2 image : step  209\n",
            "loop 2 image : step  210\n",
            "loop 2 image : step  211\n",
            "loop 2 image : step  212\n",
            "loop 2 image : step  213\n",
            "loop 2 image : step  214\n",
            "loop 2 image : step  215\n",
            "loop 2 image : step  216\n",
            "loop 2 image : step  217\n",
            "loop 2 image : step  218\n",
            "loop 2 image : step  219\n",
            "loop 2 image : step  220\n",
            "loop 2 image : step  221\n",
            "loop 2 image : step  222\n",
            "loop 2 image : step  223\n",
            "loop 2 image : step  224\n",
            "loop 2 image : step  225\n",
            "loop 2 image : step  226\n",
            "loop 2 image : step  227\n",
            "loop 2 image : step  228\n",
            "loop 2 image : step  229\n",
            "loop 2 image : step  230\n",
            "loop 2 image : step  231\n",
            "loop 2 image : step  232\n",
            "loop 2 image : step  233\n",
            "loop 2 image : step  234\n",
            "loop 2 image : step  235\n",
            "loop 2 image : step  236\n",
            "loop 2 image : step  237\n",
            "loop 2 image : step  238\n",
            "loop 2 image : step  239\n",
            "loop 2 image : step  240\n",
            "loop 2 image : step  241\n",
            "loop 2 image : step  242\n",
            "loop 2 image : step  243\n",
            "loop 2 image : step  244\n",
            "loop 2 image : step  245\n",
            "loop 2 image : step  246\n",
            "loop 2 image : step  247\n",
            "loop 2 image : step  248\n",
            "loop 2 image : step  249\n",
            "loop 2 image : step  250\n",
            "loop 2 image : step  251\n",
            "loop 2 image : step  252\n",
            "loop 2 image : step  253\n",
            "loop 2 image : step  254\n",
            "loop 2 image : step  255\n",
            "loop 2 image : step  256\n",
            "loop 2 image : step  257\n",
            "loop 2 image : step  258\n",
            "loop 2 image : step  259\n",
            "loop 2 image : step  260\n",
            "loop 2 image : step  261\n",
            "loop 2 image : step  262\n",
            "loop 2 image : step  263\n",
            "loop 2 image : step  264\n",
            "loop 2 image : step  265\n",
            "loop 2 image : step  266\n",
            "loop 2 image : step  267\n",
            "loop 2 image : step  268\n",
            "loop 2 image : step  269\n",
            "loop 2 image : step  270\n",
            "loop 2 image : step  271\n",
            "loop 2 image : step  272\n",
            "loop 2 image : step  273\n",
            "loop 2 image : step  274\n",
            "loop 2 image : step  275\n",
            "loop 2 image : step  276\n",
            "loop 2 image : step  277\n",
            "loop 2 image : step  278\n",
            "loop 2 image : step  279\n",
            "loop 2 image : step  280\n",
            "loop 2 image : step  281\n",
            "loop 2 image : step  282\n",
            "loop 2 image : step  283\n",
            "loop 2 image : step  284\n",
            "loop 2 image : step  285\n",
            "loop 2 image : step  286\n",
            "loop 2 image : step  287\n",
            "loop 2 image : step  288\n",
            "loop 2 image : step  289\n",
            "loop 2 image : step  290\n",
            "loop 2 image : step  291\n",
            "loop 2 image : step  292\n",
            "loop 2 image : step  293\n",
            "loop 2 image : step  294\n",
            "loop 2 image : step  295\n",
            "loop 2 image : step  296\n",
            "loop 2 image : step  297\n",
            "loop 2 image : step  298\n",
            "loop 2 image : step  299\n",
            "loop 2 image : step  300\n",
            "loop 2 image : step  301\n",
            "loop 2 image : step  302\n",
            "loop 2 image : step  303\n",
            "loop 2 image : step  304\n",
            "loop 2 image : step  305\n",
            "loop 2 image : step  306\n",
            "loop 2 image : step  307\n",
            "loop 2 image : step  308\n",
            "loop 2 image : step  309\n",
            "loop 2 image : step  310\n",
            "loop 2 image : step  311\n",
            "loop 2 image : step  312\n",
            "loop 2 image : step  313\n",
            "loop 2 image : step  314\n",
            "loop 2 image : step  315\n",
            "loop 2 image : step  316\n",
            "loop 2 image : step  317\n",
            "loop 2 image : step  318\n",
            "loop 2 image : step  319\n",
            "loop 2 image : step  320\n",
            "loop 2 image : step  321\n",
            "loop 2 image : step  322\n",
            "loop 2 image : step  323\n",
            "loop 2 image : step  324\n",
            "loop 2 image : step  325\n",
            "loop 2 image : step  326\n",
            "loop 2 image : step  327\n",
            "loop 2 image : step  328\n",
            "loop 2 image : step  329\n",
            "loop 2 image : step  330\n",
            "loop 2 image : step  331\n",
            "loop 2 image : step  332\n",
            "loop 2 image : step  333\n",
            "loop 2 image : step  334\n",
            "loop 2 image : step  335\n",
            "loop 2 image : step  336\n",
            "loop 2 image : step  337\n",
            "loop 2 image : step  338\n",
            "loop 2 image : step  339\n",
            "loop 2 image : step  340\n",
            "loop 2 image : step  341\n",
            "loop 2 image : step  342\n",
            "loop 2 image : step  343\n",
            "loop 2 image : step  344\n",
            "loop 2 image : step  345\n",
            "loop 2 image : step  346\n",
            "loop 2 image : step  347\n",
            "loop 2 image : step  348\n",
            "loop 2 image : step  349\n",
            "loop 2 image : step  350\n",
            "loop 2 image : step  351\n",
            "loop 2 image : step  352\n",
            "loop 2 image : step  353\n",
            "loop 2 image : step  354\n",
            "loop 2 image : step  355\n",
            "loop 2 image : step  356\n",
            "loop 2 image : step  357\n",
            "loop 2 image : step  358\n",
            "loop 2 image : step  359\n",
            "loop 2 image : step  360\n",
            "loop 2 image : step  361\n",
            "loop 2 image : step  362\n",
            "loop 2 image : step  363\n",
            "loop 2 image : step  364\n",
            "loop 2 image : step  365\n",
            "loop 2 image : step  366\n",
            "loop 2 image : step  367\n",
            "loop 2 image : step  368\n",
            "loop 2 image : step  369\n",
            "loop 2 image : step  370\n",
            "loop 2 image : step  371\n",
            "loop 2 image : step  372\n",
            "loop 2 image : step  373\n",
            "loop 2 image : step  374\n",
            "loop 2 image : step  375\n",
            "loop 2 image : step  376\n",
            "loop 2 image : step  377\n",
            "loop 2 image : step  378\n",
            "loop 2 image : step  379\n",
            "loop 2 image : step  380\n",
            "loop 2 image : step  381\n",
            "loop 2 image : step  382\n",
            "loop 2 image : step  383\n",
            "loop 2 image : step  384\n",
            "loop 2 image : step  385\n",
            "loop 2 image : step  386\n",
            "loop 2 image : step  387\n",
            "loop 2 image : step  388\n",
            "loop 2 image : step  389\n",
            "loop 2 image : step  390\n",
            "loop 2 image : step  391\n",
            "loop 2 image : step  392\n",
            "loop 2 image : step  393\n",
            "loop 2 image : step  394\n",
            "loop 2 image : step  395\n",
            "loop 2 image : step  396\n",
            "loop 2 image : step  397\n",
            "loop 2 image : step  398\n",
            "loop 2 image : step  399\n",
            "loop 2 image : step  400\n",
            "loop 2 image : step  401\n",
            "loop 2 image : step  402\n",
            "loop 2 image : step  403\n",
            "loop 2 image : step  404\n",
            "loop 2 image : step  405\n",
            "loop 2 image : step  406\n",
            "loop 2 image : step  407\n",
            "loop 2 image : step  408\n",
            "loop 2 image : step  409\n",
            "loop 2 image : step  410\n",
            "loop 2 image : step  411\n",
            "loop 1 market state : step  0 market state fut market_state    2.0\n",
            "Name: 1999-03-29 00:00:00, dtype: float64  future value 0.8627984028069475\n",
            "loop 1 market state : step  1 market state fut market_state    2.0\n",
            "Name: 1999-03-30 00:00:00, dtype: float64  future value 0.8686905376125473\n",
            "loop 1 market state : step  2 market state fut market_state    2.0\n",
            "Name: 1999-03-31 00:00:00, dtype: float64  future value 0.8798790245499224\n",
            "loop 1 market state : step  3 market state fut market_state    2.0\n",
            "Name: 1999-04-01 00:00:00, dtype: float64  future value 0.8827399801421759\n",
            "loop 1 market state : step  4 market state fut market_state   -1.0\n",
            "Name: 1999-04-05 00:00:00, dtype: float64  future value 0.8894701266335874\n",
            "loop 1 market state : step  5 market state fut market_state    2.0\n",
            "Name: 1999-04-06 00:00:00, dtype: float64  future value 0.8837023429802975\n",
            "loop 1 market state : step  6 market state fut market_state    2.0\n",
            "Name: 1999-04-07 00:00:00, dtype: float64  future value 0.8697052462120849\n",
            "loop 1 market state : step  7 market state fut market_state    1.0\n",
            "Name: 1999-04-08 00:00:00, dtype: float64  future value 0.8660455981929763\n",
            "loop 1 market state : step  8 market state fut market_state    1.0\n",
            "Name: 1999-04-09 00:00:00, dtype: float64  future value 0.8635250898429083\n",
            "loop 1 market state : step  9 market state fut market_state    1.0\n",
            "Name: 1999-04-12 00:00:00, dtype: float64  future value 0.8441988748937901\n",
            "loop 1 market state : step  10 market state fut market_state    1.0\n",
            "Name: 1999-04-13 00:00:00, dtype: float64  future value 0.8551255531068928\n",
            "loop 1 market state : step  11 market state fut market_state    2.0\n",
            "Name: 1999-04-14 00:00:00, dtype: float64  future value 0.8747332364097616\n",
            "loop 1 market state : step  12 market state fut market_state    2.0\n",
            "Name: 1999-04-15 00:00:00, dtype: float64  future value 0.8895944777858974\n",
            "loop 1 market state : step  13 market state fut market_state    2.0\n",
            "Name: 1999-04-16 00:00:00, dtype: float64  future value 0.8883047741252423\n",
            "loop 1 market state : step  14 market state fut market_state    2.0\n",
            "Name: 1999-04-19 00:00:00, dtype: float64  future value 0.8903932501299453\n",
            "loop 1 market state : step  15 market state fut market_state    2.0\n",
            "Name: 1999-04-20 00:00:00, dtype: float64  future value 0.8922001778637048\n",
            "loop 1 market state : step  16 market state fut market_state    2.0\n",
            "Name: 1999-04-21 00:00:00, dtype: float64  future value 0.8844160035137991\n",
            "loop 1 market state : step  17 market state fut market_state    1.0\n",
            "Name: 1999-04-22 00:00:00, dtype: float64  future value 0.8791261246746571\n",
            "loop 1 market state : step  18 market state fut market_state    1.0\n",
            "Name: 1999-04-23 00:00:00, dtype: float64  future value 0.8741178740236516\n",
            "loop 1 market state : step  19 market state fut market_state    1.0\n",
            "Name: 1999-04-26 00:00:00, dtype: float64  future value 0.8868514000533207\n",
            "loop 1 market state : step  20 market state fut market_state    1.0\n",
            "Name: 1999-04-27 00:00:00, dtype: float64  future value 0.8720359512287748\n",
            "loop 1 market state : step  21 market state fut market_state    1.0\n",
            "Name: 1999-04-28 00:00:00, dtype: float64  future value 0.8820591655749979\n",
            "loop 1 market state : step  22 market state fut market_state    1.0\n",
            "Name: 1999-04-29 00:00:00, dtype: float64  future value 0.8720687172779054\n",
            "loop 1 market state : step  23 market state fut market_state    2.0\n",
            "Name: 1999-04-30 00:00:00, dtype: float64  future value 0.8805468126146412\n",
            "loop 1 market state : step  24 market state fut market_state    1.0\n",
            "Name: 1999-05-03 00:00:00, dtype: float64  future value 0.8774698408497051\n",
            "loop 1 market state : step  25 market state fut market_state    3.0\n",
            "Name: 1999-05-04 00:00:00, dtype: float64  future value 0.8874929752787352\n",
            "loop 1 market state : step  26 market state fut market_state    4.0\n",
            "Name: 1999-05-05 00:00:00, dtype: float64  future value 0.8929857638709076\n",
            "loop 1 market state : step  27 market state fut market_state    4.0\n",
            "Name: 1999-05-06 00:00:00, dtype: float64  future value 0.8953164688875974\n",
            "loop 1 market state : step  28 market state fut market_state    2.0\n",
            "Name: 1999-05-07 00:00:00, dtype: float64  future value 0.8758331367370386\n",
            "loop 1 market state : step  29 market state fut market_state    2.0\n",
            "Name: 1999-05-10 00:00:00, dtype: float64  future value 0.8769395103569485\n",
            "loop 1 market state : step  30 market state fut market_state    2.0\n",
            "Name: 1999-05-11 00:00:00, dtype: float64  future value 0.8729000958366978\n",
            "loop 1 market state : step  31 market state fut market_state    2.0\n",
            "Name: 1999-05-12 00:00:00, dtype: float64  future value 0.880042694961189\n",
            "loop 1 market state : step  32 market state fut market_state    1.0\n",
            "Name: 1999-05-13 00:00:00, dtype: float64  future value 0.8765073980943905\n",
            "loop 1 market state : step  33 market state fut market_state    1.0\n",
            "Name: 1999-05-14 00:00:00, dtype: float64  future value 0.8709164711892126\n",
            "loop 1 market state : step  34 market state fut market_state    1.0\n",
            "Name: 1999-05-17 00:00:00, dtype: float64  future value 0.8554397875097739\n",
            "loop 1 market state : step  35 market state fut market_state    0.0\n",
            "Name: 1999-05-18 00:00:00, dtype: float64  future value 0.8408731209070409\n",
            "loop 1 market state : step  36 market state fut market_state    1.0\n",
            "Name: 1999-05-19 00:00:00, dtype: float64  future value 0.8542024296105347\n",
            "loop 1 market state : step  37 market state fut market_state    0.0\n",
            "Name: 1999-05-20 00:00:00, dtype: float64  future value 0.8389156291816672\n",
            "loop 1 market state : step  38 market state fut market_state    1.0\n",
            "Name: 1999-05-21 00:00:00, dtype: float64  future value 0.8522907304367506\n",
            "loop 1 market state : step  39 market state fut market_state    2.0\n",
            "Name: 1999-05-24 00:00:00, dtype: float64  future value 0.847328272337335\n",
            "loop 1 market state : step  40 market state fut market_state    0.0\n",
            "Name: 1999-05-25 00:00:00, dtype: float64  future value 0.8476883792089988\n",
            "loop 1 market state : step  41 market state fut market_state    1.0\n",
            "Name: 1999-05-26 00:00:00, dtype: float64  future value 0.8507850106034132\n",
            "loop 1 market state : step  42 market state fut market_state    2.0\n",
            "Name: 1999-05-27 00:00:00, dtype: float64  future value 0.8692535542372415\n",
            "loop 1 market state : step  43 market state fut market_state    2.0\n",
            "Name: 1999-05-28 00:00:00, dtype: float64  future value 0.8736857617610936\n",
            "loop 1 market state : step  44 market state fut market_state    2.0\n",
            "Name: 1999-06-01 00:00:00, dtype: float64  future value 0.8624317427254576\n",
            "loop 1 market state : step  45 market state fut market_state    2.0\n",
            "Name: 1999-06-02 00:00:00, dtype: float64  future value 0.8632894140407475\n",
            "loop 1 market state : step  46 market state fut market_state    2.0\n",
            "Name: 1999-06-03 00:00:00, dtype: float64  future value 0.852932305662165\n",
            "loop 1 market state : step  47 market state fut market_state    1.0\n",
            "Name: 1999-06-04 00:00:00, dtype: float64  future value 0.8469223729140813\n",
            "loop 1 market state : step  48 market state fut market_state    1.0\n",
            "Name: 1999-06-07 00:00:00, dtype: float64  future value 0.8471580487162422\n",
            "loop 1 market state : step  49 market state fut market_state    1.0\n",
            "Name: 1999-06-08 00:00:00, dtype: float64  future value 0.8518455916717335\n",
            "loop 1 market state : step  50 market state fut market_state    2.0\n",
            "Name: 1999-06-09 00:00:00, dtype: float64  future value 0.8709950297899328\n",
            "loop 1 market state : step  51 market state fut market_state    2.0\n",
            "Name: 1999-06-10 00:00:00, dtype: float64  future value 0.8772079522082399\n",
            "loop 1 market state : step  52 market state fut market_state    3.0\n",
            "Name: 1999-06-11 00:00:00, dtype: float64  future value 0.8791326778844832\n",
            "loop 1 market state : step  53 market state fut market_state    3.0\n",
            "Name: 1999-06-14 00:00:00, dtype: float64  future value 0.8831655391949078\n",
            "loop 1 market state : step  54 market state fut market_state    2.0\n",
            "Name: 1999-06-15 00:00:00, dtype: float64  future value 0.8745761192083211\n",
            "loop 1 market state : step  55 market state fut market_state    2.0\n",
            "Name: 1999-06-16 00:00:00, dtype: float64  future value 0.872729952132798\n",
            "loop 1 market state : step  56 market state fut market_state    1.0\n",
            "Name: 1999-06-17 00:00:00, dtype: float64  future value 0.8614170341259201\n",
            "loop 1 market state : step  57 market state fut market_state    1.0\n",
            "Name: 1999-06-18 00:00:00, dtype: float64  future value 0.8611093529328651\n",
            "loop 1 market state : step  58 market state fut market_state    1.0\n",
            "Name: 1999-06-21 00:00:00, dtype: float64  future value 0.8716103921760429\n",
            "loop 1 market state : step  59 market state fut market_state    4.0\n",
            "Name: 1999-06-22 00:00:00, dtype: float64  future value 0.8847694772584439\n",
            "loop 1 market state : step  60 market state fut market_state    4.0\n",
            "Name: 1999-06-23 00:00:00, dtype: float64  future value 0.8986880154259362\n",
            "loop 1 market state : step  61 market state fut market_state    4.0\n",
            "Name: 1999-06-24 00:00:00, dtype: float64  future value 0.9040891389977361\n",
            "loop 1 market state : step  62 market state fut market_state    4.0\n",
            "Name: 1999-06-25 00:00:00, dtype: float64  future value 0.9108061790694953\n",
            "loop 1 market state : step  63 market state fut market_state    4.0\n",
            "Name: 1999-06-28 00:00:00, dtype: float64  future value 0.9087766819532274\n",
            "loop 1 market state : step  64 market state fut market_state    4.0\n",
            "Name: 1999-06-29 00:00:00, dtype: float64  future value 0.9138439114926677\n",
            "loop 1 market state : step  65 market state fut market_state    4.0\n",
            "Name: 1999-06-30 00:00:00, dtype: float64  future value 0.9129012082840244\n",
            "loop 1 market state : step  66 market state fut market_state    4.0\n",
            "Name: 1999-07-01 00:00:00, dtype: float64  future value 0.9187016780692517\n",
            "loop 1 market state : step  67 market state fut market_state    4.0\n",
            "Name: 1999-07-02 00:00:00, dtype: float64  future value 0.9159650736293082\n",
            "loop 1 market state : step  68 market state fut market_state    4.0\n",
            "Name: 1999-07-06 00:00:00, dtype: float64  future value 0.9123381916593303\n",
            "loop 1 market state : step  69 market state fut market_state    4.0\n",
            "Name: 1999-07-07 00:00:00, dtype: float64  future value 0.9153562644530243\n",
            "loop 1 market state : step  70 market state fut market_state    4.0\n",
            "Name: 1999-07-08 00:00:00, dtype: float64  future value 0.9228523373221603\n",
            "loop 1 market state : step  71 market state fut market_state    4.0\n",
            "Name: 1999-07-09 00:00:00, dtype: float64  future value 0.9288492435677848\n",
            "loop 1 market state : step  72 market state fut market_state    4.0\n",
            "Name: 1999-07-12 00:00:00, dtype: float64  future value 0.9215626336615053\n",
            "loop 1 market state : step  73 market state fut market_state    2.0\n",
            "Name: 1999-07-13 00:00:00, dtype: float64  future value 0.901562077437842\n",
            "loop 1 market state : step  74 market state fut market_state    2.0\n",
            "Name: 1999-07-14 00:00:00, dtype: float64  future value 0.9029958717974783\n",
            "loop 1 market state : step  75 market state fut market_state    2.0\n",
            "Name: 1999-07-15 00:00:00, dtype: float64  future value 0.8910020593062292\n",
            "loop 1 market state : step  76 market state fut market_state    2.0\n",
            "Name: 1999-07-16 00:00:00, dtype: float64  future value 0.8883636730964843\n",
            "loop 1 market state : step  77 market state fut market_state    2.0\n",
            "Name: 1999-07-19 00:00:00, dtype: float64  future value 0.8823537403484006\n",
            "loop 1 market state : step  78 market state fut market_state    2.0\n",
            "Name: 1999-07-20 00:00:00, dtype: float64  future value 0.8922263107858162\n",
            "loop 1 market state : step  79 market state fut market_state    2.0\n",
            "Name: 1999-07-21 00:00:00, dtype: float64  future value 0.8939023341574395\n",
            "loop 1 market state : step  80 market state fut market_state    2.0\n",
            "Name: 1999-07-22 00:00:00, dtype: float64  future value 0.8779477456638529\n",
            "loop 1 market state : step  81 market state fut market_state    1.0\n",
            "Name: 1999-07-23 00:00:00, dtype: float64  future value 0.8698885762528299\n",
            "loop 1 market state : step  82 market state fut market_state    1.0\n",
            "Name: 1999-07-26 00:00:00, dtype: float64  future value 0.8694499906976387\n",
            "loop 1 market state : step  83 market state fut market_state    1.0\n",
            "Name: 1999-07-27 00:00:00, dtype: float64  future value 0.8656070126377853\n",
            "loop 1 market state : step  84 market state fut market_state    0.0\n",
            "Name: 1999-07-28 00:00:00, dtype: float64  future value 0.8545755629846579\n",
            "loop 1 market state : step  85 market state fut market_state    0.0\n",
            "Name: 1999-07-29 00:00:00, dtype: float64  future value 0.860061798367004\n",
            "loop 1 market state : step  86 market state fut market_state    0.0\n",
            "Name: 1999-07-30 00:00:00, dtype: float64  future value 0.8512760218372132\n",
            "loop 1 market state : step  87 market state fut market_state    0.0\n",
            "Name: 1999-08-02 00:00:00, dtype: float64  future value 0.8496458709343726\n",
            "loop 1 market state : step  88 market state fut market_state    0.0\n",
            "Name: 1999-08-03 00:00:00, dtype: float64  future value 0.8389287356013193\n",
            "loop 1 market state : step  89 market state fut market_state    0.0\n",
            "Name: 1999-08-04 00:00:00, dtype: float64  future value 0.8523497093251856\n",
            "loop 1 market state : step  90 market state fut market_state    0.0\n",
            "Name: 1999-08-05 00:00:00, dtype: float64  future value 0.8498815467365335\n",
            "loop 1 market state : step  91 market state fut market_state    2.0\n",
            "Name: 1999-08-06 00:00:00, dtype: float64  future value 0.8692077616856518\n",
            "loop 1 market state : step  92 market state fut market_state    2.0\n",
            "Name: 1999-08-09 00:00:00, dtype: float64  future value 0.8712307055920937\n",
            "loop 1 market state : step  93 market state fut market_state    2.0\n",
            "Name: 1999-08-10 00:00:00, dtype: float64  future value 0.8799969024095993\n",
            "loop 1 market state : step  94 market state fut market_state    2.0\n",
            "Name: 1999-08-11 00:00:00, dtype: float64  future value 0.8725858614338168\n",
            "loop 1 market state : step  95 market state fut market_state    2.0\n",
            "Name: 1999-08-12 00:00:00, dtype: float64  future value 0.8665300562169502\n",
            "loop 1 market state : step  96 market state fut market_state    2.0\n",
            "Name: 1999-08-13 00:00:00, dtype: float64  future value 0.8750540240224689\n",
            "loop 1 market state : step  97 market state fut market_state    2.0\n",
            "Name: 1999-08-16 00:00:00, dtype: float64  future value 0.8905110480724293\n",
            "loop 1 market state : step  98 market state fut market_state    2.0\n",
            "Name: 1999-08-17 00:00:00, dtype: float64  future value 0.8926584230483742\n",
            "loop 1 market state : step  99 market state fut market_state    2.0\n",
            "Name: 1999-08-18 00:00:00, dtype: float64  future value 0.9046325759101449\n",
            "loop 1 market state : step  100 market state fut market_state    2.0\n",
            "Name: 1999-08-19 00:00:00, dtype: float64  future value 0.8916829537906004\n",
            "loop 1 market state : step  101 market state fut market_state    2.0\n",
            "Name: 1999-08-20 00:00:00, dtype: float64  future value 0.88268763438076\n",
            "loop 1 market state : step  102 market state fut market_state    1.0\n",
            "Name: 1999-08-23 00:00:00, dtype: float64  future value 0.8668116044878938\n",
            "loop 1 market state : step  103 market state fut market_state    1.0\n",
            "Name: 1999-08-24 00:00:00, dtype: float64  future value 0.8644482133392664\n",
            "loop 1 market state : step  104 market state fut market_state    1.0\n",
            "Name: 1999-08-25 00:00:00, dtype: float64  future value 0.8714270621352979\n",
            "loop 1 market state : step  105 market state fut market_state    1.0\n",
            "Name: 1999-08-26 00:00:00, dtype: float64  future value 0.8635970952338026\n",
            "loop 1 market state : step  106 market state fut market_state    3.0\n",
            "Name: 1999-08-27 00:00:00, dtype: float64  future value 0.8885601095568816\n",
            "loop 1 market state : step  107 market state fut market_state    3.0\n",
            "Name: 1999-08-30 00:00:00, dtype: float64  future value 0.8841147956133772\n",
            "loop 1 market state : step  108 market state fut market_state    2.0\n",
            "Name: 1999-08-31 00:00:00, dtype: float64  future value 0.8799903491997731\n",
            "loop 1 market state : step  109 market state fut market_state    2.0\n",
            "Name: 1999-09-01 00:00:00, dtype: float64  future value 0.8822882881673325\n",
            "loop 1 market state : step  110 market state fut market_state    3.0\n",
            "Name: 1999-09-02 00:00:00, dtype: float64  future value 0.8849070147475991\n",
            "loop 1 market state : step  111 market state fut market_state    2.0\n",
            "Name: 1999-09-03 00:00:00, dtype: float64  future value 0.8799772427801209\n",
            "loop 1 market state : step  112 market state fut market_state    2.0\n",
            "Name: 1999-09-07 00:00:00, dtype: float64  future value 0.8748445610596125\n",
            "loop 1 market state : step  113 market state fut market_state    1.0\n",
            "Name: 1999-09-08 00:00:00, dtype: float64  future value 0.8628507485683634\n",
            "loop 1 market state : step  114 market state fut market_state    2.0\n",
            "Name: 1999-09-09 00:00:00, dtype: float64  future value 0.8631846426007228\n",
            "loop 1 market state : step  115 market state fut market_state    2.0\n",
            "Name: 1999-09-10 00:00:00, dtype: float64  future value 0.8742749912250922\n",
            "loop 1 market state : step  116 market state fut market_state    2.0\n",
            "Name: 1999-09-13 00:00:00, dtype: float64  future value 0.8743469966159864\n",
            "loop 1 market state : step  117 market state fut market_state    2.0\n",
            "Name: 1999-09-14 00:00:00, dtype: float64  future value 0.8560485966860578\n",
            "loop 1 market state : step  118 market state fut market_state    2.0\n",
            "Name: 1999-09-15 00:00:00, dtype: float64  future value 0.857966849069668\n",
            "loop 1 market state : step  119 market state fut market_state    0.0\n",
            "Name: 1999-09-16 00:00:00, dtype: float64  future value 0.8382609475366005\n",
            "loop 1 market state : step  120 market state fut market_state    0.0\n",
            "Name: 1999-09-17 00:00:00, dtype: float64  future value 0.83626413655227\n",
            "loop 1 market state : step  121 market state fut market_state    0.0\n",
            "Name: 1999-09-20 00:00:00, dtype: float64  future value 0.8401595402907324\n",
            "loop 1 market state : step  122 market state fut market_state    0.0\n",
            "Name: 1999-09-21 00:00:00, dtype: float64  future value 0.8394327733375785\n",
            "loop 1 market state : step  123 market state fut market_state    0.0\n",
            "Name: 1999-09-22 00:00:00, dtype: float64  future value 0.8303785549564963\n",
            "loop 1 market state : step  124 market state fut market_state    0.0\n",
            "Name: 1999-09-23 00:00:00, dtype: float64  future value 0.839766667369938\n",
            "loop 1 market state : step  125 market state fut market_state    2.0\n",
            "Name: 1999-09-24 00:00:00, dtype: float64  future value 0.839832199468199\n",
            "loop 1 market state : step  126 market state fut market_state    2.0\n",
            "Name: 1999-09-27 00:00:00, dtype: float64  future value 0.8540976581705101\n",
            "loop 1 market state : step  127 market state fut market_state    2.0\n",
            "Name: 1999-09-28 00:00:00, dtype: float64  future value 0.8519699428240435\n",
            "loop 1 market state : step  128 market state fut market_state    2.0\n",
            "Name: 1999-09-29 00:00:00, dtype: float64  future value 0.8677150683547735\n",
            "loop 1 market state : step  129 market state fut market_state    2.0\n",
            "Name: 1999-09-30 00:00:00, dtype: float64  future value 0.8626347323956809\n",
            "loop 1 market state : step  130 market state fut market_state    2.0\n",
            "Name: 1999-10-01 00:00:00, dtype: float64  future value 0.8746677842286936\n",
            "loop 1 market state : step  131 market state fut market_state    2.0\n",
            "Name: 1999-10-04 00:00:00, dtype: float64  future value 0.874137453735937\n",
            "loop 1 market state : step  132 market state fut market_state    2.0\n",
            "Name: 1999-10-05 00:00:00, dtype: float64  future value 0.8596232128118129\n",
            "loop 1 market state : step  133 market state fut market_state    1.0\n",
            "Name: 1999-10-06 00:00:00, dtype: float64  future value 0.8416260207823062\n",
            "loop 1 market state : step  134 market state fut market_state    1.0\n",
            "Name: 1999-10-07 00:00:00, dtype: float64  future value 0.8402315456816265\n",
            "loop 1 market state : step  135 market state fut market_state    0.0\n",
            "Name: 1999-10-08 00:00:00, dtype: float64  future value 0.8166564532494011\n",
            "loop 1 market state : step  136 market state fut market_state    0.0\n",
            "Name: 1999-10-11 00:00:00, dtype: float64  future value 0.8210558947241227\n",
            "loop 1 market state : step  137 market state fut market_state    0.0\n",
            "Name: 1999-10-12 00:00:00, dtype: float64  future value 0.8257630173918992\n",
            "loop 1 market state : step  138 market state fut market_state    2.0\n",
            "Name: 1999-10-13 00:00:00, dtype: float64  future value 0.8441661887618526\n",
            "loop 1 market state : step  139 market state fut market_state    2.0\n",
            "Name: 1999-10-14 00:00:00, dtype: float64  future value 0.8403558968339365\n",
            "loop 1 market state : step  140 market state fut market_state    2.0\n",
            "Name: 1999-10-15 00:00:00, dtype: float64  future value 0.8521663792844406\n",
            "loop 1 market state : step  141 market state fut market_state    2.0\n",
            "Name: 1999-10-18 00:00:00, dtype: float64  future value 0.8469158197042552\n",
            "loop 1 market state : step  142 market state fut market_state    2.0\n",
            "Name: 1999-10-19 00:00:00, dtype: float64  future value 0.8392429700042005\n",
            "loop 1 market state : step  143 market state fut market_state    2.0\n",
            "Name: 1999-10-20 00:00:00, dtype: float64  future value 0.8489322104008711\n",
            "loop 1 market state : step  144 market state fut market_state    4.0\n",
            "Name: 1999-10-21 00:00:00, dtype: float64  future value 0.878870789243018\n",
            "loop 1 market state : step  145 market state fut market_state    4.0\n",
            "Name: 1999-10-22 00:00:00, dtype: float64  future value 0.8922852896742511\n",
            "loop 1 market state : step  146 market state fut market_state    4.0\n",
            "Name: 1999-10-25 00:00:00, dtype: float64  future value 0.8865175060209614\n",
            "loop 1 market state : step  147 market state fut market_state    4.0\n",
            "Name: 1999-10-26 00:00:00, dtype: float64  future value 0.8823406339287484\n",
            "loop 1 market state : step  148 market state fut market_state    4.0\n",
            "Name: 1999-10-27 00:00:00, dtype: float64  future value 0.887047836513718\n",
            "loop 1 market state : step  149 market state fut market_state    4.0\n",
            "Name: 1999-10-28 00:00:00, dtype: float64  future value 0.89209540642368\n",
            "loop 1 market state : step  150 market state fut market_state    4.0\n",
            "Name: 1999-10-29 00:00:00, dtype: float64  future value 0.8970644177329219\n",
            "loop 1 market state : step  151 market state fut market_state    4.0\n",
            "Name: 1999-11-01 00:00:00, dtype: float64  future value 0.9015031784666\n",
            "loop 1 market state : step  152 market state fut market_state    4.0\n",
            "Name: 1999-11-02 00:00:00, dtype: float64  future value 0.8938237755567191\n",
            "loop 1 market state : step  153 market state fut market_state    4.0\n",
            "Name: 1999-11-03 00:00:00, dtype: float64  future value 0.8991790266597363\n",
            "loop 1 market state : step  154 market state fut market_state    4.0\n",
            "Name: 1999-11-04 00:00:00, dtype: float64  future value 0.9044164798202694\n",
            "loop 1 market state : step  155 market state fut market_state    4.0\n",
            "Name: 1999-11-05 00:00:00, dtype: float64  future value 0.9139748957719969\n",
            "loop 1 market state : step  156 market state fut market_state    4.0\n",
            "Name: 1999-11-08 00:00:00, dtype: float64  future value 0.9128815486545461\n",
            "loop 1 market state : step  157 market state fut market_state    4.0\n",
            "Name: 1999-11-09 00:00:00, dtype: float64  future value 0.9296937285462296\n",
            "loop 1 market state : step  158 market state fut market_state    4.0\n",
            "Name: 1999-11-10 00:00:00, dtype: float64  future value 0.9235659179384689\n",
            "loop 1 market state : step  159 market state fut market_state    4.0\n",
            "Name: 1999-11-11 00:00:00, dtype: float64  future value 0.9328820249610164\n",
            "loop 1 market state : step  160 market state fut market_state    4.0\n",
            "Name: 1999-11-12 00:00:00, dtype: float64  future value 0.9309572992847731\n",
            "loop 1 market state : step  161 market state fut market_state    4.0\n",
            "Name: 1999-11-15 00:00:00, dtype: float64  future value 0.9302632983807498\n",
            "loop 1 market state : step  162 market state fut market_state    2.0\n",
            "Name: 1999-11-16 00:00:00, dtype: float64  future value 0.9195920355164793\n",
            "loop 1 market state : step  163 market state fut market_state    3.0\n",
            "Name: 1999-11-17 00:00:00, dtype: float64  future value 0.9277362368208557\n",
            "loop 1 market state : step  164 market state fut market_state    2.0\n",
            "Name: 1999-11-18 00:00:00, dtype: float64  future value 0.9274351088376268\n",
            "loop 1 market state : step  165 market state fut market_state    2.0\n",
            "Name: 1999-11-19 00:00:00, dtype: float64  future value 0.9216804316039892\n",
            "loop 1 market state : step  166 market state fut market_state    2.0\n",
            "Name: 1999-11-22 00:00:00, dtype: float64  future value 0.9092939060263318\n",
            "loop 1 market state : step  167 market state fut market_state    2.0\n",
            "Name: 1999-11-23 00:00:00, dtype: float64  future value 0.9150616097624286\n",
            "loop 1 market state : step  168 market state fut market_state    2.0\n",
            "Name: 1999-11-24 00:00:00, dtype: float64  future value 0.922472650738211\n",
            "loop 1 market state : step  169 market state fut market_state    4.0\n",
            "Name: 1999-11-26 00:00:00, dtype: float64  future value 0.9383552338409034\n",
            "loop 1 market state : step  170 market state fut market_state    3.0\n",
            "Name: 1999-11-29 00:00:00, dtype: float64  future value 0.9318279971025223\n",
            "loop 1 market state : step  171 market state fut market_state    3.0\n",
            "Name: 1999-11-30 00:00:00, dtype: float64  future value 0.9225577625487574\n",
            "loop 1 market state : step  172 market state fut market_state    3.0\n",
            "Name: 1999-12-01 00:00:00, dtype: float64  future value 0.9190944710728531\n",
            "loop 1 market state : step  173 market state fut market_state    2.0\n",
            "Name: 1999-12-02 00:00:00, dtype: float64  future value 0.9218637616447342\n",
            "loop 1 market state : step  174 market state fut market_state    2.0\n",
            "Name: 1999-12-03 00:00:00, dtype: float64  future value 0.9277101038987443\n",
            "loop 1 market state : step  175 market state fut market_state    2.0\n",
            "Name: 1999-12-06 00:00:00, dtype: float64  future value 0.9265185385510949\n",
            "loop 1 market state : step  176 market state fut market_state    2.0\n",
            "Name: 1999-12-07 00:00:00, dtype: float64  future value 0.9186296726783576\n",
            "loop 1 market state : step  177 market state fut market_state    3.0\n",
            "Name: 1999-12-08 00:00:00, dtype: float64  future value 0.9252811806518558\n",
            "loop 1 market state : step  178 market state fut market_state    3.0\n",
            "Name: 1999-12-09 00:00:00, dtype: float64  future value 0.9288492435677848\n",
            "loop 1 market state : step  179 market state fut market_state    3.0\n",
            "Name: 1999-12-10 00:00:00, dtype: float64  future value 0.9303222772691848\n",
            "loop 1 market state : step  180 market state fut market_state    3.0\n",
            "Name: 1999-12-13 00:00:00, dtype: float64  future value 0.9283974716757485\n",
            "loop 1 market state : step  181 market state fut market_state    4.0\n",
            "Name: 1999-12-14 00:00:00, dtype: float64  future value 0.9384403456514498\n",
            "loop 1 market state : step  182 market state fut market_state    4.0\n",
            "Name: 1999-12-15 00:00:00, dtype: float64  future value 0.9402079541262526\n",
            "loop 1 market state : step  183 market state fut market_state    4.0\n",
            "Name: 1999-12-16 00:00:00, dtype: float64  future value 0.9547484078896811\n",
            "loop 1 market state : step  184 market state fut market_state    4.0\n",
            "Name: 1999-12-17 00:00:00, dtype: float64  future value 0.9539366090431738\n",
            "loop 1 market state : step  185 market state fut market_state    4.0\n",
            "Name: 1999-12-20 00:00:00, dtype: float64  future value 0.9543032691246638\n",
            "loop 1 market state : step  186 market state fut market_state    4.0\n",
            "Name: 1999-12-21 00:00:00, dtype: float64  future value 0.9581003747157345\n",
            "loop 1 market state : step  187 market state fut market_state    4.0\n",
            "Name: 1999-12-22 00:00:00, dtype: float64  future value 0.9587616095706273\n",
            "loop 1 market state : step  188 market state fut market_state    4.0\n",
            "Name: 1999-12-23 00:00:00, dtype: float64  future value 0.9618910070141722\n",
            "loop 1 market state : step  189 market state fut market_state    2.0\n",
            "Name: 1999-12-27 00:00:00, dtype: float64  future value 0.9527058043537608\n",
            "loop 1 market state : step  190 market state fut market_state    1.0\n",
            "Name: 1999-12-28 00:00:00, dtype: float64  future value 0.9161746165093576\n",
            "loop 1 market state : step  191 market state fut market_state    1.0\n",
            "Name: 1999-12-29 00:00:00, dtype: float64  future value 0.9179356717743343\n",
            "loop 1 market state : step  192 market state fut market_state    1.0\n",
            "Name: 1999-12-30 00:00:00, dtype: float64  future value 0.9188129228019096\n",
            "loop 1 market state : step  193 market state fut market_state    2.0\n",
            "Name: 1999-12-31 00:00:00, dtype: float64  future value 0.9437039317340944\n",
            "loop 1 market state : step  194 market state fut market_state    3.0\n",
            "Name: 2000-01-03 00:00:00, dtype: float64  future value 0.9542639498657072\n",
            "loop 1 market state : step  195 market state fut market_state    3.0\n",
            "Name: 2000-01-04 00:00:00, dtype: float64  future value 0.9417988656873294\n",
            "loop 1 market state : step  196 market state fut market_state    3.0\n",
            "Name: 2000-01-05 00:00:00, dtype: float64  future value 0.9376677861467062\n",
            "loop 1 market state : step  197 market state fut market_state    3.0\n",
            "Name: 2000-01-06 00:00:00, dtype: float64  future value 0.9490789223837828\n",
            "loop 1 market state : step  198 market state fut market_state    3.0\n",
            "Name: 2000-01-07 00:00:00, dtype: float64  future value 0.9592068282528375\n",
            "loop 1 market state : step  199 market state fut market_state    2.0\n",
            "Name: 2000-01-10 00:00:00, dtype: float64  future value 0.952653458592345\n",
            "loop 1 market state : step  200 market state fut market_state    3.0\n",
            "Name: 2000-01-11 00:00:00, dtype: float64  future value 0.9531510230359711\n",
            "loop 1 market state : step  201 market state fut market_state    3.0\n",
            "Name: 2000-01-12 00:00:00, dtype: float64  future value 0.946388110495429\n",
            "loop 1 market state : step  202 market state fut market_state    2.0\n",
            "Name: 2000-01-13 00:00:00, dtype: float64  future value 0.9436319263432003\n",
            "loop 1 market state : step  203 market state fut market_state    1.0\n",
            "Name: 2000-01-14 00:00:00, dtype: float64  future value 0.9175559851903852\n",
            "loop 1 market state : step  204 market state fut market_state    1.0\n",
            "Name: 2000-01-18 00:00:00, dtype: float64  future value 0.9231207791734516\n",
            "loop 1 market state : step  205 market state fut market_state    1.0\n",
            "Name: 2000-01-19 00:00:00, dtype: float64  future value 0.9192319286448154\n",
            "loop 1 market state : step  206 market state fut market_state    0.0\n",
            "Name: 2000-01-20 00:00:00, dtype: float64  future value 0.9156115998846635\n",
            "loop 1 market state : step  207 market state fut market_state    0.0\n",
            "Name: 2000-01-21 00:00:00, dtype: float64  future value 0.8904718087306657\n",
            "loop 1 market state : step  208 market state fut market_state    0.0\n",
            "Name: 2000-01-24 00:00:00, dtype: float64  future value 0.9129273412061358\n",
            "loop 1 market state : step  209 market state fut market_state    1.0\n",
            "Name: 2000-01-25 00:00:00, dtype: float64  future value 0.9226297679396517\n",
            "loop 1 market state : step  210 market state fut market_state    2.0\n",
            "Name: 2000-01-26 00:00:00, dtype: float64  future value 0.9225249964996269\n",
            "loop 1 market state : step  211 market state fut market_state    2.0\n",
            "Name: 2000-01-27 00:00:00, dtype: float64  future value 0.9329016845904947\n",
            "loop 1 market state : step  212 market state fut market_state    2.0\n",
            "Name: 2000-01-28 00:00:00, dtype: float64  future value 0.9325088915868933\n",
            "loop 1 market state : step  213 market state fut market_state    2.0\n",
            "Name: 2000-01-31 00:00:00, dtype: float64  future value 0.9324237797763469\n",
            "loop 1 market state : step  214 market state fut market_state    2.0\n",
            "Name: 2000-02-01 00:00:00, dtype: float64  future value 0.943867602145361\n",
            "loop 1 market state : step  215 market state fut market_state    2.0\n",
            "Name: 2000-02-02 00:00:00, dtype: float64  future value 0.9242205995835355\n",
            "loop 1 market state : step  216 market state fut market_state    1.0\n",
            "Name: 2000-02-03 00:00:00, dtype: float64  future value 0.9275725664095891\n",
            "loop 1 market state : step  217 market state fut market_state    1.0\n",
            "Name: 2000-02-04 00:00:00, dtype: float64  future value 0.9081220003081607\n",
            "loop 1 market state : step  218 market state fut market_state    1.0\n",
            "Name: 2000-02-07 00:00:00, dtype: float64  future value 0.9099681673836837\n",
            "loop 1 market state : step  219 market state fut market_state    1.0\n",
            "Name: 2000-02-08 00:00:00, dtype: float64  future value 0.9178964324325707\n",
            "loop 1 market state : step  220 market state fut market_state    1.0\n",
            "Name: 2000-02-09 00:00:00, dtype: float64  future value 0.9084821071798246\n",
            "loop 1 market state : step  221 market state fut market_state    1.0\n",
            "Name: 2000-02-10 00:00:00, dtype: float64  future value 0.9088683469735999\n",
            "loop 1 market state : step  222 market state fut market_state    0.0\n",
            "Name: 2000-02-11 00:00:00, dtype: float64  future value 0.8812603932309498\n",
            "loop 1 market state : step  223 market state fut market_state    0.0\n",
            "Name: 2000-02-14 00:00:00, dtype: float64  future value 0.8852409087799585\n",
            "loop 1 market state : step  224 market state fut market_state    1.0\n",
            "Name: 2000-02-15 00:00:00, dtype: float64  future value 0.8908187292654842\n",
            "loop 1 market state : step  225 market state fut market_state    0.0\n",
            "Name: 2000-02-16 00:00:00, dtype: float64  future value 0.886065814046118\n",
            "loop 1 market state : step  226 market state fut market_state    0.0\n",
            "Name: 2000-02-17 00:00:00, dtype: float64  future value 0.8729263086760023\n",
            "loop 1 market state : step  227 market state fut market_state    0.0\n",
            "Name: 2000-02-18 00:00:00, dtype: float64  future value 0.8825436235989716\n",
            "loop 1 market state : step  228 market state fut market_state    2.0\n",
            "Name: 2000-02-22 00:00:00, dtype: float64  future value 0.8945701222221583\n",
            "loop 1 market state : step  229 market state fut market_state    2.0\n",
            "Name: 2000-02-23 00:00:00, dtype: float64  future value 0.9029303396992172\n",
            "loop 1 market state : step  230 market state fut market_state    2.0\n",
            "Name: 2000-02-24 00:00:00, dtype: float64  future value 0.9046129162806666\n",
            "loop 1 market state : step  231 market state fut market_state    2.0\n",
            "Name: 2000-02-25 00:00:00, dtype: float64  future value 0.9225577625487574\n",
            "loop 1 market state : step  232 market state fut market_state    2.0\n",
            "Name: 2000-02-28 00:00:00, dtype: float64  future value 0.910845498328452\n",
            "loop 1 market state : step  233 market state fut market_state    1.0\n",
            "Name: 2000-02-29 00:00:00, dtype: float64  future value 0.8874995284885613\n",
            "loop 1 market state : step  234 market state fut market_state    1.0\n",
            "Name: 2000-03-01 00:00:00, dtype: float64  future value 0.8947533723457103\n",
            "loop 1 market state : step  235 market state fut market_state    2.0\n",
            "Name: 2000-03-02 00:00:00, dtype: float64  future value 0.9176606767132168\n",
            "loop 1 market state : step  236 market state fut market_state    1.0\n",
            "Name: 2000-03-03 00:00:00, dtype: float64  future value 0.9133266874195634\n",
            "loop 1 market state : step  237 market state fut market_state    2.0\n",
            "Name: 2000-03-06 00:00:00, dtype: float64  future value 0.9058306145504275\n",
            "loop 1 market state : step  238 market state fut market_state    2.0\n",
            "Name: 2000-03-07 00:00:00, dtype: float64  future value 0.8898105738757729\n",
            "loop 1 market state : step  239 market state fut market_state    2.0\n",
            "Name: 2000-03-08 00:00:00, dtype: float64  future value 0.9114085149531461\n",
            "loop 1 market state : step  240 market state fut market_state    4.0\n",
            "Name: 2000-03-09 00:00:00, dtype: float64  future value 0.9548335197002275\n",
            "loop 1 market state : step  241 market state fut market_state    4.0\n",
            "Name: 2000-03-10 00:00:00, dtype: float64  future value 0.9587616095706273\n",
            "loop 1 market state : step  242 market state fut market_state    4.0\n",
            "Name: 2000-03-13 00:00:00, dtype: float64  future value 0.9536289278501188\n",
            "loop 1 market state : step  243 market state fut market_state    4.0\n",
            "Name: 2000-03-14 00:00:00, dtype: float64  future value 0.9780092659190254\n",
            "loop 1 market state : step  244 market state fut market_state    4.0\n",
            "Name: 2000-03-15 00:00:00, dtype: float64  future value 0.9824414734428775\n",
            "loop 1 market state : step  245 market state fut market_state    4.0\n",
            "Name: 2000-03-16 00:00:00, dtype: float64  future value 0.9999279946091059\n",
            "loop 1 market state : step  246 market state fut market_state    4.0\n",
            "Name: 2000-03-17 00:00:00, dtype: float64  future value 1.0\n",
            "loop 1 market state : step  247 market state fut market_state    4.0\n",
            "Name: 2000-03-20 00:00:00, dtype: float64  future value 0.9976431620611986\n",
            "loop 1 market state : step  248 market state fut market_state    4.0\n",
            "Name: 2000-03-21 00:00:00, dtype: float64  future value 0.987083143929586\n",
            "loop 1 market state : step  249 market state fut market_state    4.0\n",
            "Name: 2000-03-22 00:00:00, dtype: float64  future value 0.9876003680026904\n",
            "loop 1 market state : step  250 market state fut market_state    2.0\n",
            "Name: 2000-03-23 00:00:00, dtype: float64  future value 0.974113942097756\n",
            "loop 1 market state : step  251 market state fut market_state    2.0\n",
            "Name: 2000-03-24 00:00:00, dtype: float64  future value 0.9810927908937875\n",
            "loop 1 market state : step  252 market state fut market_state    2.0\n",
            "Name: 2000-03-27 00:00:00, dtype: float64  future value 0.9859308978408932\n",
            "loop 1 market state : step  253 market state fut market_state    2.0\n",
            "Name: 2000-03-28 00:00:00, dtype: float64  future value 0.9785722825437195\n",
            "loop 1 market state : step  254 market state fut market_state    2.0\n",
            "Name: 2000-03-29 00:00:00, dtype: float64  future value 0.9737538352260922\n",
            "loop 1 market state : step  255 market state fut market_state    3.0\n",
            "Name: 2000-03-30 00:00:00, dtype: float64  future value 0.9828997186275469\n",
            "loop 1 market state : step  256 market state fut market_state    3.0\n",
            "Name: 2000-03-31 00:00:00, dtype: float64  future value 0.9927264965133727\n",
            "loop 1 market state : step  257 market state fut market_state    2.0\n",
            "Name: 2000-04-03 00:00:00, dtype: float64  future value 0.9849423221634671\n",
            "loop 1 market state : step  258 market state fut market_state    3.0\n",
            "Name: 2000-04-04 00:00:00, dtype: float64  future value 0.9824087073937469\n",
            "loop 1 market state : step  259 market state fut market_state    2.0\n",
            "Name: 2000-04-05 00:00:00, dtype: float64  future value 0.9605292979626231\n",
            "loop 1 market state : step  260 market state fut market_state    2.0\n",
            "Name: 2000-04-06 00:00:00, dtype: float64  future value 0.9430754629283322\n",
            "loop 1 market state : step  261 market state fut market_state    1.0\n",
            "Name: 2000-04-07 00:00:00, dtype: float64  future value 0.8881149707918643\n",
            "loop 1 market state : step  262 market state fut market_state    2.0\n",
            "Name: 2000-04-10 00:00:00, dtype: float64  future value 0.9174970063019502\n",
            "loop 1 market state : step  263 market state fut market_state    2.0\n",
            "Name: 2000-04-11 00:00:00, dtype: float64  future value 0.9437955967544669\n",
            "loop 1 market state : step  264 market state fut market_state    2.0\n",
            "Name: 2000-04-12 00:00:00, dtype: float64  future value 0.9345383887031613\n",
            "loop 1 market state : step  265 market state fut market_state    2.0\n",
            "Name: 2000-04-13 00:00:00, dtype: float64  future value 0.9391670326874106\n",
            "loop 1 market state : step  266 market state fut market_state    3.0\n",
            "Name: 2000-04-14 00:00:00, dtype: float64  future value 0.9361030874249338\n",
            "loop 1 market state : step  267 market state fut market_state    3.0\n",
            "Name: 2000-04-17 00:00:00, dtype: float64  future value 0.9672528113270153\n",
            "loop 1 market state : step  268 market state fut market_state    3.0\n",
            "Name: 2000-04-18 00:00:00, dtype: float64  future value 0.9564833302325463\n",
            "loop 1 market state : step  269 market state fut market_state    3.0\n",
            "Name: 2000-04-19 00:00:00, dtype: float64  future value 0.9590562642612231\n",
            "loop 1 market state : step  270 market state fut market_state    2.0\n",
            "Name: 2000-04-20 00:00:00, dtype: float64  future value 0.9508792969077161\n",
            "loop 1 market state : step  271 market state fut market_state    3.0\n",
            "Name: 2000-04-24 00:00:00, dtype: float64  future value 0.9612363253691055\n",
            "loop 1 market state : step  272 market state fut market_state    1.0\n",
            "Name: 2000-04-25 00:00:00, dtype: float64  future value 0.9468595420169438\n",
            "loop 1 market state : step  273 market state fut market_state    1.0\n",
            "Name: 2000-04-26 00:00:00, dtype: float64  future value 0.9264399799503746\n",
            "loop 1 market state : step  274 market state fut market_state    1.0\n",
            "Name: 2000-04-27 00:00:00, dtype: float64  future value 0.9228195712730297\n",
            "loop 1 market state : step  275 market state fut market_state    1.0\n",
            "Name: 2000-04-28 00:00:00, dtype: float64  future value 0.9379165683685192\n",
            "loop 1 market state : step  276 market state fut market_state    1.0\n",
            "Name: 2000-05-01 00:00:00, dtype: float64  future value 0.9323779872247572\n",
            "loop 1 market state : step  277 market state fut market_state    1.0\n",
            "Name: 2000-05-02 00:00:00, dtype: float64  future value 0.9245021478544792\n",
            "loop 1 market state : step  278 market state fut market_state    1.0\n",
            "Name: 2000-05-03 00:00:00, dtype: float64  future value 0.9054574811763043\n",
            "loop 1 market state : step  279 market state fut market_state    1.0\n",
            "Name: 2000-05-04 00:00:00, dtype: float64  future value 0.92166740510153\n",
            "loop 1 market state : step  280 market state fut market_state    1.0\n",
            "Name: 2000-05-05 00:00:00, dtype: float64  future value 0.930276404800402\n",
            "loop 1 market state : step  281 market state fut market_state    2.0\n",
            "Name: 2000-05-08 00:00:00, dtype: float64  future value 0.9508334244389334\n",
            "loop 1 market state : step  282 market state fut market_state    2.0\n",
            "Name: 2000-05-09 00:00:00, dtype: float64  future value 0.9597895045070101\n",
            "loop 1 market state : step  283 market state fut market_state    2.0\n",
            "Name: 2000-05-10 00:00:00, dtype: float64  future value 0.9478481176943698\n",
            "loop 1 market state : step  284 market state fut market_state    2.0\n",
            "Name: 2000-05-11 00:00:00, dtype: float64  future value 0.940914981532735\n",
            "loop 1 market state : step  285 market state fut market_state    1.0\n",
            "Name: 2000-05-12 00:00:00, dtype: float64  future value 0.9211043085596429\n",
            "loop 1 market state : step  286 market state fut market_state    1.0\n",
            "Name: 2000-05-15 00:00:00, dtype: float64  future value 0.9170256546976285\n",
            "loop 1 market state : step  287 market state fut market_state    1.0\n",
            "Name: 2000-05-16 00:00:00, dtype: float64  future value 0.8994409153012015\n",
            "loop 1 market state : step  288 market state fut market_state    1.0\n",
            "Name: 2000-05-17 00:00:00, dtype: float64  future value 0.9159323874973707\n",
            "loop 1 market state : step  289 market state fut market_state    1.0\n",
            "Name: 2000-05-18 00:00:00, dtype: float64  future value 0.904455799079226\n",
            "loop 1 market state : step  290 market state fut market_state    1.0\n",
            "Name: 2000-05-19 00:00:00, dtype: float64  future value 0.9021644133214928\n",
            "loop 1 market state : step  291 market state fut market_state    3.0\n",
            "Name: 2000-05-22 00:00:00, dtype: float64  future value 0.9312518740581759\n",
            "loop 1 market state : step  292 market state fut market_state    3.0\n",
            "Name: 2000-05-23 00:00:00, dtype: float64  future value 0.9300407289982412\n",
            "loop 1 market state : step  293 market state fut market_state    3.0\n",
            "Name: 2000-05-24 00:00:00, dtype: float64  future value 0.9485093525492626\n",
            "loop 1 market state : step  294 market state fut market_state    3.0\n",
            "Name: 2000-05-25 00:00:00, dtype: float64  future value 0.9671350133845315\n",
            "loop 1 market state : step  295 market state fut market_state    3.0\n",
            "Name: 2000-05-26 00:00:00, dtype: float64  future value 0.960830425945852\n",
            "loop 1 market state : step  296 market state fut market_state    3.0\n",
            "Name: 2000-05-30 00:00:00, dtype: float64  future value 0.9544210670671477\n",
            "loop 1 market state : step  297 market state fut market_state    2.0\n",
            "Name: 2000-05-31 00:00:00, dtype: float64  future value 0.9632723756951996\n",
            "loop 1 market state : step  298 market state fut market_state    3.0\n",
            "Name: 2000-06-01 00:00:00, dtype: float64  future value 0.9569285489147565\n",
            "loop 1 market state : step  299 market state fut market_state    1.0\n",
            "Name: 2000-06-02 00:00:00, dtype: float64  future value 0.9538383908129753\n",
            "loop 1 market state : step  300 market state fut market_state    1.0\n",
            "Name: 2000-06-05 00:00:00, dtype: float64  future value 0.9466696587663727\n",
            "loop 1 market state : step  301 market state fut market_state    3.0\n",
            "Name: 2000-06-06 00:00:00, dtype: float64  future value 0.9620153581664822\n",
            "loop 1 market state : step  302 market state fut market_state    2.0\n",
            "Name: 2000-06-07 00:00:00, dtype: float64  future value 0.96273557190981\n",
            "loop 1 market state : step  303 market state fut market_state    4.0\n",
            "Name: 2000-06-08 00:00:00, dtype: float64  future value 0.9680973762226531\n",
            "loop 1 market state : step  304 market state fut market_state    3.0\n",
            "Name: 2000-06-09 00:00:00, dtype: float64  future value 0.9587550563608012\n",
            "loop 1 market state : step  305 market state fut market_state    4.0\n",
            "Name: 2000-06-12 00:00:00, dtype: float64  future value 0.9728569245690385\n",
            "loop 1 market state : step  306 market state fut market_state    3.0\n",
            "Name: 2000-06-13 00:00:00, dtype: float64  future value 0.9662773420692415\n",
            "loop 1 market state : step  307 market state fut market_state    4.0\n",
            "Name: 2000-06-14 00:00:00, dtype: float64  future value 0.9683592648641184\n",
            "loop 1 market state : step  308 market state fut market_state    2.0\n",
            "Name: 2000-06-15 00:00:00, dtype: float64  future value 0.9507156264964495\n",
            "loop 1 market state : step  309 market state fut market_state    2.0\n",
            "Name: 2000-06-16 00:00:00, dtype: float64  future value 0.9437104849439205\n",
            "loop 1 market state : step  310 market state fut market_state    2.0\n",
            "Name: 2000-06-19 00:00:00, dtype: float64  future value 0.9527647832421957\n",
            "loop 1 market state : step  311 market state fut market_state    1.0\n",
            "Name: 2000-06-20 00:00:00, dtype: float64  future value 0.949648492218303\n",
            "loop 1 market state : step  312 market state fut market_state    1.0\n",
            "Name: 2000-06-21 00:00:00, dtype: float64  future value 0.9524439157122956\n",
            "loop 1 market state : step  313 market state fut market_state    1.0\n",
            "Name: 2000-06-22 00:00:00, dtype: float64  future value 0.9443062676177452\n",
            "loop 1 market state : step  314 market state fut market_state    3.0\n",
            "Name: 2000-06-23 00:00:00, dtype: float64  future value 0.9522999049305072\n",
            "loop 1 market state : step  315 market state fut market_state    3.0\n",
            "Name: 2000-06-26 00:00:00, dtype: float64  future value 0.9620808902647433\n",
            "loop 1 market state : step  316 market state fut market_state    2.0\n",
            "Name: 2000-06-27 00:00:00, dtype: float64  future value 0.946820222757987\n",
            "loop 1 market state : step  317 market state fut market_state    3.0\n",
            "Name: 2000-06-28 00:00:00, dtype: float64  future value 0.9536551406894233\n",
            "loop 1 market state : step  318 market state fut market_state    3.0\n",
            "Name: 2000-06-29 00:00:00, dtype: float64  future value 0.968208700872504\n",
            "loop 1 market state : step  319 market state fut market_state    3.0\n",
            "Name: 2000-06-30 00:00:00, dtype: float64  future value 0.966061325896559\n",
            "loop 1 market state : step  320 market state fut market_state    3.0\n",
            "Name: 2000-07-03 00:00:00, dtype: float64  future value 0.9695049577429851\n",
            "loop 1 market state : step  321 market state fut market_state    4.0\n",
            "Name: 2000-07-05 00:00:00, dtype: float64  future value 0.9773873503230892\n",
            "loop 1 market state : step  322 market state fut market_state    4.0\n",
            "Name: 2000-07-06 00:00:00, dtype: float64  future value 0.9792989695796803\n",
            "loop 1 market state : step  323 market state fut market_state    4.0\n",
            "Name: 2000-07-07 00:00:00, dtype: float64  future value 0.9885561776309859\n",
            "loop 1 market state : step  324 market state fut market_state    4.0\n",
            "Name: 2000-07-10 00:00:00, dtype: float64  future value 0.9888900716633453\n",
            "loop 1 market state : step  325 market state fut market_state    4.0\n",
            "Name: 2000-07-11 00:00:00, dtype: float64  future value 0.977924154108479\n",
            "loop 1 market state : step  326 market state fut market_state    2.0\n",
            "Name: 2000-07-12 00:00:00, dtype: float64  future value 0.9702119851494675\n",
            "loop 1 market state : step  327 market state fut market_state    2.0\n",
            "Name: 2000-07-13 00:00:00, dtype: float64  future value 0.9791221927487614\n",
            "loop 1 market state : step  328 market state fut market_state    2.0\n",
            "Name: 2000-07-14 00:00:00, dtype: float64  future value 0.9690531858509487\n",
            "loop 1 market state : step  329 market state fut market_state    2.0\n",
            "Name: 2000-07-17 00:00:00, dtype: float64  future value 0.9586438116281434\n",
            "loop 1 market state : step  330 market state fut market_state    2.0\n",
            "Name: 2000-07-18 00:00:00, dtype: float64  future value 0.9653084260212937\n",
            "loop 1 market state : step  331 market state fut market_state    1.0\n",
            "Name: 2000-07-19 00:00:00, dtype: float64  future value 0.95087274369789\n",
            "loop 1 market state : step  332 market state fut market_state    1.0\n",
            "Name: 2000-07-20 00:00:00, dtype: float64  future value 0.9490396031248262\n",
            "loop 1 market state : step  333 market state fut market_state    0.0\n",
            "Name: 2000-07-21 00:00:00, dtype: float64  future value 0.9295759306037457\n",
            "loop 1 market state : step  334 market state fut market_state    0.0\n",
            "Name: 2000-07-24 00:00:00, dtype: float64  future value 0.9367381094405222\n",
            "loop 1 market state : step  335 market state fut market_state    0.0\n",
            "Name: 2000-07-25 00:00:00, dtype: float64  future value 0.9414976577869075\n",
            "loop 1 market state : step  336 market state fut market_state    0.0\n",
            "Name: 2000-07-26 00:00:00, dtype: float64  future value 0.9418904507905089\n",
            "loop 1 market state : step  337 market state fut market_state    2.0\n",
            "Name: 2000-07-27 00:00:00, dtype: float64  future value 0.9509644087182625\n",
            "loop 1 market state : step  338 market state fut market_state    3.0\n",
            "Name: 2000-07-28 00:00:00, dtype: float64  future value 0.957753454180916\n",
            "loop 1 market state : step  339 market state fut market_state    3.0\n",
            "Name: 2000-07-31 00:00:00, dtype: float64  future value 0.9684836160164284\n",
            "loop 1 market state : step  340 market state fut market_state    3.0\n",
            "Name: 2000-08-01 00:00:00, dtype: float64  future value 0.9707619752717025\n",
            "loop 1 market state : step  341 market state fut market_state    3.0\n",
            "Name: 2000-08-02 00:00:00, dtype: float64  future value 0.9642609513726257\n",
            "loop 1 market state : step  342 market state fut market_state    3.0\n",
            "Name: 2000-08-03 00:00:00, dtype: float64  future value 0.9559988722085724\n",
            "loop 1 market state : step  343 market state fut market_state    3.0\n",
            "Name: 2000-08-04 00:00:00, dtype: float64  future value 0.9635866100980808\n",
            "loop 1 market state : step  344 market state fut market_state    3.0\n",
            "Name: 2000-08-07 00:00:00, dtype: float64  future value 0.9764969928758618\n",
            "loop 1 market state : step  345 market state fut market_state    3.0\n",
            "Name: 2000-08-08 00:00:00, dtype: float64  future value 0.9718291095498488\n",
            "loop 1 market state : step  346 market state fut market_state    3.0\n",
            "Name: 2000-08-09 00:00:00, dtype: float64  future value 0.96883061646844\n",
            "loop 1 market state : step  347 market state fut market_state    3.0\n",
            "Name: 2000-08-10 00:00:00, dtype: float64  future value 0.9794495335712947\n",
            "loop 1 market state : step  348 market state fut market_state    3.0\n",
            "Name: 2000-08-11 00:00:00, dtype: float64  future value 0.9766016843986934\n",
            "loop 1 market state : step  349 market state fut market_state    3.0\n",
            "Name: 2000-08-14 00:00:00, dtype: float64  future value 0.9816820203577861\n",
            "loop 1 market state : step  350 market state fut market_state    3.0\n",
            "Name: 2000-08-15 00:00:00, dtype: float64  future value 0.9807982161203846\n",
            "loop 1 market state : step  351 market state fut market_state    3.0\n",
            "Name: 2000-08-16 00:00:00, dtype: float64  future value 0.9859308978408932\n",
            "loop 1 market state : step  352 market state fut market_state    3.0\n",
            "Name: 2000-08-17 00:00:00, dtype: float64  future value 0.9874629104307281\n",
            "loop 1 market state : step  353 market state fut market_state    2.0\n",
            "Name: 2000-08-18 00:00:00, dtype: float64  future value 0.9862451322437743\n",
            "loop 1 market state : step  354 market state fut market_state    4.0\n",
            "Name: 2000-08-21 00:00:00, dtype: float64  future value 0.9912469096021467\n",
            "loop 1 market state : step  355 market state fut market_state    4.0\n",
            "Name: 2000-08-22 00:00:00, dtype: float64  future value 0.9884645126106134\n",
            "loop 1 market state : step  356 market state fut market_state    2.0\n",
            "Name: 2000-08-23 00:00:00, dtype: float64  future value 0.9837180706838802\n",
            "loop 1 market state : step  357 market state fut market_state    4.0\n",
            "Name: 2000-08-24 00:00:00, dtype: float64  future value 0.9935972742483149\n",
            "loop 1 market state : step  358 market state fut market_state    4.0\n",
            "Name: 2000-08-25 00:00:00, dtype: float64  future value 0.9956202181547568\n",
            "loop 1 market state : step  359 market state fut market_state    2.0\n",
            "Name: 2000-08-28 00:00:00, dtype: float64  future value 0.986657584876854\n",
            "loop 1 market state : step  360 market state fut market_state    2.0\n",
            "Name: 2000-08-29 00:00:00, dtype: float64  future value 0.9769486848507051\n",
            "loop 1 market state : step  361 market state fut market_state    2.0\n",
            "Name: 2000-08-30 00:00:00, dtype: float64  future value 0.9836657249224643\n",
            "loop 1 market state : step  362 market state fut market_state    2.0\n",
            "Name: 2000-08-31 00:00:00, dtype: float64  future value 0.978421718552105\n",
            "loop 1 market state : step  363 market state fut market_state    2.0\n",
            "Name: 2000-09-01 00:00:00, dtype: float64  future value 0.9749911931253312\n",
            "loop 1 market state : step  364 market state fut market_state    2.0\n",
            "Name: 2000-09-05 00:00:00, dtype: float64  future value 0.9702316447789459\n",
            "loop 1 market state : step  365 market state fut market_state    2.0\n",
            "Name: 2000-09-06 00:00:00, dtype: float64  future value 0.9721433439527299\n",
            "loop 1 market state : step  366 market state fut market_state    2.0\n",
            "Name: 2000-09-07 00:00:00, dtype: float64  future value 0.9694984045331589\n",
            "loop 1 market state : step  367 market state fut market_state    2.0\n",
            "Name: 2000-09-08 00:00:00, dtype: float64  future value 0.9596389405153956\n",
            "loop 1 market state : step  368 market state fut market_state    0.0\n",
            "Name: 2000-09-11 00:00:00, dtype: float64  future value 0.9456941895085987\n",
            "loop 1 market state : step  369 market state fut market_state    0.0\n",
            "Name: 2000-09-12 00:00:00, dtype: float64  future value 0.9557697496162376\n",
            "loop 1 market state : step  370 market state fut market_state    0.0\n",
            "Name: 2000-09-13 00:00:00, dtype: float64  future value 0.9501656363742145\n",
            "loop 1 market state : step  371 market state fut market_state    0.0\n",
            "Name: 2000-09-14 00:00:00, dtype: float64  future value 0.9486664697507031\n",
            "loop 1 market state : step  372 market state fut market_state    0.0\n",
            "Name: 2000-09-15 00:00:00, dtype: float64  future value 0.9484503736608276\n",
            "loop 1 market state : step  373 market state fut market_state    0.0\n",
            "Name: 2000-09-18 00:00:00, dtype: float64  future value 0.9421065468803844\n",
            "loop 1 market state : step  374 market state fut market_state    0.0\n",
            "Name: 2000-09-19 00:00:00, dtype: float64  future value 0.9343681650820685\n",
            "loop 1 market state : step  375 market state fut market_state    0.0\n",
            "Name: 2000-09-20 00:00:00, dtype: float64  future value 0.9339491592391628\n",
            "loop 1 market state : step  376 market state fut market_state    2.0\n",
            "Name: 2000-09-21 00:00:00, dtype: float64  future value 0.9547157217577436\n",
            "loop 1 market state : step  377 market state fut market_state    0.0\n",
            "Name: 2000-09-22 00:00:00, dtype: float64  future value 0.9404567363480656\n",
            "loop 1 market state : step  378 market state fut market_state    0.0\n",
            "Name: 2000-09-25 00:00:00, dtype: float64  future value 0.9402734063073206\n",
            "loop 1 market state : step  379 market state fut market_state    0.0\n",
            "Name: 2000-09-26 00:00:00, dtype: float64  future value 0.9338771538482685\n",
            "loop 1 market state : step  380 market state fut market_state    2.0\n",
            "Name: 2000-09-27 00:00:00, dtype: float64  future value 0.9390229419884293\n",
            "loop 1 market state : step  381 market state fut market_state    1.0\n",
            "Name: 2000-09-28 00:00:00, dtype: float64  future value 0.9403061723564511\n",
            "loop 1 market state : step  382 market state fut market_state    0.0\n",
            "Name: 2000-09-29 00:00:00, dtype: float64  future value 0.9224398846890806\n",
            "loop 1 market state : step  383 market state fut market_state    0.0\n",
            "Name: 2000-10-02 00:00:00, dtype: float64  future value 0.9178833260129184\n",
            "loop 1 market state : step  384 market state fut market_state    0.0\n",
            "Name: 2000-10-03 00:00:00, dtype: float64  future value 0.9080565481270926\n",
            "loop 1 market state : step  385 market state fut market_state    0.0\n",
            "Name: 2000-10-04 00:00:00, dtype: float64  future value 0.8933720036646828\n",
            "loop 1 market state : step  386 market state fut market_state    0.0\n",
            "Name: 2000-10-05 00:00:00, dtype: float64  future value 0.8705825771568532\n",
            "loop 1 market state : step  387 market state fut market_state    0.0\n",
            "Name: 2000-10-06 00:00:00, dtype: float64  future value 0.8996439049714248\n",
            "loop 1 market state : step  388 market state fut market_state    0.0\n",
            "Name: 2000-10-09 00:00:00, dtype: float64  future value 0.8999384797448275\n",
            "loop 1 market state : step  389 market state fut market_state    0.0\n",
            "Name: 2000-10-10 00:00:00, dtype: float64  future value 0.8838005612104961\n",
            "loop 1 market state : step  390 market state fut market_state    0.0\n",
            "Name: 2000-10-11 00:00:00, dtype: float64  future value 0.8786678794899877\n",
            "loop 1 market state : step  391 market state fut market_state    2.0\n",
            "Name: 2000-10-12 00:00:00, dtype: float64  future value 0.9091956877961331\n",
            "loop 1 market state : step  392 market state fut market_state    2.0\n",
            "Name: 2000-10-13 00:00:00, dtype: float64  future value 0.9145444656065171\n",
            "loop 1 market state : step  393 market state fut market_state    2.0\n",
            "Name: 2000-10-16 00:00:00, dtype: float64  future value 0.9137915657312519\n",
            "loop 1 market state : step  394 market state fut market_state    2.0\n",
            "Name: 2000-10-17 00:00:00, dtype: float64  future value 0.9153300516137199\n",
            "loop 1 market state : step  395 market state fut market_state    2.0\n",
            "Name: 2000-10-18 00:00:00, dtype: float64  future value 0.8935749933349061\n",
            "loop 1 market state : step  396 market state fut market_state    1.0\n",
            "Name: 2000-10-19 00:00:00, dtype: float64  future value 0.8932737854344842\n",
            "loop 1 market state : step  397 market state fut market_state    1.0\n",
            "Name: 2000-10-20 00:00:00, dtype: float64  future value 0.9031856751308565\n",
            "loop 1 market state : step  398 market state fut market_state    2.0\n",
            "Name: 2000-10-23 00:00:00, dtype: float64  future value 0.9156770520657316\n",
            "loop 1 market state : step  399 market state fut market_state    2.0\n",
            "Name: 2000-10-24 00:00:00, dtype: float64  future value 0.9358019594417049\n",
            "loop 1 market state : step  400 market state fut market_state    2.0\n",
            "Name: 2000-10-25 00:00:00, dtype: float64  future value 0.9304466284214948\n",
            "loop 1 market state : step  401 market state fut market_state    2.0\n",
            "Name: 2000-10-26 00:00:00, dtype: float64  future value 0.9350948521180293\n",
            "loop 1 market state : step  402 market state fut market_state    2.0\n",
            "Name: 2000-10-27 00:00:00, dtype: float64  future value 0.934027717839883\n",
            "loop 1 market state : step  403 market state fut market_state    2.0\n",
            "Name: 2000-10-30 00:00:00, dtype: float64  future value 0.9376284668877496\n",
            "loop 1 market state : step  404 market state fut market_state    3.0\n",
            "Name: 2000-10-31 00:00:00, dtype: float64  future value 0.9374190039248932\n",
            "loop 1 market state : step  405 market state fut market_state    1.0\n",
            "Name: 2000-11-01 00:00:00, dtype: float64  future value 0.9226297679396517\n",
            "loop 1 market state : step  406 market state fut market_state    1.0\n",
            "Name: 2000-11-02 00:00:00, dtype: float64  future value 0.9166459681136794\n",
            "loop 1 market state : step  407 market state fut market_state   -1.0\n",
            "Name: 2000-11-03 00:00:00, dtype: float64  future value nan\n",
            "loop 1 market state : step  408 market state fut market_state   -1.0\n",
            "Name: 2000-11-06 00:00:00, dtype: float64  future value nan\n",
            "loop 1 market state : step  409 market state fut market_state   -1.0\n",
            "Name: 2000-11-07 00:00:00, dtype: float64  future value nan\n",
            "loop 1 market state : step  410 market state fut market_state   -1.0\n",
            "Name: 2000-11-08 00:00:00, dtype: float64  future value nan\n",
            "loop 1 market state : step  411 market state fut market_state   -1.0\n",
            "Name: 2000-11-09 00:00:00, dtype: float64  future value nan\n",
            "loop 2 image : step  0\n",
            "loop 2 image : step  1\n",
            "loop 2 image : step  2\n",
            "loop 2 image : step  3\n",
            "loop 2 image : step  4\n",
            "loop 2 image : step  5\n",
            "loop 2 image : step  6\n",
            "loop 2 image : step  7\n",
            "loop 2 image : step  8\n",
            "loop 2 image : step  9\n",
            "loop 2 image : step  10\n",
            "loop 2 image : step  11\n",
            "loop 2 image : step  12\n",
            "loop 2 image : step  13\n",
            "loop 2 image : step  14\n",
            "loop 2 image : step  15\n",
            "loop 2 image : step  16\n",
            "loop 2 image : step  17\n",
            "loop 2 image : step  18\n",
            "loop 2 image : step  19\n",
            "loop 2 image : step  20\n",
            "loop 2 image : step  21\n",
            "loop 2 image : step  22\n",
            "loop 2 image : step  23\n",
            "loop 2 image : step  24\n",
            "loop 2 image : step  25\n",
            "loop 2 image : step  26\n",
            "loop 2 image : step  27\n",
            "loop 2 image : step  28\n",
            "loop 2 image : step  29\n",
            "loop 2 image : step  30\n",
            "loop 2 image : step  31\n",
            "loop 2 image : step  32\n",
            "loop 2 image : step  33\n",
            "loop 2 image : step  34\n",
            "loop 2 image : step  35\n",
            "loop 2 image : step  36\n",
            "loop 2 image : step  37\n",
            "loop 2 image : step  38\n",
            "loop 2 image : step  39\n",
            "loop 2 image : step  40\n",
            "loop 2 image : step  41\n",
            "loop 2 image : step  42\n",
            "loop 2 image : step  43\n",
            "loop 2 image : step  44\n",
            "loop 2 image : step  45\n",
            "loop 2 image : step  46\n",
            "loop 2 image : step  47\n",
            "loop 2 image : step  48\n",
            "loop 2 image : step  49\n",
            "loop 2 image : step  50\n",
            "loop 2 image : step  51\n",
            "loop 2 image : step  52\n",
            "loop 2 image : step  53\n",
            "loop 2 image : step  54\n",
            "loop 2 image : step  55\n",
            "loop 2 image : step  56\n",
            "loop 2 image : step  57\n",
            "loop 2 image : step  58\n",
            "loop 2 image : step  59\n",
            "loop 2 image : step  60\n",
            "loop 2 image : step  61\n",
            "loop 2 image : step  62\n",
            "loop 2 image : step  63\n",
            "loop 2 image : step  64\n",
            "loop 2 image : step  65\n",
            "loop 2 image : step  66\n",
            "loop 2 image : step  67\n",
            "loop 2 image : step  68\n",
            "loop 2 image : step  69\n",
            "loop 2 image : step  70\n",
            "loop 2 image : step  71\n",
            "loop 2 image : step  72\n",
            "loop 2 image : step  73\n",
            "loop 2 image : step  74\n",
            "loop 2 image : step  75\n",
            "loop 2 image : step  76\n",
            "loop 2 image : step  77\n",
            "loop 2 image : step  78\n",
            "loop 2 image : step  79\n",
            "loop 2 image : step  80\n",
            "loop 2 image : step  81\n",
            "loop 2 image : step  82\n",
            "loop 2 image : step  83\n",
            "loop 2 image : step  84\n",
            "loop 2 image : step  85\n",
            "loop 2 image : step  86\n",
            "loop 2 image : step  87\n",
            "loop 2 image : step  88\n",
            "loop 2 image : step  89\n",
            "loop 2 image : step  90\n",
            "loop 2 image : step  91\n",
            "loop 2 image : step  92\n",
            "loop 2 image : step  93\n",
            "loop 2 image : step  94\n",
            "loop 2 image : step  95\n",
            "loop 2 image : step  96\n",
            "loop 2 image : step  97\n",
            "loop 2 image : step  98\n",
            "loop 2 image : step  99\n",
            "loop 2 image : step  100\n",
            "loop 2 image : step  101\n",
            "loop 2 image : step  102\n",
            "loop 2 image : step  103\n",
            "loop 2 image : step  104\n",
            "loop 2 image : step  105\n",
            "loop 2 image : step  106\n",
            "loop 2 image : step  107\n",
            "loop 2 image : step  108\n",
            "loop 2 image : step  109\n",
            "loop 2 image : step  110\n",
            "loop 2 image : step  111\n",
            "loop 2 image : step  112\n",
            "loop 2 image : step  113\n",
            "loop 2 image : step  114\n",
            "loop 2 image : step  115\n",
            "loop 2 image : step  116\n",
            "loop 2 image : step  117\n",
            "loop 2 image : step  118\n",
            "loop 2 image : step  119\n",
            "loop 2 image : step  120\n",
            "loop 2 image : step  121\n",
            "loop 2 image : step  122\n",
            "loop 2 image : step  123\n",
            "loop 2 image : step  124\n",
            "loop 2 image : step  125\n",
            "loop 2 image : step  126\n",
            "loop 2 image : step  127\n",
            "loop 2 image : step  128\n",
            "loop 2 image : step  129\n",
            "loop 2 image : step  130\n",
            "loop 2 image : step  131\n",
            "loop 2 image : step  132\n",
            "loop 2 image : step  133\n",
            "loop 2 image : step  134\n",
            "loop 2 image : step  135\n",
            "loop 2 image : step  136\n",
            "loop 2 image : step  137\n",
            "loop 2 image : step  138\n",
            "loop 2 image : step  139\n",
            "loop 2 image : step  140\n",
            "loop 2 image : step  141\n",
            "loop 2 image : step  142\n",
            "loop 2 image : step  143\n",
            "loop 2 image : step  144\n",
            "loop 2 image : step  145\n",
            "loop 2 image : step  146\n",
            "loop 2 image : step  147\n",
            "loop 2 image : step  148\n",
            "loop 2 image : step  149\n",
            "loop 2 image : step  150\n",
            "loop 2 image : step  151\n",
            "loop 2 image : step  152\n",
            "loop 2 image : step  153\n",
            "loop 2 image : step  154\n",
            "loop 2 image : step  155\n",
            "loop 2 image : step  156\n",
            "loop 2 image : step  157\n",
            "loop 2 image : step  158\n",
            "loop 2 image : step  159\n",
            "loop 2 image : step  160\n",
            "loop 2 image : step  161\n",
            "loop 2 image : step  162\n",
            "loop 2 image : step  163\n",
            "loop 2 image : step  164\n",
            "loop 2 image : step  165\n",
            "loop 2 image : step  166\n",
            "loop 2 image : step  167\n",
            "loop 2 image : step  168\n",
            "loop 2 image : step  169\n",
            "loop 2 image : step  170\n",
            "loop 2 image : step  171\n",
            "loop 2 image : step  172\n",
            "loop 2 image : step  173\n",
            "loop 2 image : step  174\n",
            "loop 2 image : step  175\n",
            "loop 2 image : step  176\n",
            "loop 2 image : step  177\n",
            "loop 2 image : step  178\n",
            "loop 2 image : step  179\n",
            "loop 2 image : step  180\n",
            "loop 2 image : step  181\n",
            "loop 2 image : step  182\n",
            "loop 2 image : step  183\n",
            "loop 2 image : step  184\n",
            "loop 2 image : step  185\n",
            "loop 2 image : step  186\n",
            "loop 2 image : step  187\n",
            "loop 2 image : step  188\n",
            "loop 2 image : step  189\n",
            "loop 2 image : step  190\n",
            "loop 2 image : step  191\n",
            "loop 2 image : step  192\n",
            "loop 2 image : step  193\n",
            "loop 2 image : step  194\n",
            "loop 2 image : step  195\n",
            "loop 2 image : step  196\n",
            "loop 2 image : step  197\n",
            "loop 2 image : step  198\n",
            "loop 2 image : step  199\n",
            "loop 2 image : step  200\n",
            "loop 2 image : step  201\n",
            "loop 2 image : step  202\n",
            "loop 2 image : step  203\n",
            "loop 2 image : step  204\n",
            "loop 2 image : step  205\n",
            "loop 2 image : step  206\n",
            "loop 2 image : step  207\n",
            "loop 2 image : step  208\n",
            "loop 2 image : step  209\n",
            "loop 2 image : step  210\n",
            "loop 2 image : step  211\n",
            "loop 2 image : step  212\n",
            "loop 2 image : step  213\n",
            "loop 2 image : step  214\n",
            "loop 2 image : step  215\n",
            "loop 2 image : step  216\n",
            "loop 2 image : step  217\n",
            "loop 2 image : step  218\n",
            "loop 2 image : step  219\n",
            "loop 2 image : step  220\n",
            "loop 2 image : step  221\n",
            "loop 2 image : step  222\n",
            "loop 2 image : step  223\n",
            "loop 2 image : step  224\n",
            "loop 2 image : step  225\n",
            "loop 2 image : step  226\n",
            "loop 2 image : step  227\n",
            "loop 2 image : step  228\n",
            "loop 2 image : step  229\n",
            "loop 2 image : step  230\n",
            "loop 2 image : step  231\n",
            "loop 2 image : step  232\n",
            "loop 2 image : step  233\n",
            "loop 2 image : step  234\n",
            "loop 2 image : step  235\n",
            "loop 2 image : step  236\n",
            "loop 2 image : step  237\n",
            "loop 2 image : step  238\n",
            "loop 2 image : step  239\n",
            "loop 2 image : step  240\n",
            "loop 2 image : step  241\n",
            "loop 2 image : step  242\n",
            "loop 2 image : step  243\n",
            "loop 2 image : step  244\n",
            "loop 2 image : step  245\n",
            "loop 2 image : step  246\n",
            "loop 2 image : step  247\n",
            "loop 2 image : step  248\n",
            "loop 2 image : step  249\n",
            "loop 2 image : step  250\n",
            "loop 2 image : step  251\n",
            "loop 2 image : step  252\n",
            "loop 2 image : step  253\n",
            "loop 2 image : step  254\n",
            "loop 2 image : step  255\n",
            "loop 2 image : step  256\n",
            "loop 2 image : step  257\n",
            "loop 2 image : step  258\n",
            "loop 2 image : step  259\n",
            "loop 2 image : step  260\n",
            "loop 2 image : step  261\n",
            "loop 2 image : step  262\n",
            "loop 2 image : step  263\n",
            "loop 2 image : step  264\n",
            "loop 2 image : step  265\n",
            "loop 2 image : step  266\n",
            "loop 2 image : step  267\n",
            "loop 2 image : step  268\n",
            "loop 2 image : step  269\n",
            "loop 2 image : step  270\n",
            "loop 2 image : step  271\n",
            "loop 2 image : step  272\n",
            "loop 2 image : step  273\n",
            "loop 2 image : step  274\n",
            "loop 2 image : step  275\n",
            "loop 2 image : step  276\n",
            "loop 2 image : step  277\n",
            "loop 2 image : step  278\n",
            "loop 2 image : step  279\n",
            "loop 2 image : step  280\n",
            "loop 2 image : step  281\n",
            "loop 2 image : step  282\n",
            "loop 2 image : step  283\n",
            "loop 2 image : step  284\n",
            "loop 2 image : step  285\n",
            "loop 2 image : step  286\n",
            "loop 2 image : step  287\n",
            "loop 2 image : step  288\n",
            "loop 2 image : step  289\n",
            "loop 2 image : step  290\n",
            "loop 2 image : step  291\n",
            "loop 2 image : step  292\n",
            "loop 2 image : step  293\n",
            "loop 2 image : step  294\n",
            "loop 2 image : step  295\n",
            "loop 2 image : step  296\n",
            "loop 2 image : step  297\n",
            "loop 2 image : step  298\n",
            "loop 2 image : step  299\n",
            "loop 2 image : step  300\n",
            "loop 2 image : step  301\n",
            "loop 2 image : step  302\n",
            "loop 2 image : step  303\n",
            "loop 2 image : step  304\n",
            "loop 2 image : step  305\n",
            "loop 2 image : step  306\n",
            "loop 2 image : step  307\n",
            "loop 2 image : step  308\n",
            "loop 2 image : step  309\n",
            "loop 2 image : step  310\n",
            "loop 2 image : step  311\n",
            "loop 2 image : step  312\n",
            "loop 2 image : step  313\n",
            "loop 2 image : step  314\n",
            "loop 2 image : step  315\n",
            "loop 2 image : step  316\n",
            "loop 2 image : step  317\n",
            "loop 2 image : step  318\n",
            "loop 2 image : step  319\n",
            "loop 2 image : step  320\n",
            "loop 2 image : step  321\n",
            "loop 2 image : step  322\n",
            "loop 2 image : step  323\n",
            "loop 2 image : step  324\n",
            "loop 2 image : step  325\n",
            "loop 2 image : step  326\n",
            "loop 2 image : step  327\n",
            "loop 2 image : step  328\n",
            "loop 2 image : step  329\n",
            "loop 2 image : step  330\n",
            "loop 2 image : step  331\n",
            "loop 2 image : step  332\n",
            "loop 2 image : step  333\n",
            "loop 2 image : step  334\n",
            "loop 2 image : step  335\n",
            "loop 2 image : step  336\n",
            "loop 2 image : step  337\n",
            "loop 2 image : step  338\n",
            "loop 2 image : step  339\n",
            "loop 2 image : step  340\n",
            "loop 2 image : step  341\n",
            "loop 2 image : step  342\n",
            "loop 2 image : step  343\n",
            "loop 2 image : step  344\n",
            "loop 2 image : step  345\n",
            "loop 2 image : step  346\n",
            "loop 2 image : step  347\n",
            "loop 2 image : step  348\n",
            "loop 2 image : step  349\n",
            "loop 2 image : step  350\n",
            "loop 2 image : step  351\n",
            "loop 2 image : step  352\n",
            "loop 2 image : step  353\n",
            "loop 2 image : step  354\n",
            "loop 2 image : step  355\n",
            "loop 2 image : step  356\n",
            "loop 2 image : step  357\n",
            "loop 2 image : step  358\n",
            "loop 2 image : step  359\n",
            "loop 2 image : step  360\n",
            "loop 2 image : step  361\n",
            "loop 2 image : step  362\n",
            "loop 2 image : step  363\n",
            "loop 2 image : step  364\n",
            "loop 2 image : step  365\n",
            "loop 2 image : step  366\n",
            "loop 2 image : step  367\n",
            "loop 2 image : step  368\n",
            "loop 2 image : step  369\n",
            "loop 2 image : step  370\n",
            "loop 2 image : step  371\n",
            "loop 2 image : step  372\n",
            "loop 2 image : step  373\n",
            "loop 2 image : step  374\n",
            "loop 2 image : step  375\n",
            "loop 2 image : step  376\n",
            "loop 2 image : step  377\n",
            "loop 2 image : step  378\n",
            "loop 2 image : step  379\n",
            "loop 2 image : step  380\n",
            "loop 2 image : step  381\n",
            "loop 2 image : step  382\n",
            "loop 2 image : step  383\n",
            "loop 2 image : step  384\n",
            "loop 2 image : step  385\n",
            "loop 2 image : step  386\n",
            "loop 2 image : step  387\n",
            "loop 2 image : step  388\n",
            "loop 2 image : step  389\n",
            "loop 2 image : step  390\n",
            "loop 2 image : step  391\n",
            "loop 2 image : step  392\n",
            "loop 2 image : step  393\n",
            "loop 2 image : step  394\n",
            "loop 2 image : step  395\n",
            "loop 2 image : step  396\n",
            "loop 2 image : step  397\n",
            "loop 2 image : step  398\n",
            "loop 2 image : step  399\n",
            "loop 2 image : step  400\n",
            "loop 2 image : step  401\n",
            "loop 2 image : step  402\n",
            "loop 2 image : step  403\n",
            "loop 2 image : step  404\n",
            "loop 2 image : step  405\n",
            "loop 2 image : step  406\n",
            "loop 2 image : step  407\n",
            "loop 2 image : step  408\n",
            "loop 2 image : step  409\n",
            "loop 2 image : step  410\n",
            "loop 2 image : step  411\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AZLYbyEmwBwF",
        "outputId": "9585ed79-86d8-4682-bc52-ccc653f038a8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "\n",
        "'''\n",
        "COMMAND NOW FOR DOWNLOADING HISTORICAL DATAS FOR SP500\n",
        "'''\n",
        "\n",
        "#Recuperation from yahoo of sp500 large history\n",
        "start = datetime(1920,1,1)\n",
        "end = datetime(2020,7,31)\n",
        "yf.pdr_override() # <== that's all it takes :-)\n",
        "sp500 = pdr.get_data_yahoo('^GSPC', \n",
        "                           start,\n",
        "                             end)\n",
        "\n",
        "#generate the dataset it can take 6 - 8 hours\n",
        "#Need to be optimzed with more time\n",
        "testsp500=(sp500['Close'])[:]\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[*********************100%***********************]  1 of 1 downloaded\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LqaU9Lp_KVcb"
      },
      "source": [
        "###Check version for parralelisation and improvement"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y2kRNyYG42Ti"
      },
      "source": [
        "value on file instead of ram memory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ii0Ozcc746po",
        "outputId": "a6e8d92b-b3b2-4f1a-9b80-c3b08eaf3a6d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "data = np.arange(12, dtype='float64')\n",
        "data.resize((3,4))\n",
        "\n",
        "from tempfile import mkdtemp\n",
        "import os.path as path\n",
        "filename = path.join(mkdtemp(), 'newfile.dat')\n",
        "fp = np.memmap(filename, dtype='float64', mode='w+', shape=(3,4))\n",
        "fp"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "memmap([[0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UVpcdN3W5PRC"
      },
      "source": [
        "fp[:] = data[:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O_lE5_x95Sl6",
        "outputId": "1431f53b-f562-45e6-e4ac-75aef68fdf7e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "fp"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "memmap([[ 0.,  1.,  2.,  3.],\n",
              "        [ 4.,  5.,  6.,  7.],\n",
              "        [ 8.,  9., 10., 11.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rCDaC78d5C9y",
        "outputId": "d0cd1e7d-2408-4e03-b958-a9bfefbbf58b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        }
      },
      "source": [
        "%ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " \u001b[0m\u001b[01;34mdatas\u001b[0m/\n",
            " \u001b[01;34mImageM\u001b[0m/\n",
            " LICENSE\n",
            " \u001b[01;34mMainNotebook\u001b[0m/\n",
            " MANIFEST.in\n",
            " \u001b[01;34mmodel\u001b[0m/\n",
            " model.zip\n",
            " \u001b[01;34mnewversion\u001b[0m/\n",
            " \u001b[01;34mNotebooks\u001b[0m/\n",
            " README.md\n",
            " requirements.txt\n",
            " setup.cfg\n",
            " setup.py\n",
            " step1_generate_dataset_IndexImage.py\n",
            " step2_loadingtrainingdatas_vgg_transfert_modelandtraining.py\n",
            " step3_evaluate_vggsp500_model.py\n",
            " step4_guess_future_marketstate_from_image.py\n",
            " \u001b[01;34mtests\u001b[0m/\n",
            "'TFM DL Tools for Finance  Application Transfert Learning Vgg16sp500.pptx'\n",
            "'TFM Imiled 2019-2020_Deep Learning application to Finance.docx'\n",
            " \u001b[01;34mtrainer\u001b[0m/\n",
            " Transfert_Learning_Reload_SP500.ipynb\n",
            " Transfert_Learning_Vgg16forSP500.ipynb\n",
            " versioneer.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "csL7DmbUK4Hb"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KoUz2iEnK4wJ"
      },
      "source": [
        "def build_image_clean(stockindex_ohlcv, ret_image_size=(32,32,3), idate=10, pastlag=32):\n",
        "  '''\n",
        "  TO BE COMPLETED\n",
        "  NOT USED NOW\n",
        "  \n",
        "  change one date into an array (32,32,3)\n",
        "  Each absciss pixel is one day\n",
        "  in ordinate the min value of ohlc shall be 0 (volume is tabled on the third image) \n",
        "  in ordinate the max value of ohlc shall be  (volume is tabled on the third image) \n",
        "  1st image: 32 x32\n",
        "    based on each day we place the open and close point\n",
        "    in ordinate int (255 * price /max ohlc)\n",
        "    with value of  255 for close and 127 for open\n",
        "  2nd image: 32 x32\n",
        "    based on each day we place the high low point \n",
        "    in ordinate int (255 * price /max ohlc)\n",
        "    with 64 for high and 32 for low\n",
        "  3rd image: 32 x32\n",
        "    each column value is a equal to int 255* volume of day / volume max period)\n",
        "  '''\n",
        "  #number of days to consider for translate\n",
        "  tsindexstock=stockindex_ohlcv.iloc[(idate-pastlag):idate]\n",
        "  valmax=np.max(np.array(tsindexstock[tsindexstock.columns[:-1]]))\n",
        "  valmin=np.min(np.array(tsindexstock[tsindexstock.columns[:-1]]))\n",
        "  vol=tsindexstock[tsindexstock.columns[-1]]\n",
        "  \n",
        "  x_datas=np.zeros(ret_image_size)\n",
        "  \n",
        "  return x_datas"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zcNAYR-zbLoS",
        "outputId": "1a2c1864-5bff-4e06-f494-96423adc0965",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 214
        }
      },
      "source": [
        "fig=plt.figure()\n",
        "x_image_test=build_image_optimfig(fig, testsp500, idate=10, pastlag=10, futlag=3)\n",
        "x_image_test.shape\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-0669f2e581c7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mx_image_test\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbuild_image_optimfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestsp500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpastlag\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfutlag\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mx_image_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'build_image_optimfig' is not defined"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CPITIKnfapXp",
        "outputId": "3b244aa3-1335-444a-aaf2-6ebc372e8ee2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        }
      },
      "source": [
        "x_test_image_1=x_image_test[:,:]\n",
        "plt.imshow(x_test_image_1)\n",
        "x_test_image_1.shape\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(255, 255)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OyzAFCoi0hFD",
        "outputId": "8d92ec0b-f488-4f82-d2b1-ec5b67c39500",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "#https://www.machinelearningplus.com/python/parallel-processing-python/\n",
        "import multiprocessing as mp\n",
        "print(\"Number of processors: \", mp.cpu_count())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of processors:  2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s3QOy0bOuvE2",
        "outputId": "9a979048-36cc-4237-be38-65b741d16a5d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import numpy as np\n",
        "from time import time\n",
        "\n",
        "# Prepare data\n",
        "np.random.RandomState(100)\n",
        "arr = np.random.randint(0, 10, size=[200000, 5])\n",
        "data = arr.tolist()\n",
        "data[:5]\n",
        "\n",
        "# Solution Without Paralleization\n",
        "\n",
        "def howmany_within_range(row, minimum, maximum):\n",
        "    \"\"\"Returns how many numbers lie within `maximum` and `minimum` in a given `row`\"\"\"\n",
        "    count = 0\n",
        "    for n in row:\n",
        "        if minimum <= n <= maximum:\n",
        "            count = count + 1\n",
        "    return count\n",
        "\n",
        "results = []\n",
        "for row in data:\n",
        "    results.append(howmany_within_range(row, minimum=4, maximum=8))\n",
        "\n",
        "print(results[:10])\n",
        "#> [3, 1, 4, 4, 4, 2, 1, 1, 3, 3]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1, 4, 3, 0, 4, 1, 3, 1, 1, 2]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uRIf4hcb8QlC",
        "outputId": "9c43904b-e569-4cc1-dbe5-6d3248d8ab9a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "200000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 171
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ZBdzGDBuPDf",
        "outputId": "227496df-22c6-4f30-e4f4-c30ab950f344",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Parallelizing using Pool.apply()\n",
        "\n",
        "import multiprocessing as mp\n",
        "\n",
        "# Step 1: Init multiprocessing.Pool()\n",
        "pool = mp.Pool(mp.cpu_count())\n",
        "\n",
        "# Step 2: `pool.apply` the `howmany_within_range()`\n",
        "results = [pool.apply(howmany_within_range, args=(row, 4, 8)) for row in data]\n",
        "\n",
        "# Step 3: Don't forget to close\n",
        "pool.close()    \n",
        "\n",
        "print(results[:10])\n",
        "#> [3, 1, 4, 4, 4, 2, 1, 1, 3, 3]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1, 4, 3, 0, 4, 1, 3, 1, 1, 2]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m8r2Npc1KbiC"
      },
      "source": [
        "###Main Launch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gj6vLE8IbxV1",
        "outputId": "17606fee-e00b-4aba-f929-2f93d3181a5a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "_ , (X_test_image, Y_test_StateClass_image, Y_test_FutPredict_image) = \\\n",
        "setup_input_NN_image(testsp500, split=0)\n",
        "\n",
        "#copy the datafrae dataset in csv format to be used after\n",
        "#dateTimeObj = datetime.now()\n",
        "#timeStr = dateTimeObj.strftime(\"%Y_%m_%d_%H_%M_%S_%f\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "loop 2 image : step  17586\n",
            "loop 2 image : step  17587\n",
            "loop 2 image : step  17588\n",
            "loop 2 image : step  17589\n",
            "loop 2 image : step  17590\n",
            "loop 2 image : step  17591\n",
            "loop 2 image : step  17592\n",
            "loop 2 image : step  17593\n",
            "loop 2 image : step  17594\n",
            "loop 2 image : step  17595\n",
            "loop 2 image : step  17596\n",
            "loop 2 image : step  17597\n",
            "loop 2 image : step  17598\n",
            "loop 2 image : step  17599\n",
            "loop 2 image : step  17600\n",
            "loop 2 image : step  17601\n",
            "loop 2 image : step  17602\n",
            "loop 2 image : step  17603\n",
            "loop 2 image : step  17604\n",
            "loop 2 image : step  17605\n",
            "loop 2 image : step  17606\n",
            "loop 2 image : step  17607\n",
            "loop 2 image : step  17608\n",
            "loop 2 image : step  17609\n",
            "loop 2 image : step  17610\n",
            "loop 2 image : step  17611\n",
            "loop 2 image : step  17612\n",
            "loop 2 image : step  17613\n",
            "loop 2 image : step  17614\n",
            "loop 2 image : step  17615\n",
            "loop 2 image : step  17616\n",
            "loop 2 image : step  17617\n",
            "loop 2 image : step  17618\n",
            "loop 2 image : step  17619\n",
            "loop 2 image : step  17620\n",
            "loop 2 image : step  17621\n",
            "loop 2 image : step  17622\n",
            "loop 2 image : step  17623\n",
            "loop 2 image : step  17624\n",
            "loop 2 image : step  17625\n",
            "loop 2 image : step  17626\n",
            "loop 2 image : step  17627\n",
            "loop 2 image : step  17628\n",
            "loop 2 image : step  17629\n",
            "loop 2 image : step  17630\n",
            "loop 2 image : step  17631\n",
            "loop 2 image : step  17632\n",
            "loop 2 image : step  17633\n",
            "loop 2 image : step  17634\n",
            "loop 2 image : step  17635\n",
            "loop 2 image : step  17636\n",
            "loop 2 image : step  17637\n",
            "loop 2 image : step  17638\n",
            "loop 2 image : step  17639\n",
            "loop 2 image : step  17640\n",
            "loop 2 image : step  17641\n",
            "loop 2 image : step  17642\n",
            "loop 2 image : step  17643\n",
            "loop 2 image : step  17644\n",
            "loop 2 image : step  17645\n",
            "loop 2 image : step  17646\n",
            "loop 2 image : step  17647\n",
            "loop 2 image : step  17648\n",
            "loop 2 image : step  17649\n",
            "loop 2 image : step  17650\n",
            "loop 2 image : step  17651\n",
            "loop 2 image : step  17652\n",
            "loop 2 image : step  17653\n",
            "loop 2 image : step  17654\n",
            "loop 2 image : step  17655\n",
            "loop 2 image : step  17656\n",
            "loop 2 image : step  17657\n",
            "loop 2 image : step  17658\n",
            "loop 2 image : step  17659\n",
            "loop 2 image : step  17660\n",
            "loop 2 image : step  17661\n",
            "loop 2 image : step  17662\n",
            "loop 2 image : step  17663\n",
            "loop 2 image : step  17664\n",
            "loop 2 image : step  17665\n",
            "loop 2 image : step  17666\n",
            "loop 2 image : step  17667\n",
            "loop 2 image : step  17668\n",
            "loop 2 image : step  17669\n",
            "loop 2 image : step  17670\n",
            "loop 2 image : step  17671\n",
            "loop 2 image : step  17672\n",
            "loop 2 image : step  17673\n",
            "loop 2 image : step  17674\n",
            "loop 2 image : step  17675\n",
            "loop 2 image : step  17676\n",
            "loop 2 image : step  17677\n",
            "loop 2 image : step  17678\n",
            "loop 2 image : step  17679\n",
            "loop 2 image : step  17680\n",
            "loop 2 image : step  17681\n",
            "loop 2 image : step  17682\n",
            "loop 2 image : step  17683\n",
            "loop 2 image : step  17684\n",
            "loop 2 image : step  17685\n",
            "loop 2 image : step  17686\n",
            "loop 2 image : step  17687\n",
            "loop 2 image : step  17688\n",
            "loop 2 image : step  17689\n",
            "loop 2 image : step  17690\n",
            "loop 2 image : step  17691\n",
            "loop 2 image : step  17692\n",
            "loop 2 image : step  17693\n",
            "loop 2 image : step  17694\n",
            "loop 2 image : step  17695\n",
            "loop 2 image : step  17696\n",
            "loop 2 image : step  17697\n",
            "loop 2 image : step  17698\n",
            "loop 2 image : step  17699\n",
            "loop 2 image : step  17700\n",
            "loop 2 image : step  17701\n",
            "loop 2 image : step  17702\n",
            "loop 2 image : step  17703\n",
            "loop 2 image : step  17704\n",
            "loop 2 image : step  17705\n",
            "loop 2 image : step  17706\n",
            "loop 2 image : step  17707\n",
            "loop 2 image : step  17708\n",
            "loop 2 image : step  17709\n",
            "loop 2 image : step  17710\n",
            "loop 2 image : step  17711\n",
            "loop 2 image : step  17712\n",
            "loop 2 image : step  17713\n",
            "loop 2 image : step  17714\n",
            "loop 2 image : step  17715\n",
            "loop 2 image : step  17716\n",
            "loop 2 image : step  17717\n",
            "loop 2 image : step  17718\n",
            "loop 2 image : step  17719\n",
            "loop 2 image : step  17720\n",
            "loop 2 image : step  17721\n",
            "loop 2 image : step  17722\n",
            "loop 2 image : step  17723\n",
            "loop 2 image : step  17724\n",
            "loop 2 image : step  17725\n",
            "loop 2 image : step  17726\n",
            "loop 2 image : step  17727\n",
            "loop 2 image : step  17728\n",
            "loop 2 image : step  17729\n",
            "loop 2 image : step  17730\n",
            "loop 2 image : step  17731\n",
            "loop 2 image : step  17732\n",
            "loop 2 image : step  17733\n",
            "loop 2 image : step  17734\n",
            "loop 2 image : step  17735\n",
            "loop 2 image : step  17736\n",
            "loop 2 image : step  17737\n",
            "loop 2 image : step  17738\n",
            "loop 2 image : step  17739\n",
            "loop 2 image : step  17740\n",
            "loop 2 image : step  17741\n",
            "loop 2 image : step  17742\n",
            "loop 2 image : step  17743\n",
            "loop 2 image : step  17744\n",
            "loop 2 image : step  17745\n",
            "loop 2 image : step  17746\n",
            "loop 2 image : step  17747\n",
            "loop 2 image : step  17748\n",
            "loop 2 image : step  17749\n",
            "loop 2 image : step  17750\n",
            "loop 2 image : step  17751\n",
            "loop 2 image : step  17752\n",
            "loop 2 image : step  17753\n",
            "loop 2 image : step  17754\n",
            "loop 2 image : step  17755\n",
            "loop 2 image : step  17756\n",
            "loop 2 image : step  17757\n",
            "loop 2 image : step  17758\n",
            "loop 2 image : step  17759\n",
            "loop 2 image : step  17760\n",
            "loop 2 image : step  17761\n",
            "loop 2 image : step  17762\n",
            "loop 2 image : step  17763\n",
            "loop 2 image : step  17764\n",
            "loop 2 image : step  17765\n",
            "loop 2 image : step  17766\n",
            "loop 2 image : step  17767\n",
            "loop 2 image : step  17768\n",
            "loop 2 image : step  17769\n",
            "loop 2 image : step  17770\n",
            "loop 2 image : step  17771\n",
            "loop 2 image : step  17772\n",
            "loop 2 image : step  17773\n",
            "loop 2 image : step  17774\n",
            "loop 2 image : step  17775\n",
            "loop 2 image : step  17776\n",
            "loop 2 image : step  17777\n",
            "loop 2 image : step  17778\n",
            "loop 2 image : step  17779\n",
            "loop 2 image : step  17780\n",
            "loop 2 image : step  17781\n",
            "loop 2 image : step  17782\n",
            "loop 2 image : step  17783\n",
            "loop 2 image : step  17784\n",
            "loop 2 image : step  17785\n",
            "loop 2 image : step  17786\n",
            "loop 2 image : step  17787\n",
            "loop 2 image : step  17788\n",
            "loop 2 image : step  17789\n",
            "loop 2 image : step  17790\n",
            "loop 2 image : step  17791\n",
            "loop 2 image : step  17792\n",
            "loop 2 image : step  17793\n",
            "loop 2 image : step  17794\n",
            "loop 2 image : step  17795\n",
            "loop 2 image : step  17796\n",
            "loop 2 image : step  17797\n",
            "loop 2 image : step  17798\n",
            "loop 2 image : step  17799\n",
            "loop 2 image : step  17800\n",
            "loop 2 image : step  17801\n",
            "loop 2 image : step  17802\n",
            "loop 2 image : step  17803\n",
            "loop 2 image : step  17804\n",
            "loop 2 image : step  17805\n",
            "loop 2 image : step  17806\n",
            "loop 2 image : step  17807\n",
            "loop 2 image : step  17808\n",
            "loop 2 image : step  17809\n",
            "loop 2 image : step  17810\n",
            "loop 2 image : step  17811\n",
            "loop 2 image : step  17812\n",
            "loop 2 image : step  17813\n",
            "loop 2 image : step  17814\n",
            "loop 2 image : step  17815\n",
            "loop 2 image : step  17816\n",
            "loop 2 image : step  17817\n",
            "loop 2 image : step  17818\n",
            "loop 2 image : step  17819\n",
            "loop 2 image : step  17820\n",
            "loop 2 image : step  17821\n",
            "loop 2 image : step  17822\n",
            "loop 2 image : step  17823\n",
            "loop 2 image : step  17824\n",
            "loop 2 image : step  17825\n",
            "loop 2 image : step  17826\n",
            "loop 2 image : step  17827\n",
            "loop 2 image : step  17828\n",
            "loop 2 image : step  17829\n",
            "loop 2 image : step  17830\n",
            "loop 2 image : step  17831\n",
            "loop 2 image : step  17832\n",
            "loop 2 image : step  17833\n",
            "loop 2 image : step  17834\n",
            "loop 2 image : step  17835\n",
            "loop 2 image : step  17836\n",
            "loop 2 image : step  17837\n",
            "loop 2 image : step  17838\n",
            "loop 2 image : step  17839\n",
            "loop 2 image : step  17840\n",
            "loop 2 image : step  17841\n",
            "loop 2 image : step  17842\n",
            "loop 2 image : step  17843\n",
            "loop 2 image : step  17844\n",
            "loop 2 image : step  17845\n",
            "loop 2 image : step  17846\n",
            "loop 2 image : step  17847\n",
            "loop 2 image : step  17848\n",
            "loop 2 image : step  17849\n",
            "loop 2 image : step  17850\n",
            "loop 2 image : step  17851\n",
            "loop 2 image : step  17852\n",
            "loop 2 image : step  17853\n",
            "loop 2 image : step  17854\n",
            "loop 2 image : step  17855\n",
            "loop 2 image : step  17856\n",
            "loop 2 image : step  17857\n",
            "loop 2 image : step  17858\n",
            "loop 2 image : step  17859\n",
            "loop 2 image : step  17860\n",
            "loop 2 image : step  17861\n",
            "loop 2 image : step  17862\n",
            "loop 2 image : step  17863\n",
            "loop 2 image : step  17864\n",
            "loop 2 image : step  17865\n",
            "loop 2 image : step  17866\n",
            "loop 2 image : step  17867\n",
            "loop 2 image : step  17868\n",
            "loop 2 image : step  17869\n",
            "loop 2 image : step  17870\n",
            "loop 2 image : step  17871\n",
            "loop 2 image : step  17872\n",
            "loop 2 image : step  17873\n",
            "loop 2 image : step  17874\n",
            "loop 2 image : step  17875\n",
            "loop 2 image : step  17876\n",
            "loop 2 image : step  17877\n",
            "loop 2 image : step  17878\n",
            "loop 2 image : step  17879\n",
            "loop 2 image : step  17880\n",
            "loop 2 image : step  17881\n",
            "loop 2 image : step  17882\n",
            "loop 2 image : step  17883\n",
            "loop 2 image : step  17884\n",
            "loop 2 image : step  17885\n",
            "loop 2 image : step  17886\n",
            "loop 2 image : step  17887\n",
            "loop 2 image : step  17888\n",
            "loop 2 image : step  17889\n",
            "loop 2 image : step  17890\n",
            "loop 2 image : step  17891\n",
            "loop 2 image : step  17892\n",
            "loop 2 image : step  17893\n",
            "loop 2 image : step  17894\n",
            "loop 2 image : step  17895\n",
            "loop 2 image : step  17896\n",
            "loop 2 image : step  17897\n",
            "loop 2 image : step  17898\n",
            "loop 2 image : step  17899\n",
            "loop 2 image : step  17900\n",
            "loop 2 image : step  17901\n",
            "loop 2 image : step  17902\n",
            "loop 2 image : step  17903\n",
            "loop 2 image : step  17904\n",
            "loop 2 image : step  17905\n",
            "loop 2 image : step  17906\n",
            "loop 2 image : step  17907\n",
            "loop 2 image : step  17908\n",
            "loop 2 image : step  17909\n",
            "loop 2 image : step  17910\n",
            "loop 2 image : step  17911\n",
            "loop 2 image : step  17912\n",
            "loop 2 image : step  17913\n",
            "loop 2 image : step  17914\n",
            "loop 2 image : step  17915\n",
            "loop 2 image : step  17916\n",
            "loop 2 image : step  17917\n",
            "loop 2 image : step  17918\n",
            "loop 2 image : step  17919\n",
            "loop 2 image : step  17920\n",
            "loop 2 image : step  17921\n",
            "loop 2 image : step  17922\n",
            "loop 2 image : step  17923\n",
            "loop 2 image : step  17924\n",
            "loop 2 image : step  17925\n",
            "loop 2 image : step  17926\n",
            "loop 2 image : step  17927\n",
            "loop 2 image : step  17928\n",
            "loop 2 image : step  17929\n",
            "loop 2 image : step  17930\n",
            "loop 2 image : step  17931\n",
            "loop 2 image : step  17932\n",
            "loop 2 image : step  17933\n",
            "loop 2 image : step  17934\n",
            "loop 2 image : step  17935\n",
            "loop 2 image : step  17936\n",
            "loop 2 image : step  17937\n",
            "loop 2 image : step  17938\n",
            "loop 2 image : step  17939\n",
            "loop 2 image : step  17940\n",
            "loop 2 image : step  17941\n",
            "loop 2 image : step  17942\n",
            "loop 2 image : step  17943\n",
            "loop 2 image : step  17944\n",
            "loop 2 image : step  17945\n",
            "loop 2 image : step  17946\n",
            "loop 2 image : step  17947\n",
            "loop 2 image : step  17948\n",
            "loop 2 image : step  17949\n",
            "loop 2 image : step  17950\n",
            "loop 2 image : step  17951\n",
            "loop 2 image : step  17952\n",
            "loop 2 image : step  17953\n",
            "loop 2 image : step  17954\n",
            "loop 2 image : step  17955\n",
            "loop 2 image : step  17956\n",
            "loop 2 image : step  17957\n",
            "loop 2 image : step  17958\n",
            "loop 2 image : step  17959\n",
            "loop 2 image : step  17960\n",
            "loop 2 image : step  17961\n",
            "loop 2 image : step  17962\n",
            "loop 2 image : step  17963\n",
            "loop 2 image : step  17964\n",
            "loop 2 image : step  17965\n",
            "loop 2 image : step  17966\n",
            "loop 2 image : step  17967\n",
            "loop 2 image : step  17968\n",
            "loop 2 image : step  17969\n",
            "loop 2 image : step  17970\n",
            "loop 2 image : step  17971\n",
            "loop 2 image : step  17972\n",
            "loop 2 image : step  17973\n",
            "loop 2 image : step  17974\n",
            "loop 2 image : step  17975\n",
            "loop 2 image : step  17976\n",
            "loop 2 image : step  17977\n",
            "loop 2 image : step  17978\n",
            "loop 2 image : step  17979\n",
            "loop 2 image : step  17980\n",
            "loop 2 image : step  17981\n",
            "loop 2 image : step  17982\n",
            "loop 2 image : step  17983\n",
            "loop 2 image : step  17984\n",
            "loop 2 image : step  17985\n",
            "loop 2 image : step  17986\n",
            "loop 2 image : step  17987\n",
            "loop 2 image : step  17988\n",
            "loop 2 image : step  17989\n",
            "loop 2 image : step  17990\n",
            "loop 2 image : step  17991\n",
            "loop 2 image : step  17992\n",
            "loop 2 image : step  17993\n",
            "loop 2 image : step  17994\n",
            "loop 2 image : step  17995\n",
            "loop 2 image : step  17996\n",
            "loop 2 image : step  17997\n",
            "loop 2 image : step  17998\n",
            "loop 2 image : step  17999\n",
            "loop 2 image : step  18000\n",
            "loop 2 image : step  18001\n",
            "loop 2 image : step  18002\n",
            "loop 2 image : step  18003\n",
            "loop 2 image : step  18004\n",
            "loop 2 image : step  18005\n",
            "loop 2 image : step  18006\n",
            "loop 2 image : step  18007\n",
            "loop 2 image : step  18008\n",
            "loop 2 image : step  18009\n",
            "loop 2 image : step  18010\n",
            "loop 2 image : step  18011\n",
            "loop 2 image : step  18012\n",
            "loop 2 image : step  18013\n",
            "loop 2 image : step  18014\n",
            "loop 2 image : step  18015\n",
            "loop 2 image : step  18016\n",
            "loop 2 image : step  18017\n",
            "loop 2 image : step  18018\n",
            "loop 2 image : step  18019\n",
            "loop 2 image : step  18020\n",
            "loop 2 image : step  18021\n",
            "loop 2 image : step  18022\n",
            "loop 2 image : step  18023\n",
            "loop 2 image : step  18024\n",
            "loop 2 image : step  18025\n",
            "loop 2 image : step  18026\n",
            "loop 2 image : step  18027\n",
            "loop 2 image : step  18028\n",
            "loop 2 image : step  18029\n",
            "loop 2 image : step  18030\n",
            "loop 2 image : step  18031\n",
            "loop 2 image : step  18032\n",
            "loop 2 image : step  18033\n",
            "loop 2 image : step  18034\n",
            "loop 2 image : step  18035\n",
            "loop 2 image : step  18036\n",
            "loop 2 image : step  18037\n",
            "loop 2 image : step  18038\n",
            "loop 2 image : step  18039\n",
            "loop 2 image : step  18040\n",
            "loop 2 image : step  18041\n",
            "loop 2 image : step  18042\n",
            "loop 2 image : step  18043\n",
            "loop 2 image : step  18044\n",
            "loop 2 image : step  18045\n",
            "loop 2 image : step  18046\n",
            "loop 2 image : step  18047\n",
            "loop 2 image : step  18048\n",
            "loop 2 image : step  18049\n",
            "loop 2 image : step  18050\n",
            "loop 2 image : step  18051\n",
            "loop 2 image : step  18052\n",
            "loop 2 image : step  18053\n",
            "loop 2 image : step  18054\n",
            "loop 2 image : step  18055\n",
            "loop 2 image : step  18056\n",
            "loop 2 image : step  18057\n",
            "loop 2 image : step  18058\n",
            "loop 2 image : step  18059\n",
            "loop 2 image : step  18060\n",
            "loop 2 image : step  18061\n",
            "loop 2 image : step  18062\n",
            "loop 2 image : step  18063\n",
            "loop 2 image : step  18064\n",
            "loop 2 image : step  18065\n",
            "loop 2 image : step  18066\n",
            "loop 2 image : step  18067\n",
            "loop 2 image : step  18068\n",
            "loop 2 image : step  18069\n",
            "loop 2 image : step  18070\n",
            "loop 2 image : step  18071\n",
            "loop 2 image : step  18072\n",
            "loop 2 image : step  18073\n",
            "loop 2 image : step  18074\n",
            "loop 2 image : step  18075\n",
            "loop 2 image : step  18076\n",
            "loop 2 image : step  18077\n",
            "loop 2 image : step  18078\n",
            "loop 2 image : step  18079\n",
            "loop 2 image : step  18080\n",
            "loop 2 image : step  18081\n",
            "loop 2 image : step  18082\n",
            "loop 2 image : step  18083\n",
            "loop 2 image : step  18084\n",
            "loop 2 image : step  18085\n",
            "loop 2 image : step  18086\n",
            "loop 2 image : step  18087\n",
            "loop 2 image : step  18088\n",
            "loop 2 image : step  18089\n",
            "loop 2 image : step  18090\n",
            "loop 2 image : step  18091\n",
            "loop 2 image : step  18092\n",
            "loop 2 image : step  18093\n",
            "loop 2 image : step  18094\n",
            "loop 2 image : step  18095\n",
            "loop 2 image : step  18096\n",
            "loop 2 image : step  18097\n",
            "loop 2 image : step  18098\n",
            "loop 2 image : step  18099\n",
            "loop 2 image : step  18100\n",
            "loop 2 image : step  18101\n",
            "loop 2 image : step  18102\n",
            "loop 2 image : step  18103\n",
            "loop 2 image : step  18104\n",
            "loop 2 image : step  18105\n",
            "loop 2 image : step  18106\n",
            "loop 2 image : step  18107\n",
            "loop 2 image : step  18108\n",
            "loop 2 image : step  18109\n",
            "loop 2 image : step  18110\n",
            "loop 2 image : step  18111\n",
            "loop 2 image : step  18112\n",
            "loop 2 image : step  18113\n",
            "loop 2 image : step  18114\n",
            "loop 2 image : step  18115\n",
            "loop 2 image : step  18116\n",
            "loop 2 image : step  18117\n",
            "loop 2 image : step  18118\n",
            "loop 2 image : step  18119\n",
            "loop 2 image : step  18120\n",
            "loop 2 image : step  18121\n",
            "loop 2 image : step  18122\n",
            "loop 2 image : step  18123\n",
            "loop 2 image : step  18124\n",
            "loop 2 image : step  18125\n",
            "loop 2 image : step  18126\n",
            "loop 2 image : step  18127\n",
            "loop 2 image : step  18128\n",
            "loop 2 image : step  18129\n",
            "loop 2 image : step  18130\n",
            "loop 2 image : step  18131\n",
            "loop 2 image : step  18132\n",
            "loop 2 image : step  18133\n",
            "loop 2 image : step  18134\n",
            "loop 2 image : step  18135\n",
            "loop 2 image : step  18136\n",
            "loop 2 image : step  18137\n",
            "loop 2 image : step  18138\n",
            "loop 2 image : step  18139\n",
            "loop 2 image : step  18140\n",
            "loop 2 image : step  18141\n",
            "loop 2 image : step  18142\n",
            "loop 2 image : step  18143\n",
            "loop 2 image : step  18144\n",
            "loop 2 image : step  18145\n",
            "loop 2 image : step  18146\n",
            "loop 2 image : step  18147\n",
            "loop 2 image : step  18148\n",
            "loop 2 image : step  18149\n",
            "loop 2 image : step  18150\n",
            "loop 2 image : step  18151\n",
            "loop 2 image : step  18152\n",
            "loop 2 image : step  18153\n",
            "loop 2 image : step  18154\n",
            "loop 2 image : step  18155\n",
            "loop 2 image : step  18156\n",
            "loop 2 image : step  18157\n",
            "loop 2 image : step  18158\n",
            "loop 2 image : step  18159\n",
            "loop 2 image : step  18160\n",
            "loop 2 image : step  18161\n",
            "loop 2 image : step  18162\n",
            "loop 2 image : step  18163\n",
            "loop 2 image : step  18164\n",
            "loop 2 image : step  18165\n",
            "loop 2 image : step  18166\n",
            "loop 2 image : step  18167\n",
            "loop 2 image : step  18168\n",
            "loop 2 image : step  18169\n",
            "loop 2 image : step  18170\n",
            "loop 2 image : step  18171\n",
            "loop 2 image : step  18172\n",
            "loop 2 image : step  18173\n",
            "loop 2 image : step  18174\n",
            "loop 2 image : step  18175\n",
            "loop 2 image : step  18176\n",
            "loop 2 image : step  18177\n",
            "loop 2 image : step  18178\n",
            "loop 2 image : step  18179\n",
            "loop 2 image : step  18180\n",
            "loop 2 image : step  18181\n",
            "loop 2 image : step  18182\n",
            "loop 2 image : step  18183\n",
            "loop 2 image : step  18184\n",
            "loop 2 image : step  18185\n",
            "loop 2 image : step  18186\n",
            "loop 2 image : step  18187\n",
            "loop 2 image : step  18188\n",
            "loop 2 image : step  18189\n",
            "loop 2 image : step  18190\n",
            "loop 2 image : step  18191\n",
            "loop 2 image : step  18192\n",
            "loop 2 image : step  18193\n",
            "loop 2 image : step  18194\n",
            "loop 2 image : step  18195\n",
            "loop 2 image : step  18196\n",
            "loop 2 image : step  18197\n",
            "loop 2 image : step  18198\n",
            "loop 2 image : step  18199\n",
            "loop 2 image : step  18200\n",
            "loop 2 image : step  18201\n",
            "loop 2 image : step  18202\n",
            "loop 2 image : step  18203\n",
            "loop 2 image : step  18204\n",
            "loop 2 image : step  18205\n",
            "loop 2 image : step  18206\n",
            "loop 2 image : step  18207\n",
            "loop 2 image : step  18208\n",
            "loop 2 image : step  18209\n",
            "loop 2 image : step  18210\n",
            "loop 2 image : step  18211\n",
            "loop 2 image : step  18212\n",
            "loop 2 image : step  18213\n",
            "loop 2 image : step  18214\n",
            "loop 2 image : step  18215\n",
            "loop 2 image : step  18216\n",
            "loop 2 image : step  18217\n",
            "loop 2 image : step  18218\n",
            "loop 2 image : step  18219\n",
            "loop 2 image : step  18220\n",
            "loop 2 image : step  18221\n",
            "loop 2 image : step  18222\n",
            "loop 2 image : step  18223\n",
            "loop 2 image : step  18224\n",
            "loop 2 image : step  18225\n",
            "loop 2 image : step  18226\n",
            "loop 2 image : step  18227\n",
            "loop 2 image : step  18228\n",
            "loop 2 image : step  18229\n",
            "loop 2 image : step  18230\n",
            "loop 2 image : step  18231\n",
            "loop 2 image : step  18232\n",
            "loop 2 image : step  18233\n",
            "loop 2 image : step  18234\n",
            "loop 2 image : step  18235\n",
            "loop 2 image : step  18236\n",
            "loop 2 image : step  18237\n",
            "loop 2 image : step  18238\n",
            "loop 2 image : step  18239\n",
            "loop 2 image : step  18240\n",
            "loop 2 image : step  18241\n",
            "loop 2 image : step  18242\n",
            "loop 2 image : step  18243\n",
            "loop 2 image : step  18244\n",
            "loop 2 image : step  18245\n",
            "loop 2 image : step  18246\n",
            "loop 2 image : step  18247\n",
            "loop 2 image : step  18248\n",
            "loop 2 image : step  18249\n",
            "loop 2 image : step  18250\n",
            "loop 2 image : step  18251\n",
            "loop 2 image : step  18252\n",
            "loop 2 image : step  18253\n",
            "loop 2 image : step  18254\n",
            "loop 2 image : step  18255\n",
            "loop 2 image : step  18256\n",
            "loop 2 image : step  18257\n",
            "loop 2 image : step  18258\n",
            "loop 2 image : step  18259\n",
            "loop 2 image : step  18260\n",
            "loop 2 image : step  18261\n",
            "loop 2 image : step  18262\n",
            "loop 2 image : step  18263\n",
            "loop 2 image : step  18264\n",
            "loop 2 image : step  18265\n",
            "loop 2 image : step  18266\n",
            "loop 2 image : step  18267\n",
            "loop 2 image : step  18268\n",
            "loop 2 image : step  18269\n",
            "loop 2 image : step  18270\n",
            "loop 2 image : step  18271\n",
            "loop 2 image : step  18272\n",
            "loop 2 image : step  18273\n",
            "loop 2 image : step  18274\n",
            "loop 2 image : step  18275\n",
            "loop 2 image : step  18276\n",
            "loop 2 image : step  18277\n",
            "loop 2 image : step  18278\n",
            "loop 2 image : step  18279\n",
            "loop 2 image : step  18280\n",
            "loop 2 image : step  18281\n",
            "loop 2 image : step  18282\n",
            "loop 2 image : step  18283\n",
            "loop 2 image : step  18284\n",
            "loop 2 image : step  18285\n",
            "loop 2 image : step  18286\n",
            "loop 2 image : step  18287\n",
            "loop 2 image : step  18288\n",
            "loop 2 image : step  18289\n",
            "loop 2 image : step  18290\n",
            "loop 2 image : step  18291\n",
            "loop 2 image : step  18292\n",
            "loop 2 image : step  18293\n",
            "loop 2 image : step  18294\n",
            "loop 2 image : step  18295\n",
            "loop 2 image : step  18296\n",
            "loop 2 image : step  18297\n",
            "loop 2 image : step  18298\n",
            "loop 2 image : step  18299\n",
            "loop 2 image : step  18300\n",
            "loop 2 image : step  18301\n",
            "loop 2 image : step  18302\n",
            "loop 2 image : step  18303\n",
            "loop 2 image : step  18304\n",
            "loop 2 image : step  18305\n",
            "loop 2 image : step  18306\n",
            "loop 2 image : step  18307\n",
            "loop 2 image : step  18308\n",
            "loop 2 image : step  18309\n",
            "loop 2 image : step  18310\n",
            "loop 2 image : step  18311\n",
            "loop 2 image : step  18312\n",
            "loop 2 image : step  18313\n",
            "loop 2 image : step  18314\n",
            "loop 2 image : step  18315\n",
            "loop 2 image : step  18316\n",
            "loop 2 image : step  18317\n",
            "loop 2 image : step  18318\n",
            "loop 2 image : step  18319\n",
            "loop 2 image : step  18320\n",
            "loop 2 image : step  18321\n",
            "loop 2 image : step  18322\n",
            "loop 2 image : step  18323\n",
            "loop 2 image : step  18324\n",
            "loop 2 image : step  18325\n",
            "loop 2 image : step  18326\n",
            "loop 2 image : step  18327\n",
            "loop 2 image : step  18328\n",
            "loop 2 image : step  18329\n",
            "loop 2 image : step  18330\n",
            "loop 2 image : step  18331\n",
            "loop 2 image : step  18332\n",
            "loop 2 image : step  18333\n",
            "loop 2 image : step  18334\n",
            "loop 2 image : step  18335\n",
            "loop 2 image : step  18336\n",
            "loop 2 image : step  18337\n",
            "loop 2 image : step  18338\n",
            "loop 2 image : step  18339\n",
            "loop 2 image : step  18340\n",
            "loop 2 image : step  18341\n",
            "loop 2 image : step  18342\n",
            "loop 2 image : step  18343\n",
            "loop 2 image : step  18344\n",
            "loop 2 image : step  18345\n",
            "loop 2 image : step  18346\n",
            "loop 2 image : step  18347\n",
            "loop 2 image : step  18348\n",
            "loop 2 image : step  18349\n",
            "loop 2 image : step  18350\n",
            "loop 2 image : step  18351\n",
            "loop 2 image : step  18352\n",
            "loop 2 image : step  18353\n",
            "loop 2 image : step  18354\n",
            "loop 2 image : step  18355\n",
            "loop 2 image : step  18356\n",
            "loop 2 image : step  18357\n",
            "loop 2 image : step  18358\n",
            "loop 2 image : step  18359\n",
            "loop 2 image : step  18360\n",
            "loop 2 image : step  18361\n",
            "loop 2 image : step  18362\n",
            "loop 2 image : step  18363\n",
            "loop 2 image : step  18364\n",
            "loop 2 image : step  18365\n",
            "loop 2 image : step  18366\n",
            "loop 2 image : step  18367\n",
            "loop 2 image : step  18368\n",
            "loop 2 image : step  18369\n",
            "loop 2 image : step  18370\n",
            "loop 2 image : step  18371\n",
            "loop 2 image : step  18372\n",
            "loop 2 image : step  18373\n",
            "loop 2 image : step  18374\n",
            "loop 2 image : step  18375\n",
            "loop 2 image : step  18376\n",
            "loop 2 image : step  18377\n",
            "loop 2 image : step  18378\n",
            "loop 2 image : step  18379\n",
            "loop 2 image : step  18380\n",
            "loop 2 image : step  18381\n",
            "loop 2 image : step  18382\n",
            "loop 2 image : step  18383\n",
            "loop 2 image : step  18384\n",
            "loop 2 image : step  18385\n",
            "loop 2 image : step  18386\n",
            "loop 2 image : step  18387\n",
            "loop 2 image : step  18388\n",
            "loop 2 image : step  18389\n",
            "loop 2 image : step  18390\n",
            "loop 2 image : step  18391\n",
            "loop 2 image : step  18392\n",
            "loop 2 image : step  18393\n",
            "loop 2 image : step  18394\n",
            "loop 2 image : step  18395\n",
            "loop 2 image : step  18396\n",
            "loop 2 image : step  18397\n",
            "loop 2 image : step  18398\n",
            "loop 2 image : step  18399\n",
            "loop 2 image : step  18400\n",
            "loop 2 image : step  18401\n",
            "loop 2 image : step  18402\n",
            "loop 2 image : step  18403\n",
            "loop 2 image : step  18404\n",
            "loop 2 image : step  18405\n",
            "loop 2 image : step  18406\n",
            "loop 2 image : step  18407\n",
            "loop 2 image : step  18408\n",
            "loop 2 image : step  18409\n",
            "loop 2 image : step  18410\n",
            "loop 2 image : step  18411\n",
            "loop 2 image : step  18412\n",
            "loop 2 image : step  18413\n",
            "loop 2 image : step  18414\n",
            "loop 2 image : step  18415\n",
            "loop 2 image : step  18416\n",
            "loop 2 image : step  18417\n",
            "loop 2 image : step  18418\n",
            "loop 2 image : step  18419\n",
            "loop 2 image : step  18420\n",
            "loop 2 image : step  18421\n",
            "loop 2 image : step  18422\n",
            "loop 2 image : step  18423\n",
            "loop 2 image : step  18424\n",
            "loop 2 image : step  18425\n",
            "loop 2 image : step  18426\n",
            "loop 2 image : step  18427\n",
            "loop 2 image : step  18428\n",
            "loop 2 image : step  18429\n",
            "loop 2 image : step  18430\n",
            "loop 2 image : step  18431\n",
            "loop 2 image : step  18432\n",
            "loop 2 image : step  18433\n",
            "loop 2 image : step  18434\n",
            "loop 2 image : step  18435\n",
            "loop 2 image : step  18436\n",
            "loop 2 image : step  18437\n",
            "loop 2 image : step  18438\n",
            "loop 2 image : step  18439\n",
            "loop 2 image : step  18440\n",
            "loop 2 image : step  18441\n",
            "loop 2 image : step  18442\n",
            "loop 2 image : step  18443\n",
            "loop 2 image : step  18444\n",
            "loop 2 image : step  18445\n",
            "loop 2 image : step  18446\n",
            "loop 2 image : step  18447\n",
            "loop 2 image : step  18448\n",
            "loop 2 image : step  18449\n",
            "loop 2 image : step  18450\n",
            "loop 2 image : step  18451\n",
            "loop 2 image : step  18452\n",
            "loop 2 image : step  18453\n",
            "loop 2 image : step  18454\n",
            "loop 2 image : step  18455\n",
            "loop 2 image : step  18456\n",
            "loop 2 image : step  18457\n",
            "loop 2 image : step  18458\n",
            "loop 2 image : step  18459\n",
            "loop 2 image : step  18460\n",
            "loop 2 image : step  18461\n",
            "loop 2 image : step  18462\n",
            "loop 2 image : step  18463\n",
            "loop 2 image : step  18464\n",
            "loop 2 image : step  18465\n",
            "loop 2 image : step  18466\n",
            "loop 2 image : step  18467\n",
            "loop 2 image : step  18468\n",
            "loop 2 image : step  18469\n",
            "loop 2 image : step  18470\n",
            "loop 2 image : step  18471\n",
            "loop 2 image : step  18472\n",
            "loop 2 image : step  18473\n",
            "loop 2 image : step  18474\n",
            "loop 2 image : step  18475\n",
            "loop 2 image : step  18476\n",
            "loop 2 image : step  18477\n",
            "loop 2 image : step  18478\n",
            "loop 2 image : step  18479\n",
            "loop 2 image : step  18480\n",
            "loop 2 image : step  18481\n",
            "loop 2 image : step  18482\n",
            "loop 2 image : step  18483\n",
            "loop 2 image : step  18484\n",
            "loop 2 image : step  18485\n",
            "loop 2 image : step  18486\n",
            "loop 2 image : step  18487\n",
            "loop 2 image : step  18488\n",
            "loop 2 image : step  18489\n",
            "loop 2 image : step  18490\n",
            "loop 2 image : step  18491\n",
            "loop 2 image : step  18492\n",
            "loop 2 image : step  18493\n",
            "loop 2 image : step  18494\n",
            "loop 2 image : step  18495\n",
            "loop 2 image : step  18496\n",
            "loop 2 image : step  18497\n",
            "loop 2 image : step  18498\n",
            "loop 2 image : step  18499\n",
            "loop 2 image : step  18500\n",
            "loop 2 image : step  18501\n",
            "loop 2 image : step  18502\n",
            "loop 2 image : step  18503\n",
            "loop 2 image : step  18504\n",
            "loop 2 image : step  18505\n",
            "loop 2 image : step  18506\n",
            "loop 2 image : step  18507\n",
            "loop 2 image : step  18508\n",
            "loop 2 image : step  18509\n",
            "loop 2 image : step  18510\n",
            "loop 2 image : step  18511\n",
            "loop 2 image : step  18512\n",
            "loop 2 image : step  18513\n",
            "loop 2 image : step  18514\n",
            "loop 2 image : step  18515\n",
            "loop 2 image : step  18516\n",
            "loop 2 image : step  18517\n",
            "loop 2 image : step  18518\n",
            "loop 2 image : step  18519\n",
            "loop 2 image : step  18520\n",
            "loop 2 image : step  18521\n",
            "loop 2 image : step  18522\n",
            "loop 2 image : step  18523\n",
            "loop 2 image : step  18524\n",
            "loop 2 image : step  18525\n",
            "loop 2 image : step  18526\n",
            "loop 2 image : step  18527\n",
            "loop 2 image : step  18528\n",
            "loop 2 image : step  18529\n",
            "loop 2 image : step  18530\n",
            "loop 2 image : step  18531\n",
            "loop 2 image : step  18532\n",
            "loop 2 image : step  18533\n",
            "loop 2 image : step  18534\n",
            "loop 2 image : step  18535\n",
            "loop 2 image : step  18536\n",
            "loop 2 image : step  18537\n",
            "loop 2 image : step  18538\n",
            "loop 2 image : step  18539\n",
            "loop 2 image : step  18540\n",
            "loop 2 image : step  18541\n",
            "loop 2 image : step  18542\n",
            "loop 2 image : step  18543\n",
            "loop 2 image : step  18544\n",
            "loop 2 image : step  18545\n",
            "loop 2 image : step  18546\n",
            "loop 2 image : step  18547\n",
            "loop 2 image : step  18548\n",
            "loop 2 image : step  18549\n",
            "loop 2 image : step  18550\n",
            "loop 2 image : step  18551\n",
            "loop 2 image : step  18552\n",
            "loop 2 image : step  18553\n",
            "loop 2 image : step  18554\n",
            "loop 2 image : step  18555\n",
            "loop 2 image : step  18556\n",
            "loop 2 image : step  18557\n",
            "loop 2 image : step  18558\n",
            "loop 2 image : step  18559\n",
            "loop 2 image : step  18560\n",
            "loop 2 image : step  18561\n",
            "loop 2 image : step  18562\n",
            "loop 2 image : step  18563\n",
            "loop 2 image : step  18564\n",
            "loop 2 image : step  18565\n",
            "loop 2 image : step  18566\n",
            "loop 2 image : step  18567\n",
            "loop 2 image : step  18568\n",
            "loop 2 image : step  18569\n",
            "loop 2 image : step  18570\n",
            "loop 2 image : step  18571\n",
            "loop 2 image : step  18572\n",
            "loop 2 image : step  18573\n",
            "loop 2 image : step  18574\n",
            "loop 2 image : step  18575\n",
            "loop 2 image : step  18576\n",
            "loop 2 image : step  18577\n",
            "loop 2 image : step  18578\n",
            "loop 2 image : step  18579\n",
            "loop 2 image : step  18580\n",
            "loop 2 image : step  18581\n",
            "loop 2 image : step  18582\n",
            "loop 2 image : step  18583\n",
            "loop 2 image : step  18584\n",
            "loop 2 image : step  18585\n",
            "loop 2 image : step  18586\n",
            "loop 2 image : step  18587\n",
            "loop 2 image : step  18588\n",
            "loop 2 image : step  18589\n",
            "loop 2 image : step  18590\n",
            "loop 2 image : step  18591\n",
            "loop 2 image : step  18592\n",
            "loop 2 image : step  18593\n",
            "loop 2 image : step  18594\n",
            "loop 2 image : step  18595\n",
            "loop 2 image : step  18596\n",
            "loop 2 image : step  18597\n",
            "loop 2 image : step  18598\n",
            "loop 2 image : step  18599\n",
            "loop 2 image : step  18600\n",
            "loop 2 image : step  18601\n",
            "loop 2 image : step  18602\n",
            "loop 2 image : step  18603\n",
            "loop 2 image : step  18604\n",
            "loop 2 image : step  18605\n",
            "loop 2 image : step  18606\n",
            "loop 2 image : step  18607\n",
            "loop 2 image : step  18608\n",
            "loop 2 image : step  18609\n",
            "loop 2 image : step  18610\n",
            "loop 2 image : step  18611\n",
            "loop 2 image : step  18612\n",
            "loop 2 image : step  18613\n",
            "loop 2 image : step  18614\n",
            "loop 2 image : step  18615\n",
            "loop 2 image : step  18616\n",
            "loop 2 image : step  18617\n",
            "loop 2 image : step  18618\n",
            "loop 2 image : step  18619\n",
            "loop 2 image : step  18620\n",
            "loop 2 image : step  18621\n",
            "loop 2 image : step  18622\n",
            "loop 2 image : step  18623\n",
            "loop 2 image : step  18624\n",
            "loop 2 image : step  18625\n",
            "loop 2 image : step  18626\n",
            "loop 2 image : step  18627\n",
            "loop 2 image : step  18628\n",
            "loop 2 image : step  18629\n",
            "loop 2 image : step  18630\n",
            "loop 2 image : step  18631\n",
            "loop 2 image : step  18632\n",
            "loop 2 image : step  18633\n",
            "loop 2 image : step  18634\n",
            "loop 2 image : step  18635\n",
            "loop 2 image : step  18636\n",
            "loop 2 image : step  18637\n",
            "loop 2 image : step  18638\n",
            "loop 2 image : step  18639\n",
            "loop 2 image : step  18640\n",
            "loop 2 image : step  18641\n",
            "loop 2 image : step  18642\n",
            "loop 2 image : step  18643\n",
            "loop 2 image : step  18644\n",
            "loop 2 image : step  18645\n",
            "loop 2 image : step  18646\n",
            "loop 2 image : step  18647\n",
            "loop 2 image : step  18648\n",
            "loop 2 image : step  18649\n",
            "loop 2 image : step  18650\n",
            "loop 2 image : step  18651\n",
            "loop 2 image : step  18652\n",
            "loop 2 image : step  18653\n",
            "loop 2 image : step  18654\n",
            "loop 2 image : step  18655\n",
            "loop 2 image : step  18656\n",
            "loop 2 image : step  18657\n",
            "loop 2 image : step  18658\n",
            "loop 2 image : step  18659\n",
            "loop 2 image : step  18660\n",
            "loop 2 image : step  18661\n",
            "loop 2 image : step  18662\n",
            "loop 2 image : step  18663\n",
            "loop 2 image : step  18664\n",
            "loop 2 image : step  18665\n",
            "loop 2 image : step  18666\n",
            "loop 2 image : step  18667\n",
            "loop 2 image : step  18668\n",
            "loop 2 image : step  18669\n",
            "loop 2 image : step  18670\n",
            "loop 2 image : step  18671\n",
            "loop 2 image : step  18672\n",
            "loop 2 image : step  18673\n",
            "loop 2 image : step  18674\n",
            "loop 2 image : step  18675\n",
            "loop 2 image : step  18676\n",
            "loop 2 image : step  18677\n",
            "loop 2 image : step  18678\n",
            "loop 2 image : step  18679\n",
            "loop 2 image : step  18680\n",
            "loop 2 image : step  18681\n",
            "loop 2 image : step  18682\n",
            "loop 2 image : step  18683\n",
            "loop 2 image : step  18684\n",
            "loop 2 image : step  18685\n",
            "loop 2 image : step  18686\n",
            "loop 2 image : step  18687\n",
            "loop 2 image : step  18688\n",
            "loop 2 image : step  18689\n",
            "loop 2 image : step  18690\n",
            "loop 2 image : step  18691\n",
            "loop 2 image : step  18692\n",
            "loop 2 image : step  18693\n",
            "loop 2 image : step  18694\n",
            "loop 2 image : step  18695\n",
            "loop 2 image : step  18696\n",
            "loop 2 image : step  18697\n",
            "loop 2 image : step  18698\n",
            "loop 2 image : step  18699\n",
            "loop 2 image : step  18700\n",
            "loop 2 image : step  18701\n",
            "loop 2 image : step  18702\n",
            "loop 2 image : step  18703\n",
            "loop 2 image : step  18704\n",
            "loop 2 image : step  18705\n",
            "loop 2 image : step  18706\n",
            "loop 2 image : step  18707\n",
            "loop 2 image : step  18708\n",
            "loop 2 image : step  18709\n",
            "loop 2 image : step  18710\n",
            "loop 2 image : step  18711\n",
            "loop 2 image : step  18712\n",
            "loop 2 image : step  18713\n",
            "loop 2 image : step  18714\n",
            "loop 2 image : step  18715\n",
            "loop 2 image : step  18716\n",
            "loop 2 image : step  18717\n",
            "loop 2 image : step  18718\n",
            "loop 2 image : step  18719\n",
            "loop 2 image : step  18720\n",
            "loop 2 image : step  18721\n",
            "loop 2 image : step  18722\n",
            "loop 2 image : step  18723\n",
            "loop 2 image : step  18724\n",
            "loop 2 image : step  18725\n",
            "loop 2 image : step  18726\n",
            "loop 2 image : step  18727\n",
            "loop 2 image : step  18728\n",
            "loop 2 image : step  18729\n",
            "loop 2 image : step  18730\n",
            "loop 2 image : step  18731\n",
            "loop 2 image : step  18732\n",
            "loop 2 image : step  18733\n",
            "loop 2 image : step  18734\n",
            "loop 2 image : step  18735\n",
            "loop 2 image : step  18736\n",
            "loop 2 image : step  18737\n",
            "loop 2 image : step  18738\n",
            "loop 2 image : step  18739\n",
            "loop 2 image : step  18740\n",
            "loop 2 image : step  18741\n",
            "loop 2 image : step  18742\n",
            "loop 2 image : step  18743\n",
            "loop 2 image : step  18744\n",
            "loop 2 image : step  18745\n",
            "loop 2 image : step  18746\n",
            "loop 2 image : step  18747\n",
            "loop 2 image : step  18748\n",
            "loop 2 image : step  18749\n",
            "loop 2 image : step  18750\n",
            "loop 2 image : step  18751\n",
            "loop 2 image : step  18752\n",
            "loop 2 image : step  18753\n",
            "loop 2 image : step  18754\n",
            "loop 2 image : step  18755\n",
            "loop 2 image : step  18756\n",
            "loop 2 image : step  18757\n",
            "loop 2 image : step  18758\n",
            "loop 2 image : step  18759\n",
            "loop 2 image : step  18760\n",
            "loop 2 image : step  18761\n",
            "loop 2 image : step  18762\n",
            "loop 2 image : step  18763\n",
            "loop 2 image : step  18764\n",
            "loop 2 image : step  18765\n",
            "loop 2 image : step  18766\n",
            "loop 2 image : step  18767\n",
            "loop 2 image : step  18768\n",
            "loop 2 image : step  18769\n",
            "loop 2 image : step  18770\n",
            "loop 2 image : step  18771\n",
            "loop 2 image : step  18772\n",
            "loop 2 image : step  18773\n",
            "loop 2 image : step  18774\n",
            "loop 2 image : step  18775\n",
            "loop 2 image : step  18776\n",
            "loop 2 image : step  18777\n",
            "loop 2 image : step  18778\n",
            "loop 2 image : step  18779\n",
            "loop 2 image : step  18780\n",
            "loop 2 image : step  18781\n",
            "loop 2 image : step  18782\n",
            "loop 2 image : step  18783\n",
            "loop 2 image : step  18784\n",
            "loop 2 image : step  18785\n",
            "loop 2 image : step  18786\n",
            "loop 2 image : step  18787\n",
            "loop 2 image : step  18788\n",
            "loop 2 image : step  18789\n",
            "loop 2 image : step  18790\n",
            "loop 2 image : step  18791\n",
            "loop 2 image : step  18792\n",
            "loop 2 image : step  18793\n",
            "loop 2 image : step  18794\n",
            "loop 2 image : step  18795\n",
            "loop 2 image : step  18796\n",
            "loop 2 image : step  18797\n",
            "loop 2 image : step  18798\n",
            "loop 2 image : step  18799\n",
            "loop 2 image : step  18800\n",
            "loop 2 image : step  18801\n",
            "loop 2 image : step  18802\n",
            "loop 2 image : step  18803\n",
            "loop 2 image : step  18804\n",
            "loop 2 image : step  18805\n",
            "loop 2 image : step  18806\n",
            "loop 2 image : step  18807\n",
            "loop 2 image : step  18808\n",
            "loop 2 image : step  18809\n",
            "loop 2 image : step  18810\n",
            "loop 2 image : step  18811\n",
            "loop 2 image : step  18812\n",
            "loop 2 image : step  18813\n",
            "loop 2 image : step  18814\n",
            "loop 2 image : step  18815\n",
            "loop 2 image : step  18816\n",
            "loop 2 image : step  18817\n",
            "loop 2 image : step  18818\n",
            "loop 2 image : step  18819\n",
            "loop 2 image : step  18820\n",
            "loop 2 image : step  18821\n",
            "loop 2 image : step  18822\n",
            "loop 2 image : step  18823\n",
            "loop 2 image : step  18824\n",
            "loop 2 image : step  18825\n",
            "loop 2 image : step  18826\n",
            "loop 2 image : step  18827\n",
            "loop 2 image : step  18828\n",
            "loop 2 image : step  18829\n",
            "loop 2 image : step  18830\n",
            "loop 2 image : step  18831\n",
            "loop 2 image : step  18832\n",
            "loop 2 image : step  18833\n",
            "loop 2 image : step  18834\n",
            "loop 2 image : step  18835\n",
            "loop 2 image : step  18836\n",
            "loop 2 image : step  18837\n",
            "loop 2 image : step  18838\n",
            "loop 2 image : step  18839\n",
            "loop 2 image : step  18840\n",
            "loop 2 image : step  18841\n",
            "loop 2 image : step  18842\n",
            "loop 2 image : step  18843\n",
            "loop 2 image : step  18844\n",
            "loop 2 image : step  18845\n",
            "loop 2 image : step  18846\n",
            "loop 2 image : step  18847\n",
            "loop 2 image : step  18848\n",
            "loop 2 image : step  18849\n",
            "loop 2 image : step  18850\n",
            "loop 2 image : step  18851\n",
            "loop 2 image : step  18852\n",
            "loop 2 image : step  18853\n",
            "loop 2 image : step  18854\n",
            "loop 2 image : step  18855\n",
            "loop 2 image : step  18856\n",
            "loop 2 image : step  18857\n",
            "loop 2 image : step  18858\n",
            "loop 2 image : step  18859\n",
            "loop 2 image : step  18860\n",
            "loop 2 image : step  18861\n",
            "loop 2 image : step  18862\n",
            "loop 2 image : step  18863\n",
            "loop 2 image : step  18864\n",
            "loop 2 image : step  18865\n",
            "loop 2 image : step  18866\n",
            "loop 2 image : step  18867\n",
            "loop 2 image : step  18868\n",
            "loop 2 image : step  18869\n",
            "loop 2 image : step  18870\n",
            "loop 2 image : step  18871\n",
            "loop 2 image : step  18872\n",
            "loop 2 image : step  18873\n",
            "loop 2 image : step  18874\n",
            "loop 2 image : step  18875\n",
            "loop 2 image : step  18876\n",
            "loop 2 image : step  18877\n",
            "loop 2 image : step  18878\n",
            "loop 2 image : step  18879\n",
            "loop 2 image : step  18880\n",
            "loop 2 image : step  18881\n",
            "loop 2 image : step  18882\n",
            "loop 2 image : step  18883\n",
            "loop 2 image : step  18884\n",
            "loop 2 image : step  18885\n",
            "loop 2 image : step  18886\n",
            "loop 2 image : step  18887\n",
            "loop 2 image : step  18888\n",
            "loop 2 image : step  18889\n",
            "loop 2 image : step  18890\n",
            "loop 2 image : step  18891\n",
            "loop 2 image : step  18892\n",
            "loop 2 image : step  18893\n",
            "loop 2 image : step  18894\n",
            "loop 2 image : step  18895\n",
            "loop 2 image : step  18896\n",
            "loop 2 image : step  18897\n",
            "loop 2 image : step  18898\n",
            "loop 2 image : step  18899\n",
            "loop 2 image : step  18900\n",
            "loop 2 image : step  18901\n",
            "loop 2 image : step  18902\n",
            "loop 2 image : step  18903\n",
            "loop 2 image : step  18904\n",
            "loop 2 image : step  18905\n",
            "loop 2 image : step  18906\n",
            "loop 2 image : step  18907\n",
            "loop 2 image : step  18908\n",
            "loop 2 image : step  18909\n",
            "loop 2 image : step  18910\n",
            "loop 2 image : step  18911\n",
            "loop 2 image : step  18912\n",
            "loop 2 image : step  18913\n",
            "loop 2 image : step  18914\n",
            "loop 2 image : step  18915\n",
            "loop 2 image : step  18916\n",
            "loop 2 image : step  18917\n",
            "loop 2 image : step  18918\n",
            "loop 2 image : step  18919\n",
            "loop 2 image : step  18920\n",
            "loop 2 image : step  18921\n",
            "loop 2 image : step  18922\n",
            "loop 2 image : step  18923\n",
            "loop 2 image : step  18924\n",
            "loop 2 image : step  18925\n",
            "loop 2 image : step  18926\n",
            "loop 2 image : step  18927\n",
            "loop 2 image : step  18928\n",
            "loop 2 image : step  18929\n",
            "loop 2 image : step  18930\n",
            "loop 2 image : step  18931\n",
            "loop 2 image : step  18932\n",
            "loop 2 image : step  18933\n",
            "loop 2 image : step  18934\n",
            "loop 2 image : step  18935\n",
            "loop 2 image : step  18936\n",
            "loop 2 image : step  18937\n",
            "loop 2 image : step  18938\n",
            "loop 2 image : step  18939\n",
            "loop 2 image : step  18940\n",
            "loop 2 image : step  18941\n",
            "loop 2 image : step  18942\n",
            "loop 2 image : step  18943\n",
            "loop 2 image : step  18944\n",
            "loop 2 image : step  18945\n",
            "loop 2 image : step  18946\n",
            "loop 2 image : step  18947\n",
            "loop 2 image : step  18948\n",
            "loop 2 image : step  18949\n",
            "loop 2 image : step  18950\n",
            "loop 2 image : step  18951\n",
            "loop 2 image : step  18952\n",
            "loop 2 image : step  18953\n",
            "loop 2 image : step  18954\n",
            "loop 2 image : step  18955\n",
            "loop 2 image : step  18956\n",
            "loop 2 image : step  18957\n",
            "loop 2 image : step  18958\n",
            "loop 2 image : step  18959\n",
            "loop 2 image : step  18960\n",
            "loop 2 image : step  18961\n",
            "loop 2 image : step  18962\n",
            "loop 2 image : step  18963\n",
            "loop 2 image : step  18964\n",
            "loop 2 image : step  18965\n",
            "loop 2 image : step  18966\n",
            "loop 2 image : step  18967\n",
            "loop 2 image : step  18968\n",
            "loop 2 image : step  18969\n",
            "loop 2 image : step  18970\n",
            "loop 2 image : step  18971\n",
            "loop 2 image : step  18972\n",
            "loop 2 image : step  18973\n",
            "loop 2 image : step  18974\n",
            "loop 2 image : step  18975\n",
            "loop 2 image : step  18976\n",
            "loop 2 image : step  18977\n",
            "loop 2 image : step  18978\n",
            "loop 2 image : step  18979\n",
            "loop 2 image : step  18980\n",
            "loop 2 image : step  18981\n",
            "loop 2 image : step  18982\n",
            "loop 2 image : step  18983\n",
            "loop 2 image : step  18984\n",
            "loop 2 image : step  18985\n",
            "loop 2 image : step  18986\n",
            "loop 2 image : step  18987\n",
            "loop 2 image : step  18988\n",
            "loop 2 image : step  18989\n",
            "loop 2 image : step  18990\n",
            "loop 2 image : step  18991\n",
            "loop 2 image : step  18992\n",
            "loop 2 image : step  18993\n",
            "loop 2 image : step  18994\n",
            "loop 2 image : step  18995\n",
            "loop 2 image : step  18996\n",
            "loop 2 image : step  18997\n",
            "loop 2 image : step  18998\n",
            "loop 2 image : step  18999\n",
            "loop 2 image : step  19000\n",
            "loop 2 image : step  19001\n",
            "loop 2 image : step  19002\n",
            "loop 2 image : step  19003\n",
            "loop 2 image : step  19004\n",
            "loop 2 image : step  19005\n",
            "loop 2 image : step  19006\n",
            "loop 2 image : step  19007\n",
            "loop 2 image : step  19008\n",
            "loop 2 image : step  19009\n",
            "loop 2 image : step  19010\n",
            "loop 2 image : step  19011\n",
            "loop 2 image : step  19012\n",
            "loop 2 image : step  19013\n",
            "loop 2 image : step  19014\n",
            "loop 2 image : step  19015\n",
            "loop 2 image : step  19016\n",
            "loop 2 image : step  19017\n",
            "loop 2 image : step  19018\n",
            "loop 2 image : step  19019\n",
            "loop 2 image : step  19020\n",
            "loop 2 image : step  19021\n",
            "loop 2 image : step  19022\n",
            "loop 2 image : step  19023\n",
            "loop 2 image : step  19024\n",
            "loop 2 image : step  19025\n",
            "loop 2 image : step  19026\n",
            "loop 2 image : step  19027\n",
            "loop 2 image : step  19028\n",
            "loop 2 image : step  19029\n",
            "loop 2 image : step  19030\n",
            "loop 2 image : step  19031\n",
            "loop 2 image : step  19032\n",
            "loop 2 image : step  19033\n",
            "loop 2 image : step  19034\n",
            "loop 2 image : step  19035\n",
            "loop 2 image : step  19036\n",
            "loop 2 image : step  19037\n",
            "loop 2 image : step  19038\n",
            "loop 2 image : step  19039\n",
            "loop 2 image : step  19040\n",
            "loop 2 image : step  19041\n",
            "loop 2 image : step  19042\n",
            "loop 2 image : step  19043\n",
            "loop 2 image : step  19044\n",
            "loop 2 image : step  19045\n",
            "loop 2 image : step  19046\n",
            "loop 2 image : step  19047\n",
            "loop 2 image : step  19048\n",
            "loop 2 image : step  19049\n",
            "loop 2 image : step  19050\n",
            "loop 2 image : step  19051\n",
            "loop 2 image : step  19052\n",
            "loop 2 image : step  19053\n",
            "loop 2 image : step  19054\n",
            "loop 2 image : step  19055\n",
            "loop 2 image : step  19056\n",
            "loop 2 image : step  19057\n",
            "loop 2 image : step  19058\n",
            "loop 2 image : step  19059\n",
            "loop 2 image : step  19060\n",
            "loop 2 image : step  19061\n",
            "loop 2 image : step  19062\n",
            "loop 2 image : step  19063\n",
            "loop 2 image : step  19064\n",
            "loop 2 image : step  19065\n",
            "loop 2 image : step  19066\n",
            "loop 2 image : step  19067\n",
            "loop 2 image : step  19068\n",
            "loop 2 image : step  19069\n",
            "loop 2 image : step  19070\n",
            "loop 2 image : step  19071\n",
            "loop 2 image : step  19072\n",
            "loop 2 image : step  19073\n",
            "loop 2 image : step  19074\n",
            "loop 2 image : step  19075\n",
            "loop 2 image : step  19076\n",
            "loop 2 image : step  19077\n",
            "loop 2 image : step  19078\n",
            "loop 2 image : step  19079\n",
            "loop 2 image : step  19080\n",
            "loop 2 image : step  19081\n",
            "loop 2 image : step  19082\n",
            "loop 2 image : step  19083\n",
            "loop 2 image : step  19084\n",
            "loop 2 image : step  19085\n",
            "loop 2 image : step  19086\n",
            "loop 2 image : step  19087\n",
            "loop 2 image : step  19088\n",
            "loop 2 image : step  19089\n",
            "loop 2 image : step  19090\n",
            "loop 2 image : step  19091\n",
            "loop 2 image : step  19092\n",
            "loop 2 image : step  19093\n",
            "loop 2 image : step  19094\n",
            "loop 2 image : step  19095\n",
            "loop 2 image : step  19096\n",
            "loop 2 image : step  19097\n",
            "loop 2 image : step  19098\n",
            "loop 2 image : step  19099\n",
            "loop 2 image : step  19100\n",
            "loop 2 image : step  19101\n",
            "loop 2 image : step  19102\n",
            "loop 2 image : step  19103\n",
            "loop 2 image : step  19104\n",
            "loop 2 image : step  19105\n",
            "loop 2 image : step  19106\n",
            "loop 2 image : step  19107\n",
            "loop 2 image : step  19108\n",
            "loop 2 image : step  19109\n",
            "loop 2 image : step  19110\n",
            "loop 2 image : step  19111\n",
            "loop 2 image : step  19112\n",
            "loop 2 image : step  19113\n",
            "loop 2 image : step  19114\n",
            "loop 2 image : step  19115\n",
            "loop 2 image : step  19116\n",
            "loop 2 image : step  19117\n",
            "loop 2 image : step  19118\n",
            "loop 2 image : step  19119\n",
            "loop 2 image : step  19120\n",
            "loop 2 image : step  19121\n",
            "loop 2 image : step  19122\n",
            "loop 2 image : step  19123\n",
            "loop 2 image : step  19124\n",
            "loop 2 image : step  19125\n",
            "loop 2 image : step  19126\n",
            "loop 2 image : step  19127\n",
            "loop 2 image : step  19128\n",
            "loop 2 image : step  19129\n",
            "loop 2 image : step  19130\n",
            "loop 2 image : step  19131\n",
            "loop 2 image : step  19132\n",
            "loop 2 image : step  19133\n",
            "loop 2 image : step  19134\n",
            "loop 2 image : step  19135\n",
            "loop 2 image : step  19136\n",
            "loop 2 image : step  19137\n",
            "loop 2 image : step  19138\n",
            "loop 2 image : step  19139\n",
            "loop 2 image : step  19140\n",
            "loop 2 image : step  19141\n",
            "loop 2 image : step  19142\n",
            "loop 2 image : step  19143\n",
            "loop 2 image : step  19144\n",
            "loop 2 image : step  19145\n",
            "loop 2 image : step  19146\n",
            "loop 2 image : step  19147\n",
            "loop 2 image : step  19148\n",
            "loop 2 image : step  19149\n",
            "loop 2 image : step  19150\n",
            "loop 2 image : step  19151\n",
            "loop 2 image : step  19152\n",
            "loop 2 image : step  19153\n",
            "loop 2 image : step  19154\n",
            "loop 2 image : step  19155\n",
            "loop 2 image : step  19156\n",
            "loop 2 image : step  19157\n",
            "loop 2 image : step  19158\n",
            "loop 2 image : step  19159\n",
            "loop 2 image : step  19160\n",
            "loop 2 image : step  19161\n",
            "loop 2 image : step  19162\n",
            "loop 2 image : step  19163\n",
            "loop 2 image : step  19164\n",
            "loop 2 image : step  19165\n",
            "loop 2 image : step  19166\n",
            "loop 2 image : step  19167\n",
            "loop 2 image : step  19168\n",
            "loop 2 image : step  19169\n",
            "loop 2 image : step  19170\n",
            "loop 2 image : step  19171\n",
            "loop 2 image : step  19172\n",
            "loop 2 image : step  19173\n",
            "loop 2 image : step  19174\n",
            "loop 2 image : step  19175\n",
            "loop 2 image : step  19176\n",
            "loop 2 image : step  19177\n",
            "loop 2 image : step  19178\n",
            "loop 2 image : step  19179\n",
            "loop 2 image : step  19180\n",
            "loop 2 image : step  19181\n",
            "loop 2 image : step  19182\n",
            "loop 2 image : step  19183\n",
            "loop 2 image : step  19184\n",
            "loop 2 image : step  19185\n",
            "loop 2 image : step  19186\n",
            "loop 2 image : step  19187\n",
            "loop 2 image : step  19188\n",
            "loop 2 image : step  19189\n",
            "loop 2 image : step  19190\n",
            "loop 2 image : step  19191\n",
            "loop 2 image : step  19192\n",
            "loop 2 image : step  19193\n",
            "loop 2 image : step  19194\n",
            "loop 2 image : step  19195\n",
            "loop 2 image : step  19196\n",
            "loop 2 image : step  19197\n",
            "loop 2 image : step  19198\n",
            "loop 2 image : step  19199\n",
            "loop 2 image : step  19200\n",
            "loop 2 image : step  19201\n",
            "loop 2 image : step  19202\n",
            "loop 2 image : step  19203\n",
            "loop 2 image : step  19204\n",
            "loop 2 image : step  19205\n",
            "loop 2 image : step  19206\n",
            "loop 2 image : step  19207\n",
            "loop 2 image : step  19208\n",
            "loop 2 image : step  19209\n",
            "loop 2 image : step  19210\n",
            "loop 2 image : step  19211\n",
            "loop 2 image : step  19212\n",
            "loop 2 image : step  19213\n",
            "loop 2 image : step  19214\n",
            "loop 2 image : step  19215\n",
            "loop 2 image : step  19216\n",
            "loop 2 image : step  19217\n",
            "loop 2 image : step  19218\n",
            "loop 2 image : step  19219\n",
            "loop 2 image : step  19220\n",
            "loop 2 image : step  19221\n",
            "loop 2 image : step  19222\n",
            "loop 2 image : step  19223\n",
            "loop 2 image : step  19224\n",
            "loop 2 image : step  19225\n",
            "loop 2 image : step  19226\n",
            "loop 2 image : step  19227\n",
            "loop 2 image : step  19228\n",
            "loop 2 image : step  19229\n",
            "loop 2 image : step  19230\n",
            "loop 2 image : step  19231\n",
            "loop 2 image : step  19232\n",
            "loop 2 image : step  19233\n",
            "loop 2 image : step  19234\n",
            "loop 2 image : step  19235\n",
            "loop 2 image : step  19236\n",
            "loop 2 image : step  19237\n",
            "loop 2 image : step  19238\n",
            "loop 2 image : step  19239\n",
            "loop 2 image : step  19240\n",
            "loop 2 image : step  19241\n",
            "loop 2 image : step  19242\n",
            "loop 2 image : step  19243\n",
            "loop 2 image : step  19244\n",
            "loop 2 image : step  19245\n",
            "loop 2 image : step  19246\n",
            "loop 2 image : step  19247\n",
            "loop 2 image : step  19248\n",
            "loop 2 image : step  19249\n",
            "loop 2 image : step  19250\n",
            "loop 2 image : step  19251\n",
            "loop 2 image : step  19252\n",
            "loop 2 image : step  19253\n",
            "loop 2 image : step  19254\n",
            "loop 2 image : step  19255\n",
            "loop 2 image : step  19256\n",
            "loop 2 image : step  19257\n",
            "loop 2 image : step  19258\n",
            "loop 2 image : step  19259\n",
            "loop 2 image : step  19260\n",
            "loop 2 image : step  19261\n",
            "loop 2 image : step  19262\n",
            "loop 2 image : step  19263\n",
            "loop 2 image : step  19264\n",
            "loop 2 image : step  19265\n",
            "loop 2 image : step  19266\n",
            "loop 2 image : step  19267\n",
            "loop 2 image : step  19268\n",
            "loop 2 image : step  19269\n",
            "loop 2 image : step  19270\n",
            "loop 2 image : step  19271\n",
            "loop 2 image : step  19272\n",
            "loop 2 image : step  19273\n",
            "loop 2 image : step  19274\n",
            "loop 2 image : step  19275\n",
            "loop 2 image : step  19276\n",
            "loop 2 image : step  19277\n",
            "loop 2 image : step  19278\n",
            "loop 2 image : step  19279\n",
            "loop 2 image : step  19280\n",
            "loop 2 image : step  19281\n",
            "loop 2 image : step  19282\n",
            "loop 2 image : step  19283\n",
            "loop 2 image : step  19284\n",
            "loop 2 image : step  19285\n",
            "loop 2 image : step  19286\n",
            "loop 2 image : step  19287\n",
            "loop 2 image : step  19288\n",
            "loop 2 image : step  19289\n",
            "loop 2 image : step  19290\n",
            "loop 2 image : step  19291\n",
            "loop 2 image : step  19292\n",
            "loop 2 image : step  19293\n",
            "loop 2 image : step  19294\n",
            "loop 2 image : step  19295\n",
            "loop 2 image : step  19296\n",
            "loop 2 image : step  19297\n",
            "loop 2 image : step  19298\n",
            "loop 2 image : step  19299\n",
            "loop 2 image : step  19300\n",
            "loop 2 image : step  19301\n",
            "loop 2 image : step  19302\n",
            "loop 2 image : step  19303\n",
            "loop 2 image : step  19304\n",
            "loop 2 image : step  19305\n",
            "loop 2 image : step  19306\n",
            "loop 2 image : step  19307\n",
            "loop 2 image : step  19308\n",
            "loop 2 image : step  19309\n",
            "loop 2 image : step  19310\n",
            "loop 2 image : step  19311\n",
            "loop 2 image : step  19312\n",
            "loop 2 image : step  19313\n",
            "loop 2 image : step  19314\n",
            "loop 2 image : step  19315\n",
            "loop 2 image : step  19316\n",
            "loop 2 image : step  19317\n",
            "loop 2 image : step  19318\n",
            "loop 2 image : step  19319\n",
            "loop 2 image : step  19320\n",
            "loop 2 image : step  19321\n",
            "loop 2 image : step  19322\n",
            "loop 2 image : step  19323\n",
            "loop 2 image : step  19324\n",
            "loop 2 image : step  19325\n",
            "loop 2 image : step  19326\n",
            "loop 2 image : step  19327\n",
            "loop 2 image : step  19328\n",
            "loop 2 image : step  19329\n",
            "loop 2 image : step  19330\n",
            "loop 2 image : step  19331\n",
            "loop 2 image : step  19332\n",
            "loop 2 image : step  19333\n",
            "loop 2 image : step  19334\n",
            "loop 2 image : step  19335\n",
            "loop 2 image : step  19336\n",
            "loop 2 image : step  19337\n",
            "loop 2 image : step  19338\n",
            "loop 2 image : step  19339\n",
            "loop 2 image : step  19340\n",
            "loop 2 image : step  19341\n",
            "loop 2 image : step  19342\n",
            "loop 2 image : step  19343\n",
            "loop 2 image : step  19344\n",
            "loop 2 image : step  19345\n",
            "loop 2 image : step  19346\n",
            "loop 2 image : step  19347\n",
            "loop 2 image : step  19348\n",
            "loop 2 image : step  19349\n",
            "loop 2 image : step  19350\n",
            "loop 2 image : step  19351\n",
            "loop 2 image : step  19352\n",
            "loop 2 image : step  19353\n",
            "loop 2 image : step  19354\n",
            "loop 2 image : step  19355\n",
            "loop 2 image : step  19356\n",
            "loop 2 image : step  19357\n",
            "loop 2 image : step  19358\n",
            "loop 2 image : step  19359\n",
            "loop 2 image : step  19360\n",
            "loop 2 image : step  19361\n",
            "loop 2 image : step  19362\n",
            "loop 2 image : step  19363\n",
            "loop 2 image : step  19364\n",
            "loop 2 image : step  19365\n",
            "loop 2 image : step  19366\n",
            "loop 2 image : step  19367\n",
            "loop 2 image : step  19368\n",
            "loop 2 image : step  19369\n",
            "loop 2 image : step  19370\n",
            "loop 2 image : step  19371\n",
            "loop 2 image : step  19372\n",
            "loop 2 image : step  19373\n",
            "loop 2 image : step  19374\n",
            "loop 2 image : step  19375\n",
            "loop 2 image : step  19376\n",
            "loop 2 image : step  19377\n",
            "loop 2 image : step  19378\n",
            "loop 2 image : step  19379\n",
            "loop 2 image : step  19380\n",
            "loop 2 image : step  19381\n",
            "loop 2 image : step  19382\n",
            "loop 2 image : step  19383\n",
            "loop 2 image : step  19384\n",
            "loop 2 image : step  19385\n",
            "loop 2 image : step  19386\n",
            "loop 2 image : step  19387\n",
            "loop 2 image : step  19388\n",
            "loop 2 image : step  19389\n",
            "loop 2 image : step  19390\n",
            "loop 2 image : step  19391\n",
            "loop 2 image : step  19392\n",
            "loop 2 image : step  19393\n",
            "loop 2 image : step  19394\n",
            "loop 2 image : step  19395\n",
            "loop 2 image : step  19396\n",
            "loop 2 image : step  19397\n",
            "loop 2 image : step  19398\n",
            "loop 2 image : step  19399\n",
            "loop 2 image : step  19400\n",
            "loop 2 image : step  19401\n",
            "loop 2 image : step  19402\n",
            "loop 2 image : step  19403\n",
            "loop 2 image : step  19404\n",
            "loop 2 image : step  19405\n",
            "loop 2 image : step  19406\n",
            "loop 2 image : step  19407\n",
            "loop 2 image : step  19408\n",
            "loop 2 image : step  19409\n",
            "loop 2 image : step  19410\n",
            "loop 2 image : step  19411\n",
            "loop 2 image : step  19412\n",
            "loop 2 image : step  19413\n",
            "loop 2 image : step  19414\n",
            "loop 2 image : step  19415\n",
            "loop 2 image : step  19416\n",
            "loop 2 image : step  19417\n",
            "loop 2 image : step  19418\n",
            "loop 2 image : step  19419\n",
            "loop 2 image : step  19420\n",
            "loop 2 image : step  19421\n",
            "loop 2 image : step  19422\n",
            "loop 2 image : step  19423\n",
            "loop 2 image : step  19424\n",
            "loop 2 image : step  19425\n",
            "loop 2 image : step  19426\n",
            "loop 2 image : step  19427\n",
            "loop 2 image : step  19428\n",
            "loop 2 image : step  19429\n",
            "loop 2 image : step  19430\n",
            "loop 2 image : step  19431\n",
            "loop 2 image : step  19432\n",
            "loop 2 image : step  19433\n",
            "loop 2 image : step  19434\n",
            "loop 2 image : step  19435\n",
            "loop 2 image : step  19436\n",
            "loop 2 image : step  19437\n",
            "loop 2 image : step  19438\n",
            "loop 2 image : step  19439\n",
            "loop 2 image : step  19440\n",
            "loop 2 image : step  19441\n",
            "loop 2 image : step  19442\n",
            "loop 2 image : step  19443\n",
            "loop 2 image : step  19444\n",
            "loop 2 image : step  19445\n",
            "loop 2 image : step  19446\n",
            "loop 2 image : step  19447\n",
            "loop 2 image : step  19448\n",
            "loop 2 image : step  19449\n",
            "loop 2 image : step  19450\n",
            "loop 2 image : step  19451\n",
            "loop 2 image : step  19452\n",
            "loop 2 image : step  19453\n",
            "loop 2 image : step  19454\n",
            "loop 2 image : step  19455\n",
            "loop 2 image : step  19456\n",
            "loop 2 image : step  19457\n",
            "loop 2 image : step  19458\n",
            "loop 2 image : step  19459\n",
            "loop 2 image : step  19460\n",
            "loop 2 image : step  19461\n",
            "loop 2 image : step  19462\n",
            "loop 2 image : step  19463\n",
            "loop 2 image : step  19464\n",
            "loop 2 image : step  19465\n",
            "loop 2 image : step  19466\n",
            "loop 2 image : step  19467\n",
            "loop 2 image : step  19468\n",
            "loop 2 image : step  19469\n",
            "loop 2 image : step  19470\n",
            "loop 2 image : step  19471\n",
            "loop 2 image : step  19472\n",
            "loop 2 image : step  19473\n",
            "loop 2 image : step  19474\n",
            "loop 2 image : step  19475\n",
            "loop 2 image : step  19476\n",
            "loop 2 image : step  19477\n",
            "loop 2 image : step  19478\n",
            "loop 2 image : step  19479\n",
            "loop 2 image : step  19480\n",
            "loop 2 image : step  19481\n",
            "loop 2 image : step  19482\n",
            "loop 2 image : step  19483\n",
            "loop 2 image : step  19484\n",
            "loop 2 image : step  19485\n",
            "loop 2 image : step  19486\n",
            "loop 2 image : step  19487\n",
            "loop 2 image : step  19488\n",
            "loop 2 image : step  19489\n",
            "loop 2 image : step  19490\n",
            "loop 2 image : step  19491\n",
            "loop 2 image : step  19492\n",
            "loop 2 image : step  19493\n",
            "loop 2 image : step  19494\n",
            "loop 2 image : step  19495\n",
            "loop 2 image : step  19496\n",
            "loop 2 image : step  19497\n",
            "loop 2 image : step  19498\n",
            "loop 2 image : step  19499\n",
            "loop 2 image : step  19500\n",
            "loop 2 image : step  19501\n",
            "loop 2 image : step  19502\n",
            "loop 2 image : step  19503\n",
            "loop 2 image : step  19504\n",
            "loop 2 image : step  19505\n",
            "loop 2 image : step  19506\n",
            "loop 2 image : step  19507\n",
            "loop 2 image : step  19508\n",
            "loop 2 image : step  19509\n",
            "loop 2 image : step  19510\n",
            "loop 2 image : step  19511\n",
            "loop 2 image : step  19512\n",
            "loop 2 image : step  19513\n",
            "loop 2 image : step  19514\n",
            "loop 2 image : step  19515\n",
            "loop 2 image : step  19516\n",
            "loop 2 image : step  19517\n",
            "loop 2 image : step  19518\n",
            "loop 2 image : step  19519\n",
            "loop 2 image : step  19520\n",
            "loop 2 image : step  19521\n",
            "loop 2 image : step  19522\n",
            "loop 2 image : step  19523\n",
            "loop 2 image : step  19524\n",
            "loop 2 image : step  19525\n",
            "loop 2 image : step  19526\n",
            "loop 2 image : step  19527\n",
            "loop 2 image : step  19528\n",
            "loop 2 image : step  19529\n",
            "loop 2 image : step  19530\n",
            "loop 2 image : step  19531\n",
            "loop 2 image : step  19532\n",
            "loop 2 image : step  19533\n",
            "loop 2 image : step  19534\n",
            "loop 2 image : step  19535\n",
            "loop 2 image : step  19536\n",
            "loop 2 image : step  19537\n",
            "loop 2 image : step  19538\n",
            "loop 2 image : step  19539\n",
            "loop 2 image : step  19540\n",
            "loop 2 image : step  19541\n",
            "loop 2 image : step  19542\n",
            "loop 2 image : step  19543\n",
            "loop 2 image : step  19544\n",
            "loop 2 image : step  19545\n",
            "loop 2 image : step  19546\n",
            "loop 2 image : step  19547\n",
            "loop 2 image : step  19548\n",
            "loop 2 image : step  19549\n",
            "loop 2 image : step  19550\n",
            "loop 2 image : step  19551\n",
            "loop 2 image : step  19552\n",
            "loop 2 image : step  19553\n",
            "loop 2 image : step  19554\n",
            "loop 2 image : step  19555\n",
            "loop 2 image : step  19556\n",
            "loop 2 image : step  19557\n",
            "loop 2 image : step  19558\n",
            "loop 2 image : step  19559\n",
            "loop 2 image : step  19560\n",
            "loop 2 image : step  19561\n",
            "loop 2 image : step  19562\n",
            "loop 2 image : step  19563\n",
            "loop 2 image : step  19564\n",
            "loop 2 image : step  19565\n",
            "loop 2 image : step  19566\n",
            "loop 2 image : step  19567\n",
            "loop 2 image : step  19568\n",
            "loop 2 image : step  19569\n",
            "loop 2 image : step  19570\n",
            "loop 2 image : step  19571\n",
            "loop 2 image : step  19572\n",
            "loop 2 image : step  19573\n",
            "loop 2 image : step  19574\n",
            "loop 2 image : step  19575\n",
            "loop 2 image : step  19576\n",
            "loop 2 image : step  19577\n",
            "loop 2 image : step  19578\n",
            "loop 2 image : step  19579\n",
            "loop 2 image : step  19580\n",
            "loop 2 image : step  19581\n",
            "loop 2 image : step  19582\n",
            "loop 2 image : step  19583\n",
            "loop 2 image : step  19584\n",
            "loop 2 image : step  19585\n",
            "loop 2 image : step  19586\n",
            "loop 2 image : step  19587\n",
            "loop 2 image : step  19588\n",
            "loop 2 image : step  19589\n",
            "loop 2 image : step  19590\n",
            "loop 2 image : step  19591\n",
            "loop 2 image : step  19592\n",
            "loop 2 image : step  19593\n",
            "loop 2 image : step  19594\n",
            "loop 2 image : step  19595\n",
            "loop 2 image : step  19596\n",
            "loop 2 image : step  19597\n",
            "loop 2 image : step  19598\n",
            "loop 2 image : step  19599\n",
            "loop 2 image : step  19600\n",
            "loop 2 image : step  19601\n",
            "loop 2 image : step  19602\n",
            "loop 2 image : step  19603\n",
            "loop 2 image : step  19604\n",
            "loop 2 image : step  19605\n",
            "loop 2 image : step  19606\n",
            "loop 2 image : step  19607\n",
            "loop 2 image : step  19608\n",
            "loop 2 image : step  19609\n",
            "loop 2 image : step  19610\n",
            "loop 2 image : step  19611\n",
            "loop 2 image : step  19612\n",
            "loop 2 image : step  19613\n",
            "loop 2 image : step  19614\n",
            "loop 2 image : step  19615\n",
            "loop 2 image : step  19616\n",
            "loop 2 image : step  19617\n",
            "loop 2 image : step  19618\n",
            "loop 2 image : step  19619\n",
            "loop 2 image : step  19620\n",
            "loop 2 image : step  19621\n",
            "loop 2 image : step  19622\n",
            "loop 2 image : step  19623\n",
            "loop 2 image : step  19624\n",
            "loop 2 image : step  19625\n",
            "loop 2 image : step  19626\n",
            "loop 2 image : step  19627\n",
            "loop 2 image : step  19628\n",
            "loop 2 image : step  19629\n",
            "loop 2 image : step  19630\n",
            "loop 2 image : step  19631\n",
            "loop 2 image : step  19632\n",
            "loop 2 image : step  19633\n",
            "loop 2 image : step  19634\n",
            "loop 2 image : step  19635\n",
            "loop 2 image : step  19636\n",
            "loop 2 image : step  19637\n",
            "loop 2 image : step  19638\n",
            "loop 2 image : step  19639\n",
            "loop 2 image : step  19640\n",
            "loop 2 image : step  19641\n",
            "loop 2 image : step  19642\n",
            "loop 2 image : step  19643\n",
            "loop 2 image : step  19644\n",
            "loop 2 image : step  19645\n",
            "loop 2 image : step  19646\n",
            "loop 2 image : step  19647\n",
            "loop 2 image : step  19648\n",
            "loop 2 image : step  19649\n",
            "loop 2 image : step  19650\n",
            "loop 2 image : step  19651\n",
            "loop 2 image : step  19652\n",
            "loop 2 image : step  19653\n",
            "loop 2 image : step  19654\n",
            "loop 2 image : step  19655\n",
            "loop 2 image : step  19656\n",
            "loop 2 image : step  19657\n",
            "loop 2 image : step  19658\n",
            "loop 2 image : step  19659\n",
            "loop 2 image : step  19660\n",
            "loop 2 image : step  19661\n",
            "loop 2 image : step  19662\n",
            "loop 2 image : step  19663\n",
            "loop 2 image : step  19664\n",
            "loop 2 image : step  19665\n",
            "loop 2 image : step  19666\n",
            "loop 2 image : step  19667\n",
            "loop 2 image : step  19668\n",
            "loop 2 image : step  19669\n",
            "loop 2 image : step  19670\n",
            "loop 2 image : step  19671\n",
            "loop 2 image : step  19672\n",
            "loop 2 image : step  19673\n",
            "loop 2 image : step  19674\n",
            "loop 2 image : step  19675\n",
            "loop 2 image : step  19676\n",
            "loop 2 image : step  19677\n",
            "loop 2 image : step  19678\n",
            "loop 2 image : step  19679\n",
            "loop 2 image : step  19680\n",
            "loop 2 image : step  19681\n",
            "loop 2 image : step  19682\n",
            "loop 2 image : step  19683\n",
            "loop 2 image : step  19684\n",
            "loop 2 image : step  19685\n",
            "loop 2 image : step  19686\n",
            "loop 2 image : step  19687\n",
            "loop 2 image : step  19688\n",
            "loop 2 image : step  19689\n",
            "loop 2 image : step  19690\n",
            "loop 2 image : step  19691\n",
            "loop 2 image : step  19692\n",
            "loop 2 image : step  19693\n",
            "loop 2 image : step  19694\n",
            "loop 2 image : step  19695\n",
            "loop 2 image : step  19696\n",
            "loop 2 image : step  19697\n",
            "loop 2 image : step  19698\n",
            "loop 2 image : step  19699\n",
            "loop 2 image : step  19700\n",
            "loop 2 image : step  19701\n",
            "loop 2 image : step  19702\n",
            "loop 2 image : step  19703\n",
            "loop 2 image : step  19704\n",
            "loop 2 image : step  19705\n",
            "loop 2 image : step  19706\n",
            "loop 2 image : step  19707\n",
            "loop 2 image : step  19708\n",
            "loop 2 image : step  19709\n",
            "loop 2 image : step  19710\n",
            "loop 2 image : step  19711\n",
            "loop 2 image : step  19712\n",
            "loop 2 image : step  19713\n",
            "loop 2 image : step  19714\n",
            "loop 2 image : step  19715\n",
            "loop 2 image : step  19716\n",
            "loop 2 image : step  19717\n",
            "loop 2 image : step  19718\n",
            "loop 2 image : step  19719\n",
            "loop 2 image : step  19720\n",
            "loop 2 image : step  19721\n",
            "loop 2 image : step  19722\n",
            "loop 2 image : step  19723\n",
            "loop 2 image : step  19724\n",
            "loop 2 image : step  19725\n",
            "loop 2 image : step  19726\n",
            "loop 2 image : step  19727\n",
            "loop 2 image : step  19728\n",
            "loop 2 image : step  19729\n",
            "loop 2 image : step  19730\n",
            "loop 2 image : step  19731\n",
            "loop 2 image : step  19732\n",
            "loop 2 image : step  19733\n",
            "loop 2 image : step  19734\n",
            "loop 2 image : step  19735\n",
            "loop 2 image : step  19736\n",
            "loop 2 image : step  19737\n",
            "loop 2 image : step  19738\n",
            "loop 2 image : step  19739\n",
            "loop 2 image : step  19740\n",
            "loop 2 image : step  19741\n",
            "loop 2 image : step  19742\n",
            "loop 2 image : step  19743\n",
            "loop 2 image : step  19744\n",
            "loop 2 image : step  19745\n",
            "loop 2 image : step  19746\n",
            "loop 2 image : step  19747\n",
            "loop 2 image : step  19748\n",
            "loop 2 image : step  19749\n",
            "loop 2 image : step  19750\n",
            "loop 2 image : step  19751\n",
            "loop 2 image : step  19752\n",
            "loop 2 image : step  19753\n",
            "loop 2 image : step  19754\n",
            "loop 2 image : step  19755\n",
            "loop 2 image : step  19756\n",
            "loop 2 image : step  19757\n",
            "loop 2 image : step  19758\n",
            "loop 2 image : step  19759\n",
            "loop 2 image : step  19760\n",
            "loop 2 image : step  19761\n",
            "loop 2 image : step  19762\n",
            "loop 2 image : step  19763\n",
            "loop 2 image : step  19764\n",
            "loop 2 image : step  19765\n",
            "loop 2 image : step  19766\n",
            "loop 2 image : step  19767\n",
            "loop 2 image : step  19768\n",
            "loop 2 image : step  19769\n",
            "loop 2 image : step  19770\n",
            "loop 2 image : step  19771\n",
            "loop 2 image : step  19772\n",
            "loop 2 image : step  19773\n",
            "loop 2 image : step  19774\n",
            "loop 2 image : step  19775\n",
            "loop 2 image : step  19776\n",
            "loop 2 image : step  19777\n",
            "loop 2 image : step  19778\n",
            "loop 2 image : step  19779\n",
            "loop 2 image : step  19780\n",
            "loop 2 image : step  19781\n",
            "loop 2 image : step  19782\n",
            "loop 2 image : step  19783\n",
            "loop 2 image : step  19784\n",
            "loop 2 image : step  19785\n",
            "loop 2 image : step  19786\n",
            "loop 2 image : step  19787\n",
            "loop 2 image : step  19788\n",
            "loop 2 image : step  19789\n",
            "loop 2 image : step  19790\n",
            "loop 2 image : step  19791\n",
            "loop 2 image : step  19792\n",
            "loop 2 image : step  19793\n",
            "loop 2 image : step  19794\n",
            "loop 2 image : step  19795\n",
            "loop 2 image : step  19796\n",
            "loop 2 image : step  19797\n",
            "loop 2 image : step  19798\n",
            "loop 2 image : step  19799\n",
            "loop 2 image : step  19800\n",
            "loop 2 image : step  19801\n",
            "loop 2 image : step  19802\n",
            "loop 2 image : step  19803\n",
            "loop 2 image : step  19804\n",
            "loop 2 image : step  19805\n",
            "loop 2 image : step  19806\n",
            "loop 2 image : step  19807\n",
            "loop 2 image : step  19808\n",
            "loop 2 image : step  19809\n",
            "loop 2 image : step  19810\n",
            "loop 2 image : step  19811\n",
            "loop 2 image : step  19812\n",
            "loop 2 image : step  19813\n",
            "loop 2 image : step  19814\n",
            "loop 2 image : step  19815\n",
            "loop 2 image : step  19816\n",
            "loop 2 image : step  19817\n",
            "loop 2 image : step  19818\n",
            "loop 2 image : step  19819\n",
            "loop 2 image : step  19820\n",
            "loop 2 image : step  19821\n",
            "loop 2 image : step  19822\n",
            "loop 2 image : step  19823\n",
            "loop 2 image : step  19824\n",
            "loop 2 image : step  19825\n",
            "loop 2 image : step  19826\n",
            "loop 2 image : step  19827\n",
            "loop 2 image : step  19828\n",
            "loop 2 image : step  19829\n",
            "loop 2 image : step  19830\n",
            "loop 2 image : step  19831\n",
            "loop 2 image : step  19832\n",
            "loop 2 image : step  19833\n",
            "loop 2 image : step  19834\n",
            "loop 2 image : step  19835\n",
            "loop 2 image : step  19836\n",
            "loop 2 image : step  19837\n",
            "loop 2 image : step  19838\n",
            "loop 2 image : step  19839\n",
            "loop 2 image : step  19840\n",
            "loop 2 image : step  19841\n",
            "loop 2 image : step  19842\n",
            "loop 2 image : step  19843\n",
            "loop 2 image : step  19844\n",
            "loop 2 image : step  19845\n",
            "loop 2 image : step  19846\n",
            "loop 2 image : step  19847\n",
            "loop 2 image : step  19848\n",
            "loop 2 image : step  19849\n",
            "loop 2 image : step  19850\n",
            "loop 2 image : step  19851\n",
            "loop 2 image : step  19852\n",
            "loop 2 image : step  19853\n",
            "loop 2 image : step  19854\n",
            "loop 2 image : step  19855\n",
            "loop 2 image : step  19856\n",
            "loop 2 image : step  19857\n",
            "loop 2 image : step  19858\n",
            "loop 2 image : step  19859\n",
            "loop 2 image : step  19860\n",
            "loop 2 image : step  19861\n",
            "loop 2 image : step  19862\n",
            "loop 2 image : step  19863\n",
            "loop 2 image : step  19864\n",
            "loop 2 image : step  19865\n",
            "loop 2 image : step  19866\n",
            "loop 2 image : step  19867\n",
            "loop 2 image : step  19868\n",
            "loop 2 image : step  19869\n",
            "loop 2 image : step  19870\n",
            "loop 2 image : step  19871\n",
            "loop 2 image : step  19872\n",
            "loop 2 image : step  19873\n",
            "loop 2 image : step  19874\n",
            "loop 2 image : step  19875\n",
            "loop 2 image : step  19876\n",
            "loop 2 image : step  19877\n",
            "loop 2 image : step  19878\n",
            "loop 2 image : step  19879\n",
            "loop 2 image : step  19880\n",
            "loop 2 image : step  19881\n",
            "loop 2 image : step  19882\n",
            "loop 2 image : step  19883\n",
            "loop 2 image : step  19884\n",
            "loop 2 image : step  19885\n",
            "loop 2 image : step  19886\n",
            "loop 2 image : step  19887\n",
            "loop 2 image : step  19888\n",
            "loop 2 image : step  19889\n",
            "loop 2 image : step  19890\n",
            "loop 2 image : step  19891\n",
            "loop 2 image : step  19892\n",
            "loop 2 image : step  19893\n",
            "loop 2 image : step  19894\n",
            "loop 2 image : step  19895\n",
            "loop 2 image : step  19896\n",
            "loop 2 image : step  19897\n",
            "loop 2 image : step  19898\n",
            "loop 2 image : step  19899\n",
            "loop 2 image : step  19900\n",
            "loop 2 image : step  19901\n",
            "loop 2 image : step  19902\n",
            "loop 2 image : step  19903\n",
            "loop 2 image : step  19904\n",
            "loop 2 image : step  19905\n",
            "loop 2 image : step  19906\n",
            "loop 2 image : step  19907\n",
            "loop 2 image : step  19908\n",
            "loop 2 image : step  19909\n",
            "loop 2 image : step  19910\n",
            "loop 2 image : step  19911\n",
            "loop 2 image : step  19912\n",
            "loop 2 image : step  19913\n",
            "loop 2 image : step  19914\n",
            "loop 2 image : step  19915\n",
            "loop 2 image : step  19916\n",
            "loop 2 image : step  19917\n",
            "loop 2 image : step  19918\n",
            "loop 2 image : step  19919\n",
            "loop 2 image : step  19920\n",
            "loop 2 image : step  19921\n",
            "loop 2 image : step  19922\n",
            "loop 2 image : step  19923\n",
            "loop 2 image : step  19924\n",
            "loop 2 image : step  19925\n",
            "loop 2 image : step  19926\n",
            "loop 2 image : step  19927\n",
            "loop 2 image : step  19928\n",
            "loop 2 image : step  19929\n",
            "loop 2 image : step  19930\n",
            "loop 2 image : step  19931\n",
            "loop 2 image : step  19932\n",
            "loop 2 image : step  19933\n",
            "loop 2 image : step  19934\n",
            "loop 2 image : step  19935\n",
            "loop 2 image : step  19936\n",
            "loop 2 image : step  19937\n",
            "loop 2 image : step  19938\n",
            "loop 2 image : step  19939\n",
            "loop 2 image : step  19940\n",
            "loop 2 image : step  19941\n",
            "loop 2 image : step  19942\n",
            "loop 2 image : step  19943\n",
            "loop 2 image : step  19944\n",
            "loop 2 image : step  19945\n",
            "loop 2 image : step  19946\n",
            "loop 2 image : step  19947\n",
            "loop 2 image : step  19948\n",
            "loop 2 image : step  19949\n",
            "loop 2 image : step  19950\n",
            "loop 2 image : step  19951\n",
            "loop 2 image : step  19952\n",
            "loop 2 image : step  19953\n",
            "loop 2 image : step  19954\n",
            "loop 2 image : step  19955\n",
            "loop 2 image : step  19956\n",
            "loop 2 image : step  19957\n",
            "loop 2 image : step  19958\n",
            "loop 2 image : step  19959\n",
            "loop 2 image : step  19960\n",
            "loop 2 image : step  19961\n",
            "loop 2 image : step  19962\n",
            "loop 2 image : step  19963\n",
            "loop 2 image : step  19964\n",
            "loop 2 image : step  19965\n",
            "loop 2 image : step  19966\n",
            "loop 2 image : step  19967\n",
            "loop 2 image : step  19968\n",
            "loop 2 image : step  19969\n",
            "loop 2 image : step  19970\n",
            "loop 2 image : step  19971\n",
            "loop 2 image : step  19972\n",
            "loop 2 image : step  19973\n",
            "loop 2 image : step  19974\n",
            "loop 2 image : step  19975\n",
            "loop 2 image : step  19976\n",
            "loop 2 image : step  19977\n",
            "loop 2 image : step  19978\n",
            "loop 2 image : step  19979\n",
            "loop 2 image : step  19980\n",
            "loop 2 image : step  19981\n",
            "loop 2 image : step  19982\n",
            "loop 2 image : step  19983\n",
            "loop 2 image : step  19984\n",
            "loop 2 image : step  19985\n",
            "loop 2 image : step  19986\n",
            "loop 2 image : step  19987\n",
            "loop 2 image : step  19988\n",
            "loop 2 image : step  19989\n",
            "loop 2 image : step  19990\n",
            "loop 2 image : step  19991\n",
            "loop 2 image : step  19992\n",
            "loop 2 image : step  19993\n",
            "loop 2 image : step  19994\n",
            "loop 2 image : step  19995\n",
            "loop 2 image : step  19996\n",
            "loop 2 image : step  19997\n",
            "loop 2 image : step  19998\n",
            "loop 2 image : step  19999\n",
            "loop 2 image : step  20000\n",
            "loop 2 image : step  20001\n",
            "loop 2 image : step  20002\n",
            "loop 2 image : step  20003\n",
            "loop 2 image : step  20004\n",
            "loop 2 image : step  20005\n",
            "loop 2 image : step  20006\n",
            "loop 2 image : step  20007\n",
            "loop 2 image : step  20008\n",
            "loop 2 image : step  20009\n",
            "loop 2 image : step  20010\n",
            "loop 2 image : step  20011\n",
            "loop 2 image : step  20012\n",
            "loop 2 image : step  20013\n",
            "loop 2 image : step  20014\n",
            "loop 2 image : step  20015\n",
            "loop 2 image : step  20016\n",
            "loop 2 image : step  20017\n",
            "loop 2 image : step  20018\n",
            "loop 2 image : step  20019\n",
            "loop 2 image : step  20020\n",
            "loop 2 image : step  20021\n",
            "loop 2 image : step  20022\n",
            "loop 2 image : step  20023\n",
            "loop 2 image : step  20024\n",
            "loop 2 image : step  20025\n",
            "loop 2 image : step  20026\n",
            "loop 2 image : step  20027\n",
            "loop 2 image : step  20028\n",
            "loop 2 image : step  20029\n",
            "loop 2 image : step  20030\n",
            "loop 2 image : step  20031\n",
            "loop 2 image : step  20032\n",
            "loop 2 image : step  20033\n",
            "loop 2 image : step  20034\n",
            "loop 2 image : step  20035\n",
            "loop 2 image : step  20036\n",
            "loop 2 image : step  20037\n",
            "loop 2 image : step  20038\n",
            "loop 2 image : step  20039\n",
            "loop 2 image : step  20040\n",
            "loop 2 image : step  20041\n",
            "loop 2 image : step  20042\n",
            "loop 2 image : step  20043\n",
            "loop 2 image : step  20044\n",
            "loop 2 image : step  20045\n",
            "loop 2 image : step  20046\n",
            "loop 2 image : step  20047\n",
            "loop 2 image : step  20048\n",
            "loop 2 image : step  20049\n",
            "loop 2 image : step  20050\n",
            "loop 2 image : step  20051\n",
            "loop 2 image : step  20052\n",
            "loop 2 image : step  20053\n",
            "loop 2 image : step  20054\n",
            "loop 2 image : step  20055\n",
            "loop 2 image : step  20056\n",
            "loop 2 image : step  20057\n",
            "loop 2 image : step  20058\n",
            "loop 2 image : step  20059\n",
            "loop 2 image : step  20060\n",
            "loop 2 image : step  20061\n",
            "loop 2 image : step  20062\n",
            "loop 2 image : step  20063\n",
            "loop 2 image : step  20064\n",
            "loop 2 image : step  20065\n",
            "loop 2 image : step  20066\n",
            "loop 2 image : step  20067\n",
            "loop 2 image : step  20068\n",
            "loop 2 image : step  20069\n",
            "loop 2 image : step  20070\n",
            "loop 2 image : step  20071\n",
            "loop 2 image : step  20072\n",
            "loop 2 image : step  20073\n",
            "loop 2 image : step  20074\n",
            "loop 2 image : step  20075\n",
            "loop 2 image : step  20076\n",
            "loop 2 image : step  20077\n",
            "loop 2 image : step  20078\n",
            "loop 2 image : step  20079\n",
            "loop 2 image : step  20080\n",
            "loop 2 image : step  20081\n",
            "loop 2 image : step  20082\n",
            "loop 2 image : step  20083\n",
            "loop 2 image : step  20084\n",
            "loop 2 image : step  20085\n",
            "loop 2 image : step  20086\n",
            "loop 2 image : step  20087\n",
            "loop 2 image : step  20088\n",
            "loop 2 image : step  20089\n",
            "loop 2 image : step  20090\n",
            "loop 2 image : step  20091\n",
            "loop 2 image : step  20092\n",
            "loop 2 image : step  20093\n",
            "loop 2 image : step  20094\n",
            "loop 2 image : step  20095\n",
            "loop 2 image : step  20096\n",
            "loop 2 image : step  20097\n",
            "loop 2 image : step  20098\n",
            "loop 2 image : step  20099\n",
            "loop 2 image : step  20100\n",
            "loop 2 image : step  20101\n",
            "loop 2 image : step  20102\n",
            "loop 2 image : step  20103\n",
            "loop 2 image : step  20104\n",
            "loop 2 image : step  20105\n",
            "loop 2 image : step  20106\n",
            "loop 2 image : step  20107\n",
            "loop 2 image : step  20108\n",
            "loop 2 image : step  20109\n",
            "loop 2 image : step  20110\n",
            "loop 2 image : step  20111\n",
            "loop 2 image : step  20112\n",
            "loop 2 image : step  20113\n",
            "loop 2 image : step  20114\n",
            "loop 2 image : step  20115\n",
            "loop 2 image : step  20116\n",
            "loop 2 image : step  20117\n",
            "loop 2 image : step  20118\n",
            "loop 2 image : step  20119\n",
            "loop 2 image : step  20120\n",
            "loop 2 image : step  20121\n",
            "loop 2 image : step  20122\n",
            "loop 2 image : step  20123\n",
            "loop 2 image : step  20124\n",
            "loop 2 image : step  20125\n",
            "loop 2 image : step  20126\n",
            "loop 2 image : step  20127\n",
            "loop 2 image : step  20128\n",
            "loop 2 image : step  20129\n",
            "loop 2 image : step  20130\n",
            "loop 2 image : step  20131\n",
            "loop 2 image : step  20132\n",
            "loop 2 image : step  20133\n",
            "loop 2 image : step  20134\n",
            "loop 2 image : step  20135\n",
            "loop 2 image : step  20136\n",
            "loop 2 image : step  20137\n",
            "loop 2 image : step  20138\n",
            "loop 2 image : step  20139\n",
            "loop 2 image : step  20140\n",
            "loop 2 image : step  20141\n",
            "loop 2 image : step  20142\n",
            "loop 2 image : step  20143\n",
            "loop 2 image : step  20144\n",
            "loop 2 image : step  20145\n",
            "loop 2 image : step  20146\n",
            "loop 2 image : step  20147\n",
            "loop 2 image : step  20148\n",
            "loop 2 image : step  20149\n",
            "loop 2 image : step  20150\n",
            "loop 2 image : step  20151\n",
            "loop 2 image : step  20152\n",
            "loop 2 image : step  20153\n",
            "loop 2 image : step  20154\n",
            "loop 2 image : step  20155\n",
            "loop 2 image : step  20156\n",
            "loop 2 image : step  20157\n",
            "loop 2 image : step  20158\n",
            "loop 2 image : step  20159\n",
            "loop 2 image : step  20160\n",
            "loop 2 image : step  20161\n",
            "loop 2 image : step  20162\n",
            "loop 2 image : step  20163\n",
            "loop 2 image : step  20164\n",
            "loop 2 image : step  20165\n",
            "loop 2 image : step  20166\n",
            "loop 2 image : step  20167\n",
            "loop 2 image : step  20168\n",
            "loop 2 image : step  20169\n",
            "loop 2 image : step  20170\n",
            "loop 2 image : step  20171\n",
            "loop 2 image : step  20172\n",
            "loop 2 image : step  20173\n",
            "loop 2 image : step  20174\n",
            "loop 2 image : step  20175\n",
            "loop 2 image : step  20176\n",
            "loop 2 image : step  20177\n",
            "loop 2 image : step  20178\n",
            "loop 2 image : step  20179\n",
            "loop 2 image : step  20180\n",
            "loop 2 image : step  20181\n",
            "loop 2 image : step  20182\n",
            "loop 2 image : step  20183\n",
            "loop 2 image : step  20184\n",
            "loop 2 image : step  20185\n",
            "loop 2 image : step  20186\n",
            "loop 2 image : step  20187\n",
            "loop 2 image : step  20188\n",
            "loop 2 image : step  20189\n",
            "loop 2 image : step  20190\n",
            "loop 2 image : step  20191\n",
            "loop 2 image : step  20192\n",
            "loop 2 image : step  20193\n",
            "loop 2 image : step  20194\n",
            "loop 2 image : step  20195\n",
            "loop 2 image : step  20196\n",
            "loop 2 image : step  20197\n",
            "loop 2 image : step  20198\n",
            "loop 2 image : step  20199\n",
            "loop 2 image : step  20200\n",
            "loop 2 image : step  20201\n",
            "loop 2 image : step  20202\n",
            "loop 2 image : step  20203\n",
            "loop 2 image : step  20204\n",
            "loop 2 image : step  20205\n",
            "loop 2 image : step  20206\n",
            "loop 2 image : step  20207\n",
            "loop 2 image : step  20208\n",
            "loop 2 image : step  20209\n",
            "loop 2 image : step  20210\n",
            "loop 2 image : step  20211\n",
            "loop 2 image : step  20212\n",
            "loop 2 image : step  20213\n",
            "loop 2 image : step  20214\n",
            "loop 2 image : step  20215\n",
            "loop 2 image : step  20216\n",
            "loop 2 image : step  20217\n",
            "loop 2 image : step  20218\n",
            "loop 2 image : step  20219\n",
            "loop 2 image : step  20220\n",
            "loop 2 image : step  20221\n",
            "loop 2 image : step  20222\n",
            "loop 2 image : step  20223\n",
            "loop 2 image : step  20224\n",
            "loop 2 image : step  20225\n",
            "loop 2 image : step  20226\n",
            "loop 2 image : step  20227\n",
            "loop 2 image : step  20228\n",
            "loop 2 image : step  20229\n",
            "loop 2 image : step  20230\n",
            "loop 2 image : step  20231\n",
            "loop 2 image : step  20232\n",
            "loop 2 image : step  20233\n",
            "loop 2 image : step  20234\n",
            "loop 2 image : step  20235\n",
            "loop 2 image : step  20236\n",
            "loop 2 image : step  20237\n",
            "loop 2 image : step  20238\n",
            "loop 2 image : step  20239\n",
            "loop 2 image : step  20240\n",
            "loop 2 image : step  20241\n",
            "loop 2 image : step  20242\n",
            "loop 2 image : step  20243\n",
            "loop 2 image : step  20244\n",
            "loop 2 image : step  20245\n",
            "loop 2 image : step  20246\n",
            "loop 2 image : step  20247\n",
            "loop 2 image : step  20248\n",
            "loop 2 image : step  20249\n",
            "loop 2 image : step  20250\n",
            "loop 2 image : step  20251\n",
            "loop 2 image : step  20252\n",
            "loop 2 image : step  20253\n",
            "loop 2 image : step  20254\n",
            "loop 2 image : step  20255\n",
            "loop 2 image : step  20256\n",
            "loop 2 image : step  20257\n",
            "loop 2 image : step  20258\n",
            "loop 2 image : step  20259\n",
            "loop 2 image : step  20260\n",
            "loop 2 image : step  20261\n",
            "loop 2 image : step  20262\n",
            "loop 2 image : step  20263\n",
            "loop 2 image : step  20264\n",
            "loop 2 image : step  20265\n",
            "loop 2 image : step  20266\n",
            "loop 2 image : step  20267\n",
            "loop 2 image : step  20268\n",
            "loop 2 image : step  20269\n",
            "loop 2 image : step  20270\n",
            "loop 2 image : step  20271\n",
            "loop 2 image : step  20272\n",
            "loop 2 image : step  20273\n",
            "loop 2 image : step  20274\n",
            "loop 2 image : step  20275\n",
            "loop 2 image : step  20276\n",
            "loop 2 image : step  20277\n",
            "loop 2 image : step  20278\n",
            "loop 2 image : step  20279\n",
            "loop 2 image : step  20280\n",
            "loop 2 image : step  20281\n",
            "loop 2 image : step  20282\n",
            "loop 2 image : step  20283\n",
            "loop 2 image : step  20284\n",
            "loop 2 image : step  20285\n",
            "loop 2 image : step  20286\n",
            "loop 2 image : step  20287\n",
            "loop 2 image : step  20288\n",
            "loop 2 image : step  20289\n",
            "loop 2 image : step  20290\n",
            "loop 2 image : step  20291\n",
            "loop 2 image : step  20292\n",
            "loop 2 image : step  20293\n",
            "loop 2 image : step  20294\n",
            "loop 2 image : step  20295\n",
            "loop 2 image : step  20296\n",
            "loop 2 image : step  20297\n",
            "loop 2 image : step  20298\n",
            "loop 2 image : step  20299\n",
            "loop 2 image : step  20300\n",
            "loop 2 image : step  20301\n",
            "loop 2 image : step  20302\n",
            "loop 2 image : step  20303\n",
            "loop 2 image : step  20304\n",
            "loop 2 image : step  20305\n",
            "loop 2 image : step  20306\n",
            "loop 2 image : step  20307\n",
            "loop 2 image : step  20308\n",
            "loop 2 image : step  20309\n",
            "loop 2 image : step  20310\n",
            "loop 2 image : step  20311\n",
            "loop 2 image : step  20312\n",
            "loop 2 image : step  20313\n",
            "loop 2 image : step  20314\n",
            "loop 2 image : step  20315\n",
            "loop 2 image : step  20316\n",
            "loop 2 image : step  20317\n",
            "loop 2 image : step  20318\n",
            "loop 2 image : step  20319\n",
            "loop 2 image : step  20320\n",
            "loop 2 image : step  20321\n",
            "loop 2 image : step  20322\n",
            "loop 2 image : step  20323\n",
            "loop 2 image : step  20324\n",
            "loop 2 image : step  20325\n",
            "loop 2 image : step  20326\n",
            "loop 2 image : step  20327\n",
            "loop 2 image : step  20328\n",
            "loop 2 image : step  20329\n",
            "loop 2 image : step  20330\n",
            "loop 2 image : step  20331\n",
            "loop 2 image : step  20332\n",
            "loop 2 image : step  20333\n",
            "loop 2 image : step  20334\n",
            "loop 2 image : step  20335\n",
            "loop 2 image : step  20336\n",
            "loop 2 image : step  20337\n",
            "loop 2 image : step  20338\n",
            "loop 2 image : step  20339\n",
            "loop 2 image : step  20340\n",
            "loop 2 image : step  20341\n",
            "loop 2 image : step  20342\n",
            "loop 2 image : step  20343\n",
            "loop 2 image : step  20344\n",
            "loop 2 image : step  20345\n",
            "loop 2 image : step  20346\n",
            "loop 2 image : step  20347\n",
            "loop 2 image : step  20348\n",
            "loop 2 image : step  20349\n",
            "loop 2 image : step  20350\n",
            "loop 2 image : step  20351\n",
            "loop 2 image : step  20352\n",
            "loop 2 image : step  20353\n",
            "loop 2 image : step  20354\n",
            "loop 2 image : step  20355\n",
            "loop 2 image : step  20356\n",
            "loop 2 image : step  20357\n",
            "loop 2 image : step  20358\n",
            "loop 2 image : step  20359\n",
            "loop 2 image : step  20360\n",
            "loop 2 image : step  20361\n",
            "loop 2 image : step  20362\n",
            "loop 2 image : step  20363\n",
            "loop 2 image : step  20364\n",
            "loop 2 image : step  20365\n",
            "loop 2 image : step  20366\n",
            "loop 2 image : step  20367\n",
            "loop 2 image : step  20368\n",
            "loop 2 image : step  20369\n",
            "loop 2 image : step  20370\n",
            "loop 2 image : step  20371\n",
            "loop 2 image : step  20372\n",
            "loop 2 image : step  20373\n",
            "loop 2 image : step  20374\n",
            "loop 2 image : step  20375\n",
            "loop 2 image : step  20376\n",
            "loop 2 image : step  20377\n",
            "loop 2 image : step  20378\n",
            "loop 2 image : step  20379\n",
            "loop 2 image : step  20380\n",
            "loop 2 image : step  20381\n",
            "loop 2 image : step  20382\n",
            "loop 2 image : step  20383\n",
            "loop 2 image : step  20384\n",
            "loop 2 image : step  20385\n",
            "loop 2 image : step  20386\n",
            "loop 2 image : step  20387\n",
            "loop 2 image : step  20388\n",
            "loop 2 image : step  20389\n",
            "loop 2 image : step  20390\n",
            "loop 2 image : step  20391\n",
            "loop 2 image : step  20392\n",
            "loop 2 image : step  20393\n",
            "loop 2 image : step  20394\n",
            "loop 2 image : step  20395\n",
            "loop 2 image : step  20396\n",
            "loop 2 image : step  20397\n",
            "loop 2 image : step  20398\n",
            "loop 2 image : step  20399\n",
            "loop 2 image : step  20400\n",
            "loop 2 image : step  20401\n",
            "loop 2 image : step  20402\n",
            "loop 2 image : step  20403\n",
            "loop 2 image : step  20404\n",
            "loop 2 image : step  20405\n",
            "loop 2 image : step  20406\n",
            "loop 2 image : step  20407\n",
            "loop 2 image : step  20408\n",
            "loop 2 image : step  20409\n",
            "loop 2 image : step  20410\n",
            "loop 2 image : step  20411\n",
            "loop 2 image : step  20412\n",
            "loop 2 image : step  20413\n",
            "loop 2 image : step  20414\n",
            "loop 2 image : step  20415\n",
            "loop 2 image : step  20416\n",
            "loop 2 image : step  20417\n",
            "loop 2 image : step  20418\n",
            "loop 2 image : step  20419\n",
            "loop 2 image : step  20420\n",
            "loop 2 image : step  20421\n",
            "loop 2 image : step  20422\n",
            "loop 2 image : step  20423\n",
            "loop 2 image : step  20424\n",
            "loop 2 image : step  20425\n",
            "loop 2 image : step  20426\n",
            "loop 2 image : step  20427\n",
            "loop 2 image : step  20428\n",
            "loop 2 image : step  20429\n",
            "loop 2 image : step  20430\n",
            "loop 2 image : step  20431\n",
            "loop 2 image : step  20432\n",
            "loop 2 image : step  20433\n",
            "loop 2 image : step  20434\n",
            "loop 2 image : step  20435\n",
            "loop 2 image : step  20436\n",
            "loop 2 image : step  20437\n",
            "loop 2 image : step  20438\n",
            "loop 2 image : step  20439\n",
            "loop 2 image : step  20440\n",
            "loop 2 image : step  20441\n",
            "loop 2 image : step  20442\n",
            "loop 2 image : step  20443\n",
            "loop 2 image : step  20444\n",
            "loop 2 image : step  20445\n",
            "loop 2 image : step  20446\n",
            "loop 2 image : step  20447\n",
            "loop 2 image : step  20448\n",
            "loop 2 image : step  20449\n",
            "loop 2 image : step  20450\n",
            "loop 2 image : step  20451\n",
            "loop 2 image : step  20452\n",
            "loop 2 image : step  20453\n",
            "loop 2 image : step  20454\n",
            "loop 2 image : step  20455\n",
            "loop 2 image : step  20456\n",
            "loop 2 image : step  20457\n",
            "loop 2 image : step  20458\n",
            "loop 2 image : step  20459\n",
            "loop 2 image : step  20460\n",
            "loop 2 image : step  20461\n",
            "loop 2 image : step  20462\n",
            "loop 2 image : step  20463\n",
            "loop 2 image : step  20464\n",
            "loop 2 image : step  20465\n",
            "loop 2 image : step  20466\n",
            "loop 2 image : step  20467\n",
            "loop 2 image : step  20468\n",
            "loop 2 image : step  20469\n",
            "loop 2 image : step  20470\n",
            "loop 2 image : step  20471\n",
            "loop 2 image : step  20472\n",
            "loop 2 image : step  20473\n",
            "loop 2 image : step  20474\n",
            "loop 2 image : step  20475\n",
            "loop 2 image : step  20476\n",
            "loop 2 image : step  20477\n",
            "loop 2 image : step  20478\n",
            "loop 2 image : step  20479\n",
            "loop 2 image : step  20480\n",
            "loop 2 image : step  20481\n",
            "loop 2 image : step  20482\n",
            "loop 2 image : step  20483\n",
            "loop 2 image : step  20484\n",
            "loop 2 image : step  20485\n",
            "loop 2 image : step  20486\n",
            "loop 2 image : step  20487\n",
            "loop 2 image : step  20488\n",
            "loop 2 image : step  20489\n",
            "loop 2 image : step  20490\n",
            "loop 2 image : step  20491\n",
            "loop 2 image : step  20492\n",
            "loop 2 image : step  20493\n",
            "loop 2 image : step  20494\n",
            "loop 2 image : step  20495\n",
            "loop 2 image : step  20496\n",
            "loop 2 image : step  20497\n",
            "loop 2 image : step  20498\n",
            "loop 2 image : step  20499\n",
            "loop 2 image : step  20500\n",
            "loop 2 image : step  20501\n",
            "loop 2 image : step  20502\n",
            "loop 2 image : step  20503\n",
            "loop 2 image : step  20504\n",
            "loop 2 image : step  20505\n",
            "loop 2 image : step  20506\n",
            "loop 2 image : step  20507\n",
            "loop 2 image : step  20508\n",
            "loop 2 image : step  20509\n",
            "loop 2 image : step  20510\n",
            "loop 2 image : step  20511\n",
            "loop 2 image : step  20512\n",
            "loop 2 image : step  20513\n",
            "loop 2 image : step  20514\n",
            "loop 2 image : step  20515\n",
            "loop 2 image : step  20516\n",
            "loop 2 image : step  20517\n",
            "loop 2 image : step  20518\n",
            "loop 2 image : step  20519\n",
            "loop 2 image : step  20520\n",
            "loop 2 image : step  20521\n",
            "loop 2 image : step  20522\n",
            "loop 2 image : step  20523\n",
            "loop 2 image : step  20524\n",
            "loop 2 image : step  20525\n",
            "loop 2 image : step  20526\n",
            "loop 2 image : step  20527\n",
            "loop 2 image : step  20528\n",
            "loop 2 image : step  20529\n",
            "loop 2 image : step  20530\n",
            "loop 2 image : step  20531\n",
            "loop 2 image : step  20532\n",
            "loop 2 image : step  20533\n",
            "loop 2 image : step  20534\n",
            "loop 2 image : step  20535\n",
            "loop 2 image : step  20536\n",
            "loop 2 image : step  20537\n",
            "loop 2 image : step  20538\n",
            "loop 2 image : step  20539\n",
            "loop 2 image : step  20540\n",
            "loop 2 image : step  20541\n",
            "loop 2 image : step  20542\n",
            "loop 2 image : step  20543\n",
            "loop 2 image : step  20544\n",
            "loop 2 image : step  20545\n",
            "loop 2 image : step  20546\n",
            "loop 2 image : step  20547\n",
            "loop 2 image : step  20548\n",
            "loop 2 image : step  20549\n",
            "loop 2 image : step  20550\n",
            "loop 2 image : step  20551\n",
            "loop 2 image : step  20552\n",
            "loop 2 image : step  20553\n",
            "loop 2 image : step  20554\n",
            "loop 2 image : step  20555\n",
            "loop 2 image : step  20556\n",
            "loop 2 image : step  20557\n",
            "loop 2 image : step  20558\n",
            "loop 2 image : step  20559\n",
            "loop 2 image : step  20560\n",
            "loop 2 image : step  20561\n",
            "loop 2 image : step  20562\n",
            "loop 2 image : step  20563\n",
            "loop 2 image : step  20564\n",
            "loop 2 image : step  20565\n",
            "loop 2 image : step  20566\n",
            "loop 2 image : step  20567\n",
            "loop 2 image : step  20568\n",
            "loop 2 image : step  20569\n",
            "loop 2 image : step  20570\n",
            "loop 2 image : step  20571\n",
            "loop 2 image : step  20572\n",
            "loop 2 image : step  20573\n",
            "loop 2 image : step  20574\n",
            "loop 2 image : step  20575\n",
            "loop 2 image : step  20576\n",
            "loop 2 image : step  20577\n",
            "loop 2 image : step  20578\n",
            "loop 2 image : step  20579\n",
            "loop 2 image : step  20580\n",
            "loop 2 image : step  20581\n",
            "loop 2 image : step  20582\n",
            "loop 2 image : step  20583\n",
            "loop 2 image : step  20584\n",
            "loop 2 image : step  20585\n",
            "loop 2 image : step  20586\n",
            "loop 2 image : step  20587\n",
            "loop 2 image : step  20588\n",
            "loop 2 image : step  20589\n",
            "loop 2 image : step  20590\n",
            "loop 2 image : step  20591\n",
            "loop 2 image : step  20592\n",
            "loop 2 image : step  20593\n",
            "loop 2 image : step  20594\n",
            "loop 2 image : step  20595\n",
            "loop 2 image : step  20596\n",
            "loop 2 image : step  20597\n",
            "loop 2 image : step  20598\n",
            "loop 2 image : step  20599\n",
            "loop 2 image : step  20600\n",
            "loop 2 image : step  20601\n",
            "loop 2 image : step  20602\n",
            "loop 2 image : step  20603\n",
            "loop 2 image : step  20604\n",
            "loop 2 image : step  20605\n",
            "loop 2 image : step  20606\n",
            "loop 2 image : step  20607\n",
            "loop 2 image : step  20608\n",
            "loop 2 image : step  20609\n",
            "loop 2 image : step  20610\n",
            "loop 2 image : step  20611\n",
            "loop 2 image : step  20612\n",
            "loop 2 image : step  20613\n",
            "loop 2 image : step  20614\n",
            "loop 2 image : step  20615\n",
            "loop 2 image : step  20616\n",
            "loop 2 image : step  20617\n",
            "loop 2 image : step  20618\n",
            "loop 2 image : step  20619\n",
            "loop 2 image : step  20620\n",
            "loop 2 image : step  20621\n",
            "loop 2 image : step  20622\n",
            "loop 2 image : step  20623\n",
            "loop 2 image : step  20624\n",
            "loop 2 image : step  20625\n",
            "loop 2 image : step  20626\n",
            "loop 2 image : step  20627\n",
            "loop 2 image : step  20628\n",
            "loop 2 image : step  20629\n",
            "loop 2 image : step  20630\n",
            "loop 2 image : step  20631\n",
            "loop 2 image : step  20632\n",
            "loop 2 image : step  20633\n",
            "loop 2 image : step  20634\n",
            "loop 2 image : step  20635\n",
            "loop 2 image : step  20636\n",
            "loop 2 image : step  20637\n",
            "loop 2 image : step  20638\n",
            "loop 2 image : step  20639\n",
            "loop 2 image : step  20640\n",
            "loop 2 image : step  20641\n",
            "loop 2 image : step  20642\n",
            "loop 2 image : step  20643\n",
            "loop 2 image : step  20644\n",
            "loop 2 image : step  20645\n",
            "loop 2 image : step  20646\n",
            "loop 2 image : step  20647\n",
            "loop 2 image : step  20648\n",
            "loop 2 image : step  20649\n",
            "loop 2 image : step  20650\n",
            "loop 2 image : step  20651\n",
            "loop 2 image : step  20652\n",
            "loop 2 image : step  20653\n",
            "loop 2 image : step  20654\n",
            "loop 2 image : step  20655\n",
            "loop 2 image : step  20656\n",
            "loop 2 image : step  20657\n",
            "loop 2 image : step  20658\n",
            "loop 2 image : step  20659\n",
            "loop 2 image : step  20660\n",
            "loop 2 image : step  20661\n",
            "loop 2 image : step  20662\n",
            "loop 2 image : step  20663\n",
            "loop 2 image : step  20664\n",
            "loop 2 image : step  20665\n",
            "loop 2 image : step  20666\n",
            "loop 2 image : step  20667\n",
            "loop 2 image : step  20668\n",
            "loop 2 image : step  20669\n",
            "loop 2 image : step  20670\n",
            "loop 2 image : step  20671\n",
            "loop 2 image : step  20672\n",
            "loop 2 image : step  20673\n",
            "loop 2 image : step  20674\n",
            "loop 2 image : step  20675\n",
            "loop 2 image : step  20676\n",
            "loop 2 image : step  20677\n",
            "loop 2 image : step  20678\n",
            "loop 2 image : step  20679\n",
            "loop 2 image : step  20680\n",
            "loop 2 image : step  20681\n",
            "loop 2 image : step  20682\n",
            "loop 2 image : step  20683\n",
            "loop 2 image : step  20684\n",
            "loop 2 image : step  20685\n",
            "loop 2 image : step  20686\n",
            "loop 2 image : step  20687\n",
            "loop 2 image : step  20688\n",
            "loop 2 image : step  20689\n",
            "loop 2 image : step  20690\n",
            "loop 2 image : step  20691\n",
            "loop 2 image : step  20692\n",
            "loop 2 image : step  20693\n",
            "loop 2 image : step  20694\n",
            "loop 2 image : step  20695\n",
            "loop 2 image : step  20696\n",
            "loop 2 image : step  20697\n",
            "loop 2 image : step  20698\n",
            "loop 2 image : step  20699\n",
            "loop 2 image : step  20700\n",
            "loop 2 image : step  20701\n",
            "loop 2 image : step  20702\n",
            "loop 2 image : step  20703\n",
            "loop 2 image : step  20704\n",
            "loop 2 image : step  20705\n",
            "loop 2 image : step  20706\n",
            "loop 2 image : step  20707\n",
            "loop 2 image : step  20708\n",
            "loop 2 image : step  20709\n",
            "loop 2 image : step  20710\n",
            "loop 2 image : step  20711\n",
            "loop 2 image : step  20712\n",
            "loop 2 image : step  20713\n",
            "loop 2 image : step  20714\n",
            "loop 2 image : step  20715\n",
            "loop 2 image : step  20716\n",
            "loop 2 image : step  20717\n",
            "loop 2 image : step  20718\n",
            "loop 2 image : step  20719\n",
            "loop 2 image : step  20720\n",
            "loop 2 image : step  20721\n",
            "loop 2 image : step  20722\n",
            "loop 2 image : step  20723\n",
            "loop 2 image : step  20724\n",
            "loop 2 image : step  20725\n",
            "loop 2 image : step  20726\n",
            "loop 2 image : step  20727\n",
            "loop 2 image : step  20728\n",
            "loop 2 image : step  20729\n",
            "loop 2 image : step  20730\n",
            "loop 2 image : step  20731\n",
            "loop 2 image : step  20732\n",
            "loop 2 image : step  20733\n",
            "loop 2 image : step  20734\n",
            "loop 2 image : step  20735\n",
            "loop 2 image : step  20736\n",
            "loop 2 image : step  20737\n",
            "loop 2 image : step  20738\n",
            "loop 2 image : step  20739\n",
            "loop 2 image : step  20740\n",
            "loop 2 image : step  20741\n",
            "loop 2 image : step  20742\n",
            "loop 2 image : step  20743\n",
            "loop 2 image : step  20744\n",
            "loop 2 image : step  20745\n",
            "loop 2 image : step  20746\n",
            "loop 2 image : step  20747\n",
            "loop 2 image : step  20748\n",
            "loop 2 image : step  20749\n",
            "loop 2 image : step  20750\n",
            "loop 2 image : step  20751\n",
            "loop 2 image : step  20752\n",
            "loop 2 image : step  20753\n",
            "loop 2 image : step  20754\n",
            "loop 2 image : step  20755\n",
            "loop 2 image : step  20756\n",
            "loop 2 image : step  20757\n",
            "loop 2 image : step  20758\n",
            "loop 2 image : step  20759\n",
            "loop 2 image : step  20760\n",
            "loop 2 image : step  20761\n",
            "loop 2 image : step  20762\n",
            "loop 2 image : step  20763\n",
            "loop 2 image : step  20764\n",
            "loop 2 image : step  20765\n",
            "loop 2 image : step  20766\n",
            "loop 2 image : step  20767\n",
            "loop 2 image : step  20768\n",
            "loop 2 image : step  20769\n",
            "loop 2 image : step  20770\n",
            "loop 2 image : step  20771\n",
            "loop 2 image : step  20772\n",
            "loop 2 image : step  20773\n",
            "loop 2 image : step  20774\n",
            "loop 2 image : step  20775\n",
            "loop 2 image : step  20776\n",
            "loop 2 image : step  20777\n",
            "loop 2 image : step  20778\n",
            "loop 2 image : step  20779\n",
            "loop 2 image : step  20780\n",
            "loop 2 image : step  20781\n",
            "loop 2 image : step  20782\n",
            "loop 2 image : step  20783\n",
            "loop 2 image : step  20784\n",
            "loop 2 image : step  20785\n",
            "loop 2 image : step  20786\n",
            "loop 2 image : step  20787\n",
            "loop 2 image : step  20788\n",
            "loop 2 image : step  20789\n",
            "loop 2 image : step  20790\n",
            "loop 2 image : step  20791\n",
            "loop 2 image : step  20792\n",
            "loop 2 image : step  20793\n",
            "loop 2 image : step  20794\n",
            "loop 2 image : step  20795\n",
            "loop 2 image : step  20796\n",
            "loop 2 image : step  20797\n",
            "loop 2 image : step  20798\n",
            "loop 2 image : step  20799\n",
            "loop 2 image : step  20800\n",
            "loop 2 image : step  20801\n",
            "loop 2 image : step  20802\n",
            "loop 2 image : step  20803\n",
            "loop 2 image : step  20804\n",
            "loop 2 image : step  20805\n",
            "loop 2 image : step  20806\n",
            "loop 2 image : step  20807\n",
            "loop 2 image : step  20808\n",
            "loop 2 image : step  20809\n",
            "loop 2 image : step  20810\n",
            "loop 2 image : step  20811\n",
            "loop 2 image : step  20812\n",
            "loop 2 image : step  20813\n",
            "loop 2 image : step  20814\n",
            "loop 2 image : step  20815\n",
            "loop 2 image : step  20816\n",
            "loop 2 image : step  20817\n",
            "loop 2 image : step  20818\n",
            "loop 2 image : step  20819\n",
            "loop 2 image : step  20820\n",
            "loop 2 image : step  20821\n",
            "loop 2 image : step  20822\n",
            "loop 2 image : step  20823\n",
            "loop 2 image : step  20824\n",
            "loop 2 image : step  20825\n",
            "loop 2 image : step  20826\n",
            "loop 2 image : step  20827\n",
            "loop 2 image : step  20828\n",
            "loop 2 image : step  20829\n",
            "loop 2 image : step  20830\n",
            "loop 2 image : step  20831\n",
            "loop 2 image : step  20832\n",
            "loop 2 image : step  20833\n",
            "loop 2 image : step  20834\n",
            "loop 2 image : step  20835\n",
            "loop 2 image : step  20836\n",
            "loop 2 image : step  20837\n",
            "loop 2 image : step  20838\n",
            "loop 2 image : step  20839\n",
            "loop 2 image : step  20840\n",
            "loop 2 image : step  20841\n",
            "loop 2 image : step  20842\n",
            "loop 2 image : step  20843\n",
            "loop 2 image : step  20844\n",
            "loop 2 image : step  20845\n",
            "loop 2 image : step  20846\n",
            "loop 2 image : step  20847\n",
            "loop 2 image : step  20848\n",
            "loop 2 image : step  20849\n",
            "loop 2 image : step  20850\n",
            "loop 2 image : step  20851\n",
            "loop 2 image : step  20852\n",
            "loop 2 image : step  20853\n",
            "loop 2 image : step  20854\n",
            "loop 2 image : step  20855\n",
            "loop 2 image : step  20856\n",
            "loop 2 image : step  20857\n",
            "loop 2 image : step  20858\n",
            "loop 2 image : step  20859\n",
            "loop 2 image : step  20860\n",
            "loop 2 image : step  20861\n",
            "loop 2 image : step  20862\n",
            "loop 2 image : step  20863\n",
            "loop 2 image : step  20864\n",
            "loop 2 image : step  20865\n",
            "loop 2 image : step  20866\n",
            "loop 2 image : step  20867\n",
            "loop 2 image : step  20868\n",
            "loop 2 image : step  20869\n",
            "loop 2 image : step  20870\n",
            "loop 2 image : step  20871\n",
            "loop 2 image : step  20872\n",
            "loop 2 image : step  20873\n",
            "loop 2 image : step  20874\n",
            "loop 2 image : step  20875\n",
            "loop 2 image : step  20876\n",
            "loop 2 image : step  20877\n",
            "loop 2 image : step  20878\n",
            "loop 2 image : step  20879\n",
            "loop 2 image : step  20880\n",
            "loop 2 image : step  20881\n",
            "loop 2 image : step  20882\n",
            "loop 2 image : step  20883\n",
            "loop 2 image : step  20884\n",
            "loop 2 image : step  20885\n",
            "loop 2 image : step  20886\n",
            "loop 2 image : step  20887\n",
            "loop 2 image : step  20888\n",
            "loop 2 image : step  20889\n",
            "loop 2 image : step  20890\n",
            "loop 2 image : step  20891\n",
            "loop 2 image : step  20892\n",
            "loop 2 image : step  20893\n",
            "loop 2 image : step  20894\n",
            "loop 2 image : step  20895\n",
            "loop 2 image : step  20896\n",
            "loop 2 image : step  20897\n",
            "loop 2 image : step  20898\n",
            "loop 2 image : step  20899\n",
            "loop 2 image : step  20900\n",
            "loop 2 image : step  20901\n",
            "loop 2 image : step  20902\n",
            "loop 2 image : step  20903\n",
            "loop 2 image : step  20904\n",
            "loop 2 image : step  20905\n",
            "loop 2 image : step  20906\n",
            "loop 2 image : step  20907\n",
            "loop 2 image : step  20908\n",
            "loop 2 image : step  20909\n",
            "loop 2 image : step  20910\n",
            "loop 2 image : step  20911\n",
            "loop 2 image : step  20912\n",
            "loop 2 image : step  20913\n",
            "loop 2 image : step  20914\n",
            "loop 2 image : step  20915\n",
            "loop 2 image : step  20916\n",
            "loop 2 image : step  20917\n",
            "loop 2 image : step  20918\n",
            "loop 2 image : step  20919\n",
            "loop 2 image : step  20920\n",
            "loop 2 image : step  20921\n",
            "loop 2 image : step  20922\n",
            "loop 2 image : step  20923\n",
            "loop 2 image : step  20924\n",
            "loop 2 image : step  20925\n",
            "loop 2 image : step  20926\n",
            "loop 2 image : step  20927\n",
            "loop 2 image : step  20928\n",
            "loop 2 image : step  20929\n",
            "loop 2 image : step  20930\n",
            "loop 2 image : step  20931\n",
            "loop 2 image : step  20932\n",
            "loop 2 image : step  20933\n",
            "loop 2 image : step  20934\n",
            "loop 2 image : step  20935\n",
            "loop 2 image : step  20936\n",
            "loop 2 image : step  20937\n",
            "loop 2 image : step  20938\n",
            "loop 2 image : step  20939\n",
            "loop 2 image : step  20940\n",
            "loop 2 image : step  20941\n",
            "loop 2 image : step  20942\n",
            "loop 2 image : step  20943\n",
            "loop 2 image : step  20944\n",
            "loop 2 image : step  20945\n",
            "loop 2 image : step  20946\n",
            "loop 2 image : step  20947\n",
            "loop 2 image : step  20948\n",
            "loop 2 image : step  20949\n",
            "loop 2 image : step  20950\n",
            "loop 2 image : step  20951\n",
            "loop 2 image : step  20952\n",
            "loop 2 image : step  20953\n",
            "loop 2 image : step  20954\n",
            "loop 2 image : step  20955\n",
            "loop 2 image : step  20956\n",
            "loop 2 image : step  20957\n",
            "loop 2 image : step  20958\n",
            "loop 2 image : step  20959\n",
            "loop 2 image : step  20960\n",
            "loop 2 image : step  20961\n",
            "loop 2 image : step  20962\n",
            "loop 2 image : step  20963\n",
            "loop 2 image : step  20964\n",
            "loop 2 image : step  20965\n",
            "loop 2 image : step  20966\n",
            "loop 2 image : step  20967\n",
            "loop 2 image : step  20968\n",
            "loop 2 image : step  20969\n",
            "loop 2 image : step  20970\n",
            "loop 2 image : step  20971\n",
            "loop 2 image : step  20972\n",
            "loop 2 image : step  20973\n",
            "loop 2 image : step  20974\n",
            "loop 2 image : step  20975\n",
            "loop 2 image : step  20976\n",
            "loop 2 image : step  20977\n",
            "loop 2 image : step  20978\n",
            "loop 2 image : step  20979\n",
            "loop 2 image : step  20980\n",
            "loop 2 image : step  20981\n",
            "loop 2 image : step  20982\n",
            "loop 2 image : step  20983\n",
            "loop 2 image : step  20984\n",
            "loop 2 image : step  20985\n",
            "loop 2 image : step  20986\n",
            "loop 2 image : step  20987\n",
            "loop 2 image : step  20988\n",
            "loop 2 image : step  20989\n",
            "loop 2 image : step  20990\n",
            "loop 2 image : step  20991\n",
            "loop 2 image : step  20992\n",
            "loop 2 image : step  20993\n",
            "loop 2 image : step  20994\n",
            "loop 2 image : step  20995\n",
            "loop 2 image : step  20996\n",
            "loop 2 image : step  20997\n",
            "loop 2 image : step  20998\n",
            "loop 2 image : step  20999\n",
            "loop 2 image : step  21000\n",
            "loop 2 image : step  21001\n",
            "loop 2 image : step  21002\n",
            "loop 2 image : step  21003\n",
            "loop 2 image : step  21004\n",
            "loop 2 image : step  21005\n",
            "loop 2 image : step  21006\n",
            "loop 2 image : step  21007\n",
            "loop 2 image : step  21008\n",
            "loop 2 image : step  21009\n",
            "loop 2 image : step  21010\n",
            "loop 2 image : step  21011\n",
            "loop 2 image : step  21012\n",
            "loop 2 image : step  21013\n",
            "loop 2 image : step  21014\n",
            "loop 2 image : step  21015\n",
            "loop 2 image : step  21016\n",
            "loop 2 image : step  21017\n",
            "loop 2 image : step  21018\n",
            "loop 2 image : step  21019\n",
            "loop 2 image : step  21020\n",
            "loop 2 image : step  21021\n",
            "loop 2 image : step  21022\n",
            "loop 2 image : step  21023\n",
            "loop 2 image : step  21024\n",
            "loop 2 image : step  21025\n",
            "loop 2 image : step  21026\n",
            "loop 2 image : step  21027\n",
            "loop 2 image : step  21028\n",
            "loop 2 image : step  21029\n",
            "loop 2 image : step  21030\n",
            "loop 2 image : step  21031\n",
            "loop 2 image : step  21032\n",
            "loop 2 image : step  21033\n",
            "loop 2 image : step  21034\n",
            "loop 2 image : step  21035\n",
            "loop 2 image : step  21036\n",
            "loop 2 image : step  21037\n",
            "loop 2 image : step  21038\n",
            "loop 2 image : step  21039\n",
            "loop 2 image : step  21040\n",
            "loop 2 image : step  21041\n",
            "loop 2 image : step  21042\n",
            "loop 2 image : step  21043\n",
            "loop 2 image : step  21044\n",
            "loop 2 image : step  21045\n",
            "loop 2 image : step  21046\n",
            "loop 2 image : step  21047\n",
            "loop 2 image : step  21048\n",
            "loop 2 image : step  21049\n",
            "loop 2 image : step  21050\n",
            "loop 2 image : step  21051\n",
            "loop 2 image : step  21052\n",
            "loop 2 image : step  21053\n",
            "loop 2 image : step  21054\n",
            "loop 2 image : step  21055\n",
            "loop 2 image : step  21056\n",
            "loop 2 image : step  21057\n",
            "loop 2 image : step  21058\n",
            "loop 2 image : step  21059\n",
            "loop 2 image : step  21060\n",
            "loop 2 image : step  21061\n",
            "loop 2 image : step  21062\n",
            "loop 2 image : step  21063\n",
            "loop 2 image : step  21064\n",
            "loop 2 image : step  21065\n",
            "loop 2 image : step  21066\n",
            "loop 2 image : step  21067\n",
            "loop 2 image : step  21068\n",
            "loop 2 image : step  21069\n",
            "loop 2 image : step  21070\n",
            "loop 2 image : step  21071\n",
            "loop 2 image : step  21072\n",
            "loop 2 image : step  21073\n",
            "loop 2 image : step  21074\n",
            "loop 2 image : step  21075\n",
            "loop 2 image : step  21076\n",
            "loop 2 image : step  21077\n",
            "loop 2 image : step  21078\n",
            "loop 2 image : step  21079\n",
            "loop 2 image : step  21080\n",
            "loop 2 image : step  21081\n",
            "loop 2 image : step  21082\n",
            "loop 2 image : step  21083\n",
            "loop 2 image : step  21084\n",
            "loop 2 image : step  21085\n",
            "loop 2 image : step  21086\n",
            "loop 2 image : step  21087\n",
            "loop 2 image : step  21088\n",
            "loop 2 image : step  21089\n",
            "loop 2 image : step  21090\n",
            "loop 2 image : step  21091\n",
            "loop 2 image : step  21092\n",
            "loop 2 image : step  21093\n",
            "loop 2 image : step  21094\n",
            "loop 2 image : step  21095\n",
            "loop 2 image : step  21096\n",
            "loop 2 image : step  21097\n",
            "loop 2 image : step  21098\n",
            "loop 2 image : step  21099\n",
            "loop 2 image : step  21100\n",
            "loop 2 image : step  21101\n",
            "loop 2 image : step  21102\n",
            "loop 2 image : step  21103\n",
            "loop 2 image : step  21104\n",
            "loop 2 image : step  21105\n",
            "loop 2 image : step  21106\n",
            "loop 2 image : step  21107\n",
            "loop 2 image : step  21108\n",
            "loop 2 image : step  21109\n",
            "loop 2 image : step  21110\n",
            "loop 2 image : step  21111\n",
            "loop 2 image : step  21112\n",
            "loop 2 image : step  21113\n",
            "loop 2 image : step  21114\n",
            "loop 2 image : step  21115\n",
            "loop 2 image : step  21116\n",
            "loop 2 image : step  21117\n",
            "loop 2 image : step  21118\n",
            "loop 2 image : step  21119\n",
            "loop 2 image : step  21120\n",
            "loop 2 image : step  21121\n",
            "loop 2 image : step  21122\n",
            "loop 2 image : step  21123\n",
            "loop 2 image : step  21124\n",
            "loop 2 image : step  21125\n",
            "loop 2 image : step  21126\n",
            "loop 2 image : step  21127\n",
            "loop 2 image : step  21128\n",
            "loop 2 image : step  21129\n",
            "loop 2 image : step  21130\n",
            "loop 2 image : step  21131\n",
            "loop 2 image : step  21132\n",
            "loop 2 image : step  21133\n",
            "loop 2 image : step  21134\n",
            "loop 2 image : step  21135\n",
            "loop 2 image : step  21136\n",
            "loop 2 image : step  21137\n",
            "loop 2 image : step  21138\n",
            "loop 2 image : step  21139\n",
            "loop 2 image : step  21140\n",
            "loop 2 image : step  21141\n",
            "loop 2 image : step  21142\n",
            "loop 2 image : step  21143\n",
            "loop 2 image : step  21144\n",
            "loop 2 image : step  21145\n",
            "loop 2 image : step  21146\n",
            "loop 2 image : step  21147\n",
            "loop 2 image : step  21148\n",
            "loop 2 image : step  21149\n",
            "loop 2 image : step  21150\n",
            "loop 2 image : step  21151\n",
            "loop 2 image : step  21152\n",
            "loop 2 image : step  21153\n",
            "loop 2 image : step  21154\n",
            "loop 2 image : step  21155\n",
            "loop 2 image : step  21156\n",
            "loop 2 image : step  21157\n",
            "loop 2 image : step  21158\n",
            "loop 2 image : step  21159\n",
            "loop 2 image : step  21160\n",
            "loop 2 image : step  21161\n",
            "loop 2 image : step  21162\n",
            "loop 2 image : step  21163\n",
            "loop 2 image : step  21164\n",
            "loop 2 image : step  21165\n",
            "loop 2 image : step  21166\n",
            "loop 2 image : step  21167\n",
            "loop 2 image : step  21168\n",
            "loop 2 image : step  21169\n",
            "loop 2 image : step  21170\n",
            "loop 2 image : step  21171\n",
            "loop 2 image : step  21172\n",
            "loop 2 image : step  21173\n",
            "loop 2 image : step  21174\n",
            "loop 2 image : step  21175\n",
            "loop 2 image : step  21176\n",
            "loop 2 image : step  21177\n",
            "loop 2 image : step  21178\n",
            "loop 2 image : step  21179\n",
            "loop 2 image : step  21180\n",
            "loop 2 image : step  21181\n",
            "loop 2 image : step  21182\n",
            "loop 2 image : step  21183\n",
            "loop 2 image : step  21184\n",
            "loop 2 image : step  21185\n",
            "loop 2 image : step  21186\n",
            "loop 2 image : step  21187\n",
            "loop 2 image : step  21188\n",
            "loop 2 image : step  21189\n",
            "loop 2 image : step  21190\n",
            "loop 2 image : step  21191\n",
            "loop 2 image : step  21192\n",
            "loop 2 image : step  21193\n",
            "loop 2 image : step  21194\n",
            "loop 2 image : step  21195\n",
            "loop 2 image : step  21196\n",
            "loop 2 image : step  21197\n",
            "loop 2 image : step  21198\n",
            "loop 2 image : step  21199\n",
            "loop 2 image : step  21200\n",
            "loop 2 image : step  21201\n",
            "loop 2 image : step  21202\n",
            "loop 2 image : step  21203\n",
            "loop 2 image : step  21204\n",
            "loop 2 image : step  21205\n",
            "loop 2 image : step  21206\n",
            "loop 2 image : step  21207\n",
            "loop 2 image : step  21208\n",
            "loop 2 image : step  21209\n",
            "loop 2 image : step  21210\n",
            "loop 2 image : step  21211\n",
            "loop 2 image : step  21212\n",
            "loop 2 image : step  21213\n",
            "loop 2 image : step  21214\n",
            "loop 2 image : step  21215\n",
            "loop 2 image : step  21216\n",
            "loop 2 image : step  21217\n",
            "loop 2 image : step  21218\n",
            "loop 2 image : step  21219\n",
            "loop 2 image : step  21220\n",
            "loop 2 image : step  21221\n",
            "loop 2 image : step  21222\n",
            "loop 2 image : step  21223\n",
            "loop 2 image : step  21224\n",
            "loop 2 image : step  21225\n",
            "loop 2 image : step  21226\n",
            "loop 2 image : step  21227\n",
            "loop 2 image : step  21228\n",
            "loop 2 image : step  21229\n",
            "loop 2 image : step  21230\n",
            "loop 2 image : step  21231\n",
            "loop 2 image : step  21232\n",
            "loop 2 image : step  21233\n",
            "loop 2 image : step  21234\n",
            "loop 2 image : step  21235\n",
            "loop 2 image : step  21236\n",
            "loop 2 image : step  21237\n",
            "loop 2 image : step  21238\n",
            "loop 2 image : step  21239\n",
            "loop 2 image : step  21240\n",
            "loop 2 image : step  21241\n",
            "loop 2 image : step  21242\n",
            "loop 2 image : step  21243\n",
            "loop 2 image : step  21244\n",
            "loop 2 image : step  21245\n",
            "loop 2 image : step  21246\n",
            "loop 2 image : step  21247\n",
            "loop 2 image : step  21248\n",
            "loop 2 image : step  21249\n",
            "loop 2 image : step  21250\n",
            "loop 2 image : step  21251\n",
            "loop 2 image : step  21252\n",
            "loop 2 image : step  21253\n",
            "loop 2 image : step  21254\n",
            "loop 2 image : step  21255\n",
            "loop 2 image : step  21256\n",
            "loop 2 image : step  21257\n",
            "loop 2 image : step  21258\n",
            "loop 2 image : step  21259\n",
            "loop 2 image : step  21260\n",
            "loop 2 image : step  21261\n",
            "loop 2 image : step  21262\n",
            "loop 2 image : step  21263\n",
            "loop 2 image : step  21264\n",
            "loop 2 image : step  21265\n",
            "loop 2 image : step  21266\n",
            "loop 2 image : step  21267\n",
            "loop 2 image : step  21268\n",
            "loop 2 image : step  21269\n",
            "loop 2 image : step  21270\n",
            "loop 2 image : step  21271\n",
            "loop 2 image : step  21272\n",
            "loop 2 image : step  21273\n",
            "loop 2 image : step  21274\n",
            "loop 2 image : step  21275\n",
            "loop 2 image : step  21276\n",
            "loop 2 image : step  21277\n",
            "loop 2 image : step  21278\n",
            "loop 2 image : step  21279\n",
            "loop 2 image : step  21280\n",
            "loop 2 image : step  21281\n",
            "loop 2 image : step  21282\n",
            "loop 2 image : step  21283\n",
            "loop 2 image : step  21284\n",
            "loop 2 image : step  21285\n",
            "loop 2 image : step  21286\n",
            "loop 2 image : step  21287\n",
            "loop 2 image : step  21288\n",
            "loop 2 image : step  21289\n",
            "loop 2 image : step  21290\n",
            "loop 2 image : step  21291\n",
            "loop 2 image : step  21292\n",
            "loop 2 image : step  21293\n",
            "loop 2 image : step  21294\n",
            "loop 2 image : step  21295\n",
            "loop 2 image : step  21296\n",
            "loop 2 image : step  21297\n",
            "loop 2 image : step  21298\n",
            "loop 2 image : step  21299\n",
            "loop 2 image : step  21300\n",
            "loop 2 image : step  21301\n",
            "loop 2 image : step  21302\n",
            "loop 2 image : step  21303\n",
            "loop 2 image : step  21304\n",
            "loop 2 image : step  21305\n",
            "loop 2 image : step  21306\n",
            "loop 2 image : step  21307\n",
            "loop 2 image : step  21308\n",
            "loop 2 image : step  21309\n",
            "loop 2 image : step  21310\n",
            "loop 2 image : step  21311\n",
            "loop 2 image : step  21312\n",
            "loop 2 image : step  21313\n",
            "loop 2 image : step  21314\n",
            "loop 2 image : step  21315\n",
            "loop 2 image : step  21316\n",
            "loop 2 image : step  21317\n",
            "loop 2 image : step  21318\n",
            "loop 2 image : step  21319\n",
            "loop 2 image : step  21320\n",
            "loop 2 image : step  21321\n",
            "loop 2 image : step  21322\n",
            "loop 2 image : step  21323\n",
            "loop 2 image : step  21324\n",
            "loop 2 image : step  21325\n",
            "loop 2 image : step  21326\n",
            "loop 2 image : step  21327\n",
            "loop 2 image : step  21328\n",
            "loop 2 image : step  21329\n",
            "loop 2 image : step  21330\n",
            "loop 2 image : step  21331\n",
            "loop 2 image : step  21332\n",
            "loop 2 image : step  21333\n",
            "loop 2 image : step  21334\n",
            "loop 2 image : step  21335\n",
            "loop 2 image : step  21336\n",
            "loop 2 image : step  21337\n",
            "loop 2 image : step  21338\n",
            "loop 2 image : step  21339\n",
            "loop 2 image : step  21340\n",
            "loop 2 image : step  21341\n",
            "loop 2 image : step  21342\n",
            "loop 2 image : step  21343\n",
            "loop 2 image : step  21344\n",
            "loop 2 image : step  21345\n",
            "loop 2 image : step  21346\n",
            "loop 2 image : step  21347\n",
            "loop 2 image : step  21348\n",
            "loop 2 image : step  21349\n",
            "loop 2 image : step  21350\n",
            "loop 2 image : step  21351\n",
            "loop 2 image : step  21352\n",
            "loop 2 image : step  21353\n",
            "loop 2 image : step  21354\n",
            "loop 2 image : step  21355\n",
            "loop 2 image : step  21356\n",
            "loop 2 image : step  21357\n",
            "loop 2 image : step  21358\n",
            "loop 2 image : step  21359\n",
            "loop 2 image : step  21360\n",
            "loop 2 image : step  21361\n",
            "loop 2 image : step  21362\n",
            "loop 2 image : step  21363\n",
            "loop 2 image : step  21364\n",
            "loop 2 image : step  21365\n",
            "loop 2 image : step  21366\n",
            "loop 2 image : step  21367\n",
            "loop 2 image : step  21368\n",
            "loop 2 image : step  21369\n",
            "loop 2 image : step  21370\n",
            "loop 2 image : step  21371\n",
            "loop 2 image : step  21372\n",
            "loop 2 image : step  21373\n",
            "loop 2 image : step  21374\n",
            "loop 2 image : step  21375\n",
            "loop 2 image : step  21376\n",
            "loop 2 image : step  21377\n",
            "loop 2 image : step  21378\n",
            "loop 2 image : step  21379\n",
            "loop 2 image : step  21380\n",
            "loop 2 image : step  21381\n",
            "loop 2 image : step  21382\n",
            "loop 2 image : step  21383\n",
            "loop 2 image : step  21384\n",
            "loop 2 image : step  21385\n",
            "loop 2 image : step  21386\n",
            "loop 2 image : step  21387\n",
            "loop 2 image : step  21388\n",
            "loop 2 image : step  21389\n",
            "loop 2 image : step  21390\n",
            "loop 2 image : step  21391\n",
            "loop 2 image : step  21392\n",
            "loop 2 image : step  21393\n",
            "loop 2 image : step  21394\n",
            "loop 2 image : step  21395\n",
            "loop 2 image : step  21396\n",
            "loop 2 image : step  21397\n",
            "loop 2 image : step  21398\n",
            "loop 2 image : step  21399\n",
            "loop 2 image : step  21400\n",
            "loop 2 image : step  21401\n",
            "loop 2 image : step  21402\n",
            "loop 2 image : step  21403\n",
            "loop 2 image : step  21404\n",
            "loop 2 image : step  21405\n",
            "loop 2 image : step  21406\n",
            "loop 2 image : step  21407\n",
            "loop 2 image : step  21408\n",
            "loop 2 image : step  21409\n",
            "loop 2 image : step  21410\n",
            "loop 2 image : step  21411\n",
            "loop 2 image : step  21412\n",
            "loop 2 image : step  21413\n",
            "loop 2 image : step  21414\n",
            "loop 2 image : step  21415\n",
            "loop 2 image : step  21416\n",
            "loop 2 image : step  21417\n",
            "loop 2 image : step  21418\n",
            "loop 2 image : step  21419\n",
            "loop 2 image : step  21420\n",
            "loop 2 image : step  21421\n",
            "loop 2 image : step  21422\n",
            "loop 2 image : step  21423\n",
            "loop 2 image : step  21424\n",
            "loop 2 image : step  21425\n",
            "loop 2 image : step  21426\n",
            "loop 2 image : step  21427\n",
            "loop 2 image : step  21428\n",
            "loop 2 image : step  21429\n",
            "loop 2 image : step  21430\n",
            "loop 2 image : step  21431\n",
            "loop 2 image : step  21432\n",
            "loop 2 image : step  21433\n",
            "loop 2 image : step  21434\n",
            "loop 2 image : step  21435\n",
            "loop 2 image : step  21436\n",
            "loop 2 image : step  21437\n",
            "loop 2 image : step  21438\n",
            "loop 2 image : step  21439\n",
            "loop 2 image : step  21440\n",
            "loop 2 image : step  21441\n",
            "loop 2 image : step  21442\n",
            "loop 2 image : step  21443\n",
            "loop 2 image : step  21444\n",
            "loop 2 image : step  21445\n",
            "loop 2 image : step  21446\n",
            "loop 2 image : step  21447\n",
            "loop 2 image : step  21448\n",
            "loop 2 image : step  21449\n",
            "loop 2 image : step  21450\n",
            "loop 2 image : step  21451\n",
            "loop 2 image : step  21452\n",
            "loop 2 image : step  21453\n",
            "loop 2 image : step  21454\n",
            "loop 2 image : step  21455\n",
            "loop 2 image : step  21456\n",
            "loop 2 image : step  21457\n",
            "loop 2 image : step  21458\n",
            "loop 2 image : step  21459\n",
            "loop 2 image : step  21460\n",
            "loop 2 image : step  21461\n",
            "loop 2 image : step  21462\n",
            "loop 2 image : step  21463\n",
            "loop 2 image : step  21464\n",
            "loop 2 image : step  21465\n",
            "loop 2 image : step  21466\n",
            "loop 2 image : step  21467\n",
            "loop 2 image : step  21468\n",
            "loop 2 image : step  21469\n",
            "loop 2 image : step  21470\n",
            "loop 2 image : step  21471\n",
            "loop 2 image : step  21472\n",
            "loop 2 image : step  21473\n",
            "loop 2 image : step  21474\n",
            "loop 2 image : step  21475\n",
            "loop 2 image : step  21476\n",
            "loop 2 image : step  21477\n",
            "loop 2 image : step  21478\n",
            "loop 2 image : step  21479\n",
            "loop 2 image : step  21480\n",
            "loop 2 image : step  21481\n",
            "loop 2 image : step  21482\n",
            "loop 2 image : step  21483\n",
            "loop 2 image : step  21484\n",
            "loop 2 image : step  21485\n",
            "loop 2 image : step  21486\n",
            "loop 2 image : step  21487\n",
            "loop 2 image : step  21488\n",
            "loop 2 image : step  21489\n",
            "loop 2 image : step  21490\n",
            "loop 2 image : step  21491\n",
            "loop 2 image : step  21492\n",
            "loop 2 image : step  21493\n",
            "loop 2 image : step  21494\n",
            "loop 2 image : step  21495\n",
            "loop 2 image : step  21496\n",
            "loop 2 image : step  21497\n",
            "loop 2 image : step  21498\n",
            "loop 2 image : step  21499\n",
            "loop 2 image : step  21500\n",
            "loop 2 image : step  21501\n",
            "loop 2 image : step  21502\n",
            "loop 2 image : step  21503\n",
            "loop 2 image : step  21504\n",
            "loop 2 image : step  21505\n",
            "loop 2 image : step  21506\n",
            "loop 2 image : step  21507\n",
            "loop 2 image : step  21508\n",
            "loop 2 image : step  21509\n",
            "loop 2 image : step  21510\n",
            "loop 2 image : step  21511\n",
            "loop 2 image : step  21512\n",
            "loop 2 image : step  21513\n",
            "loop 2 image : step  21514\n",
            "loop 2 image : step  21515\n",
            "loop 2 image : step  21516\n",
            "loop 2 image : step  21517\n",
            "loop 2 image : step  21518\n",
            "loop 2 image : step  21519\n",
            "loop 2 image : step  21520\n",
            "loop 2 image : step  21521\n",
            "loop 2 image : step  21522\n",
            "loop 2 image : step  21523\n",
            "loop 2 image : step  21524\n",
            "loop 2 image : step  21525\n",
            "loop 2 image : step  21526\n",
            "loop 2 image : step  21527\n",
            "loop 2 image : step  21528\n",
            "loop 2 image : step  21529\n",
            "loop 2 image : step  21530\n",
            "loop 2 image : step  21531\n",
            "loop 2 image : step  21532\n",
            "loop 2 image : step  21533\n",
            "loop 2 image : step  21534\n",
            "loop 2 image : step  21535\n",
            "loop 2 image : step  21536\n",
            "loop 2 image : step  21537\n",
            "loop 2 image : step  21538\n",
            "loop 2 image : step  21539\n",
            "loop 2 image : step  21540\n",
            "loop 2 image : step  21541\n",
            "loop 2 image : step  21542\n",
            "loop 2 image : step  21543\n",
            "loop 2 image : step  21544\n",
            "loop 2 image : step  21545\n",
            "loop 2 image : step  21546\n",
            "loop 2 image : step  21547\n",
            "loop 2 image : step  21548\n",
            "loop 2 image : step  21549\n",
            "loop 2 image : step  21550\n",
            "loop 2 image : step  21551\n",
            "loop 2 image : step  21552\n",
            "loop 2 image : step  21553\n",
            "loop 2 image : step  21554\n",
            "loop 2 image : step  21555\n",
            "loop 2 image : step  21556\n",
            "loop 2 image : step  21557\n",
            "loop 2 image : step  21558\n",
            "loop 2 image : step  21559\n",
            "loop 2 image : step  21560\n",
            "loop 2 image : step  21561\n",
            "loop 2 image : step  21562\n",
            "loop 2 image : step  21563\n",
            "loop 2 image : step  21564\n",
            "loop 2 image : step  21565\n",
            "loop 2 image : step  21566\n",
            "loop 2 image : step  21567\n",
            "loop 2 image : step  21568\n",
            "loop 2 image : step  21569\n",
            "loop 2 image : step  21570\n",
            "loop 2 image : step  21571\n",
            "loop 2 image : step  21572\n",
            "loop 2 image : step  21573\n",
            "loop 2 image : step  21574\n",
            "loop 2 image : step  21575\n",
            "loop 2 image : step  21576\n",
            "loop 2 image : step  21577\n",
            "loop 2 image : step  21578\n",
            "loop 2 image : step  21579\n",
            "loop 2 image : step  21580\n",
            "loop 2 image : step  21581\n",
            "loop 2 image : step  21582\n",
            "loop 2 image : step  21583\n",
            "loop 2 image : step  21584\n",
            "loop 2 image : step  21585\n",
            "loop 2 image : step  21586\n",
            "loop 2 image : step  21587\n",
            "loop 2 image : step  21588\n",
            "loop 2 image : step  21589\n",
            "loop 2 image : step  21590\n",
            "loop 2 image : step  21591\n",
            "loop 2 image : step  21592\n",
            "loop 2 image : step  21593\n",
            "loop 2 image : step  21594\n",
            "loop 2 image : step  21595\n",
            "loop 2 image : step  21596\n",
            "loop 2 image : step  21597\n",
            "loop 2 image : step  21598\n",
            "loop 2 image : step  21599\n",
            "loop 2 image : step  21600\n",
            "loop 2 image : step  21601\n",
            "loop 2 image : step  21602\n",
            "loop 2 image : step  21603\n",
            "loop 2 image : step  21604\n",
            "loop 2 image : step  21605\n",
            "loop 2 image : step  21606\n",
            "loop 2 image : step  21607\n",
            "loop 2 image : step  21608\n",
            "loop 2 image : step  21609\n",
            "loop 2 image : step  21610\n",
            "loop 2 image : step  21611\n",
            "loop 2 image : step  21612\n",
            "loop 2 image : step  21613\n",
            "loop 2 image : step  21614\n",
            "loop 2 image : step  21615\n",
            "loop 2 image : step  21616\n",
            "loop 2 image : step  21617\n",
            "loop 2 image : step  21618\n",
            "loop 2 image : step  21619\n",
            "loop 2 image : step  21620\n",
            "loop 2 image : step  21621\n",
            "loop 2 image : step  21622\n",
            "loop 2 image : step  21623\n",
            "loop 2 image : step  21624\n",
            "loop 2 image : step  21625\n",
            "loop 2 image : step  21626\n",
            "loop 2 image : step  21627\n",
            "loop 2 image : step  21628\n",
            "loop 2 image : step  21629\n",
            "loop 2 image : step  21630\n",
            "loop 2 image : step  21631\n",
            "loop 2 image : step  21632\n",
            "loop 2 image : step  21633\n",
            "loop 2 image : step  21634\n",
            "loop 2 image : step  21635\n",
            "loop 2 image : step  21636\n",
            "loop 2 image : step  21637\n",
            "loop 2 image : step  21638\n",
            "loop 2 image : step  21639\n",
            "loop 2 image : step  21640\n",
            "loop 2 image : step  21641\n",
            "loop 2 image : step  21642\n",
            "loop 2 image : step  21643\n",
            "loop 2 image : step  21644\n",
            "loop 2 image : step  21645\n",
            "loop 2 image : step  21646\n",
            "loop 2 image : step  21647\n",
            "loop 2 image : step  21648\n",
            "loop 2 image : step  21649\n",
            "loop 2 image : step  21650\n",
            "loop 2 image : step  21651\n",
            "loop 2 image : step  21652\n",
            "loop 2 image : step  21653\n",
            "loop 2 image : step  21654\n",
            "loop 2 image : step  21655\n",
            "loop 2 image : step  21656\n",
            "loop 2 image : step  21657\n",
            "loop 2 image : step  21658\n",
            "loop 2 image : step  21659\n",
            "loop 2 image : step  21660\n",
            "loop 2 image : step  21661\n",
            "loop 2 image : step  21662\n",
            "loop 2 image : step  21663\n",
            "loop 2 image : step  21664\n",
            "loop 2 image : step  21665\n",
            "loop 2 image : step  21666\n",
            "loop 2 image : step  21667\n",
            "loop 2 image : step  21668\n",
            "loop 2 image : step  21669\n",
            "loop 2 image : step  21670\n",
            "loop 2 image : step  21671\n",
            "loop 2 image : step  21672\n",
            "loop 2 image : step  21673\n",
            "loop 2 image : step  21674\n",
            "loop 2 image : step  21675\n",
            "loop 2 image : step  21676\n",
            "loop 2 image : step  21677\n",
            "loop 2 image : step  21678\n",
            "loop 2 image : step  21679\n",
            "loop 2 image : step  21680\n",
            "loop 2 image : step  21681\n",
            "loop 2 image : step  21682\n",
            "loop 2 image : step  21683\n",
            "loop 2 image : step  21684\n",
            "loop 2 image : step  21685\n",
            "loop 2 image : step  21686\n",
            "loop 2 image : step  21687\n",
            "loop 2 image : step  21688\n",
            "loop 2 image : step  21689\n",
            "loop 2 image : step  21690\n",
            "loop 2 image : step  21691\n",
            "loop 2 image : step  21692\n",
            "loop 2 image : step  21693\n",
            "loop 2 image : step  21694\n",
            "loop 2 image : step  21695\n",
            "loop 2 image : step  21696\n",
            "loop 2 image : step  21697\n",
            "loop 2 image : step  21698\n",
            "loop 2 image : step  21699\n",
            "loop 2 image : step  21700\n",
            "loop 2 image : step  21701\n",
            "loop 2 image : step  21702\n",
            "loop 2 image : step  21703\n",
            "loop 2 image : step  21704\n",
            "loop 2 image : step  21705\n",
            "loop 2 image : step  21706\n",
            "loop 2 image : step  21707\n",
            "loop 2 image : step  21708\n",
            "loop 2 image : step  21709\n",
            "loop 2 image : step  21710\n",
            "loop 2 image : step  21711\n",
            "loop 2 image : step  21712\n",
            "loop 2 image : step  21713\n",
            "loop 2 image : step  21714\n",
            "loop 2 image : step  21715\n",
            "loop 2 image : step  21716\n",
            "loop 2 image : step  21717\n",
            "loop 2 image : step  21718\n",
            "loop 2 image : step  21719\n",
            "loop 2 image : step  21720\n",
            "loop 2 image : step  21721\n",
            "loop 2 image : step  21722\n",
            "loop 2 image : step  21723\n",
            "loop 2 image : step  21724\n",
            "loop 2 image : step  21725\n",
            "loop 2 image : step  21726\n",
            "loop 2 image : step  21727\n",
            "loop 2 image : step  21728\n",
            "loop 2 image : step  21729\n",
            "loop 2 image : step  21730\n",
            "loop 2 image : step  21731\n",
            "loop 2 image : step  21732\n",
            "loop 2 image : step  21733\n",
            "loop 2 image : step  21734\n",
            "loop 2 image : step  21735\n",
            "loop 2 image : step  21736\n",
            "loop 2 image : step  21737\n",
            "loop 2 image : step  21738\n",
            "loop 2 image : step  21739\n",
            "loop 2 image : step  21740\n",
            "loop 2 image : step  21741\n",
            "loop 2 image : step  21742\n",
            "loop 2 image : step  21743\n",
            "loop 2 image : step  21744\n",
            "loop 2 image : step  21745\n",
            "loop 2 image : step  21746\n",
            "loop 2 image : step  21747\n",
            "loop 2 image : step  21748\n",
            "loop 2 image : step  21749\n",
            "loop 2 image : step  21750\n",
            "loop 2 image : step  21751\n",
            "loop 2 image : step  21752\n",
            "loop 2 image : step  21753\n",
            "loop 2 image : step  21754\n",
            "loop 2 image : step  21755\n",
            "loop 2 image : step  21756\n",
            "loop 2 image : step  21757\n",
            "loop 2 image : step  21758\n",
            "loop 2 image : step  21759\n",
            "loop 2 image : step  21760\n",
            "loop 2 image : step  21761\n",
            "loop 2 image : step  21762\n",
            "loop 2 image : step  21763\n",
            "loop 2 image : step  21764\n",
            "loop 2 image : step  21765\n",
            "loop 2 image : step  21766\n",
            "loop 2 image : step  21767\n",
            "loop 2 image : step  21768\n",
            "loop 2 image : step  21769\n",
            "loop 2 image : step  21770\n",
            "loop 2 image : step  21771\n",
            "loop 2 image : step  21772\n",
            "loop 2 image : step  21773\n",
            "loop 2 image : step  21774\n",
            "loop 2 image : step  21775\n",
            "loop 2 image : step  21776\n",
            "loop 2 image : step  21777\n",
            "loop 2 image : step  21778\n",
            "loop 2 image : step  21779\n",
            "loop 2 image : step  21780\n",
            "loop 2 image : step  21781\n",
            "loop 2 image : step  21782\n",
            "loop 2 image : step  21783\n",
            "loop 2 image : step  21784\n",
            "loop 2 image : step  21785\n",
            "loop 2 image : step  21786\n",
            "loop 2 image : step  21787\n",
            "loop 2 image : step  21788\n",
            "loop 2 image : step  21789\n",
            "loop 2 image : step  21790\n",
            "loop 2 image : step  21791\n",
            "loop 2 image : step  21792\n",
            "loop 2 image : step  21793\n",
            "loop 2 image : step  21794\n",
            "loop 2 image : step  21795\n",
            "loop 2 image : step  21796\n",
            "loop 2 image : step  21797\n",
            "loop 2 image : step  21798\n",
            "loop 2 image : step  21799\n",
            "loop 2 image : step  21800\n",
            "loop 2 image : step  21801\n",
            "loop 2 image : step  21802\n",
            "loop 2 image : step  21803\n",
            "loop 2 image : step  21804\n",
            "loop 2 image : step  21805\n",
            "loop 2 image : step  21806\n",
            "loop 2 image : step  21807\n",
            "loop 2 image : step  21808\n",
            "loop 2 image : step  21809\n",
            "loop 2 image : step  21810\n",
            "loop 2 image : step  21811\n",
            "loop 2 image : step  21812\n",
            "loop 2 image : step  21813\n",
            "loop 2 image : step  21814\n",
            "loop 2 image : step  21815\n",
            "loop 2 image : step  21816\n",
            "loop 2 image : step  21817\n",
            "loop 2 image : step  21818\n",
            "loop 2 image : step  21819\n",
            "loop 2 image : step  21820\n",
            "loop 2 image : step  21821\n",
            "loop 2 image : step  21822\n",
            "loop 2 image : step  21823\n",
            "loop 2 image : step  21824\n",
            "loop 2 image : step  21825\n",
            "loop 2 image : step  21826\n",
            "loop 2 image : step  21827\n",
            "loop 2 image : step  21828\n",
            "loop 2 image : step  21829\n",
            "loop 2 image : step  21830\n",
            "loop 2 image : step  21831\n",
            "loop 2 image : step  21832\n",
            "loop 2 image : step  21833\n",
            "loop 2 image : step  21834\n",
            "loop 2 image : step  21835\n",
            "loop 2 image : step  21836\n",
            "loop 2 image : step  21837\n",
            "loop 2 image : step  21838\n",
            "loop 2 image : step  21839\n",
            "loop 2 image : step  21840\n",
            "loop 2 image : step  21841\n",
            "loop 2 image : step  21842\n",
            "loop 2 image : step  21843\n",
            "loop 2 image : step  21844\n",
            "loop 2 image : step  21845\n",
            "loop 2 image : step  21846\n",
            "loop 2 image : step  21847\n",
            "loop 2 image : step  21848\n",
            "loop 2 image : step  21849\n",
            "loop 2 image : step  21850\n",
            "loop 2 image : step  21851\n",
            "loop 2 image : step  21852\n",
            "loop 2 image : step  21853\n",
            "loop 2 image : step  21854\n",
            "loop 2 image : step  21855\n",
            "loop 2 image : step  21856\n",
            "loop 2 image : step  21857\n",
            "loop 2 image : step  21858\n",
            "loop 2 image : step  21859\n",
            "loop 2 image : step  21860\n",
            "loop 2 image : step  21861\n",
            "loop 2 image : step  21862\n",
            "loop 2 image : step  21863\n",
            "loop 2 image : step  21864\n",
            "loop 2 image : step  21865\n",
            "loop 2 image : step  21866\n",
            "loop 2 image : step  21867\n",
            "loop 2 image : step  21868\n",
            "loop 2 image : step  21869\n",
            "loop 2 image : step  21870\n",
            "loop 2 image : step  21871\n",
            "loop 2 image : step  21872\n",
            "loop 2 image : step  21873\n",
            "loop 2 image : step  21874\n",
            "loop 2 image : step  21875\n",
            "loop 2 image : step  21876\n",
            "loop 2 image : step  21877\n",
            "loop 2 image : step  21878\n",
            "loop 2 image : step  21879\n",
            "loop 2 image : step  21880\n",
            "loop 2 image : step  21881\n",
            "loop 2 image : step  21882\n",
            "loop 2 image : step  21883\n",
            "loop 2 image : step  21884\n",
            "loop 2 image : step  21885\n",
            "loop 2 image : step  21886\n",
            "loop 2 image : step  21887\n",
            "loop 2 image : step  21888\n",
            "loop 2 image : step  21889\n",
            "loop 2 image : step  21890\n",
            "loop 2 image : step  21891\n",
            "loop 2 image : step  21892\n",
            "loop 2 image : step  21893\n",
            "loop 2 image : step  21894\n",
            "loop 2 image : step  21895\n",
            "loop 2 image : step  21896\n",
            "loop 2 image : step  21897\n",
            "loop 2 image : step  21898\n",
            "loop 2 image : step  21899\n",
            "loop 2 image : step  21900\n",
            "loop 2 image : step  21901\n",
            "loop 2 image : step  21902\n",
            "loop 2 image : step  21903\n",
            "loop 2 image : step  21904\n",
            "loop 2 image : step  21905\n",
            "loop 2 image : step  21906\n",
            "loop 2 image : step  21907\n",
            "loop 2 image : step  21908\n",
            "loop 2 image : step  21909\n",
            "loop 2 image : step  21910\n",
            "loop 2 image : step  21911\n",
            "loop 2 image : step  21912\n",
            "loop 2 image : step  21913\n",
            "loop 2 image : step  21914\n",
            "loop 2 image : step  21915\n",
            "loop 2 image : step  21916\n",
            "loop 2 image : step  21917\n",
            "loop 2 image : step  21918\n",
            "loop 2 image : step  21919\n",
            "loop 2 image : step  21920\n",
            "loop 2 image : step  21921\n",
            "loop 2 image : step  21922\n",
            "loop 2 image : step  21923\n",
            "loop 2 image : step  21924\n",
            "loop 2 image : step  21925\n",
            "loop 2 image : step  21926\n",
            "loop 2 image : step  21927\n",
            "loop 2 image : step  21928\n",
            "loop 2 image : step  21929\n",
            "loop 2 image : step  21930\n",
            "loop 2 image : step  21931\n",
            "loop 2 image : step  21932\n",
            "loop 2 image : step  21933\n",
            "loop 2 image : step  21934\n",
            "loop 2 image : step  21935\n",
            "loop 2 image : step  21936\n",
            "loop 2 image : step  21937\n",
            "loop 2 image : step  21938\n",
            "loop 2 image : step  21939\n",
            "loop 2 image : step  21940\n",
            "loop 2 image : step  21941\n",
            "loop 2 image : step  21942\n",
            "loop 2 image : step  21943\n",
            "loop 2 image : step  21944\n",
            "loop 2 image : step  21945\n",
            "loop 2 image : step  21946\n",
            "loop 2 image : step  21947\n",
            "loop 2 image : step  21948\n",
            "loop 2 image : step  21949\n",
            "loop 2 image : step  21950\n",
            "loop 2 image : step  21951\n",
            "loop 2 image : step  21952\n",
            "loop 2 image : step  21953\n",
            "loop 2 image : step  21954\n",
            "loop 2 image : step  21955\n",
            "loop 2 image : step  21956\n",
            "loop 2 image : step  21957\n",
            "loop 2 image : step  21958\n",
            "loop 2 image : step  21959\n",
            "loop 2 image : step  21960\n",
            "loop 2 image : step  21961\n",
            "loop 2 image : step  21962\n",
            "loop 2 image : step  21963\n",
            "loop 2 image : step  21964\n",
            "loop 2 image : step  21965\n",
            "loop 2 image : step  21966\n",
            "loop 2 image : step  21967\n",
            "loop 2 image : step  21968\n",
            "loop 2 image : step  21969\n",
            "loop 2 image : step  21970\n",
            "loop 2 image : step  21971\n",
            "loop 2 image : step  21972\n",
            "loop 2 image : step  21973\n",
            "loop 2 image : step  21974\n",
            "loop 2 image : step  21975\n",
            "loop 2 image : step  21976\n",
            "loop 2 image : step  21977\n",
            "loop 2 image : step  21978\n",
            "loop 2 image : step  21979\n",
            "loop 2 image : step  21980\n",
            "loop 2 image : step  21981\n",
            "loop 2 image : step  21982\n",
            "loop 2 image : step  21983\n",
            "loop 2 image : step  21984\n",
            "loop 2 image : step  21985\n",
            "loop 2 image : step  21986\n",
            "loop 2 image : step  21987\n",
            "loop 2 image : step  21988\n",
            "loop 2 image : step  21989\n",
            "loop 2 image : step  21990\n",
            "loop 2 image : step  21991\n",
            "loop 2 image : step  21992\n",
            "loop 2 image : step  21993\n",
            "loop 2 image : step  21994\n",
            "loop 2 image : step  21995\n",
            "loop 2 image : step  21996\n",
            "loop 2 image : step  21997\n",
            "loop 2 image : step  21998\n",
            "loop 2 image : step  21999\n",
            "loop 2 image : step  22000\n",
            "loop 2 image : step  22001\n",
            "loop 2 image : step  22002\n",
            "loop 2 image : step  22003\n",
            "loop 2 image : step  22004\n",
            "loop 2 image : step  22005\n",
            "loop 2 image : step  22006\n",
            "loop 2 image : step  22007\n",
            "loop 2 image : step  22008\n",
            "loop 2 image : step  22009\n",
            "loop 2 image : step  22010\n",
            "loop 2 image : step  22011\n",
            "loop 2 image : step  22012\n",
            "loop 2 image : step  22013\n",
            "loop 2 image : step  22014\n",
            "loop 2 image : step  22015\n",
            "loop 2 image : step  22016\n",
            "loop 2 image : step  22017\n",
            "loop 2 image : step  22018\n",
            "loop 2 image : step  22019\n",
            "loop 2 image : step  22020\n",
            "loop 2 image : step  22021\n",
            "loop 2 image : step  22022\n",
            "loop 2 image : step  22023\n",
            "loop 2 image : step  22024\n",
            "loop 2 image : step  22025\n",
            "loop 2 image : step  22026\n",
            "loop 2 image : step  22027\n",
            "loop 2 image : step  22028\n",
            "loop 2 image : step  22029\n",
            "loop 2 image : step  22030\n",
            "loop 2 image : step  22031\n",
            "loop 2 image : step  22032\n",
            "loop 2 image : step  22033\n",
            "loop 2 image : step  22034\n",
            "loop 2 image : step  22035\n",
            "loop 2 image : step  22036\n",
            "loop 2 image : step  22037\n",
            "loop 2 image : step  22038\n",
            "loop 2 image : step  22039\n",
            "loop 2 image : step  22040\n",
            "loop 2 image : step  22041\n",
            "loop 2 image : step  22042\n",
            "loop 2 image : step  22043\n",
            "loop 2 image : step  22044\n",
            "loop 2 image : step  22045\n",
            "loop 2 image : step  22046\n",
            "loop 2 image : step  22047\n",
            "loop 2 image : step  22048\n",
            "loop 2 image : step  22049\n",
            "loop 2 image : step  22050\n",
            "loop 2 image : step  22051\n",
            "loop 2 image : step  22052\n",
            "loop 2 image : step  22053\n",
            "loop 2 image : step  22054\n",
            "loop 2 image : step  22055\n",
            "loop 2 image : step  22056\n",
            "loop 2 image : step  22057\n",
            "loop 2 image : step  22058\n",
            "loop 2 image : step  22059\n",
            "loop 2 image : step  22060\n",
            "loop 2 image : step  22061\n",
            "loop 2 image : step  22062\n",
            "loop 2 image : step  22063\n",
            "loop 2 image : step  22064\n",
            "loop 2 image : step  22065\n",
            "loop 2 image : step  22066\n",
            "loop 2 image : step  22067\n",
            "loop 2 image : step  22068\n",
            "loop 2 image : step  22069\n",
            "loop 2 image : step  22070\n",
            "loop 2 image : step  22071\n",
            "loop 2 image : step  22072\n",
            "loop 2 image : step  22073\n",
            "loop 2 image : step  22074\n",
            "loop 2 image : step  22075\n",
            "loop 2 image : step  22076\n",
            "loop 2 image : step  22077\n",
            "loop 2 image : step  22078\n",
            "loop 2 image : step  22079\n",
            "loop 2 image : step  22080\n",
            "loop 2 image : step  22081\n",
            "loop 2 image : step  22082\n",
            "loop 2 image : step  22083\n",
            "loop 2 image : step  22084\n",
            "loop 2 image : step  22085\n",
            "loop 2 image : step  22086\n",
            "loop 2 image : step  22087\n",
            "loop 2 image : step  22088\n",
            "loop 2 image : step  22089\n",
            "loop 2 image : step  22090\n",
            "loop 2 image : step  22091\n",
            "loop 2 image : step  22092\n",
            "loop 2 image : step  22093\n",
            "loop 2 image : step  22094\n",
            "loop 2 image : step  22095\n",
            "loop 2 image : step  22096\n",
            "loop 2 image : step  22097\n",
            "loop 2 image : step  22098\n",
            "loop 2 image : step  22099\n",
            "loop 2 image : step  22100\n",
            "loop 2 image : step  22101\n",
            "loop 2 image : step  22102\n",
            "loop 2 image : step  22103\n",
            "loop 2 image : step  22104\n",
            "loop 2 image : step  22105\n",
            "loop 2 image : step  22106\n",
            "loop 2 image : step  22107\n",
            "loop 2 image : step  22108\n",
            "loop 2 image : step  22109\n",
            "loop 2 image : step  22110\n",
            "loop 2 image : step  22111\n",
            "loop 2 image : step  22112\n",
            "loop 2 image : step  22113\n",
            "loop 2 image : step  22114\n",
            "loop 2 image : step  22115\n",
            "loop 2 image : step  22116\n",
            "loop 2 image : step  22117\n",
            "loop 2 image : step  22118\n",
            "loop 2 image : step  22119\n",
            "loop 2 image : step  22120\n",
            "loop 2 image : step  22121\n",
            "loop 2 image : step  22122\n",
            "loop 2 image : step  22123\n",
            "loop 2 image : step  22124\n",
            "loop 2 image : step  22125\n",
            "loop 2 image : step  22126\n",
            "loop 2 image : step  22127\n",
            "loop 2 image : step  22128\n",
            "loop 2 image : step  22129\n",
            "loop 2 image : step  22130\n",
            "loop 2 image : step  22131\n",
            "loop 2 image : step  22132\n",
            "loop 2 image : step  22133\n",
            "loop 2 image : step  22134\n",
            "loop 2 image : step  22135\n",
            "loop 2 image : step  22136\n",
            "loop 2 image : step  22137\n",
            "loop 2 image : step  22138\n",
            "loop 2 image : step  22139\n",
            "loop 2 image : step  22140\n",
            "loop 2 image : step  22141\n",
            "loop 2 image : step  22142\n",
            "loop 2 image : step  22143\n",
            "loop 2 image : step  22144\n",
            "loop 2 image : step  22145\n",
            "loop 2 image : step  22146\n",
            "loop 2 image : step  22147\n",
            "loop 2 image : step  22148\n",
            "loop 2 image : step  22149\n",
            "loop 2 image : step  22150\n",
            "loop 2 image : step  22151\n",
            "loop 2 image : step  22152\n",
            "loop 2 image : step  22153\n",
            "loop 2 image : step  22154\n",
            "loop 2 image : step  22155\n",
            "loop 2 image : step  22156\n",
            "loop 2 image : step  22157\n",
            "loop 2 image : step  22158\n",
            "loop 2 image : step  22159\n",
            "loop 2 image : step  22160\n",
            "loop 2 image : step  22161\n",
            "loop 2 image : step  22162\n",
            "loop 2 image : step  22163\n",
            "loop 2 image : step  22164\n",
            "loop 2 image : step  22165\n",
            "loop 2 image : step  22166\n",
            "loop 2 image : step  22167\n",
            "loop 2 image : step  22168\n",
            "loop 2 image : step  22169\n",
            "loop 2 image : step  22170\n",
            "loop 2 image : step  22171\n",
            "loop 2 image : step  22172\n",
            "loop 2 image : step  22173\n",
            "loop 2 image : step  22174\n",
            "loop 2 image : step  22175\n",
            "loop 2 image : step  22176\n",
            "loop 2 image : step  22177\n",
            "loop 2 image : step  22178\n",
            "loop 2 image : step  22179\n",
            "loop 2 image : step  22180\n",
            "loop 2 image : step  22181\n",
            "loop 2 image : step  22182\n",
            "loop 2 image : step  22183\n",
            "loop 2 image : step  22184\n",
            "loop 2 image : step  22185\n",
            "loop 2 image : step  22186\n",
            "loop 2 image : step  22187\n",
            "loop 2 image : step  22188\n",
            "loop 2 image : step  22189\n",
            "loop 2 image : step  22190\n",
            "loop 2 image : step  22191\n",
            "loop 2 image : step  22192\n",
            "loop 2 image : step  22193\n",
            "loop 2 image : step  22194\n",
            "loop 2 image : step  22195\n",
            "loop 2 image : step  22196\n",
            "loop 2 image : step  22197\n",
            "loop 2 image : step  22198\n",
            "loop 2 image : step  22199\n",
            "loop 2 image : step  22200\n",
            "loop 2 image : step  22201\n",
            "loop 2 image : step  22202\n",
            "loop 2 image : step  22203\n",
            "loop 2 image : step  22204\n",
            "loop 2 image : step  22205\n",
            "loop 2 image : step  22206\n",
            "loop 2 image : step  22207\n",
            "loop 2 image : step  22208\n",
            "loop 2 image : step  22209\n",
            "loop 2 image : step  22210\n",
            "loop 2 image : step  22211\n",
            "loop 2 image : step  22212\n",
            "loop 2 image : step  22213\n",
            "loop 2 image : step  22214\n",
            "loop 2 image : step  22215\n",
            "loop 2 image : step  22216\n",
            "loop 2 image : step  22217\n",
            "loop 2 image : step  22218\n",
            "loop 2 image : step  22219\n",
            "loop 2 image : step  22220\n",
            "loop 2 image : step  22221\n",
            "loop 2 image : step  22222\n",
            "loop 2 image : step  22223\n",
            "loop 2 image : step  22224\n",
            "loop 2 image : step  22225\n",
            "loop 2 image : step  22226\n",
            "loop 2 image : step  22227\n",
            "loop 2 image : step  22228\n",
            "loop 2 image : step  22229\n",
            "loop 2 image : step  22230\n",
            "loop 2 image : step  22231\n",
            "loop 2 image : step  22232\n",
            "loop 2 image : step  22233\n",
            "loop 2 image : step  22234\n",
            "loop 2 image : step  22235\n",
            "loop 2 image : step  22236\n",
            "loop 2 image : step  22237\n",
            "loop 2 image : step  22238\n",
            "loop 2 image : step  22239\n",
            "loop 2 image : step  22240\n",
            "loop 2 image : step  22241\n",
            "loop 2 image : step  22242\n",
            "loop 2 image : step  22243\n",
            "loop 2 image : step  22244\n",
            "loop 2 image : step  22245\n",
            "loop 2 image : step  22246\n",
            "loop 2 image : step  22247\n",
            "loop 2 image : step  22248\n",
            "loop 2 image : step  22249\n",
            "loop 2 image : step  22250\n",
            "loop 2 image : step  22251\n",
            "loop 2 image : step  22252\n",
            "loop 2 image : step  22253\n",
            "loop 2 image : step  22254\n",
            "loop 2 image : step  22255\n",
            "loop 2 image : step  22256\n",
            "loop 2 image : step  22257\n",
            "loop 2 image : step  22258\n",
            "loop 2 image : step  22259\n",
            "loop 2 image : step  22260\n",
            "loop 2 image : step  22261\n",
            "loop 2 image : step  22262\n",
            "loop 2 image : step  22263\n",
            "loop 2 image : step  22264\n",
            "loop 2 image : step  22265\n",
            "loop 2 image : step  22266\n",
            "loop 2 image : step  22267\n",
            "loop 2 image : step  22268\n",
            "loop 2 image : step  22269\n",
            "loop 2 image : step  22270\n",
            "loop 2 image : step  22271\n",
            "loop 2 image : step  22272\n",
            "loop 2 image : step  22273\n",
            "loop 2 image : step  22274\n",
            "loop 2 image : step  22275\n",
            "loop 2 image : step  22276\n",
            "loop 2 image : step  22277\n",
            "loop 2 image : step  22278\n",
            "loop 2 image : step  22279\n",
            "loop 2 image : step  22280\n",
            "loop 2 image : step  22281\n",
            "loop 2 image : step  22282\n",
            "loop 2 image : step  22283\n",
            "loop 2 image : step  22284\n",
            "loop 2 image : step  22285\n",
            "loop 2 image : step  22286\n",
            "loop 2 image : step  22287\n",
            "loop 2 image : step  22288\n",
            "loop 2 image : step  22289\n",
            "loop 2 image : step  22290\n",
            "loop 2 image : step  22291\n",
            "loop 2 image : step  22292\n",
            "loop 2 image : step  22293\n",
            "loop 2 image : step  22294\n",
            "loop 2 image : step  22295\n",
            "loop 2 image : step  22296\n",
            "loop 2 image : step  22297\n",
            "loop 2 image : step  22298\n",
            "loop 2 image : step  22299\n",
            "loop 2 image : step  22300\n",
            "loop 2 image : step  22301\n",
            "loop 2 image : step  22302\n",
            "loop 2 image : step  22303\n",
            "loop 2 image : step  22304\n",
            "loop 2 image : step  22305\n",
            "loop 2 image : step  22306\n",
            "loop 2 image : step  22307\n",
            "loop 2 image : step  22308\n",
            "loop 2 image : step  22309\n",
            "loop 2 image : step  22310\n",
            "loop 2 image : step  22311\n",
            "loop 2 image : step  22312\n",
            "loop 2 image : step  22313\n",
            "loop 2 image : step  22314\n",
            "loop 2 image : step  22315\n",
            "loop 2 image : step  22316\n",
            "loop 2 image : step  22317\n",
            "loop 2 image : step  22318\n",
            "loop 2 image : step  22319\n",
            "loop 2 image : step  22320\n",
            "loop 2 image : step  22321\n",
            "loop 2 image : step  22322\n",
            "loop 2 image : step  22323\n",
            "loop 2 image : step  22324\n",
            "loop 2 image : step  22325\n",
            "loop 2 image : step  22326\n",
            "loop 2 image : step  22327\n",
            "loop 2 image : step  22328\n",
            "loop 2 image : step  22329\n",
            "loop 2 image : step  22330\n",
            "loop 2 image : step  22331\n",
            "loop 2 image : step  22332\n",
            "loop 2 image : step  22333\n",
            "loop 2 image : step  22334\n",
            "loop 2 image : step  22335\n",
            "loop 2 image : step  22336\n",
            "loop 2 image : step  22337\n",
            "loop 2 image : step  22338\n",
            "loop 2 image : step  22339\n",
            "loop 2 image : step  22340\n",
            "loop 2 image : step  22341\n",
            "loop 2 image : step  22342\n",
            "loop 2 image : step  22343\n",
            "loop 2 image : step  22344\n",
            "loop 2 image : step  22345\n",
            "loop 2 image : step  22346\n",
            "loop 2 image : step  22347\n",
            "loop 2 image : step  22348\n",
            "loop 2 image : step  22349\n",
            "loop 2 image : step  22350\n",
            "loop 2 image : step  22351\n",
            "loop 2 image : step  22352\n",
            "loop 2 image : step  22353\n",
            "loop 2 image : step  22354\n",
            "loop 2 image : step  22355\n",
            "loop 2 image : step  22356\n",
            "loop 2 image : step  22357\n",
            "loop 2 image : step  22358\n",
            "loop 2 image : step  22359\n",
            "loop 2 image : step  22360\n",
            "loop 2 image : step  22361\n",
            "loop 2 image : step  22362\n",
            "loop 2 image : step  22363\n",
            "loop 2 image : step  22364\n",
            "loop 2 image : step  22365\n",
            "loop 2 image : step  22366\n",
            "loop 2 image : step  22367\n",
            "loop 2 image : step  22368\n",
            "loop 2 image : step  22369\n",
            "loop 2 image : step  22370\n",
            "loop 2 image : step  22371\n",
            "loop 2 image : step  22372\n",
            "loop 2 image : step  22373\n",
            "loop 2 image : step  22374\n",
            "loop 2 image : step  22375\n",
            "loop 2 image : step  22376\n",
            "loop 2 image : step  22377\n",
            "loop 2 image : step  22378\n",
            "loop 2 image : step  22379\n",
            "loop 2 image : step  22380\n",
            "loop 2 image : step  22381\n",
            "loop 2 image : step  22382\n",
            "loop 2 image : step  22383\n",
            "loop 2 image : step  22384\n",
            "loop 2 image : step  22385\n",
            "loop 2 image : step  22386\n",
            "loop 2 image : step  22387\n",
            "loop 2 image : step  22388\n",
            "loop 2 image : step  22389\n",
            "loop 2 image : step  22390\n",
            "loop 2 image : step  22391\n",
            "loop 2 image : step  22392\n",
            "loop 2 image : step  22393\n",
            "loop 2 image : step  22394\n",
            "loop 2 image : step  22395\n",
            "loop 2 image : step  22396\n",
            "loop 2 image : step  22397\n",
            "loop 2 image : step  22398\n",
            "loop 2 image : step  22399\n",
            "loop 2 image : step  22400\n",
            "loop 2 image : step  22401\n",
            "loop 2 image : step  22402\n",
            "loop 2 image : step  22403\n",
            "loop 2 image : step  22404\n",
            "loop 2 image : step  22405\n",
            "loop 2 image : step  22406\n",
            "loop 2 image : step  22407\n",
            "loop 2 image : step  22408\n",
            "loop 2 image : step  22409\n",
            "loop 2 image : step  22410\n",
            "loop 2 image : step  22411\n",
            "loop 2 image : step  22412\n",
            "loop 2 image : step  22413\n",
            "loop 2 image : step  22414\n",
            "loop 2 image : step  22415\n",
            "loop 2 image : step  22416\n",
            "loop 2 image : step  22417\n",
            "loop 2 image : step  22418\n",
            "loop 2 image : step  22419\n",
            "loop 2 image : step  22420\n",
            "loop 2 image : step  22421\n",
            "loop 2 image : step  22422\n",
            "loop 2 image : step  22423\n",
            "loop 2 image : step  22424\n",
            "loop 2 image : step  22425\n",
            "loop 2 image : step  22426\n",
            "loop 2 image : step  22427\n",
            "loop 2 image : step  22428\n",
            "loop 2 image : step  22429\n",
            "loop 2 image : step  22430\n",
            "loop 2 image : step  22431\n",
            "loop 2 image : step  22432\n",
            "loop 2 image : step  22433\n",
            "loop 2 image : step  22434\n",
            "loop 2 image : step  22435\n",
            "loop 2 image : step  22436\n",
            "loop 2 image : step  22437\n",
            "loop 2 image : step  22438\n",
            "loop 2 image : step  22439\n",
            "loop 2 image : step  22440\n",
            "loop 2 image : step  22441\n",
            "loop 2 image : step  22442\n",
            "loop 2 image : step  22443\n",
            "loop 2 image : step  22444\n",
            "loop 2 image : step  22445\n",
            "loop 2 image : step  22446\n",
            "loop 2 image : step  22447\n",
            "loop 2 image : step  22448\n",
            "loop 2 image : step  22449\n",
            "loop 2 image : step  22450\n",
            "loop 2 image : step  22451\n",
            "loop 2 image : step  22452\n",
            "loop 2 image : step  22453\n",
            "loop 2 image : step  22454\n",
            "loop 2 image : step  22455\n",
            "loop 2 image : step  22456\n",
            "loop 2 image : step  22457\n",
            "loop 2 image : step  22458\n",
            "loop 2 image : step  22459\n",
            "loop 2 image : step  22460\n",
            "loop 2 image : step  22461\n",
            "loop 2 image : step  22462\n",
            "loop 2 image : step  22463\n",
            "loop 2 image : step  22464\n",
            "loop 2 image : step  22465\n",
            "loop 2 image : step  22466\n",
            "loop 2 image : step  22467\n",
            "loop 2 image : step  22468\n",
            "loop 2 image : step  22469\n",
            "loop 2 image : step  22470\n",
            "loop 2 image : step  22471\n",
            "loop 2 image : step  22472\n",
            "loop 2 image : step  22473\n",
            "loop 2 image : step  22474\n",
            "loop 2 image : step  22475\n",
            "loop 2 image : step  22476\n",
            "loop 2 image : step  22477\n",
            "loop 2 image : step  22478\n",
            "loop 2 image : step  22479\n",
            "loop 2 image : step  22480\n",
            "loop 2 image : step  22481\n",
            "loop 2 image : step  22482\n",
            "loop 2 image : step  22483\n",
            "loop 2 image : step  22484\n",
            "loop 2 image : step  22485\n",
            "loop 2 image : step  22486\n",
            "loop 2 image : step  22487\n",
            "loop 2 image : step  22488\n",
            "loop 2 image : step  22489\n",
            "loop 2 image : step  22490\n",
            "loop 2 image : step  22491\n",
            "loop 2 image : step  22492\n",
            "loop 2 image : step  22493\n",
            "loop 2 image : step  22494\n",
            "loop 2 image : step  22495\n",
            "loop 2 image : step  22496\n",
            "loop 2 image : step  22497\n",
            "loop 2 image : step  22498\n",
            "loop 2 image : step  22499\n",
            "loop 2 image : step  22500\n",
            "loop 2 image : step  22501\n",
            "loop 2 image : step  22502\n",
            "loop 2 image : step  22503\n",
            "loop 2 image : step  22504\n",
            "loop 2 image : step  22505\n",
            "loop 2 image : step  22506\n",
            "loop 2 image : step  22507\n",
            "loop 2 image : step  22508\n",
            "loop 2 image : step  22509\n",
            "loop 2 image : step  22510\n",
            "loop 2 image : step  22511\n",
            "loop 2 image : step  22512\n",
            "loop 2 image : step  22513\n",
            "loop 2 image : step  22514\n",
            "loop 2 image : step  22515\n",
            "loop 2 image : step  22516\n",
            "loop 2 image : step  22517\n",
            "loop 2 image : step  22518\n",
            "loop 2 image : step  22519\n",
            "loop 2 image : step  22520\n",
            "loop 2 image : step  22521\n",
            "loop 2 image : step  22522\n",
            "loop 2 image : step  22523\n",
            "loop 2 image : step  22524\n",
            "loop 2 image : step  22525\n",
            "loop 2 image : step  22526\n",
            "loop 2 image : step  22527\n",
            "loop 2 image : step  22528\n",
            "loop 2 image : step  22529\n",
            "loop 2 image : step  22530\n",
            "loop 2 image : step  22531\n",
            "loop 2 image : step  22532\n",
            "loop 2 image : step  22533\n",
            "loop 2 image : step  22534\n",
            "loop 2 image : step  22535\n",
            "loop 2 image : step  22536\n",
            "loop 2 image : step  22537\n",
            "loop 2 image : step  22538\n",
            "loop 2 image : step  22539\n",
            "loop 2 image : step  22540\n",
            "loop 2 image : step  22541\n",
            "loop 2 image : step  22542\n",
            "loop 2 image : step  22543\n",
            "loop 2 image : step  22544\n",
            "loop 2 image : step  22545\n",
            "loop 2 image : step  22546\n",
            "loop 2 image : step  22547\n",
            "loop 2 image : step  22548\n",
            "loop 2 image : step  22549\n",
            "loop 2 image : step  22550\n",
            "loop 2 image : step  22551\n",
            "loop 2 image : step  22552\n",
            "loop 2 image : step  22553\n",
            "loop 2 image : step  22554\n",
            "loop 2 image : step  22555\n",
            "loop 2 image : step  22556\n",
            "loop 2 image : step  22557\n",
            "loop 2 image : step  22558\n",
            "loop 2 image : step  22559\n",
            "loop 2 image : step  22560\n",
            "loop 2 image : step  22561\n",
            "loop 2 image : step  22562\n",
            "loop 2 image : step  22563\n",
            "loop 2 image : step  22564\n",
            "loop 2 image : step  22565\n",
            "loop 2 image : step  22566\n",
            "loop 2 image : step  22567\n",
            "loop 2 image : step  22568\n",
            "loop 2 image : step  22569\n",
            "loop 2 image : step  22570\n",
            "loop 2 image : step  22571\n",
            "loop 2 image : step  22572\n",
            "loop 2 image : step  22573\n",
            "loop 2 image : step  22574\n",
            "loop 2 image : step  22575\n",
            "loop 2 image : step  22576\n",
            "loop 2 image : step  22577\n",
            "loop 2 image : step  22578\n",
            "loop 2 image : step  22579\n",
            "loop 2 image : step  22580\n",
            "loop 2 image : step  22581\n",
            "loop 2 image : step  22582\n",
            "loop 2 image : step  22583\n",
            "loop 2 image : step  22584\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bUtiN9yGtBNY",
        "outputId": "49ae7318-8895-4ced-de5a-2b4f3ecabff5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        }
      },
      "source": [
        "#classification of the states to get a equally distributed dataset\n",
        "y_StateClass_image=Y_test_StateClass_image\n",
        "x_image=X_test_image\n",
        "y_futurepredict_image=Y_test_FutPredict_image\n",
        "#group by y state the x_image \n",
        "#count the min of the of each state \n",
        "#construct a directory for each block like cat, dog etc\n",
        "\n",
        "#we need dataset of train, validation, test \n",
        "\n",
        "y_StateClass_image.describe()\n",
        "y_StateClass_image.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-cef23a790903>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#classification of the states to get a equally distributed dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0my_StateClass_image\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mY_test_StateClass_image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mx_image\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX_test_image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0my_futurepredict_image\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mY_test_FutPredict_image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#group by y state the x_image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'Y_test_StateClass_image' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wVaekoeJwOK_",
        "outputId": "9d3622f6-acff-4df0-f2c5-8770b509e0a2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "non_monotonic_index =pd.Index(list(y_StateClass_image))\n",
        "\n",
        "def localize_index_from_state(non_monotonic_index, state=0):\n",
        "  state_loc=non_monotonic_index.get_loc(state)\n",
        "  return [i for i in range(0,state_loc.size) if state_loc[i]]\n",
        "\n",
        "try : \n",
        "  state_error_loc=localize_index_from_state(non_monotonic_index,-1) \n",
        "  y_StateClass_image_error =y_StateClass_image.iloc[state_error_loc]\n",
        "  x_image_State_is_error =x_image.iloc[state_error_loc].head()\n",
        "except :\n",
        "  print(\"No value for error state\")\n",
        "\n",
        "state_zero_loc=localize_index_from_state(non_monotonic_index, 0)\n",
        "state_one_loc=localize_index_from_state(non_monotonic_index, 1)\n",
        "state_two_loc=localize_index_from_state(non_monotonic_index, 2)\n",
        "state_three_loc=localize_index_from_state(non_monotonic_index, 3)\n",
        "state_four_loc=localize_index_from_state(non_monotonic_index, 4)\n",
        "\n",
        "#Build up class for the dataset\n",
        "y_StateClass_image_0 =y_StateClass_image.iloc[state_zero_loc]\n",
        "y_StateClass_image_1 =y_StateClass_image.iloc[state_one_loc]\n",
        "y_StateClass_image_2 =y_StateClass_image.iloc[state_two_loc]\n",
        "y_StateClass_image_3 =y_StateClass_image.iloc[state_three_loc]\n",
        "y_StateClass_image_4 =y_StateClass_image.iloc[state_four_loc]\n",
        "\n",
        "x_image_State_is_0 =x_image.iloc[state_zero_loc]\n",
        "x_image_State_is_1 =x_image.iloc[state_one_loc]\n",
        "x_image_State_is_2 =x_image.iloc[state_two_loc]\n",
        "x_image_State_is_3 =x_image.iloc[state_three_loc]\n",
        "x_image_State_is_4 =x_image.iloc[state_four_loc]\n",
        "\n",
        "y_futpredict_image_0 =y_futurepredict_image.iloc[state_zero_loc]\n",
        "y_futpredict_image_1 =y_futurepredict_image.iloc[state_one_loc]\n",
        "y_futpredict_image_2 =y_futurepredict_image.iloc[state_two_loc]\n",
        "y_futpredict_image_3 =y_futurepredict_image.iloc[state_three_loc]\n",
        "y_futpredict_image_4 =y_futurepredict_image.iloc[state_four_loc]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "No value for error state\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cHV8BR52UXyw",
        "outputId": "dc194b2f-44be-4b3e-dc0a-8056ccb6c8f8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "print(\"dataset class 0 size is :\",y_StateClass_image_0.size, \"and for x \", x_image_State_is_0.index.size)\n",
        "print(\"dataset class 1 size is :\",y_StateClass_image_1.size, \"and for x \", x_image_State_is_1.index.size)\n",
        "print(\"dataset class 2 size is :\",y_StateClass_image_2.size, \"and for x \", x_image_State_is_2.index.size)\n",
        "print(\"dataset class 3 size is :\",y_StateClass_image_3.size, \"and for x \", x_image_State_is_3.index.size)\n",
        "print(\"dataset class 4 size is :\",y_StateClass_image_4.size, \"and for x \", x_image_State_is_4.index.size)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dataset class 0 size is : 195 and for x  195\n",
            "dataset class 1 size is : 205 and for x  205\n",
            "dataset class 2 size is : 760 and for x  760\n",
            "dataset class 3 size is : 328 and for x  328\n",
            "dataset class 4 size is : 759 and for x  759\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_B0t9M5EBD3-"
      },
      "source": [
        "####test write"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Akj--T1pU3jp",
        "outputId": "266fef4a-3a81-452e-910a-0248e59ebfba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        }
      },
      "source": [
        "%cd /content/drive/My Drive/A_transfertTFM\n",
        "X_train_image.to_csv('datas/X_train_image4.csv')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/A_transfertTFM\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-e0b895a51161>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmagic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cd /content/drive/My Drive/A_transfertTFM'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mX_train_image\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'datas/X_train_image4.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'X_train_image' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m1M6bviBZ2XR",
        "outputId": "68027ed9-2f4d-4d1b-98ef-ad4e7a1a9a78",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "write_path='datas/state_is_'\n",
        "write_path+str(2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'datas/state_is_2'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dh6EqvRiBH5K"
      },
      "source": [
        "####end test write"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yFniKbGsp5Bh",
        "outputId": "259993bd-ce12-4189-cea4-d35f861a1eee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%cd /content/drive/My Drive/A_transfertTFM\n",
        "def print_data_class(state=0,write_path='datas/state_is_') :\n",
        "  state_zero_loc=localize_index_from_state(non_monotonic_index, state)\n",
        "  y_StateClass_image_0 =y_StateClass_image.iloc[state_zero_loc]\n",
        "  x_image_State_is_0 =x_image.iloc[state_zero_loc]\n",
        "  y_futpredict_image_0 =y_futurepredict_image.iloc[state_zero_loc]\n",
        "  y_StateClass_image_0.to_csv(write_path+str(state)+'/y_stateclass.csv')\n",
        "  x_image_State_is_0.to_csv(write_path+str(state)+'/x_image.csv')\n",
        "  y_futpredict_image_0.to_csv(write_path+str(state)+'/y_future.csv')\n",
        "\n",
        "print_data_class(state=0)\n",
        "print_data_class(state=1)\n",
        "print_data_class(state=2)\n",
        "print_data_class(state=3)\n",
        "print_data_class(state=4)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/A_transfertTFM\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bD676WJOcE1j"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "okVHT-6wvehF",
        "outputId": "93917f54-cbb4-4bd8-c1bd-d70a43b23f48",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 146
        }
      },
      "source": [
        "\n",
        "X_train_image.to_csv('datas/X_train_image4.csv')\n",
        "Y_train_StateClass_image.to_csv('datas/Y_train_StateClass_image4.csv')\n",
        "Y_train_FutPredict_image.to_csv('datas/Y_train_FutPredict_image4.csv')\n",
        "\n",
        "X_test_image.to_csv('datas/X_test_image4.csv')\n",
        "Y_test_StateClass_image.to_csv('datas/Y_test_StateClass_image4.csv')_\n",
        "Y_test_FutPredict_image.to_csv('datas/Y_test_FutPredict_image4.csv')X_train_image"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Image Col0</th>\n",
              "      <th>Image Col1</th>\n",
              "      <th>Image Col2</th>\n",
              "      <th>Image Col3</th>\n",
              "      <th>Image Col4</th>\n",
              "      <th>Image Col5</th>\n",
              "      <th>Image Col6</th>\n",
              "      <th>Image Col7</th>\n",
              "      <th>Image Col8</th>\n",
              "      <th>Image Col9</th>\n",
              "      <th>Image Col10</th>\n",
              "      <th>Image Col11</th>\n",
              "      <th>Image Col12</th>\n",
              "      <th>Image Col13</th>\n",
              "      <th>Image Col14</th>\n",
              "      <th>Image Col15</th>\n",
              "      <th>Image Col16</th>\n",
              "      <th>Image Col17</th>\n",
              "      <th>Image Col18</th>\n",
              "      <th>Image Col19</th>\n",
              "      <th>Image Col20</th>\n",
              "      <th>Image Col21</th>\n",
              "      <th>Image Col22</th>\n",
              "      <th>Image Col23</th>\n",
              "      <th>Image Col24</th>\n",
              "      <th>Image Col25</th>\n",
              "      <th>Image Col26</th>\n",
              "      <th>Image Col27</th>\n",
              "      <th>Image Col28</th>\n",
              "      <th>Image Col29</th>\n",
              "      <th>Image Col30</th>\n",
              "      <th>Image Col31</th>\n",
              "      <th>Image Col32</th>\n",
              "      <th>Image Col33</th>\n",
              "      <th>Image Col34</th>\n",
              "      <th>Image Col35</th>\n",
              "      <th>Image Col36</th>\n",
              "      <th>Image Col37</th>\n",
              "      <th>Image Col38</th>\n",
              "      <th>Image Col39</th>\n",
              "      <th>...</th>\n",
              "      <th>Image Col64985</th>\n",
              "      <th>Image Col64986</th>\n",
              "      <th>Image Col64987</th>\n",
              "      <th>Image Col64988</th>\n",
              "      <th>Image Col64989</th>\n",
              "      <th>Image Col64990</th>\n",
              "      <th>Image Col64991</th>\n",
              "      <th>Image Col64992</th>\n",
              "      <th>Image Col64993</th>\n",
              "      <th>Image Col64994</th>\n",
              "      <th>Image Col64995</th>\n",
              "      <th>Image Col64996</th>\n",
              "      <th>Image Col64997</th>\n",
              "      <th>Image Col64998</th>\n",
              "      <th>Image Col64999</th>\n",
              "      <th>Image Col65000</th>\n",
              "      <th>Image Col65001</th>\n",
              "      <th>Image Col65002</th>\n",
              "      <th>Image Col65003</th>\n",
              "      <th>Image Col65004</th>\n",
              "      <th>Image Col65005</th>\n",
              "      <th>Image Col65006</th>\n",
              "      <th>Image Col65007</th>\n",
              "      <th>Image Col65008</th>\n",
              "      <th>Image Col65009</th>\n",
              "      <th>Image Col65010</th>\n",
              "      <th>Image Col65011</th>\n",
              "      <th>Image Col65012</th>\n",
              "      <th>Image Col65013</th>\n",
              "      <th>Image Col65014</th>\n",
              "      <th>Image Col65015</th>\n",
              "      <th>Image Col65016</th>\n",
              "      <th>Image Col65017</th>\n",
              "      <th>Image Col65018</th>\n",
              "      <th>Image Col65019</th>\n",
              "      <th>Image Col65020</th>\n",
              "      <th>Image Col65021</th>\n",
              "      <th>Image Col65022</th>\n",
              "      <th>Image Col65023</th>\n",
              "      <th>Image Col65024</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Date</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>0 rows × 65025 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [Image Col0, Image Col1, Image Col2, Image Col3, Image Col4, Image Col5, Image Col6, Image Col7, Image Col8, Image Col9, Image Col10, Image Col11, Image Col12, Image Col13, Image Col14, Image Col15, Image Col16, Image Col17, Image Col18, Image Col19, Image Col20, Image Col21, Image Col22, Image Col23, Image Col24, Image Col25, Image Col26, Image Col27, Image Col28, Image Col29, Image Col30, Image Col31, Image Col32, Image Col33, Image Col34, Image Col35, Image Col36, Image Col37, Image Col38, Image Col39, Image Col40, Image Col41, Image Col42, Image Col43, Image Col44, Image Col45, Image Col46, Image Col47, Image Col48, Image Col49, Image Col50, Image Col51, Image Col52, Image Col53, Image Col54, Image Col55, Image Col56, Image Col57, Image Col58, Image Col59, Image Col60, Image Col61, Image Col62, Image Col63, Image Col64, Image Col65, Image Col66, Image Col67, Image Col68, Image Col69, Image Col70, Image Col71, Image Col72, Image Col73, Image Col74, Image Col75, Image Col76, Image Col77, Image Col78, Image Col79, Image Col80, Image Col81, Image Col82, Image Col83, Image Col84, Image Col85, Image Col86, Image Col87, Image Col88, Image Col89, Image Col90, Image Col91, Image Col92, Image Col93, Image Col94, Image Col95, Image Col96, Image Col97, Image Col98, Image Col99, ...]\n",
              "Index: []\n",
              "\n",
              "[0 rows x 65025 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V_QcOSjqMHSy"
      },
      "source": [
        "##Step 2: Loading training datas with vgg16 transfert model and training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R98_pBo02L2-"
      },
      "source": [
        "####List of model utilisation of main example resnet \n",
        "https://towardsdatascience.com/understanding-and-coding-a-resnet-in-keras-446d7ff84d33"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fI7gHLdU15Bb"
      },
      "source": [
        "\n",
        "![1_NdCntZms6S2pBmQ3j_wyuw.png](data:image/png;base64,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)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1R3sN7G1IWIK"
      },
      "source": [
        "####Loading and Cleaning Training Datas "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MTfVYoKBaQzZ"
      },
      "source": [
        "This part is for loading the training dataset as it is better to generate it once for all in step 1 because of it time consuming process.\n",
        "\n",
        "This part also configure back the X_train datas from dataframe based on columns to a (32,32,3) np. array for the input of the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zqU-LK4JTEOM",
        "outputId": "f4e7a4e8-e4fb-4dfd-f387-93a58ee1b1f2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "%pwd"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/drive/My Drive/A_transfertTFMresnetSP500'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jp0_855PpN3S",
        "outputId": "2ddcc167-0b3e-42db-b7a2-d34496a2fc31",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "%cd /content/DL_Tools_For_Finance\n",
        "\n",
        "import numpy as np\n",
        "import pylab as plt\n",
        "import pandas as pd\n",
        "\n",
        "from keras.datasets import cifar10\n",
        "from keras.utils import np_utils\n",
        "from keras.models import Sequential\n",
        "from keras import optimizers\n",
        "from keras.layers import Dense, Flatten, Conv2D, MaxPooling2D\n",
        "from keras.layers import Dropout, Flatten, GlobalAveragePooling2D\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "'''\n",
        "UTILITY FUNCTIONS\n",
        "'''\n",
        "\n",
        "def change_X_df__nparray_image(df_X_train_image_flattened ):\n",
        "  '''\n",
        "  setup_input_NN_image returns a dataframe of flaten image for x train and xtest\n",
        "  then this function will change each date into a nparray list of images with 32, 32, 3 size \n",
        "  '''\n",
        "  X_train_image=df_X_train_image_flattened\n",
        "  nb_train=len(X_train_image.index)\n",
        "  \n",
        "  x_train=np.zeros((nb_train,32,32,3))\n",
        "  for i in range(nb_train):\n",
        "    tmp=np.array(X_train_image.iloc[i])\n",
        "    tmp=tmp.reshape(32,32,3)\n",
        "    x_train[i]=tmp\n",
        "  return x_train\n",
        "\n",
        "'''\n",
        "MAIN EXECUTIONS\n",
        "'''\n",
        "#recuperation of datas \n",
        "X_train_image=pd.read_csv('datas/X_train_image.csv')\n",
        "Y_train_StateClass_image=pd.read_csv('datas/Y_train_StateClass_image.csv')\n",
        "Y_train_FutPredict_image=pd.read_csv('datas/Y_train_FutPredict_image.csv')\n",
        "\n",
        "\n",
        "#setting up the index to Date\n",
        "X_train_image=X_train_image.set_index(\"Date\")\n",
        "Y_train_StateClass_image=Y_train_StateClass_image.set_index(\"Date\")\n",
        "Y_train_FutPredict_image=Y_train_FutPredict_image.set_index(\"Date\")\n",
        "\n",
        "#modify dataset to np array for input to NN\n",
        "x_train=change_X_df__nparray_image(X_train_image)\n",
        "y_train_state=np.array(Y_train_StateClass_image)\n",
        "y_train_value=np.array(Y_train_FutPredict_image)\n",
        "\n",
        "##Setting up xtrain and ytrain\n",
        "#Here we focus on predicting the future state Y_train_StateClass_image\n",
        "nb_train=len(X_train_image.index)\n",
        "x_train=np.zeros((nb_train,32,32,3))\n",
        "for i in range(nb_train):\n",
        "  tmp=np.array(X_train_image.iloc[i])\n",
        "  tmp=tmp.reshape(32,32,3)\n",
        "  x_train[i]=tmp\n",
        "  \n",
        "y_train=np.array(Y_train_StateClass_image)\n",
        "#y_train=np.array(Y_train_FutPredict_image)\n",
        "\n",
        "nb_train=len(X_train_image.index)\n",
        "x_train=np.zeros((nb_train,32,32,3))\n",
        "for i in range(nb_train):\n",
        "  tmp=np.array(X_train_image.iloc[i])\n",
        "  tmp=tmp.reshape(32,32,3)\n",
        "  x_train[i]=tmp\n",
        "\n",
        "y_train=np.array(Y_train_StateClass_image)\n",
        "#y_train=np.array(Y_train_FutPredict_image)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/DL_Tools_For_Finance\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OiGYdJFX-aEC"
      },
      "source": [
        "Now we check the datas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fwiNBaTN-Ufq",
        "outputId": "4cd5a6af-6f2b-422a-e535-0cad1bddf1ca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "print(\"number of train datas\",len(y_train))\n",
        "print(\"number of dates actual\",len(y_train))\n",
        "print(\"shape of y and x train\", y_train.shape, \" and \", x_train.shape)\n",
        "print(\"min value of datas\", np.min(y_train), \", max value of datas\",np.max(y_train))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "number of train datas 18580\n",
            "number of dates actual 18580\n",
            "shape of y and x train (18580, 1)  and  (18580, 32, 32, 3)\n",
            "min value of datas -1.0 , max value of datas 4.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ewhz7vXwxzf4",
        "outputId": "f8cd89cc-224e-4acc-970c-43bb39bdaa7c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig2 = plt.figure(figsize=(10, 6))\n",
        "x_datas=x_train[np.random.randint(low=10,high=1000,size=8)]\n",
        "for i in range(0,8,1):\n",
        "    img = x_datas[i][:-1][:-1]\n",
        "    fig2.add_subplot(2, 4, i+1)\n",
        "    plt.imshow(img)\n",
        "\n",
        "\n",
        "print('Shape of each image in the training data: ', x_datas.shape[:])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of each image in the training data:  (8, 32, 32, 3)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x432 with 8 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F6Z8846EWC3F"
      },
      "source": [
        "####Build up and training of the NN model "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tbRtrFYtaDI5"
      },
      "source": [
        "In this part we suppose that we have the training dataset taken from step 2. We use a Transfert model for vgg16 and some other layers. we use for this example a categorical_crossentropy loss and rmsprop optimizer. This part can be fined tuned for each financial index or stock index (layers, optimmizer, metrics, dropout) but in this case we introduced a simplier case. We train and save the model, please refer to XX to see the convergence of the model.\n",
        "\n",
        "We have 14.7M parameters and 66k trainable parametres. the size of training input is 571M only for the image not including rolling volatility, moving average etc\n",
        "\n",
        "NB: If you have an error on size of input of the ytrain you have to reload the input executing  step 2 once again"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gTHxq8RVg-vE"
      },
      "source": [
        "import numpy as np\n",
        "import pylab as plt\n",
        "import pandas as pd\n",
        "\n",
        "from keras.utils import np_utils\n",
        "from keras.models import Sequential, Model\n",
        "from keras import optimizers\n",
        "from keras.layers import Dense, Flatten, Conv2D, MaxPooling2D\n",
        "from keras.layers import Dropout, Flatten, GlobalAveragePooling2D\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras import applications\n",
        "\n",
        "from datetime import datetime\n",
        "from packaging import version\n",
        "import tensorboard\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V-aVAzmKU9t8",
        "outputId": "99551224-3b1a-4400-b57d-a59a533cc0fd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "x_train.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(18580, 32, 32, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D-2BAtxzmPpe",
        "outputId": "c7a15e80-ae42-4855-ba7e-06618763406a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "'''\n",
        "PARAMETERS to change so as to improve the training\n",
        "'''\n",
        "\n",
        "#We can modify batch size and epochs to adjust improve the training\n",
        "batch_size=64\n",
        "epochs=5\n",
        "sp500_learning_rate=0.001\n",
        "\n",
        "#Use of exponential decay for learning rate\n",
        "#https://keras.io/api/optimizers/learning_rate_schedules/exponential_decay/\n",
        "\n",
        "lr_schedule = sp500_learning_rate  #or lr_scheduleExp\n",
        "\n",
        "sp500_decay_rate=0.90\n",
        "\n",
        "\n",
        "lr_scheduleExp = tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "    sp500_learning_rate,\n",
        "    decay_steps=100,\n",
        "    decay_rate=vggsp500_decay_rate,\n",
        "    staircase=True)\n",
        "\n",
        "##https://keras.io/api/losses/\n",
        "sp500loss='categorical_crossentropy'                                 \n",
        "\n",
        "##https://keras.io/api/optimizers/\n",
        "sp500optimizer_name='Adam'\n",
        "sp500optimizer=keras.optimizers.Adam(learning_rate=lr_schedule)  \n",
        "\n",
        "##https://keras.io/api/metrics/\n",
        "sp500metrics=['accuracy']                                           \n",
        "\n",
        "\n",
        "\n",
        "'''\n",
        "model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=lr_schedule),\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(data, labels, epochs=5)\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\nmodel.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=lr_schedule),\\n              loss='sparse_categorical_crossentropy',\\n              metrics=['accuracy'])\\n\\nmodel.fit(data, labels, epochs=5)\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7cRVj5gY5R3E"
      },
      "source": [
        "###Version with resnet\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vw_E83udAoLa",
        "outputId": "87dda025-7131-4d02-af71-a715eb0054a3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "transfer_model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"functional_3\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_20 (InputLayer)           [(None, 32, 32, 3)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv1_pad (ZeroPadding2D)       (None, 38, 38, 3)    0           input_20[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv1_conv (Conv2D)             (None, 16, 16, 64)   9472        conv1_pad[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1_bn (BatchNormalization)   (None, 16, 16, 64)   256         conv1_conv[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv1_relu (Activation)         (None, 16, 16, 64)   0           conv1_bn[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "pool1_pad (ZeroPadding2D)       (None, 18, 18, 64)   0           conv1_relu[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "pool1_pool (MaxPooling2D)       (None, 8, 8, 64)     0           pool1_pad[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_1_conv (Conv2D)    (None, 8, 8, 64)     4160        pool1_pool[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_1_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_1_relu (Activation (None, 8, 8, 64)     0           conv2_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_2_conv (Conv2D)    (None, 8, 8, 64)     36928       conv2_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_2_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_2_relu (Activation (None, 8, 8, 64)     0           conv2_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_0_conv (Conv2D)    (None, 8, 8, 256)    16640       pool1_pool[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_3_conv (Conv2D)    (None, 8, 8, 256)    16640       conv2_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_0_bn (BatchNormali (None, 8, 8, 256)    1024        conv2_block1_0_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_3_bn (BatchNormali (None, 8, 8, 256)    1024        conv2_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_add (Add)          (None, 8, 8, 256)    0           conv2_block1_0_bn[0][0]          \n",
            "                                                                 conv2_block1_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_out (Activation)   (None, 8, 8, 256)    0           conv2_block1_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_1_conv (Conv2D)    (None, 8, 8, 64)     16448       conv2_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_1_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_1_relu (Activation (None, 8, 8, 64)     0           conv2_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_2_conv (Conv2D)    (None, 8, 8, 64)     36928       conv2_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_2_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_2_relu (Activation (None, 8, 8, 64)     0           conv2_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_3_conv (Conv2D)    (None, 8, 8, 256)    16640       conv2_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_3_bn (BatchNormali (None, 8, 8, 256)    1024        conv2_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_add (Add)          (None, 8, 8, 256)    0           conv2_block1_out[0][0]           \n",
            "                                                                 conv2_block2_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_out (Activation)   (None, 8, 8, 256)    0           conv2_block2_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_1_conv (Conv2D)    (None, 8, 8, 64)     16448       conv2_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_1_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_1_relu (Activation (None, 8, 8, 64)     0           conv2_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_2_conv (Conv2D)    (None, 8, 8, 64)     36928       conv2_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_2_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_2_relu (Activation (None, 8, 8, 64)     0           conv2_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_3_conv (Conv2D)    (None, 8, 8, 256)    16640       conv2_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_3_bn (BatchNormali (None, 8, 8, 256)    1024        conv2_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_add (Add)          (None, 8, 8, 256)    0           conv2_block2_out[0][0]           \n",
            "                                                                 conv2_block3_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_out (Activation)   (None, 8, 8, 256)    0           conv2_block3_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_1_conv (Conv2D)    (None, 4, 4, 128)    32896       conv2_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_1_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_1_relu (Activation (None, 4, 4, 128)    0           conv3_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_2_conv (Conv2D)    (None, 4, 4, 128)    147584      conv3_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_2_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_2_relu (Activation (None, 4, 4, 128)    0           conv3_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_0_conv (Conv2D)    (None, 4, 4, 512)    131584      conv2_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_3_conv (Conv2D)    (None, 4, 4, 512)    66048       conv3_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_0_bn (BatchNormali (None, 4, 4, 512)    2048        conv3_block1_0_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_3_bn (BatchNormali (None, 4, 4, 512)    2048        conv3_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_add (Add)          (None, 4, 4, 512)    0           conv3_block1_0_bn[0][0]          \n",
            "                                                                 conv3_block1_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_out (Activation)   (None, 4, 4, 512)    0           conv3_block1_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_1_conv (Conv2D)    (None, 4, 4, 128)    65664       conv3_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_1_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_1_relu (Activation (None, 4, 4, 128)    0           conv3_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_2_conv (Conv2D)    (None, 4, 4, 128)    147584      conv3_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_2_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_2_relu (Activation (None, 4, 4, 128)    0           conv3_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_3_conv (Conv2D)    (None, 4, 4, 512)    66048       conv3_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_3_bn (BatchNormali (None, 4, 4, 512)    2048        conv3_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_add (Add)          (None, 4, 4, 512)    0           conv3_block1_out[0][0]           \n",
            "                                                                 conv3_block2_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_out (Activation)   (None, 4, 4, 512)    0           conv3_block2_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_1_conv (Conv2D)    (None, 4, 4, 128)    65664       conv3_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_1_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_1_relu (Activation (None, 4, 4, 128)    0           conv3_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_2_conv (Conv2D)    (None, 4, 4, 128)    147584      conv3_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_2_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_2_relu (Activation (None, 4, 4, 128)    0           conv3_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_3_conv (Conv2D)    (None, 4, 4, 512)    66048       conv3_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_3_bn (BatchNormali (None, 4, 4, 512)    2048        conv3_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_add (Add)          (None, 4, 4, 512)    0           conv3_block2_out[0][0]           \n",
            "                                                                 conv3_block3_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_out (Activation)   (None, 4, 4, 512)    0           conv3_block3_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_1_conv (Conv2D)    (None, 4, 4, 128)    65664       conv3_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_1_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block4_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_1_relu (Activation (None, 4, 4, 128)    0           conv3_block4_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_2_conv (Conv2D)    (None, 4, 4, 128)    147584      conv3_block4_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_2_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block4_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_2_relu (Activation (None, 4, 4, 128)    0           conv3_block4_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_3_conv (Conv2D)    (None, 4, 4, 512)    66048       conv3_block4_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_3_bn (BatchNormali (None, 4, 4, 512)    2048        conv3_block4_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_add (Add)          (None, 4, 4, 512)    0           conv3_block3_out[0][0]           \n",
            "                                                                 conv3_block4_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_out (Activation)   (None, 4, 4, 512)    0           conv3_block4_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_1_conv (Conv2D)    (None, 2, 2, 256)    131328      conv3_block4_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_1_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_1_relu (Activation (None, 2, 2, 256)    0           conv4_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_2_conv (Conv2D)    (None, 2, 2, 256)    590080      conv4_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_2_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_2_relu (Activation (None, 2, 2, 256)    0           conv4_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_0_conv (Conv2D)    (None, 2, 2, 1024)   525312      conv3_block4_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_3_conv (Conv2D)    (None, 2, 2, 1024)   263168      conv4_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_0_bn (BatchNormali (None, 2, 2, 1024)   4096        conv4_block1_0_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_3_bn (BatchNormali (None, 2, 2, 1024)   4096        conv4_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_add (Add)          (None, 2, 2, 1024)   0           conv4_block1_0_bn[0][0]          \n",
            "                                                                 conv4_block1_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_out (Activation)   (None, 2, 2, 1024)   0           conv4_block1_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_1_conv (Conv2D)    (None, 2, 2, 256)    262400      conv4_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_1_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_1_relu (Activation (None, 2, 2, 256)    0           conv4_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_2_conv (Conv2D)    (None, 2, 2, 256)    590080      conv4_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_2_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_2_relu (Activation (None, 2, 2, 256)    0           conv4_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_3_conv (Conv2D)    (None, 2, 2, 1024)   263168      conv4_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_3_bn (BatchNormali (None, 2, 2, 1024)   4096        conv4_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_add (Add)          (None, 2, 2, 1024)   0           conv4_block1_out[0][0]           \n",
            "                                                                 conv4_block2_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_out (Activation)   (None, 2, 2, 1024)   0           conv4_block2_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_1_conv (Conv2D)    (None, 2, 2, 256)    262400      conv4_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_1_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_1_relu (Activation (None, 2, 2, 256)    0           conv4_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_2_conv (Conv2D)    (None, 2, 2, 256)    590080      conv4_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_2_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_2_relu (Activation (None, 2, 2, 256)    0           conv4_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_3_conv (Conv2D)    (None, 2, 2, 1024)   263168      conv4_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_3_bn (BatchNormali (None, 2, 2, 1024)   4096        conv4_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_add (Add)          (None, 2, 2, 1024)   0           conv4_block2_out[0][0]           \n",
            "                                                                 conv4_block3_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_out (Activation)   (None, 2, 2, 1024)   0           conv4_block3_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_1_conv (Conv2D)    (None, 2, 2, 256)    262400      conv4_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_1_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block4_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_1_relu (Activation (None, 2, 2, 256)    0           conv4_block4_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_2_conv (Conv2D)    (None, 2, 2, 256)    590080      conv4_block4_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_2_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block4_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_2_relu (Activation (None, 2, 2, 256)    0           conv4_block4_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_3_conv (Conv2D)    (None, 2, 2, 1024)   263168      conv4_block4_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_3_bn (BatchNormali (None, 2, 2, 1024)   4096        conv4_block4_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_add (Add)          (None, 2, 2, 1024)   0           conv4_block3_out[0][0]           \n",
            "                                                                 conv4_block4_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_out (Activation)   (None, 2, 2, 1024)   0           conv4_block4_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_1_conv (Conv2D)    (None, 2, 2, 256)    262400      conv4_block4_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_1_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block5_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_1_relu (Activation (None, 2, 2, 256)    0           conv4_block5_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_2_conv (Conv2D)    (None, 2, 2, 256)    590080      conv4_block5_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_2_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block5_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_2_relu (Activation (None, 2, 2, 256)    0           conv4_block5_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_3_conv (Conv2D)    (None, 2, 2, 1024)   263168      conv4_block5_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_3_bn (BatchNormali (None, 2, 2, 1024)   4096        conv4_block5_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_add (Add)          (None, 2, 2, 1024)   0           conv4_block4_out[0][0]           \n",
            "                                                                 conv4_block5_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_out (Activation)   (None, 2, 2, 1024)   0           conv4_block5_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_1_conv (Conv2D)    (None, 2, 2, 256)    262400      conv4_block5_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_1_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block6_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_1_relu (Activation (None, 2, 2, 256)    0           conv4_block6_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_2_conv (Conv2D)    (None, 2, 2, 256)    590080      conv4_block6_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_2_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block6_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_2_relu (Activation (None, 2, 2, 256)    0           conv4_block6_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_3_conv (Conv2D)    (None, 2, 2, 1024)   263168      conv4_block6_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_3_bn (BatchNormali (None, 2, 2, 1024)   4096        conv4_block6_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_add (Add)          (None, 2, 2, 1024)   0           conv4_block5_out[0][0]           \n",
            "                                                                 conv4_block6_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_out (Activation)   (None, 2, 2, 1024)   0           conv4_block6_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_1_conv (Conv2D)    (None, 1, 1, 512)    524800      conv4_block6_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_1_bn (BatchNormali (None, 1, 1, 512)    2048        conv5_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_1_relu (Activation (None, 1, 1, 512)    0           conv5_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_2_conv (Conv2D)    (None, 1, 1, 512)    2359808     conv5_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_2_bn (BatchNormali (None, 1, 1, 512)    2048        conv5_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_2_relu (Activation (None, 1, 1, 512)    0           conv5_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_0_conv (Conv2D)    (None, 1, 1, 2048)   2099200     conv4_block6_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_3_conv (Conv2D)    (None, 1, 1, 2048)   1050624     conv5_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_0_bn (BatchNormali (None, 1, 1, 2048)   8192        conv5_block1_0_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_3_bn (BatchNormali (None, 1, 1, 2048)   8192        conv5_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_add (Add)          (None, 1, 1, 2048)   0           conv5_block1_0_bn[0][0]          \n",
            "                                                                 conv5_block1_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_out (Activation)   (None, 1, 1, 2048)   0           conv5_block1_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_1_conv (Conv2D)    (None, 1, 1, 512)    1049088     conv5_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_1_bn (BatchNormali (None, 1, 1, 512)    2048        conv5_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_1_relu (Activation (None, 1, 1, 512)    0           conv5_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_2_conv (Conv2D)    (None, 1, 1, 512)    2359808     conv5_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_2_bn (BatchNormali (None, 1, 1, 512)    2048        conv5_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_2_relu (Activation (None, 1, 1, 512)    0           conv5_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_3_conv (Conv2D)    (None, 1, 1, 2048)   1050624     conv5_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_3_bn (BatchNormali (None, 1, 1, 2048)   8192        conv5_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_add (Add)          (None, 1, 1, 2048)   0           conv5_block1_out[0][0]           \n",
            "                                                                 conv5_block2_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_out (Activation)   (None, 1, 1, 2048)   0           conv5_block2_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_1_conv (Conv2D)    (None, 1, 1, 512)    1049088     conv5_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_1_bn (BatchNormali (None, 1, 1, 512)    2048        conv5_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_1_relu (Activation (None, 1, 1, 512)    0           conv5_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_2_conv (Conv2D)    (None, 1, 1, 512)    2359808     conv5_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_2_bn (BatchNormali (None, 1, 1, 512)    2048        conv5_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_2_relu (Activation (None, 1, 1, 512)    0           conv5_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_3_conv (Conv2D)    (None, 1, 1, 2048)   1050624     conv5_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_3_bn (BatchNormali (None, 1, 1, 2048)   8192        conv5_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_add (Add)          (None, 1, 1, 2048)   0           conv5_block2_out[0][0]           \n",
            "                                                                 conv5_block3_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_out (Activation)   (None, 1, 1, 2048)   0           conv5_block3_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling2d_3 (Glo (None, 2048)         0           conv5_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "dropout_19 (Dropout)            (None, 2048)         0           global_average_pooling2d_3[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "dense_34 (Dense)                (None, 6)            12294       dropout_19[0][0]                 \n",
            "==================================================================================================\n",
            "Total params: 23,600,006\n",
            "Trainable params: 23,546,886\n",
            "Non-trainable params: 53,120\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9dBDC-Pb5Vi1",
        "outputId": "b5d106d2-d402-451c-8778-dd79eca040a2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "%cd /content/drive/My Drive/A_transfertTFMresnetSP500\n",
        "\n",
        "'''\n",
        "PART 3 Resnet TRAINING AND SAVING\n",
        "we suppose that we have loaded xtrain and ytrain\n",
        "This part is based on the Design of the NN\n",
        "Her we find the  quite usefull\n",
        "'''\n",
        "\n",
        "#Importing the resnet model\n",
        "from keras.applications.resnet import ResNet50, preprocess_input\n",
        "\n",
        "#In our example we need to y into categorical as it has 6 categories\n",
        "nb_classes=6\n",
        "y_train = np_utils.to_categorical(y_train, nb_classes)\n",
        "\n",
        "\n",
        "#Loading the resnet16 model with pre-trained ImageNet weights\n",
        "resnet_model = ResNet50(weights=None, include_top=False, input_shape=(32, 32, 3))\n",
        "resnet_model.trainable = False # remove if you want to retrain resnet weights\n",
        "\n",
        "resnet_model.summary()\n",
        "\n",
        "\n",
        "##Transfert model from resnet\n",
        "transfer_model = Sequential()\n",
        "transfer_model.add(resnet_model)\n",
        "transfer_model.add(Flatten())\n",
        "transfer_model.add(Dense(128, activation='relu'))\n",
        "transfer_model.add(Dropout(0.7))\n",
        "transfer_model.add(Dense(6, activation='softmax'))\n",
        "\n",
        "\n",
        "#Transfert model 2\n",
        "img_height,img_width=32,32\n",
        "num_classes=6\n",
        "base_model = applications.resnet50.ResNet50(weights= None, include_top=False, input_shape= (img_height,img_width,3))\n",
        "\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dropout(0.7)(x)\n",
        "predictions = Dense(num_classes, activation= 'softmax')(x)\n",
        "transfer_model = Model(inputs = base_model.input, outputs = predictions)\n",
        "\n",
        "###compilation model\n",
        "\n",
        "transfer_model.compile(loss=sp500loss, optimizer=sp500optimizer,\n",
        "              metrics=sp500metrics)\n",
        "\n",
        "#Save initial weight to reinitialize it after when we trying to find the best set of parameters\n",
        "transfer_model.save_weights('model/initial_weights.h5')\n",
        "#model.load_weights('my_model_weights.h5'\n",
        "\n",
        "##Saving the best model for each parameters\n",
        "checkpoint = ModelCheckpoint(\"model/best_model\"+sp500loss+\"_\"+sp500optimizer_name+\"_Batch\"+\\\n",
        "                             str(batch_size)+\"_LR\"+str(sp500_learning_rate)+\"_\"+str(sp500_decay_rate)+\".hdf5\", \\\n",
        "                                monitor='loss', verbose=1, \\\n",
        "                                save_best_only=True, mode='auto', period=1)\n",
        "\n",
        "# Load the TensorBoard notebook extension.\n",
        "%load_ext tensorboard\n",
        "#!rm -rf ./logs/ \n",
        "\n",
        " # Define the Keras TensorBoard callback.\n",
        "logdir=\"logs/fit/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir, histogram_freq=1)\n",
        "\n",
        "##Fitting the model on the train data and labels.\n",
        "#reinitialise xtrain, ytrain to avoid change of type from np.array to tensor by keras\n",
        "m_x_train=x_train\n",
        "m_y_train=y_train\n",
        "\n",
        "history = transfer_model.fit(m_x_train, m_y_train, \\\n",
        "                              batch_size=batch_size, epochs=epochs, \\\n",
        "                              validation_split=0.2, verbose=1, shuffle=True, \\\n",
        "                              callbacks=[checkpoint, tensorboard_callback])\n",
        "\n",
        "# Saving themodel\n",
        "transfer_model.save('model/resnetforsp500.h5')\n",
        "\n",
        "#Display the graph of the model\n",
        "tf.keras.utils.plot_model(transfer_model)\n",
        "\n",
        "##Display summary of neural network\n",
        "transfer_model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/A_transfertTFMresnetSP500\n",
            "Model: \"resnet50\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_21 (InputLayer)           [(None, 32, 32, 3)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv1_pad (ZeroPadding2D)       (None, 38, 38, 3)    0           input_21[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv1_conv (Conv2D)             (None, 16, 16, 64)   9472        conv1_pad[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1_bn (BatchNormalization)   (None, 16, 16, 64)   256         conv1_conv[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv1_relu (Activation)         (None, 16, 16, 64)   0           conv1_bn[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "pool1_pad (ZeroPadding2D)       (None, 18, 18, 64)   0           conv1_relu[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "pool1_pool (MaxPooling2D)       (None, 8, 8, 64)     0           pool1_pad[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_1_conv (Conv2D)    (None, 8, 8, 64)     4160        pool1_pool[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_1_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_1_relu (Activation (None, 8, 8, 64)     0           conv2_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_2_conv (Conv2D)    (None, 8, 8, 64)     36928       conv2_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_2_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_2_relu (Activation (None, 8, 8, 64)     0           conv2_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_0_conv (Conv2D)    (None, 8, 8, 256)    16640       pool1_pool[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_3_conv (Conv2D)    (None, 8, 8, 256)    16640       conv2_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_0_bn (BatchNormali (None, 8, 8, 256)    1024        conv2_block1_0_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_3_bn (BatchNormali (None, 8, 8, 256)    1024        conv2_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_add (Add)          (None, 8, 8, 256)    0           conv2_block1_0_bn[0][0]          \n",
            "                                                                 conv2_block1_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_out (Activation)   (None, 8, 8, 256)    0           conv2_block1_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_1_conv (Conv2D)    (None, 8, 8, 64)     16448       conv2_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_1_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_1_relu (Activation (None, 8, 8, 64)     0           conv2_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_2_conv (Conv2D)    (None, 8, 8, 64)     36928       conv2_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_2_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_2_relu (Activation (None, 8, 8, 64)     0           conv2_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_3_conv (Conv2D)    (None, 8, 8, 256)    16640       conv2_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_3_bn (BatchNormali (None, 8, 8, 256)    1024        conv2_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_add (Add)          (None, 8, 8, 256)    0           conv2_block1_out[0][0]           \n",
            "                                                                 conv2_block2_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_out (Activation)   (None, 8, 8, 256)    0           conv2_block2_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_1_conv (Conv2D)    (None, 8, 8, 64)     16448       conv2_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_1_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_1_relu (Activation (None, 8, 8, 64)     0           conv2_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_2_conv (Conv2D)    (None, 8, 8, 64)     36928       conv2_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_2_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_2_relu (Activation (None, 8, 8, 64)     0           conv2_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_3_conv (Conv2D)    (None, 8, 8, 256)    16640       conv2_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_3_bn (BatchNormali (None, 8, 8, 256)    1024        conv2_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_add (Add)          (None, 8, 8, 256)    0           conv2_block2_out[0][0]           \n",
            "                                                                 conv2_block3_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_out (Activation)   (None, 8, 8, 256)    0           conv2_block3_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_1_conv (Conv2D)    (None, 4, 4, 128)    32896       conv2_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_1_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_1_relu (Activation (None, 4, 4, 128)    0           conv3_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_2_conv (Conv2D)    (None, 4, 4, 128)    147584      conv3_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_2_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_2_relu (Activation (None, 4, 4, 128)    0           conv3_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_0_conv (Conv2D)    (None, 4, 4, 512)    131584      conv2_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_3_conv (Conv2D)    (None, 4, 4, 512)    66048       conv3_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_0_bn (BatchNormali (None, 4, 4, 512)    2048        conv3_block1_0_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_3_bn (BatchNormali (None, 4, 4, 512)    2048        conv3_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_add (Add)          (None, 4, 4, 512)    0           conv3_block1_0_bn[0][0]          \n",
            "                                                                 conv3_block1_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_out (Activation)   (None, 4, 4, 512)    0           conv3_block1_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_1_conv (Conv2D)    (None, 4, 4, 128)    65664       conv3_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_1_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_1_relu (Activation (None, 4, 4, 128)    0           conv3_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_2_conv (Conv2D)    (None, 4, 4, 128)    147584      conv3_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_2_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_2_relu (Activation (None, 4, 4, 128)    0           conv3_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_3_conv (Conv2D)    (None, 4, 4, 512)    66048       conv3_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_3_bn (BatchNormali (None, 4, 4, 512)    2048        conv3_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_add (Add)          (None, 4, 4, 512)    0           conv3_block1_out[0][0]           \n",
            "                                                                 conv3_block2_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_out (Activation)   (None, 4, 4, 512)    0           conv3_block2_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_1_conv (Conv2D)    (None, 4, 4, 128)    65664       conv3_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_1_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_1_relu (Activation (None, 4, 4, 128)    0           conv3_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_2_conv (Conv2D)    (None, 4, 4, 128)    147584      conv3_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_2_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_2_relu (Activation (None, 4, 4, 128)    0           conv3_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_3_conv (Conv2D)    (None, 4, 4, 512)    66048       conv3_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_3_bn (BatchNormali (None, 4, 4, 512)    2048        conv3_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_add (Add)          (None, 4, 4, 512)    0           conv3_block2_out[0][0]           \n",
            "                                                                 conv3_block3_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_out (Activation)   (None, 4, 4, 512)    0           conv3_block3_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_1_conv (Conv2D)    (None, 4, 4, 128)    65664       conv3_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_1_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block4_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_1_relu (Activation (None, 4, 4, 128)    0           conv3_block4_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_2_conv (Conv2D)    (None, 4, 4, 128)    147584      conv3_block4_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_2_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block4_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_2_relu (Activation (None, 4, 4, 128)    0           conv3_block4_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_3_conv (Conv2D)    (None, 4, 4, 512)    66048       conv3_block4_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_3_bn (BatchNormali (None, 4, 4, 512)    2048        conv3_block4_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_add (Add)          (None, 4, 4, 512)    0           conv3_block3_out[0][0]           \n",
            "                                                                 conv3_block4_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_out (Activation)   (None, 4, 4, 512)    0           conv3_block4_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_1_conv (Conv2D)    (None, 2, 2, 256)    131328      conv3_block4_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_1_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_1_relu (Activation (None, 2, 2, 256)    0           conv4_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_2_conv (Conv2D)    (None, 2, 2, 256)    590080      conv4_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_2_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_2_relu (Activation (None, 2, 2, 256)    0           conv4_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_0_conv (Conv2D)    (None, 2, 2, 1024)   525312      conv3_block4_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_3_conv (Conv2D)    (None, 2, 2, 1024)   263168      conv4_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_0_bn (BatchNormali (None, 2, 2, 1024)   4096        conv4_block1_0_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_3_bn (BatchNormali (None, 2, 2, 1024)   4096        conv4_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_add (Add)          (None, 2, 2, 1024)   0           conv4_block1_0_bn[0][0]          \n",
            "                                                                 conv4_block1_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_out (Activation)   (None, 2, 2, 1024)   0           conv4_block1_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_1_conv (Conv2D)    (None, 2, 2, 256)    262400      conv4_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_1_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_1_relu (Activation (None, 2, 2, 256)    0           conv4_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_2_conv (Conv2D)    (None, 2, 2, 256)    590080      conv4_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_2_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_2_relu (Activation (None, 2, 2, 256)    0           conv4_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_3_conv (Conv2D)    (None, 2, 2, 1024)   263168      conv4_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_3_bn (BatchNormali (None, 2, 2, 1024)   4096        conv4_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_add (Add)          (None, 2, 2, 1024)   0           conv4_block1_out[0][0]           \n",
            "                                                                 conv4_block2_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_out (Activation)   (None, 2, 2, 1024)   0           conv4_block2_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_1_conv (Conv2D)    (None, 2, 2, 256)    262400      conv4_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_1_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_1_relu (Activation (None, 2, 2, 256)    0           conv4_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_2_conv (Conv2D)    (None, 2, 2, 256)    590080      conv4_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_2_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_2_relu (Activation (None, 2, 2, 256)    0           conv4_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_3_conv (Conv2D)    (None, 2, 2, 1024)   263168      conv4_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_3_bn (BatchNormali (None, 2, 2, 1024)   4096        conv4_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_add (Add)          (None, 2, 2, 1024)   0           conv4_block2_out[0][0]           \n",
            "                                                                 conv4_block3_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_out (Activation)   (None, 2, 2, 1024)   0           conv4_block3_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_1_conv (Conv2D)    (None, 2, 2, 256)    262400      conv4_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_1_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block4_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_1_relu (Activation (None, 2, 2, 256)    0           conv4_block4_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_2_conv (Conv2D)    (None, 2, 2, 256)    590080      conv4_block4_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_2_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block4_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_2_relu (Activation (None, 2, 2, 256)    0           conv4_block4_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_3_conv (Conv2D)    (None, 2, 2, 1024)   263168      conv4_block4_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_3_bn (BatchNormali (None, 2, 2, 1024)   4096        conv4_block4_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_add (Add)          (None, 2, 2, 1024)   0           conv4_block3_out[0][0]           \n",
            "                                                                 conv4_block4_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_out (Activation)   (None, 2, 2, 1024)   0           conv4_block4_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_1_conv (Conv2D)    (None, 2, 2, 256)    262400      conv4_block4_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_1_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block5_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_1_relu (Activation (None, 2, 2, 256)    0           conv4_block5_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_2_conv (Conv2D)    (None, 2, 2, 256)    590080      conv4_block5_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_2_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block5_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_2_relu (Activation (None, 2, 2, 256)    0           conv4_block5_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_3_conv (Conv2D)    (None, 2, 2, 1024)   263168      conv4_block5_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_3_bn (BatchNormali (None, 2, 2, 1024)   4096        conv4_block5_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_add (Add)          (None, 2, 2, 1024)   0           conv4_block4_out[0][0]           \n",
            "                                                                 conv4_block5_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_out (Activation)   (None, 2, 2, 1024)   0           conv4_block5_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_1_conv (Conv2D)    (None, 2, 2, 256)    262400      conv4_block5_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_1_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block6_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_1_relu (Activation (None, 2, 2, 256)    0           conv4_block6_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_2_conv (Conv2D)    (None, 2, 2, 256)    590080      conv4_block6_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_2_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block6_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_2_relu (Activation (None, 2, 2, 256)    0           conv4_block6_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_3_conv (Conv2D)    (None, 2, 2, 1024)   263168      conv4_block6_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_3_bn (BatchNormali (None, 2, 2, 1024)   4096        conv4_block6_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_add (Add)          (None, 2, 2, 1024)   0           conv4_block5_out[0][0]           \n",
            "                                                                 conv4_block6_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_out (Activation)   (None, 2, 2, 1024)   0           conv4_block6_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_1_conv (Conv2D)    (None, 1, 1, 512)    524800      conv4_block6_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_1_bn (BatchNormali (None, 1, 1, 512)    2048        conv5_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_1_relu (Activation (None, 1, 1, 512)    0           conv5_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_2_conv (Conv2D)    (None, 1, 1, 512)    2359808     conv5_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_2_bn (BatchNormali (None, 1, 1, 512)    2048        conv5_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_2_relu (Activation (None, 1, 1, 512)    0           conv5_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_0_conv (Conv2D)    (None, 1, 1, 2048)   2099200     conv4_block6_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_3_conv (Conv2D)    (None, 1, 1, 2048)   1050624     conv5_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_0_bn (BatchNormali (None, 1, 1, 2048)   8192        conv5_block1_0_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_3_bn (BatchNormali (None, 1, 1, 2048)   8192        conv5_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_add (Add)          (None, 1, 1, 2048)   0           conv5_block1_0_bn[0][0]          \n",
            "                                                                 conv5_block1_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_out (Activation)   (None, 1, 1, 2048)   0           conv5_block1_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_1_conv (Conv2D)    (None, 1, 1, 512)    1049088     conv5_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_1_bn (BatchNormali (None, 1, 1, 512)    2048        conv5_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_1_relu (Activation (None, 1, 1, 512)    0           conv5_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_2_conv (Conv2D)    (None, 1, 1, 512)    2359808     conv5_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_2_bn (BatchNormali (None, 1, 1, 512)    2048        conv5_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_2_relu (Activation (None, 1, 1, 512)    0           conv5_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_3_conv (Conv2D)    (None, 1, 1, 2048)   1050624     conv5_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_3_bn (BatchNormali (None, 1, 1, 2048)   8192        conv5_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_add (Add)          (None, 1, 1, 2048)   0           conv5_block1_out[0][0]           \n",
            "                                                                 conv5_block2_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_out (Activation)   (None, 1, 1, 2048)   0           conv5_block2_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_1_conv (Conv2D)    (None, 1, 1, 512)    1049088     conv5_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_1_bn (BatchNormali (None, 1, 1, 512)    2048        conv5_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_1_relu (Activation (None, 1, 1, 512)    0           conv5_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_2_conv (Conv2D)    (None, 1, 1, 512)    2359808     conv5_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_2_bn (BatchNormali (None, 1, 1, 512)    2048        conv5_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_2_relu (Activation (None, 1, 1, 512)    0           conv5_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_3_conv (Conv2D)    (None, 1, 1, 2048)   1050624     conv5_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_3_bn (BatchNormali (None, 1, 1, 2048)   8192        conv5_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_add (Add)          (None, 1, 1, 2048)   0           conv5_block2_out[0][0]           \n",
            "                                                                 conv5_block3_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_out (Activation)   (None, 1, 1, 2048)   0           conv5_block3_add[0][0]           \n",
            "==================================================================================================\n",
            "Total params: 23,587,712\n",
            "Trainable params: 0\n",
            "Non-trainable params: 23,587,712\n",
            "__________________________________________________________________________________________________\n",
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
            "The tensorboard extension is already loaded. To reload it, use:\n",
            "  %reload_ext tensorboard\n",
            "Epoch 1/5\n",
            "233/233 [==============================] - ETA: 0s - loss: 3.1298 - accuracy: 0.2633\n",
            "Epoch 00001: loss improved from inf to 3.12980, saving model to model/best_modelcategorical_crossentropy_Adam_Batch64_LR0.001_0.9.hdf5\n",
            "233/233 [==============================] - 1057s 5s/step - loss: 3.1298 - accuracy: 0.2633 - val_loss: 2.8547 - val_accuracy: 0.3022\n",
            "Epoch 2/5\n",
            "233/233 [==============================] - ETA: 0s - loss: 2.1033 - accuracy: 0.3033\n",
            "Epoch 00002: loss improved from 3.12980 to 2.10330, saving model to model/best_modelcategorical_crossentropy_Adam_Batch64_LR0.001_0.9.hdf5\n",
            "233/233 [==============================] - 1060s 5s/step - loss: 2.1033 - accuracy: 0.3033 - val_loss: 2.0254 - val_accuracy: 0.3399\n",
            "Epoch 3/5\n",
            "233/233 [==============================] - ETA: 0s - loss: 2.0591 - accuracy: 0.3144\n",
            "Epoch 00003: loss improved from 2.10330 to 2.05908, saving model to model/best_modelcategorical_crossentropy_Adam_Batch64_LR0.001_0.9.hdf5\n",
            "233/233 [==============================] - 1058s 5s/step - loss: 2.0591 - accuracy: 0.3144 - val_loss: 2.0103 - val_accuracy: 0.2896\n",
            "Epoch 4/5\n",
            "233/233 [==============================] - ETA: 0s - loss: 1.9791 - accuracy: 0.3219\n",
            "Epoch 00004: loss improved from 2.05908 to 1.97908, saving model to model/best_modelcategorical_crossentropy_Adam_Batch64_LR0.001_0.9.hdf5\n",
            "233/233 [==============================] - 1050s 5s/step - loss: 1.9791 - accuracy: 0.3219 - val_loss: 2.3768 - val_accuracy: 0.3385\n",
            "Epoch 5/5\n",
            "233/233 [==============================] - ETA: 0s - loss: 1.9411 - accuracy: 0.3229\n",
            "Epoch 00005: loss improved from 1.97908 to 1.94115, saving model to model/best_modelcategorical_crossentropy_Adam_Batch64_LR0.001_0.9.hdf5\n",
            "233/233 [==============================] - 1044s 4s/step - loss: 1.9411 - accuracy: 0.3229 - val_loss: 1.5205 - val_accuracy: 0.3022\n",
            "Model: \"functional_5\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_22 (InputLayer)           [(None, 32, 32, 3)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv1_pad (ZeroPadding2D)       (None, 38, 38, 3)    0           input_22[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv1_conv (Conv2D)             (None, 16, 16, 64)   9472        conv1_pad[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1_bn (BatchNormalization)   (None, 16, 16, 64)   256         conv1_conv[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv1_relu (Activation)         (None, 16, 16, 64)   0           conv1_bn[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "pool1_pad (ZeroPadding2D)       (None, 18, 18, 64)   0           conv1_relu[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "pool1_pool (MaxPooling2D)       (None, 8, 8, 64)     0           pool1_pad[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_1_conv (Conv2D)    (None, 8, 8, 64)     4160        pool1_pool[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_1_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_1_relu (Activation (None, 8, 8, 64)     0           conv2_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_2_conv (Conv2D)    (None, 8, 8, 64)     36928       conv2_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_2_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_2_relu (Activation (None, 8, 8, 64)     0           conv2_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_0_conv (Conv2D)    (None, 8, 8, 256)    16640       pool1_pool[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_3_conv (Conv2D)    (None, 8, 8, 256)    16640       conv2_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_0_bn (BatchNormali (None, 8, 8, 256)    1024        conv2_block1_0_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_3_bn (BatchNormali (None, 8, 8, 256)    1024        conv2_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_add (Add)          (None, 8, 8, 256)    0           conv2_block1_0_bn[0][0]          \n",
            "                                                                 conv2_block1_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_out (Activation)   (None, 8, 8, 256)    0           conv2_block1_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_1_conv (Conv2D)    (None, 8, 8, 64)     16448       conv2_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_1_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_1_relu (Activation (None, 8, 8, 64)     0           conv2_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_2_conv (Conv2D)    (None, 8, 8, 64)     36928       conv2_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_2_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_2_relu (Activation (None, 8, 8, 64)     0           conv2_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_3_conv (Conv2D)    (None, 8, 8, 256)    16640       conv2_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_3_bn (BatchNormali (None, 8, 8, 256)    1024        conv2_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_add (Add)          (None, 8, 8, 256)    0           conv2_block1_out[0][0]           \n",
            "                                                                 conv2_block2_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_out (Activation)   (None, 8, 8, 256)    0           conv2_block2_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_1_conv (Conv2D)    (None, 8, 8, 64)     16448       conv2_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_1_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_1_relu (Activation (None, 8, 8, 64)     0           conv2_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_2_conv (Conv2D)    (None, 8, 8, 64)     36928       conv2_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_2_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_2_relu (Activation (None, 8, 8, 64)     0           conv2_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_3_conv (Conv2D)    (None, 8, 8, 256)    16640       conv2_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_3_bn (BatchNormali (None, 8, 8, 256)    1024        conv2_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_add (Add)          (None, 8, 8, 256)    0           conv2_block2_out[0][0]           \n",
            "                                                                 conv2_block3_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_out (Activation)   (None, 8, 8, 256)    0           conv2_block3_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_1_conv (Conv2D)    (None, 4, 4, 128)    32896       conv2_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_1_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_1_relu (Activation (None, 4, 4, 128)    0           conv3_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_2_conv (Conv2D)    (None, 4, 4, 128)    147584      conv3_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_2_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_2_relu (Activation (None, 4, 4, 128)    0           conv3_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_0_conv (Conv2D)    (None, 4, 4, 512)    131584      conv2_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_3_conv (Conv2D)    (None, 4, 4, 512)    66048       conv3_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_0_bn (BatchNormali (None, 4, 4, 512)    2048        conv3_block1_0_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_3_bn (BatchNormali (None, 4, 4, 512)    2048        conv3_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_add (Add)          (None, 4, 4, 512)    0           conv3_block1_0_bn[0][0]          \n",
            "                                                                 conv3_block1_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_out (Activation)   (None, 4, 4, 512)    0           conv3_block1_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_1_conv (Conv2D)    (None, 4, 4, 128)    65664       conv3_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_1_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_1_relu (Activation (None, 4, 4, 128)    0           conv3_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_2_conv (Conv2D)    (None, 4, 4, 128)    147584      conv3_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_2_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_2_relu (Activation (None, 4, 4, 128)    0           conv3_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_3_conv (Conv2D)    (None, 4, 4, 512)    66048       conv3_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_3_bn (BatchNormali (None, 4, 4, 512)    2048        conv3_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_add (Add)          (None, 4, 4, 512)    0           conv3_block1_out[0][0]           \n",
            "                                                                 conv3_block2_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_out (Activation)   (None, 4, 4, 512)    0           conv3_block2_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_1_conv (Conv2D)    (None, 4, 4, 128)    65664       conv3_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_1_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_1_relu (Activation (None, 4, 4, 128)    0           conv3_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_2_conv (Conv2D)    (None, 4, 4, 128)    147584      conv3_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_2_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_2_relu (Activation (None, 4, 4, 128)    0           conv3_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_3_conv (Conv2D)    (None, 4, 4, 512)    66048       conv3_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_3_bn (BatchNormali (None, 4, 4, 512)    2048        conv3_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_add (Add)          (None, 4, 4, 512)    0           conv3_block2_out[0][0]           \n",
            "                                                                 conv3_block3_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_out (Activation)   (None, 4, 4, 512)    0           conv3_block3_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_1_conv (Conv2D)    (None, 4, 4, 128)    65664       conv3_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_1_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block4_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_1_relu (Activation (None, 4, 4, 128)    0           conv3_block4_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_2_conv (Conv2D)    (None, 4, 4, 128)    147584      conv3_block4_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_2_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block4_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_2_relu (Activation (None, 4, 4, 128)    0           conv3_block4_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_3_conv (Conv2D)    (None, 4, 4, 512)    66048       conv3_block4_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_3_bn (BatchNormali (None, 4, 4, 512)    2048        conv3_block4_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_add (Add)          (None, 4, 4, 512)    0           conv3_block3_out[0][0]           \n",
            "                                                                 conv3_block4_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_out (Activation)   (None, 4, 4, 512)    0           conv3_block4_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_1_conv (Conv2D)    (None, 2, 2, 256)    131328      conv3_block4_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_1_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_1_relu (Activation (None, 2, 2, 256)    0           conv4_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_2_conv (Conv2D)    (None, 2, 2, 256)    590080      conv4_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_2_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_2_relu (Activation (None, 2, 2, 256)    0           conv4_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_0_conv (Conv2D)    (None, 2, 2, 1024)   525312      conv3_block4_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_3_conv (Conv2D)    (None, 2, 2, 1024)   263168      conv4_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_0_bn (BatchNormali (None, 2, 2, 1024)   4096        conv4_block1_0_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_3_bn (BatchNormali (None, 2, 2, 1024)   4096        conv4_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_add (Add)          (None, 2, 2, 1024)   0           conv4_block1_0_bn[0][0]          \n",
            "                                                                 conv4_block1_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_out (Activation)   (None, 2, 2, 1024)   0           conv4_block1_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_1_conv (Conv2D)    (None, 2, 2, 256)    262400      conv4_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_1_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_1_relu (Activation (None, 2, 2, 256)    0           conv4_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_2_conv (Conv2D)    (None, 2, 2, 256)    590080      conv4_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_2_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_2_relu (Activation (None, 2, 2, 256)    0           conv4_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_3_conv (Conv2D)    (None, 2, 2, 1024)   263168      conv4_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_3_bn (BatchNormali (None, 2, 2, 1024)   4096        conv4_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_add (Add)          (None, 2, 2, 1024)   0           conv4_block1_out[0][0]           \n",
            "                                                                 conv4_block2_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_out (Activation)   (None, 2, 2, 1024)   0           conv4_block2_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_1_conv (Conv2D)    (None, 2, 2, 256)    262400      conv4_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_1_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_1_relu (Activation (None, 2, 2, 256)    0           conv4_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_2_conv (Conv2D)    (None, 2, 2, 256)    590080      conv4_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_2_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_2_relu (Activation (None, 2, 2, 256)    0           conv4_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_3_conv (Conv2D)    (None, 2, 2, 1024)   263168      conv4_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_3_bn (BatchNormali (None, 2, 2, 1024)   4096        conv4_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_add (Add)          (None, 2, 2, 1024)   0           conv4_block2_out[0][0]           \n",
            "                                                                 conv4_block3_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_out (Activation)   (None, 2, 2, 1024)   0           conv4_block3_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_1_conv (Conv2D)    (None, 2, 2, 256)    262400      conv4_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_1_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block4_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_1_relu (Activation (None, 2, 2, 256)    0           conv4_block4_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_2_conv (Conv2D)    (None, 2, 2, 256)    590080      conv4_block4_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_2_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block4_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_2_relu (Activation (None, 2, 2, 256)    0           conv4_block4_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_3_conv (Conv2D)    (None, 2, 2, 1024)   263168      conv4_block4_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_3_bn (BatchNormali (None, 2, 2, 1024)   4096        conv4_block4_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_add (Add)          (None, 2, 2, 1024)   0           conv4_block3_out[0][0]           \n",
            "                                                                 conv4_block4_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_out (Activation)   (None, 2, 2, 1024)   0           conv4_block4_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_1_conv (Conv2D)    (None, 2, 2, 256)    262400      conv4_block4_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_1_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block5_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_1_relu (Activation (None, 2, 2, 256)    0           conv4_block5_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_2_conv (Conv2D)    (None, 2, 2, 256)    590080      conv4_block5_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_2_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block5_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_2_relu (Activation (None, 2, 2, 256)    0           conv4_block5_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_3_conv (Conv2D)    (None, 2, 2, 1024)   263168      conv4_block5_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_3_bn (BatchNormali (None, 2, 2, 1024)   4096        conv4_block5_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_add (Add)          (None, 2, 2, 1024)   0           conv4_block4_out[0][0]           \n",
            "                                                                 conv4_block5_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_out (Activation)   (None, 2, 2, 1024)   0           conv4_block5_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_1_conv (Conv2D)    (None, 2, 2, 256)    262400      conv4_block5_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_1_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block6_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_1_relu (Activation (None, 2, 2, 256)    0           conv4_block6_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_2_conv (Conv2D)    (None, 2, 2, 256)    590080      conv4_block6_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_2_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block6_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_2_relu (Activation (None, 2, 2, 256)    0           conv4_block6_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_3_conv (Conv2D)    (None, 2, 2, 1024)   263168      conv4_block6_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_3_bn (BatchNormali (None, 2, 2, 1024)   4096        conv4_block6_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_add (Add)          (None, 2, 2, 1024)   0           conv4_block5_out[0][0]           \n",
            "                                                                 conv4_block6_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_out (Activation)   (None, 2, 2, 1024)   0           conv4_block6_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_1_conv (Conv2D)    (None, 1, 1, 512)    524800      conv4_block6_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_1_bn (BatchNormali (None, 1, 1, 512)    2048        conv5_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_1_relu (Activation (None, 1, 1, 512)    0           conv5_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_2_conv (Conv2D)    (None, 1, 1, 512)    2359808     conv5_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_2_bn (BatchNormali (None, 1, 1, 512)    2048        conv5_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_2_relu (Activation (None, 1, 1, 512)    0           conv5_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_0_conv (Conv2D)    (None, 1, 1, 2048)   2099200     conv4_block6_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_3_conv (Conv2D)    (None, 1, 1, 2048)   1050624     conv5_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_0_bn (BatchNormali (None, 1, 1, 2048)   8192        conv5_block1_0_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_3_bn (BatchNormali (None, 1, 1, 2048)   8192        conv5_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_add (Add)          (None, 1, 1, 2048)   0           conv5_block1_0_bn[0][0]          \n",
            "                                                                 conv5_block1_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_out (Activation)   (None, 1, 1, 2048)   0           conv5_block1_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_1_conv (Conv2D)    (None, 1, 1, 512)    1049088     conv5_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_1_bn (BatchNormali (None, 1, 1, 512)    2048        conv5_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_1_relu (Activation (None, 1, 1, 512)    0           conv5_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_2_conv (Conv2D)    (None, 1, 1, 512)    2359808     conv5_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_2_bn (BatchNormali (None, 1, 1, 512)    2048        conv5_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_2_relu (Activation (None, 1, 1, 512)    0           conv5_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_3_conv (Conv2D)    (None, 1, 1, 2048)   1050624     conv5_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_3_bn (BatchNormali (None, 1, 1, 2048)   8192        conv5_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_add (Add)          (None, 1, 1, 2048)   0           conv5_block1_out[0][0]           \n",
            "                                                                 conv5_block2_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_out (Activation)   (None, 1, 1, 2048)   0           conv5_block2_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_1_conv (Conv2D)    (None, 1, 1, 512)    1049088     conv5_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_1_bn (BatchNormali (None, 1, 1, 512)    2048        conv5_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_1_relu (Activation (None, 1, 1, 512)    0           conv5_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_2_conv (Conv2D)    (None, 1, 1, 512)    2359808     conv5_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_2_bn (BatchNormali (None, 1, 1, 512)    2048        conv5_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_2_relu (Activation (None, 1, 1, 512)    0           conv5_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_3_conv (Conv2D)    (None, 1, 1, 2048)   1050624     conv5_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_3_bn (BatchNormali (None, 1, 1, 2048)   8192        conv5_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_add (Add)          (None, 1, 1, 2048)   0           conv5_block2_out[0][0]           \n",
            "                                                                 conv5_block3_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_out (Activation)   (None, 1, 1, 2048)   0           conv5_block3_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling2d_4 (Glo (None, 2048)         0           conv5_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "dropout_21 (Dropout)            (None, 2048)         0           global_average_pooling2d_4[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "dense_37 (Dense)                (None, 6)            12294       dropout_21[0][0]                 \n",
            "==================================================================================================\n",
            "Total params: 23,600,006\n",
            "Trainable params: 23,546,886\n",
            "Non-trainable params: 53,120\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wa7aDNZGWE6h",
        "outputId": "b9acec98-2994-4cff-b751-7d7643dc8d0d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "%pwd"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/drive/My Drive/A_transfertTFMresnetSP500'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sqRbJ1lTWRD7",
        "outputId": "cce77f57-a0b0-49da-8b6f-1ae87b0bf723",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "ls -l model/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 837925\n",
            "-rw------- 1 root root  97839904 Sep 19 14:40 best_modelcategorical_crossentropy_Adam_Batch32_LR0.0001_0.98.hdf5\n",
            "-rw------- 1 root root  97839896 Sep 19 13:15 best_modelcategorical_crossentropy_Adam_Batch32_LR0.01_0.98.hdf5\n",
            "-rw------- 1 root root 283765328 Sep 19 17:08 best_modelcategorical_crossentropy_Adam_Batch64_LR0.001_0.9.hdf5\n",
            "-rw------- 1 root root  94822472 Sep 19 15:40 initial_weights.h5\n",
            "-rw------- 1 root root 283765328 Sep 19 17:08 resnetforsp500.h5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uhuotgb6WcNZ"
      },
      "source": [
        "%cp model/best_modelcategorical_crossentropy_Adam_Batch64_LR0.001_0.9.hdf5 "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "weqAP7PdW1On"
      },
      "source": [
        "transfer_model.save('model/initial_weights.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H6eFmzP24-vc"
      },
      "source": [
        "###Version with vgg16"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WQNbUuu_ofxC",
        "outputId": "9930dc87-6f44-4727-a4fa-892dbc144628",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "%cd /content/drive/My Drive/A_transfertTFMVggSP500\n",
        "\n",
        "'''\n",
        "PART 3 VGGSP500 TRAINING AND SAVING\n",
        "we suppose that we have loaded xtrain and ytrain\n",
        "This part is based on the Design of the NN\n",
        "Her we find the Vgg16 quite usefull\n",
        "'''\n",
        "\n",
        "#Importing the VGG16 model\n",
        "from keras.applications.vgg16 import VGG16, preprocess_input\n",
        "from keras.applications.resnet import ResNet50\n",
        "\n",
        "#In our example we need to y into categorical as it has 6 categories\n",
        "nb_classes=6\n",
        "y_train = np_utils.to_categorical(y_train, nb_classes)\n",
        "\n",
        "\n",
        "#Loading the VGG16 model with pre-trained ImageNet weights\n",
        "vgg_model = VGG16(weights='imagenet', include_top=False, input_shape=(32, 32, 3))\n",
        "vgg_model.trainable = False # remove if you want to retrain vgg weights\n",
        "\n",
        "vgg_model.summary()\n",
        "\n",
        "\n",
        "##Transfert model from vgg\n",
        "transfer_model = Sequential()\n",
        "transfer_model.add(vgg_model)\n",
        "transfer_model.add(Flatten())\n",
        "transfer_model.add(Dense(128, activation='relu'))\n",
        "transfer_model.add(Dropout(0.2))\n",
        "transfer_model.add(Dense(6, activation='softmax'))\n",
        "\n",
        "\n",
        "\n",
        "transfer_model.compile(loss=vggsp500loss, optimizer=vggsp500optimizer,\n",
        "              metrics=vggsp500metrics)\n",
        "\n",
        "#Save initial weight to reinitialize it after when we trying to find the best set of parameters\n",
        "transfer_model.save_weights('model/initial_weights.h5')\n",
        "#model.load_weights('my_model_weights.h5'\n",
        "\n",
        "##Saving the best model for each parameters\n",
        "checkpoint = ModelCheckpoint(\"model/best_model\"+vggsp500loss+\"_\"+vggsp500optimizer_name+\"_Batch\"+\\\n",
        "                             str(batch_size)+\"_LR\"+str(vggsp500_learning_rate)+\"_\"+str(vggsp500_decay_rate)+\".hdf5\", \\\n",
        "                                monitor='loss', verbose=1, \\\n",
        "                                save_best_only=True, mode='auto', period=1)\n",
        "\n",
        "# Load the TensorBoard notebook extension.\n",
        "%load_ext tensorboard\n",
        "#!rm -rf ./logs/ \n",
        "\n",
        " # Define the Keras TensorBoard callback.\n",
        "logdir=\"logs/fit/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir, histogram_freq=1)\n",
        "\n",
        "##Fitting the model on the train data and labels.\n",
        "#reinitialise xtrain, ytrain to avoid change of type from np.array to tensor by keras\n",
        "m_x_train=x_train\n",
        "m_y_train=y_train\n",
        "\n",
        "history = transfer_model.fit(m_x_train, m_y_train, \\\n",
        "                              batch_size=batch_size, epochs=epochs, \\\n",
        "                              validation_split=0.2, verbose=1, shuffle=True, \\\n",
        "                              callbacks=[checkpoint, tensorboard_callback])\n",
        "\n",
        "# Saving themodel\n",
        "transfer_model.save('model/vggforsp500.h5')\n",
        "\n",
        "#Display the graph of the model\n",
        "tf.keras.utils.plot_model(transfer_model)\n",
        "\n",
        "##Display summary of neural network\n",
        "transfer_model.summary()\n",
        "\n",
        "#Display Tensorboard\n",
        "%tensorboard --logdir logs"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/A_transfertTFMVggSP500\n",
            "Model: \"vgg16\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_2 (InputLayer)         [(None, 32, 32, 3)]       0         \n",
            "_________________________________________________________________\n",
            "block1_conv1 (Conv2D)        (None, 32, 32, 64)        1792      \n",
            "_________________________________________________________________\n",
            "block1_conv2 (Conv2D)        (None, 32, 32, 64)        36928     \n",
            "_________________________________________________________________\n",
            "block1_pool (MaxPooling2D)   (None, 16, 16, 64)        0         \n",
            "_________________________________________________________________\n",
            "block2_conv1 (Conv2D)        (None, 16, 16, 128)       73856     \n",
            "_________________________________________________________________\n",
            "block2_conv2 (Conv2D)        (None, 16, 16, 128)       147584    \n",
            "_________________________________________________________________\n",
            "block2_pool (MaxPooling2D)   (None, 8, 8, 128)         0         \n",
            "_________________________________________________________________\n",
            "block3_conv1 (Conv2D)        (None, 8, 8, 256)         295168    \n",
            "_________________________________________________________________\n",
            "block3_conv2 (Conv2D)        (None, 8, 8, 256)         590080    \n",
            "_________________________________________________________________\n",
            "block3_conv3 (Conv2D)        (None, 8, 8, 256)         590080    \n",
            "_________________________________________________________________\n",
            "block3_pool (MaxPooling2D)   (None, 4, 4, 256)         0         \n",
            "_________________________________________________________________\n",
            "block4_conv1 (Conv2D)        (None, 4, 4, 512)         1180160   \n",
            "_________________________________________________________________\n",
            "block4_conv2 (Conv2D)        (None, 4, 4, 512)         2359808   \n",
            "_________________________________________________________________\n",
            "block4_conv3 (Conv2D)        (None, 4, 4, 512)         2359808   \n",
            "_________________________________________________________________\n",
            "block4_pool (MaxPooling2D)   (None, 2, 2, 512)         0         \n",
            "_________________________________________________________________\n",
            "block5_conv1 (Conv2D)        (None, 2, 2, 512)         2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv2 (Conv2D)        (None, 2, 2, 512)         2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv3 (Conv2D)        (None, 2, 2, 512)         2359808   \n",
            "_________________________________________________________________\n",
            "block5_pool (MaxPooling2D)   (None, 1, 1, 512)         0         \n",
            "=================================================================\n",
            "Total params: 14,714,688\n",
            "Trainable params: 0\n",
            "Non-trainable params: 14,714,688\n",
            "_________________________________________________________________\n",
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
            "The tensorboard extension is already loaded. To reload it, use:\n",
            "  %reload_ext tensorboard\n",
            "Epoch 1/3\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-f479dc028c0d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0mm_y_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransfer_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm_x_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm_y_train\u001b[0m\u001b[0;34m,\u001b[0m                               \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m                               \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m                               \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensorboard_callback\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;31m# Saving themodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1098\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1099\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    821\u001b[0m       \u001b[0;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 823\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    824\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m       \u001b[0;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    695\u001b[0m     self._concrete_stateful_fn = (\n\u001b[1;32m    696\u001b[0m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0;32m--> 697\u001b[0;31m             *args, **kwds))\n\u001b[0m\u001b[1;32m    698\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    699\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minvalid_creator_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0munused_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0munused_kwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2853\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2854\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2855\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2856\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2857\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3212\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3213\u001b[0;31m       \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3214\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3215\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   3073\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3074\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3075\u001b[0;31m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[1;32m   3076\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3077\u001b[0m         \u001b[0mfunction_spec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_spec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    985\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 986\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    987\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    988\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    598\u001b[0m         \u001b[0;31m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    599\u001b[0m         \u001b[0;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 600\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    601\u001b[0m     \u001b[0mweak_wrapped_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweakref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapped_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    971\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    972\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 973\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    974\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    975\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:806 train_function  *\n        return step_function(self, iterator)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:796 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:1211 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:2585 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:2945 _call_for_each_replica\n        return fn(*args, **kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:789 run_step  **\n        outputs = model.train_step(data)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:749 train_step\n        y, y_pred, sample_weight, regularization_losses=self.losses)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/compile_utils.py:204 __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/losses.py:149 __call__\n        losses = ag_call(y_true, y_pred)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/losses.py:253 call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py:201 wrapper\n        return target(*args, **kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/losses.py:1535 categorical_crossentropy\n        return K.categorical_crossentropy(y_true, y_pred, from_logits=from_logits)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py:201 wrapper\n        return target(*args, **kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/backend.py:4687 categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/tensor_shape.py:1134 assert_is_compatible_with\n        raise ValueError(\"Shapes %s and %s are incompatible\" % (self, other))\n\n    ValueError: Shapes (None, 6, 6) and (None, 6) are incompatible\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FiDWq98sOAb-"
      },
      "source": [
        "%load_ext tensorboard"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pJfc1EgdOIn8"
      },
      "source": [
        "#here you have with the logs in drive link where you can launch tensorboard and see the training process\n",
        "%cd /content/drive/My Drive/A_transfertTFMVggSP500\n",
        "%tensorboard --logdir logs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n4hYLydW_XH2"
      },
      "source": [
        "history.history['loss']\n",
        "history.history"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SgkpclptyV7A"
      },
      "source": [
        "**IN CASE OF ERROR OF SHAPE OF INPUT**\n",
        "\n",
        "please note that if some error in shape of the input you have to execute the loading datas once again\n",
        "there is a update of type between tensor and np.array of y_train and x_train\n",
        "\n",
        "Go back to\n",
        "\n",
        "Step 2: Loading training datas with vgg16 transfert model and training\n",
        "\n",
        "Loading and Cleaning Training Datas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IuzuOOsaLAsO"
      },
      "source": [
        "####Search of the best parameters for training and evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WQD_FVuIK--C",
        "outputId": "450c342a-0c91-41ed-c32a-a902c34f04bd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "%cd /content/drive/My Drive/A_transfertTFMVggSP500\n",
        "# Load the TensorBoard notebook extension.\n",
        "%reload_ext tensorboard\n",
        "!rm -rf ./logs/ \n",
        "\n",
        "#We can modify batch size and epochs to adjust improve the training\n",
        "l_batch_size=[25,50,100]\n",
        "l_epochs=[25,50,100]\n",
        "l_learning_rate=[0.001,0.01,0.1]\n",
        "l_optimizer_name=[keras.optimizers.SGD, keras.optimizers.RMSprop, keras.optimizers.Adam, keras.optimizers.Adagrad, keras.optimizers.Adamax, keras.optimizers.Ftrl]\n",
        "\n",
        "Tabres={}\n",
        "\n",
        "for x_batch_size in l_batch_size :\n",
        "  for  x_epochs in l_epochs:\n",
        "    for x_learning_rate in l_learning_rate:\n",
        "      for x_optimizer_name in l_optimizer_name :\n",
        "        batch_size= x_batch_size\n",
        "        epochs= x_epochs\n",
        "        lr_schedule =  x_learning_rate\n",
        "        \n",
        "        ##https://keras.io/api/losses/\n",
        "        vggsp500loss='categorical_crossentropy'                             \n",
        "\n",
        "        ##https://keras.io/api/optimizers/\n",
        "        vggsp500optimizer_name=x_optimizer_name(learning_rate=x_learning_rate)\n",
        "        \n",
        "        ##https://keras.io/api/metrics/\n",
        "        vggsp500metrics=['accuracy']                                           \n",
        "        \n",
        "        \n",
        "        ##ACTUALISATION OF THE MODEL AND BY EACH SET OF PARAMETERS\n",
        "        \n",
        "        #First we initialize weight for each set of param see abpve where we had saved it\n",
        "        transfer_model.load_weights('model/initial_weights.h5')\n",
        "        transfer_model.compile(loss=vggsp500loss, optimizer=vggsp500optimizer_name,\n",
        "                      metrics=vggsp500metrics)\n",
        "\n",
        "        ##Saving the best model for each parameters\n",
        "        nameof_intermediarymodel=\"best_model\"+vggsp500loss+\"_\"+str(vggsp500optimizer_name)+\"_Batch\"+\\\n",
        "                                    str(batch_size)+\"_LR\"+str(x_learning_rate)+\"_Epochs\"+str(epochs)\n",
        "        checkpoint = ModelCheckpoint('model/'+nameof_intermediarymodel+\".hdf5\", \\\n",
        "                                        monitor='loss', verbose=1, \\\n",
        "                                        save_best_only=True, mode='auto', period=1)\n",
        "\n",
        "        \n",
        "        # Define the Keras TensorBoard callback.\n",
        "        logdir=\"logs/fit/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "        tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir, histogram_freq=1)\n",
        "\n",
        "        ##Fitting the model on the train data and labels.\n",
        "        m_x_train=x_train\n",
        "        m_y_train=y_train\n",
        "\n",
        "        history = transfer_model.fit(m_x_train, m_y_train, \\\n",
        "                                      batch_size=batch_size, epochs=epochs, \\\n",
        "                                      validation_split=0.2, verbose=1, shuffle=True, \\\n",
        "                                      callbacks=[checkpoint,tensorboard_callback])\n",
        "        # Saving themodel\n",
        "        transfer_model.save('model/'+nameof_intermediarymodel+'vggforsp500'+'.h5')\n",
        "        Tabres.update({nameof_intermediarymodel:history.history})\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/A_transfertTFMVggSP500\n",
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
            "Epoch 1/25\n",
            "  2/595 [..............................] - ETA: 34s - loss: 2.4315 - accuracy: 0.0400WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0145s vs `on_train_batch_end` time: 0.1003s). Check your callbacks.\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.6045 - accuracy: 0.3101\n",
            "Epoch 00001: loss improved from inf to 1.60393, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f5b6204e8d0>_Batch25_LR0.001_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.6039 - accuracy: 0.3110 - val_loss: 1.5135 - val_accuracy: 0.3399\n",
            "Epoch 2/25\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.5611 - accuracy: 0.3199\n",
            "Epoch 00002: loss improved from 1.60393 to 1.56054, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f5b6204e8d0>_Batch25_LR0.001_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5605 - accuracy: 0.3201 - val_loss: 1.5102 - val_accuracy: 0.3399\n",
            "Epoch 3/25\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.5548 - accuracy: 0.3293\n",
            "Epoch 00003: loss improved from 1.56054 to 1.55448, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f5b6204e8d0>_Batch25_LR0.001_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5545 - accuracy: 0.3294 - val_loss: 1.5035 - val_accuracy: 0.3399\n",
            "Epoch 4/25\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5459 - accuracy: 0.3327\n",
            "Epoch 00004: loss improved from 1.55448 to 1.54585, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f5b6204e8d0>_Batch25_LR0.001_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5459 - accuracy: 0.3326 - val_loss: 1.5065 - val_accuracy: 0.3399\n",
            "Epoch 5/25\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5430 - accuracy: 0.3321\n",
            "Epoch 00005: loss improved from 1.54585 to 1.54320, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f5b6204e8d0>_Batch25_LR0.001_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5432 - accuracy: 0.3318 - val_loss: 1.5075 - val_accuracy: 0.3399\n",
            "Epoch 6/25\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5411 - accuracy: 0.3384\n",
            "Epoch 00006: loss improved from 1.54320 to 1.54099, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f5b6204e8d0>_Batch25_LR0.001_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5410 - accuracy: 0.3387 - val_loss: 1.5053 - val_accuracy: 0.3399\n",
            "Epoch 7/25\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5378 - accuracy: 0.3389\n",
            "Epoch 00007: loss improved from 1.54099 to 1.53811, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f5b6204e8d0>_Batch25_LR0.001_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5381 - accuracy: 0.3387 - val_loss: 1.5027 - val_accuracy: 0.3399\n",
            "Epoch 8/25\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5351 - accuracy: 0.3378\n",
            "Epoch 00008: loss improved from 1.53811 to 1.53514, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f5b6204e8d0>_Batch25_LR0.001_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5351 - accuracy: 0.3379 - val_loss: 1.5048 - val_accuracy: 0.3399\n",
            "Epoch 9/25\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5381 - accuracy: 0.3386\n",
            "Epoch 00009: loss did not improve from 1.53514\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5382 - accuracy: 0.3388 - val_loss: 1.5061 - val_accuracy: 0.3399\n",
            "Epoch 10/25\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5370 - accuracy: 0.3422\n",
            "Epoch 00010: loss did not improve from 1.53514\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5366 - accuracy: 0.3422 - val_loss: 1.5034 - val_accuracy: 0.3399\n",
            "Epoch 11/25\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.5342 - accuracy: 0.3403\n",
            "Epoch 00011: loss improved from 1.53514 to 1.53396, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f5b6204e8d0>_Batch25_LR0.001_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5340 - accuracy: 0.3404 - val_loss: 1.5014 - val_accuracy: 0.3399\n",
            "Epoch 12/25\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.5354 - accuracy: 0.3381\n",
            "Epoch 00012: loss did not improve from 1.53396\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5354 - accuracy: 0.3381 - val_loss: 1.5010 - val_accuracy: 0.3399\n",
            "Epoch 13/25\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.5326 - accuracy: 0.3406\n",
            "Epoch 00013: loss improved from 1.53396 to 1.53260, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f5b6204e8d0>_Batch25_LR0.001_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5326 - accuracy: 0.3406 - val_loss: 1.5043 - val_accuracy: 0.3399\n",
            "Epoch 14/25\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5333 - accuracy: 0.3436\n",
            "Epoch 00014: loss did not improve from 1.53260\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5329 - accuracy: 0.3442 - val_loss: 1.5033 - val_accuracy: 0.3399\n",
            "Epoch 15/25\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5314 - accuracy: 0.3414\n",
            "Epoch 00015: loss improved from 1.53260 to 1.53111, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f5b6204e8d0>_Batch25_LR0.001_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5311 - accuracy: 0.3417 - val_loss: 1.5036 - val_accuracy: 0.3399\n",
            "Epoch 16/25\n",
            "589/595 [============================>.] - ETA: 0s - loss: 1.5308 - accuracy: 0.3419\n",
            "Epoch 00016: loss improved from 1.53111 to 1.53053, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f5b6204e8d0>_Batch25_LR0.001_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5305 - accuracy: 0.3425 - val_loss: 1.5035 - val_accuracy: 0.3399\n",
            "Epoch 17/25\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.5306 - accuracy: 0.3430\n",
            "Epoch 00017: loss did not improve from 1.53053\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5306 - accuracy: 0.3430 - val_loss: 1.5025 - val_accuracy: 0.3399\n",
            "Epoch 18/25\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.5305 - accuracy: 0.3426\n",
            "Epoch 00018: loss did not improve from 1.53053\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5305 - accuracy: 0.3426 - val_loss: 1.5024 - val_accuracy: 0.3399\n",
            "Epoch 19/25\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5305 - accuracy: 0.3420\n",
            "Epoch 00019: loss improved from 1.53053 to 1.53039, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f5b6204e8d0>_Batch25_LR0.001_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5304 - accuracy: 0.3421 - val_loss: 1.5016 - val_accuracy: 0.3399\n",
            "Epoch 20/25\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.5282 - accuracy: 0.3417\n",
            "Epoch 00020: loss improved from 1.53039 to 1.52811, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f5b6204e8d0>_Batch25_LR0.001_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5281 - accuracy: 0.3420 - val_loss: 1.5025 - val_accuracy: 0.3399\n",
            "Epoch 21/25\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5292 - accuracy: 0.3424\n",
            "Epoch 00021: loss did not improve from 1.52811\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5293 - accuracy: 0.3422 - val_loss: 1.5011 - val_accuracy: 0.3399\n",
            "Epoch 22/25\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5287 - accuracy: 0.3414\n",
            "Epoch 00022: loss did not improve from 1.52811\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5283 - accuracy: 0.3416 - val_loss: 1.5029 - val_accuracy: 0.3399\n",
            "Epoch 23/25\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.5282 - accuracy: 0.3419\n",
            "Epoch 00023: loss did not improve from 1.52811\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5282 - accuracy: 0.3419 - val_loss: 1.4992 - val_accuracy: 0.3399\n",
            "Epoch 24/25\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5283 - accuracy: 0.3428\n",
            "Epoch 00024: loss did not improve from 1.52811\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5281 - accuracy: 0.3428 - val_loss: 1.4994 - val_accuracy: 0.3399\n",
            "Epoch 25/25\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5271 - accuracy: 0.3417\n",
            "Epoch 00025: loss improved from 1.52811 to 1.52711, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f5b6204e8d0>_Batch25_LR0.001_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5271 - accuracy: 0.3418 - val_loss: 1.4991 - val_accuracy: 0.3399\n",
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
            "Epoch 1/25\n",
            "  2/595 [..............................] - ETA: 39s - loss: 2.0196 - accuracy: 0.0800WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0155s vs `on_train_batch_end` time: 0.1178s). Check your callbacks.\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5473 - accuracy: 0.3327\n",
            "Epoch 00001: loss improved from inf to 1.54702, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.rmsprop.RMSprop object at 0x7f59cc45efd0>_Batch25_LR0.001_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5470 - accuracy: 0.3329 - val_loss: 1.5068 - val_accuracy: 0.3399\n",
            "Epoch 2/25\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5284 - accuracy: 0.3414\n",
            "Epoch 00002: loss improved from 1.54702 to 1.52771, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.rmsprop.RMSprop object at 0x7f59cc45efd0>_Batch25_LR0.001_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5277 - accuracy: 0.3420 - val_loss: 1.4969 - val_accuracy: 0.3399\n",
            "Epoch 3/25\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.5241 - accuracy: 0.3425\n",
            "Epoch 00003: loss improved from 1.52771 to 1.52421, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.rmsprop.RMSprop object at 0x7f59cc45efd0>_Batch25_LR0.001_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5242 - accuracy: 0.3423 - val_loss: 1.5041 - val_accuracy: 0.3399\n",
            "Epoch 4/25\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5199 - accuracy: 0.3434\n",
            "Epoch 00004: loss improved from 1.52421 to 1.52004, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.rmsprop.RMSprop object at 0x7f59cc45efd0>_Batch25_LR0.001_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5200 - accuracy: 0.3435 - val_loss: 1.4919 - val_accuracy: 0.3442\n",
            "Epoch 5/25\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.5180 - accuracy: 0.3408\n",
            "Epoch 00005: loss improved from 1.52004 to 1.51803, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.rmsprop.RMSprop object at 0x7f59cc45efd0>_Batch25_LR0.001_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5180 - accuracy: 0.3408 - val_loss: 1.4974 - val_accuracy: 0.3358\n",
            "Epoch 6/25\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.5138 - accuracy: 0.3458\n",
            "Epoch 00006: loss improved from 1.51803 to 1.51346, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.rmsprop.RMSprop object at 0x7f59cc45efd0>_Batch25_LR0.001_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5135 - accuracy: 0.3451 - val_loss: 1.4893 - val_accuracy: 0.3415\n",
            "Epoch 7/25\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5116 - accuracy: 0.3461\n",
            "Epoch 00007: loss improved from 1.51346 to 1.51201, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.rmsprop.RMSprop object at 0x7f59cc45efd0>_Batch25_LR0.001_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5120 - accuracy: 0.3457 - val_loss: 1.4988 - val_accuracy: 0.3286\n",
            "Epoch 8/25\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5121 - accuracy: 0.3473\n",
            "Epoch 00008: loss improved from 1.51201 to 1.51195, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.rmsprop.RMSprop object at 0x7f59cc45efd0>_Batch25_LR0.001_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5120 - accuracy: 0.3475 - val_loss: 1.5162 - val_accuracy: 0.3372\n",
            "Epoch 9/25\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5079 - accuracy: 0.3443\n",
            "Epoch 00009: loss improved from 1.51195 to 1.50823, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.rmsprop.RMSprop object at 0x7f59cc45efd0>_Batch25_LR0.001_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5082 - accuracy: 0.3443 - val_loss: 1.5151 - val_accuracy: 0.3224\n",
            "Epoch 10/25\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.5068 - accuracy: 0.3448\n",
            "Epoch 00010: loss improved from 1.50823 to 1.50675, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.rmsprop.RMSprop object at 0x7f59cc45efd0>_Batch25_LR0.001_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5068 - accuracy: 0.3448 - val_loss: 1.5074 - val_accuracy: 0.3243\n",
            "Epoch 11/25\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5046 - accuracy: 0.3450\n",
            "Epoch 00011: loss improved from 1.50675 to 1.50467, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.rmsprop.RMSprop object at 0x7f59cc45efd0>_Batch25_LR0.001_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5047 - accuracy: 0.3451 - val_loss: 1.5108 - val_accuracy: 0.3229\n",
            "Epoch 12/25\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5039 - accuracy: 0.3482\n",
            "Epoch 00012: loss improved from 1.50467 to 1.50432, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.rmsprop.RMSprop object at 0x7f59cc45efd0>_Batch25_LR0.001_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5043 - accuracy: 0.3480 - val_loss: 1.4885 - val_accuracy: 0.3202\n",
            "Epoch 13/25\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.5011 - accuracy: 0.3495\n",
            "Epoch 00013: loss improved from 1.50432 to 1.50177, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.rmsprop.RMSprop object at 0x7f59cc45efd0>_Batch25_LR0.001_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5018 - accuracy: 0.3493 - val_loss: 1.4918 - val_accuracy: 0.3294\n",
            "Epoch 14/25\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5013 - accuracy: 0.3525\n",
            "Epoch 00014: loss improved from 1.50177 to 1.50128, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.rmsprop.RMSprop object at 0x7f59cc45efd0>_Batch25_LR0.001_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5013 - accuracy: 0.3525 - val_loss: 1.5210 - val_accuracy: 0.3221\n",
            "Epoch 15/25\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.4995 - accuracy: 0.3475\n",
            "Epoch 00015: loss improved from 1.50128 to 1.49957, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.rmsprop.RMSprop object at 0x7f59cc45efd0>_Batch25_LR0.001_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.4996 - accuracy: 0.3473 - val_loss: 1.4979 - val_accuracy: 0.3200\n",
            "Epoch 16/25\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.4982 - accuracy: 0.3465\n",
            "Epoch 00016: loss improved from 1.49957 to 1.49825, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.rmsprop.RMSprop object at 0x7f59cc45efd0>_Batch25_LR0.001_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.4982 - accuracy: 0.3465 - val_loss: 1.4987 - val_accuracy: 0.3305\n",
            "Epoch 17/25\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.4980 - accuracy: 0.3498\n",
            "Epoch 00017: loss improved from 1.49825 to 1.49805, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.rmsprop.RMSprop object at 0x7f59cc45efd0>_Batch25_LR0.001_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.4980 - accuracy: 0.3498 - val_loss: 1.5270 - val_accuracy: 0.3251\n",
            "Epoch 18/25\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.4963 - accuracy: 0.3502\n",
            "Epoch 00018: loss improved from 1.49805 to 1.49669, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.rmsprop.RMSprop object at 0x7f59cc45efd0>_Batch25_LR0.001_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.4967 - accuracy: 0.3495 - val_loss: 1.5085 - val_accuracy: 0.3272\n",
            "Epoch 19/25\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.4960 - accuracy: 0.3529\n",
            "Epoch 00019: loss improved from 1.49669 to 1.49595, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.rmsprop.RMSprop object at 0x7f59cc45efd0>_Batch25_LR0.001_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.4960 - accuracy: 0.3529 - val_loss: 1.5233 - val_accuracy: 0.3369\n",
            "Epoch 20/25\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.4945 - accuracy: 0.3533\n",
            "Epoch 00020: loss improved from 1.49595 to 1.49452, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.rmsprop.RMSprop object at 0x7f59cc45efd0>_Batch25_LR0.001_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.4945 - accuracy: 0.3533 - val_loss: 1.5195 - val_accuracy: 0.3267\n",
            "Epoch 21/25\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.4946 - accuracy: 0.3554\n",
            "Epoch 00021: loss improved from 1.49452 to 1.49425, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.rmsprop.RMSprop object at 0x7f59cc45efd0>_Batch25_LR0.001_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.4942 - accuracy: 0.3559 - val_loss: 1.5107 - val_accuracy: 0.3173\n",
            "Epoch 22/25\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.4921 - accuracy: 0.3575\n",
            "Epoch 00022: loss improved from 1.49425 to 1.49210, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.rmsprop.RMSprop object at 0x7f59cc45efd0>_Batch25_LR0.001_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.4921 - accuracy: 0.3574 - val_loss: 1.5283 - val_accuracy: 0.3375\n",
            "Epoch 23/25\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.4911 - accuracy: 0.3549\n",
            "Epoch 00023: loss improved from 1.49210 to 1.49081, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.rmsprop.RMSprop object at 0x7f59cc45efd0>_Batch25_LR0.001_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.4908 - accuracy: 0.3552 - val_loss: 1.5110 - val_accuracy: 0.3393\n",
            "Epoch 24/25\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.4900 - accuracy: 0.3575\n",
            "Epoch 00024: loss improved from 1.49081 to 1.48945, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.rmsprop.RMSprop object at 0x7f59cc45efd0>_Batch25_LR0.001_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.4894 - accuracy: 0.3578 - val_loss: 1.5388 - val_accuracy: 0.3122\n",
            "Epoch 25/25\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.4890 - accuracy: 0.3592\n",
            "Epoch 00025: loss improved from 1.48945 to 1.48899, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.rmsprop.RMSprop object at 0x7f59cc45efd0>_Batch25_LR0.001_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.4890 - accuracy: 0.3591 - val_loss: 1.5243 - val_accuracy: 0.3253\n",
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
            "Epoch 1/25\n",
            "  2/595 [..............................] - ETA: 34s - loss: 2.1772 - accuracy: 0.0800WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0163s vs `on_train_batch_end` time: 0.0981s). Check your callbacks.\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.5496 - accuracy: 0.3283\n",
            "Epoch 00001: loss improved from inf to 1.54961, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f5b61ded208>_Batch25_LR0.001_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5496 - accuracy: 0.3283 - val_loss: 1.5036 - val_accuracy: 0.3399\n",
            "Epoch 2/25\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5277 - accuracy: 0.3429\n",
            "Epoch 00002: loss improved from 1.54961 to 1.52753, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f5b61ded208>_Batch25_LR0.001_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5275 - accuracy: 0.3429 - val_loss: 1.4948 - val_accuracy: 0.3399\n",
            "Epoch 3/25\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.5233 - accuracy: 0.3418\n",
            "Epoch 00003: loss improved from 1.52753 to 1.52306, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f5b61ded208>_Batch25_LR0.001_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5231 - accuracy: 0.3420 - val_loss: 1.4904 - val_accuracy: 0.3224\n",
            "Epoch 4/25\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5196 - accuracy: 0.3423\n",
            "Epoch 00004: loss improved from 1.52306 to 1.51946, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f5b61ded208>_Batch25_LR0.001_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5195 - accuracy: 0.3424 - val_loss: 1.4875 - val_accuracy: 0.3372\n",
            "Epoch 5/25\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5168 - accuracy: 0.3432\n",
            "Epoch 00005: loss improved from 1.51946 to 1.51672, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f5b61ded208>_Batch25_LR0.001_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5167 - accuracy: 0.3432 - val_loss: 1.5036 - val_accuracy: 0.3372\n",
            "Epoch 6/25\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.5151 - accuracy: 0.3447\n",
            "Epoch 00006: loss improved from 1.51672 to 1.51510, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f5b61ded208>_Batch25_LR0.001_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5151 - accuracy: 0.3447 - val_loss: 1.4885 - val_accuracy: 0.3372\n",
            "Epoch 7/25\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.5128 - accuracy: 0.3461\n",
            "Epoch 00007: loss improved from 1.51510 to 1.51256, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f5b61ded208>_Batch25_LR0.001_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5126 - accuracy: 0.3463 - val_loss: 1.5083 - val_accuracy: 0.3361\n",
            "Epoch 8/25\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5114 - accuracy: 0.3440\n",
            "Epoch 00008: loss improved from 1.51256 to 1.51128, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f5b61ded208>_Batch25_LR0.001_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5113 - accuracy: 0.3439 - val_loss: 1.5006 - val_accuracy: 0.3348\n",
            "Epoch 9/25\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5086 - accuracy: 0.3468\n",
            "Epoch 00009: loss improved from 1.51128 to 1.50812, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f5b61ded208>_Batch25_LR0.001_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5081 - accuracy: 0.3469 - val_loss: 1.4871 - val_accuracy: 0.3302\n",
            "Epoch 10/25\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5065 - accuracy: 0.3476\n",
            "Epoch 00010: loss improved from 1.50812 to 1.50689, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f5b61ded208>_Batch25_LR0.001_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5069 - accuracy: 0.3473 - val_loss: 1.4937 - val_accuracy: 0.3369\n",
            "Epoch 11/25\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.5041 - accuracy: 0.3462\n",
            "Epoch 00011: loss improved from 1.50689 to 1.50414, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f5b61ded208>_Batch25_LR0.001_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5041 - accuracy: 0.3462 - val_loss: 1.4974 - val_accuracy: 0.3345\n",
            "Epoch 12/25\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5026 - accuracy: 0.3454\n",
            "Epoch 00012: loss improved from 1.50414 to 1.50225, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f5b61ded208>_Batch25_LR0.001_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5023 - accuracy: 0.3457 - val_loss: 1.4946 - val_accuracy: 0.3219\n",
            "Epoch 13/25\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5018 - accuracy: 0.3485\n",
            "Epoch 00013: loss improved from 1.50225 to 1.50216, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f5b61ded208>_Batch25_LR0.001_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5022 - accuracy: 0.3481 - val_loss: 1.4966 - val_accuracy: 0.3294\n",
            "Epoch 14/25\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.4981 - accuracy: 0.3499\n",
            "Epoch 00014: loss improved from 1.50216 to 1.49781, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f5b61ded208>_Batch25_LR0.001_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.4978 - accuracy: 0.3500 - val_loss: 1.5049 - val_accuracy: 0.3124\n",
            "Epoch 15/25\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.4988 - accuracy: 0.3509\n",
            "Epoch 00015: loss did not improve from 1.49781\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.4990 - accuracy: 0.3507 - val_loss: 1.5013 - val_accuracy: 0.3200\n",
            "Epoch 16/25\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.4970 - accuracy: 0.3486\n",
            "Epoch 00016: loss improved from 1.49781 to 1.49698, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f5b61ded208>_Batch25_LR0.001_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.4970 - accuracy: 0.3484 - val_loss: 1.4958 - val_accuracy: 0.3294\n",
            "Epoch 17/25\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.4939 - accuracy: 0.3497\n",
            "Epoch 00017: loss improved from 1.49698 to 1.49405, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f5b61ded208>_Batch25_LR0.001_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.4941 - accuracy: 0.3496 - val_loss: 1.5069 - val_accuracy: 0.3087\n",
            "Epoch 18/25\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.4930 - accuracy: 0.3517\n",
            "Epoch 00018: loss improved from 1.49405 to 1.49286, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f5b61ded208>_Batch25_LR0.001_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.4929 - accuracy: 0.3519 - val_loss: 1.4970 - val_accuracy: 0.3291\n",
            "Epoch 19/25\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.4932 - accuracy: 0.3467\n",
            "Epoch 00019: loss did not improve from 1.49286\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.4931 - accuracy: 0.3466 - val_loss: 1.4949 - val_accuracy: 0.3130\n",
            "Epoch 20/25\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.4915 - accuracy: 0.3534\n",
            "Epoch 00020: loss improved from 1.49286 to 1.49150, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f5b61ded208>_Batch25_LR0.001_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.4915 - accuracy: 0.3534 - val_loss: 1.5029 - val_accuracy: 0.3081\n",
            "Epoch 21/25\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.4885 - accuracy: 0.3549\n",
            "Epoch 00021: loss improved from 1.49150 to 1.48841, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f5b61ded208>_Batch25_LR0.001_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.4884 - accuracy: 0.3549 - val_loss: 1.5132 - val_accuracy: 0.3130\n",
            "Epoch 22/25\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.4871 - accuracy: 0.3521\n",
            "Epoch 00022: loss improved from 1.48841 to 1.48767, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f5b61ded208>_Batch25_LR0.001_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.4877 - accuracy: 0.3516 - val_loss: 1.4946 - val_accuracy: 0.3315\n",
            "Epoch 23/25\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.4849 - accuracy: 0.3528\n",
            "Epoch 00023: loss improved from 1.48767 to 1.48474, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f5b61ded208>_Batch25_LR0.001_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.4847 - accuracy: 0.3528 - val_loss: 1.5146 - val_accuracy: 0.3221\n",
            "Epoch 24/25\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.4834 - accuracy: 0.3567\n",
            "Epoch 00024: loss improved from 1.48474 to 1.48375, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f5b61ded208>_Batch25_LR0.001_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.4837 - accuracy: 0.3569 - val_loss: 1.5243 - val_accuracy: 0.3122\n",
            "Epoch 25/25\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.4833 - accuracy: 0.3565\n",
            "Epoch 00025: loss improved from 1.48375 to 1.48263, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f5b61ded208>_Batch25_LR0.001_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.4826 - accuracy: 0.3568 - val_loss: 1.5092 - val_accuracy: 0.3399\n",
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
            "Epoch 1/25\n",
            "  2/595 [..............................] - ETA: 37s - loss: 2.1424 - accuracy: 0.1400WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0170s vs `on_train_batch_end` time: 0.1080s). Check your callbacks.\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5985 - accuracy: 0.3146\n",
            "Epoch 00001: loss improved from inf to 1.59869, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adagrad.Adagrad object at 0x7f59d1b66c50>_Batch25_LR0.001_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5987 - accuracy: 0.3144 - val_loss: 1.5194 - val_accuracy: 0.3399\n",
            "Epoch 2/25\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.5618 - accuracy: 0.3284\n",
            "Epoch 00002: loss improved from 1.59869 to 1.56176, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adagrad.Adagrad object at 0x7f59d1b66c50>_Batch25_LR0.001_Epochs25.hdf5\n",
            "595/595 [==============================] - 9s 15ms/step - loss: 1.5618 - accuracy: 0.3284 - val_loss: 1.5115 - val_accuracy: 0.3399\n",
            "Epoch 3/25\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5547 - accuracy: 0.3271\n",
            "Epoch 00003: loss improved from 1.56176 to 1.55475, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adagrad.Adagrad object at 0x7f59d1b66c50>_Batch25_LR0.001_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5548 - accuracy: 0.3272 - val_loss: 1.5069 - val_accuracy: 0.3399\n",
            "Epoch 4/25\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5489 - accuracy: 0.3295\n",
            "Epoch 00004: loss improved from 1.55475 to 1.54948, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adagrad.Adagrad object at 0x7f59d1b66c50>_Batch25_LR0.001_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5495 - accuracy: 0.3291 - val_loss: 1.5093 - val_accuracy: 0.3399\n",
            "Epoch 5/25\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5484 - accuracy: 0.3290\n",
            "Epoch 00005: loss improved from 1.54948 to 1.54797, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adagrad.Adagrad object at 0x7f59d1b66c50>_Batch25_LR0.001_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5480 - accuracy: 0.3295 - val_loss: 1.5071 - val_accuracy: 0.3399\n",
            "Epoch 6/25\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5460 - accuracy: 0.3311\n",
            "Epoch 00006: loss improved from 1.54797 to 1.54594, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adagrad.Adagrad object at 0x7f59d1b66c50>_Batch25_LR0.001_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5459 - accuracy: 0.3311 - val_loss: 1.5058 - val_accuracy: 0.3399\n",
            "Epoch 7/25\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5434 - accuracy: 0.3353\n",
            "Epoch 00007: loss improved from 1.54594 to 1.54391, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adagrad.Adagrad object at 0x7f59d1b66c50>_Batch25_LR0.001_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5439 - accuracy: 0.3349 - val_loss: 1.5040 - val_accuracy: 0.3399\n",
            "Epoch 8/25\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5402 - accuracy: 0.3349\n",
            "Epoch 00008: loss improved from 1.54391 to 1.54047, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adagrad.Adagrad object at 0x7f59d1b66c50>_Batch25_LR0.001_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5405 - accuracy: 0.3346 - val_loss: 1.5042 - val_accuracy: 0.3399\n",
            "Epoch 9/25\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5377 - accuracy: 0.3320\n",
            "Epoch 00009: loss improved from 1.54047 to 1.53778, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adagrad.Adagrad object at 0x7f59d1b66c50>_Batch25_LR0.001_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5378 - accuracy: 0.3321 - val_loss: 1.5055 - val_accuracy: 0.3399\n",
            "Epoch 10/25\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5346 - accuracy: 0.3371\n",
            "Epoch 00010: loss improved from 1.53778 to 1.53518, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adagrad.Adagrad object at 0x7f59d1b66c50>_Batch25_LR0.001_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5352 - accuracy: 0.3367 - val_loss: 1.5045 - val_accuracy: 0.3399\n",
            "Epoch 11/25\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5357 - accuracy: 0.3319\n",
            "Epoch 00011: loss did not improve from 1.53518\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5357 - accuracy: 0.3319 - val_loss: 1.5063 - val_accuracy: 0.3399\n",
            "Epoch 12/25\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5368 - accuracy: 0.3347\n",
            "Epoch 00012: loss did not improve from 1.53518\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5367 - accuracy: 0.3347 - val_loss: 1.5044 - val_accuracy: 0.3399\n",
            "Epoch 13/25\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5375 - accuracy: 0.3338\n",
            "Epoch 00013: loss did not improve from 1.53518\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5377 - accuracy: 0.3339 - val_loss: 1.5049 - val_accuracy: 0.3399\n",
            "Epoch 14/25\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5358 - accuracy: 0.3367\n",
            "Epoch 00014: loss did not improve from 1.53518\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5357 - accuracy: 0.3369 - val_loss: 1.5035 - val_accuracy: 0.3399\n",
            "Epoch 15/25\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.5363 - accuracy: 0.3365\n",
            "Epoch 00015: loss did not improve from 1.53518\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5357 - accuracy: 0.3371 - val_loss: 1.5039 - val_accuracy: 0.3399\n",
            "Epoch 16/25\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5348 - accuracy: 0.3356\n",
            "Epoch 00016: loss improved from 1.53518 to 1.53487, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adagrad.Adagrad object at 0x7f59d1b66c50>_Batch25_LR0.001_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5349 - accuracy: 0.3355 - val_loss: 1.5029 - val_accuracy: 0.3399\n",
            "Epoch 17/25\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5344 - accuracy: 0.3354\n",
            "Epoch 00017: loss improved from 1.53487 to 1.53424, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adagrad.Adagrad object at 0x7f59d1b66c50>_Batch25_LR0.001_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5342 - accuracy: 0.3356 - val_loss: 1.5028 - val_accuracy: 0.3399\n",
            "Epoch 18/25\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5347 - accuracy: 0.3370\n",
            "Epoch 00018: loss did not improve from 1.53424\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5347 - accuracy: 0.3369 - val_loss: 1.5032 - val_accuracy: 0.3399\n",
            "Epoch 19/25\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.5332 - accuracy: 0.3408\n",
            "Epoch 00019: loss improved from 1.53424 to 1.53361, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adagrad.Adagrad object at 0x7f59d1b66c50>_Batch25_LR0.001_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5336 - accuracy: 0.3405 - val_loss: 1.5025 - val_accuracy: 0.3399\n",
            "Epoch 20/25\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5320 - accuracy: 0.3413\n",
            "Epoch 00020: loss improved from 1.53361 to 1.53176, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adagrad.Adagrad object at 0x7f59d1b66c50>_Batch25_LR0.001_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5318 - accuracy: 0.3415 - val_loss: 1.5033 - val_accuracy: 0.3399\n",
            "Epoch 21/25\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5315 - accuracy: 0.3372\n",
            "Epoch 00021: loss improved from 1.53176 to 1.53164, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adagrad.Adagrad object at 0x7f59d1b66c50>_Batch25_LR0.001_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5316 - accuracy: 0.3371 - val_loss: 1.5039 - val_accuracy: 0.3399\n",
            "Epoch 22/25\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5318 - accuracy: 0.3401\n",
            "Epoch 00022: loss did not improve from 1.53164\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5320 - accuracy: 0.3403 - val_loss: 1.5035 - val_accuracy: 0.3399\n",
            "Epoch 23/25\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5325 - accuracy: 0.3372\n",
            "Epoch 00023: loss did not improve from 1.53164\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5327 - accuracy: 0.3370 - val_loss: 1.5038 - val_accuracy: 0.3399\n",
            "Epoch 24/25\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5291 - accuracy: 0.3414\n",
            "Epoch 00024: loss improved from 1.53164 to 1.52933, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adagrad.Adagrad object at 0x7f59d1b66c50>_Batch25_LR0.001_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5293 - accuracy: 0.3410 - val_loss: 1.5028 - val_accuracy: 0.3399\n",
            "Epoch 25/25\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5315 - accuracy: 0.3377\n",
            "Epoch 00025: loss did not improve from 1.52933\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5317 - accuracy: 0.3380 - val_loss: 1.5038 - val_accuracy: 0.3399\n",
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
            "Epoch 1/25\n",
            "  2/595 [..............................] - ETA: 35s - loss: 2.2563 - accuracy: 0.1000WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0149s vs `on_train_batch_end` time: 0.1030s). Check your callbacks.\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.5564 - accuracy: 0.3263\n",
            "Epoch 00001: loss improved from inf to 1.55637, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adamax.Adamax object at 0x7f59c909e1d0>_Batch25_LR0.001_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5564 - accuracy: 0.3263 - val_loss: 1.5220 - val_accuracy: 0.3399\n",
            "Epoch 2/25\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5350 - accuracy: 0.3360\n",
            "Epoch 00002: loss improved from 1.55637 to 1.53484, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adamax.Adamax object at 0x7f59c909e1d0>_Batch25_LR0.001_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5348 - accuracy: 0.3358 - val_loss: 1.4980 - val_accuracy: 0.3399\n",
            "Epoch 3/25\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5294 - accuracy: 0.3424\n",
            "Epoch 00003: loss improved from 1.53484 to 1.52974, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adamax.Adamax object at 0x7f59c909e1d0>_Batch25_LR0.001_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5297 - accuracy: 0.3421 - val_loss: 1.4993 - val_accuracy: 0.3399\n",
            "Epoch 4/25\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5241 - accuracy: 0.3391\n",
            "Epoch 00004: loss improved from 1.52974 to 1.52419, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adamax.Adamax object at 0x7f59c909e1d0>_Batch25_LR0.001_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5242 - accuracy: 0.3391 - val_loss: 1.5081 - val_accuracy: 0.3399\n",
            "Epoch 5/25\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5219 - accuracy: 0.3428\n",
            "Epoch 00005: loss improved from 1.52419 to 1.52203, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adamax.Adamax object at 0x7f59c909e1d0>_Batch25_LR0.001_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5220 - accuracy: 0.3426 - val_loss: 1.4936 - val_accuracy: 0.3399\n",
            "Epoch 6/25\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5191 - accuracy: 0.3419\n",
            "Epoch 00006: loss improved from 1.52203 to 1.51928, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adamax.Adamax object at 0x7f59c909e1d0>_Batch25_LR0.001_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5193 - accuracy: 0.3418 - val_loss: 1.4924 - val_accuracy: 0.3399\n",
            "Epoch 7/25\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5171 - accuracy: 0.3422\n",
            "Epoch 00007: loss improved from 1.51928 to 1.51689, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adamax.Adamax object at 0x7f59c909e1d0>_Batch25_LR0.001_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5169 - accuracy: 0.3423 - val_loss: 1.4939 - val_accuracy: 0.3399\n",
            "Epoch 8/25\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5163 - accuracy: 0.3422\n",
            "Epoch 00008: loss improved from 1.51689 to 1.51610, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adamax.Adamax object at 0x7f59c909e1d0>_Batch25_LR0.001_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5161 - accuracy: 0.3422 - val_loss: 1.4942 - val_accuracy: 0.3372\n",
            "Epoch 9/25\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5140 - accuracy: 0.3438\n",
            "Epoch 00009: loss improved from 1.51610 to 1.51376, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adamax.Adamax object at 0x7f59c909e1d0>_Batch25_LR0.001_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5138 - accuracy: 0.3437 - val_loss: 1.4962 - val_accuracy: 0.3372\n",
            "Epoch 10/25\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5101 - accuracy: 0.3447\n",
            "Epoch 00010: loss improved from 1.51376 to 1.51026, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adamax.Adamax object at 0x7f59c909e1d0>_Batch25_LR0.001_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5103 - accuracy: 0.3445 - val_loss: 1.4937 - val_accuracy: 0.3380\n",
            "Epoch 11/25\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.5095 - accuracy: 0.3437\n",
            "Epoch 00011: loss improved from 1.51026 to 1.50954, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adamax.Adamax object at 0x7f59c909e1d0>_Batch25_LR0.001_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5095 - accuracy: 0.3437 - val_loss: 1.4921 - val_accuracy: 0.3385\n",
            "Epoch 12/25\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5082 - accuracy: 0.3401\n",
            "Epoch 00012: loss improved from 1.50954 to 1.50815, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adamax.Adamax object at 0x7f59c909e1d0>_Batch25_LR0.001_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5081 - accuracy: 0.3402 - val_loss: 1.5015 - val_accuracy: 0.3375\n",
            "Epoch 13/25\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5081 - accuracy: 0.3456\n",
            "Epoch 00013: loss improved from 1.50815 to 1.50805, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adamax.Adamax object at 0x7f59c909e1d0>_Batch25_LR0.001_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5081 - accuracy: 0.3457 - val_loss: 1.4902 - val_accuracy: 0.3364\n",
            "Epoch 14/25\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5045 - accuracy: 0.3453\n",
            "Epoch 00014: loss improved from 1.50805 to 1.50479, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adamax.Adamax object at 0x7f59c909e1d0>_Batch25_LR0.001_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5048 - accuracy: 0.3457 - val_loss: 1.4946 - val_accuracy: 0.3356\n",
            "Epoch 15/25\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5048 - accuracy: 0.3452\n",
            "Epoch 00015: loss did not improve from 1.50479\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5049 - accuracy: 0.3452 - val_loss: 1.4905 - val_accuracy: 0.3361\n",
            "Epoch 16/25\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.5027 - accuracy: 0.3489\n",
            "Epoch 00016: loss improved from 1.50479 to 1.50248, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adamax.Adamax object at 0x7f59c909e1d0>_Batch25_LR0.001_Epochs25.hdf5\n",
            "595/595 [==============================] - 9s 14ms/step - loss: 1.5025 - accuracy: 0.3484 - val_loss: 1.4910 - val_accuracy: 0.3227\n",
            "Epoch 17/25\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5026 - accuracy: 0.3457\n",
            "Epoch 00017: loss did not improve from 1.50248\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5027 - accuracy: 0.3458 - val_loss: 1.4920 - val_accuracy: 0.3358\n",
            "Epoch 18/25\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5007 - accuracy: 0.3466\n",
            "Epoch 00018: loss improved from 1.50248 to 1.50078, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adamax.Adamax object at 0x7f59c909e1d0>_Batch25_LR0.001_Epochs25.hdf5\n",
            "595/595 [==============================] - 9s 15ms/step - loss: 1.5008 - accuracy: 0.3465 - val_loss: 1.4932 - val_accuracy: 0.3237\n",
            "Epoch 19/25\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5007 - accuracy: 0.3476\n",
            "Epoch 00019: loss improved from 1.50078 to 1.50071, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adamax.Adamax object at 0x7f59c909e1d0>_Batch25_LR0.001_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5007 - accuracy: 0.3472 - val_loss: 1.4969 - val_accuracy: 0.3353\n",
            "Epoch 20/25\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.4983 - accuracy: 0.3464\n",
            "Epoch 00020: loss improved from 1.50071 to 1.49821, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adamax.Adamax object at 0x7f59c909e1d0>_Batch25_LR0.001_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.4982 - accuracy: 0.3469 - val_loss: 1.5061 - val_accuracy: 0.3367\n",
            "Epoch 21/25\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.4974 - accuracy: 0.3492\n",
            "Epoch 00021: loss improved from 1.49821 to 1.49756, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adamax.Adamax object at 0x7f59c909e1d0>_Batch25_LR0.001_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.4976 - accuracy: 0.3494 - val_loss: 1.4918 - val_accuracy: 0.3410\n",
            "Epoch 22/25\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.4949 - accuracy: 0.3558\n",
            "Epoch 00022: loss improved from 1.49756 to 1.49498, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adamax.Adamax object at 0x7f59c909e1d0>_Batch25_LR0.001_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.4950 - accuracy: 0.3557 - val_loss: 1.4959 - val_accuracy: 0.3375\n",
            "Epoch 23/25\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.4952 - accuracy: 0.3508\n",
            "Epoch 00023: loss did not improve from 1.49498\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.4954 - accuracy: 0.3505 - val_loss: 1.4967 - val_accuracy: 0.3356\n",
            "Epoch 24/25\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.4957 - accuracy: 0.3497\n",
            "Epoch 00024: loss did not improve from 1.49498\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.4956 - accuracy: 0.3499 - val_loss: 1.4963 - val_accuracy: 0.3377\n",
            "Epoch 25/25\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.4926 - accuracy: 0.3525\n",
            "Epoch 00025: loss improved from 1.49498 to 1.49258, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adamax.Adamax object at 0x7f59c909e1d0>_Batch25_LR0.001_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.4926 - accuracy: 0.3526 - val_loss: 1.4902 - val_accuracy: 0.3283\n",
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
            "Epoch 1/25\n",
            "  2/595 [..............................] - ETA: 33s - loss: 2.1209 - accuracy: 0.0800WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0158s vs `on_train_batch_end` time: 0.0965s). Check your callbacks.\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.6698 - accuracy: 0.3214\n",
            "Epoch 00001: loss improved from inf to 1.66937, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.ftrl.Ftrl object at 0x7f59c8cb1e80>_Batch25_LR0.001_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.6694 - accuracy: 0.3216 - val_loss: 1.5491 - val_accuracy: 0.3399\n",
            "Epoch 2/25\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5571 - accuracy: 0.3437\n",
            "Epoch 00002: loss improved from 1.66937 to 1.55727, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.ftrl.Ftrl object at 0x7f59c8cb1e80>_Batch25_LR0.001_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5573 - accuracy: 0.3436 - val_loss: 1.5178 - val_accuracy: 0.3399\n",
            "Epoch 3/25\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5417 - accuracy: 0.3437\n",
            "Epoch 00003: loss improved from 1.55727 to 1.54183, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.ftrl.Ftrl object at 0x7f59c8cb1e80>_Batch25_LR0.001_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5418 - accuracy: 0.3436 - val_loss: 1.5095 - val_accuracy: 0.3399\n",
            "Epoch 4/25\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.5361 - accuracy: 0.3436\n",
            "Epoch 00004: loss improved from 1.54183 to 1.53613, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.ftrl.Ftrl object at 0x7f59c8cb1e80>_Batch25_LR0.001_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5361 - accuracy: 0.3436 - val_loss: 1.5045 - val_accuracy: 0.3399\n",
            "Epoch 5/25\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5332 - accuracy: 0.3438\n",
            "Epoch 00005: loss improved from 1.53613 to 1.53350, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.ftrl.Ftrl object at 0x7f59c8cb1e80>_Batch25_LR0.001_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5335 - accuracy: 0.3436 - val_loss: 1.5022 - val_accuracy: 0.3399\n",
            "Epoch 6/25\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5329 - accuracy: 0.3436\n",
            "Epoch 00006: loss improved from 1.53350 to 1.53267, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.ftrl.Ftrl object at 0x7f59c8cb1e80>_Batch25_LR0.001_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5327 - accuracy: 0.3436 - val_loss: 1.5012 - val_accuracy: 0.3399\n",
            "Epoch 7/25\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5309 - accuracy: 0.3436\n",
            "Epoch 00007: loss improved from 1.53267 to 1.53082, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.ftrl.Ftrl object at 0x7f59c8cb1e80>_Batch25_LR0.001_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5308 - accuracy: 0.3436 - val_loss: 1.4997 - val_accuracy: 0.3399\n",
            "Epoch 8/25\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.5305 - accuracy: 0.3437\n",
            "Epoch 00008: loss improved from 1.53082 to 1.53056, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.ftrl.Ftrl object at 0x7f59c8cb1e80>_Batch25_LR0.001_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5306 - accuracy: 0.3436 - val_loss: 1.4995 - val_accuracy: 0.3399\n",
            "Epoch 9/25\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5303 - accuracy: 0.3432\n",
            "Epoch 00009: loss improved from 1.53056 to 1.52996, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.ftrl.Ftrl object at 0x7f59c8cb1e80>_Batch25_LR0.001_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5300 - accuracy: 0.3436 - val_loss: 1.4995 - val_accuracy: 0.3399\n",
            "Epoch 10/25\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5296 - accuracy: 0.3436\n",
            "Epoch 00010: loss improved from 1.52996 to 1.52963, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.ftrl.Ftrl object at 0x7f59c8cb1e80>_Batch25_LR0.001_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5296 - accuracy: 0.3436 - val_loss: 1.4984 - val_accuracy: 0.3399\n",
            "Epoch 11/25\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5298 - accuracy: 0.3438\n",
            "Epoch 00011: loss did not improve from 1.52963\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5297 - accuracy: 0.3436 - val_loss: 1.4986 - val_accuracy: 0.3399\n",
            "Epoch 12/25\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5284 - accuracy: 0.3436\n",
            "Epoch 00012: loss improved from 1.52963 to 1.52869, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.ftrl.Ftrl object at 0x7f59c8cb1e80>_Batch25_LR0.001_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5287 - accuracy: 0.3436 - val_loss: 1.4984 - val_accuracy: 0.3399\n",
            "Epoch 13/25\n",
            "589/595 [============================>.] - ETA: 0s - loss: 1.5289 - accuracy: 0.3438\n",
            "Epoch 00013: loss did not improve from 1.52869\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5289 - accuracy: 0.3436 - val_loss: 1.4982 - val_accuracy: 0.3399\n",
            "Epoch 14/25\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5291 - accuracy: 0.3441\n",
            "Epoch 00014: loss did not improve from 1.52869\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5294 - accuracy: 0.3436 - val_loss: 1.4981 - val_accuracy: 0.3399\n",
            "Epoch 15/25\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5292 - accuracy: 0.3434\n",
            "Epoch 00015: loss did not improve from 1.52869\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5291 - accuracy: 0.3436 - val_loss: 1.4987 - val_accuracy: 0.3399\n",
            "Epoch 16/25\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5290 - accuracy: 0.3437\n",
            "Epoch 00016: loss did not improve from 1.52869\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5291 - accuracy: 0.3436 - val_loss: 1.4984 - val_accuracy: 0.3399\n",
            "Epoch 17/25\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.5290 - accuracy: 0.3436\n",
            "Epoch 00017: loss did not improve from 1.52869\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5290 - accuracy: 0.3436 - val_loss: 1.4980 - val_accuracy: 0.3399\n",
            "Epoch 18/25\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5293 - accuracy: 0.3434\n",
            "Epoch 00018: loss did not improve from 1.52869\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5292 - accuracy: 0.3436 - val_loss: 1.4981 - val_accuracy: 0.3399\n",
            "Epoch 19/25\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.5284 - accuracy: 0.3436\n",
            "Epoch 00019: loss improved from 1.52869 to 1.52842, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.ftrl.Ftrl object at 0x7f59c8cb1e80>_Batch25_LR0.001_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5284 - accuracy: 0.3436 - val_loss: 1.4979 - val_accuracy: 0.3399\n",
            "Epoch 20/25\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5287 - accuracy: 0.3438\n",
            "Epoch 00020: loss did not improve from 1.52842\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5285 - accuracy: 0.3437 - val_loss: 1.4980 - val_accuracy: 0.3399\n",
            "Epoch 21/25\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5288 - accuracy: 0.3436\n",
            "Epoch 00021: loss did not improve from 1.52842\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5287 - accuracy: 0.3436 - val_loss: 1.4978 - val_accuracy: 0.3399\n",
            "Epoch 22/25\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5292 - accuracy: 0.3433\n",
            "Epoch 00022: loss did not improve from 1.52842\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5291 - accuracy: 0.3436 - val_loss: 1.4980 - val_accuracy: 0.3399\n",
            "Epoch 23/25\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5290 - accuracy: 0.3436\n",
            "Epoch 00023: loss did not improve from 1.52842\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5291 - accuracy: 0.3436 - val_loss: 1.4981 - val_accuracy: 0.3399\n",
            "Epoch 24/25\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.5285 - accuracy: 0.3436\n",
            "Epoch 00024: loss did not improve from 1.52842\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5285 - accuracy: 0.3436 - val_loss: 1.4979 - val_accuracy: 0.3399\n",
            "Epoch 25/25\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5289 - accuracy: 0.3436\n",
            "Epoch 00025: loss did not improve from 1.52842\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5287 - accuracy: 0.3436 - val_loss: 1.4977 - val_accuracy: 0.3399\n",
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
            "Epoch 1/25\n",
            "  2/595 [..............................] - ETA: 40s - loss: 2.1576 - accuracy: 0.0600WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0153s vs `on_train_batch_end` time: 0.1207s). Check your callbacks.\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.5562 - accuracy: 0.3308\n",
            "Epoch 00001: loss improved from inf to 1.55579, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f59c8dc0f60>_Batch25_LR0.01_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5558 - accuracy: 0.3309 - val_loss: 1.4990 - val_accuracy: 0.3399\n",
            "Epoch 2/25\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.5389 - accuracy: 0.3372\n",
            "Epoch 00002: loss improved from 1.55579 to 1.53883, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f59c8dc0f60>_Batch25_LR0.01_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5388 - accuracy: 0.3374 - val_loss: 1.5072 - val_accuracy: 0.3399\n",
            "Epoch 3/25\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.5325 - accuracy: 0.3380\n",
            "Epoch 00003: loss improved from 1.53883 to 1.53253, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f59c8dc0f60>_Batch25_LR0.01_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5325 - accuracy: 0.3380 - val_loss: 1.5163 - val_accuracy: 0.3399\n",
            "Epoch 4/25\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5314 - accuracy: 0.3427\n",
            "Epoch 00004: loss improved from 1.53253 to 1.53152, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f59c8dc0f60>_Batch25_LR0.01_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5315 - accuracy: 0.3425 - val_loss: 1.5035 - val_accuracy: 0.3399\n",
            "Epoch 5/25\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5286 - accuracy: 0.3412\n",
            "Epoch 00005: loss improved from 1.53152 to 1.52848, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f59c8dc0f60>_Batch25_LR0.01_Epochs25.hdf5\n",
            "595/595 [==============================] - 9s 15ms/step - loss: 1.5285 - accuracy: 0.3415 - val_loss: 1.5018 - val_accuracy: 0.3399\n",
            "Epoch 6/25\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5268 - accuracy: 0.3419\n",
            "Epoch 00006: loss improved from 1.52848 to 1.52661, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f59c8dc0f60>_Batch25_LR0.01_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5266 - accuracy: 0.3420 - val_loss: 1.4989 - val_accuracy: 0.3399\n",
            "Epoch 7/25\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.5255 - accuracy: 0.3433\n",
            "Epoch 00007: loss improved from 1.52661 to 1.52547, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f59c8dc0f60>_Batch25_LR0.01_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5255 - accuracy: 0.3433 - val_loss: 1.5056 - val_accuracy: 0.3399\n",
            "Epoch 8/25\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.5247 - accuracy: 0.3436\n",
            "Epoch 00008: loss improved from 1.52547 to 1.52494, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f59c8dc0f60>_Batch25_LR0.01_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5249 - accuracy: 0.3434 - val_loss: 1.5007 - val_accuracy: 0.3399\n",
            "Epoch 9/25\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.5257 - accuracy: 0.3439\n",
            "Epoch 00009: loss did not improve from 1.52494\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5254 - accuracy: 0.3444 - val_loss: 1.5045 - val_accuracy: 0.3399\n",
            "Epoch 10/25\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5248 - accuracy: 0.3420\n",
            "Epoch 00010: loss improved from 1.52494 to 1.52479, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f59c8dc0f60>_Batch25_LR0.01_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5248 - accuracy: 0.3418 - val_loss: 1.4934 - val_accuracy: 0.3396\n",
            "Epoch 11/25\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5240 - accuracy: 0.3430\n",
            "Epoch 00011: loss improved from 1.52479 to 1.52407, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f59c8dc0f60>_Batch25_LR0.01_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5241 - accuracy: 0.3428 - val_loss: 1.5073 - val_accuracy: 0.3399\n",
            "Epoch 12/25\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.5242 - accuracy: 0.3437\n",
            "Epoch 00012: loss improved from 1.52407 to 1.52375, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f59c8dc0f60>_Batch25_LR0.01_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5238 - accuracy: 0.3440 - val_loss: 1.5033 - val_accuracy: 0.3399\n",
            "Epoch 13/25\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5226 - accuracy: 0.3438\n",
            "Epoch 00013: loss improved from 1.52375 to 1.52267, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f59c8dc0f60>_Batch25_LR0.01_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5227 - accuracy: 0.3434 - val_loss: 1.4944 - val_accuracy: 0.3399\n",
            "Epoch 14/25\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.5216 - accuracy: 0.3436\n",
            "Epoch 00014: loss improved from 1.52267 to 1.52157, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f59c8dc0f60>_Batch25_LR0.01_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5216 - accuracy: 0.3436 - val_loss: 1.4969 - val_accuracy: 0.3399\n",
            "Epoch 15/25\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5215 - accuracy: 0.3424\n",
            "Epoch 00015: loss did not improve from 1.52157\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5217 - accuracy: 0.3422 - val_loss: 1.5020 - val_accuracy: 0.3399\n",
            "Epoch 16/25\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.5214 - accuracy: 0.3443\n",
            "Epoch 00016: loss did not improve from 1.52157\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5217 - accuracy: 0.3442 - val_loss: 1.4994 - val_accuracy: 0.3399\n",
            "Epoch 17/25\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5212 - accuracy: 0.3424\n",
            "Epoch 00017: loss improved from 1.52157 to 1.52112, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f59c8dc0f60>_Batch25_LR0.01_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5211 - accuracy: 0.3424 - val_loss: 1.4951 - val_accuracy: 0.3399\n",
            "Epoch 18/25\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5206 - accuracy: 0.3428\n",
            "Epoch 00018: loss improved from 1.52112 to 1.52049, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f59c8dc0f60>_Batch25_LR0.01_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5205 - accuracy: 0.3429 - val_loss: 1.4941 - val_accuracy: 0.3399\n",
            "Epoch 19/25\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.5202 - accuracy: 0.3428\n",
            "Epoch 00019: loss improved from 1.52049 to 1.52019, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f59c8dc0f60>_Batch25_LR0.01_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5202 - accuracy: 0.3428 - val_loss: 1.5056 - val_accuracy: 0.3399\n",
            "Epoch 20/25\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5191 - accuracy: 0.3427\n",
            "Epoch 00020: loss improved from 1.52019 to 1.51922, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f59c8dc0f60>_Batch25_LR0.01_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5192 - accuracy: 0.3430 - val_loss: 1.5182 - val_accuracy: 0.3399\n",
            "Epoch 21/25\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.5188 - accuracy: 0.3432\n",
            "Epoch 00021: loss improved from 1.51922 to 1.51883, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f59c8dc0f60>_Batch25_LR0.01_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5188 - accuracy: 0.3432 - val_loss: 1.4992 - val_accuracy: 0.3399\n",
            "Epoch 22/25\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.5177 - accuracy: 0.3433\n",
            "Epoch 00022: loss improved from 1.51883 to 1.51818, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f59c8dc0f60>_Batch25_LR0.01_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5182 - accuracy: 0.3432 - val_loss: 1.5033 - val_accuracy: 0.3399\n",
            "Epoch 23/25\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5161 - accuracy: 0.3443\n",
            "Epoch 00023: loss improved from 1.51818 to 1.51637, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f59c8dc0f60>_Batch25_LR0.01_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5164 - accuracy: 0.3447 - val_loss: 1.5086 - val_accuracy: 0.3399\n",
            "Epoch 24/25\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5167 - accuracy: 0.3447\n",
            "Epoch 00024: loss did not improve from 1.51637\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5168 - accuracy: 0.3446 - val_loss: 1.4969 - val_accuracy: 0.3399\n",
            "Epoch 25/25\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5174 - accuracy: 0.3442\n",
            "Epoch 00025: loss did not improve from 1.51637\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5173 - accuracy: 0.3443 - val_loss: 1.5104 - val_accuracy: 0.3399\n",
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
            "Epoch 1/25\n",
            "  2/595 [..............................] - ETA: 50s - loss: 3.5273 - accuracy: 0.2000WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0208s vs `on_train_batch_end` time: 0.1441s). Check your callbacks.\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.5615 - accuracy: 0.3328\n",
            "Epoch 00001: loss improved from inf to 1.56149, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.rmsprop.RMSprop object at 0x7f59c8aa62e8>_Batch25_LR0.01_Epochs25.hdf5\n",
            "595/595 [==============================] - 9s 14ms/step - loss: 1.5615 - accuracy: 0.3328 - val_loss: 1.5115 - val_accuracy: 0.3399\n",
            "Epoch 2/25\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5301 - accuracy: 0.3432\n",
            "Epoch 00002: loss improved from 1.56149 to 1.52966, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.rmsprop.RMSprop object at 0x7f59c8aa62e8>_Batch25_LR0.01_Epochs25.hdf5\n",
            "595/595 [==============================] - 9s 14ms/step - loss: 1.5297 - accuracy: 0.3436 - val_loss: 1.4937 - val_accuracy: 0.3399\n",
            "Epoch 3/25\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.5294 - accuracy: 0.3435\n",
            "Epoch 00003: loss improved from 1.52966 to 1.52937, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.rmsprop.RMSprop object at 0x7f59c8aa62e8>_Batch25_LR0.01_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5294 - accuracy: 0.3436 - val_loss: 1.4940 - val_accuracy: 0.3399\n",
            "Epoch 4/25\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5294 - accuracy: 0.3441\n",
            "Epoch 00004: loss did not improve from 1.52937\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5295 - accuracy: 0.3436 - val_loss: 1.4967 - val_accuracy: 0.3399\n",
            "Epoch 5/25\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5297 - accuracy: 0.3436\n",
            "Epoch 00005: loss did not improve from 1.52937\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5295 - accuracy: 0.3436 - val_loss: 1.4932 - val_accuracy: 0.3399\n",
            "Epoch 6/25\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5296 - accuracy: 0.3437\n",
            "Epoch 00006: loss did not improve from 1.52937\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5297 - accuracy: 0.3436 - val_loss: 1.4963 - val_accuracy: 0.3399\n",
            "Epoch 7/25\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.5295 - accuracy: 0.3439\n",
            "Epoch 00007: loss did not improve from 1.52937\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5295 - accuracy: 0.3436 - val_loss: 1.4966 - val_accuracy: 0.3399\n",
            "Epoch 8/25\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5300 - accuracy: 0.3431\n",
            "Epoch 00008: loss did not improve from 1.52937\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5298 - accuracy: 0.3436 - val_loss: 1.4957 - val_accuracy: 0.3399\n",
            "Epoch 9/25\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5299 - accuracy: 0.3436\n",
            "Epoch 00009: loss did not improve from 1.52937\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5297 - accuracy: 0.3436 - val_loss: 1.4959 - val_accuracy: 0.3399\n",
            "Epoch 10/25\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.5302 - accuracy: 0.3436\n",
            "Epoch 00010: loss did not improve from 1.52937\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5302 - accuracy: 0.3436 - val_loss: 1.4923 - val_accuracy: 0.3399\n",
            "Epoch 11/25\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5304 - accuracy: 0.3436\n",
            "Epoch 00011: loss did not improve from 1.52937\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5305 - accuracy: 0.3436 - val_loss: 1.4912 - val_accuracy: 0.3399\n",
            "Epoch 12/25\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5307 - accuracy: 0.3435\n",
            "Epoch 00012: loss did not improve from 1.52937\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5306 - accuracy: 0.3436 - val_loss: 1.4987 - val_accuracy: 0.3399\n",
            "Epoch 13/25\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.5302 - accuracy: 0.3436\n",
            "Epoch 00013: loss did not improve from 1.52937\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5302 - accuracy: 0.3436 - val_loss: 1.4929 - val_accuracy: 0.3399\n",
            "Epoch 14/25\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5303 - accuracy: 0.3438\n",
            "Epoch 00014: loss did not improve from 1.52937\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5302 - accuracy: 0.3436 - val_loss: 1.4968 - val_accuracy: 0.3399\n",
            "Epoch 15/25\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.5299 - accuracy: 0.3436\n",
            "Epoch 00015: loss did not improve from 1.52937\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5299 - accuracy: 0.3436 - val_loss: 1.4945 - val_accuracy: 0.3399\n",
            "Epoch 16/25\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5303 - accuracy: 0.3436\n",
            "Epoch 00016: loss did not improve from 1.52937\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5303 - accuracy: 0.3436 - val_loss: 1.4920 - val_accuracy: 0.3399\n",
            "Epoch 17/25\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.5296 - accuracy: 0.3439\n",
            "Epoch 00017: loss did not improve from 1.52937\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5299 - accuracy: 0.3436 - val_loss: 1.4910 - val_accuracy: 0.3399\n",
            "Epoch 18/25\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5309 - accuracy: 0.3438\n",
            "Epoch 00018: loss did not improve from 1.52937\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5310 - accuracy: 0.3436 - val_loss: 1.4943 - val_accuracy: 0.3399\n",
            "Epoch 19/25\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5307 - accuracy: 0.3431\n",
            "Epoch 00019: loss did not improve from 1.52937\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5303 - accuracy: 0.3436 - val_loss: 1.4973 - val_accuracy: 0.3399\n",
            "Epoch 20/25\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5301 - accuracy: 0.3438\n",
            "Epoch 00020: loss did not improve from 1.52937\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5301 - accuracy: 0.3436 - val_loss: 1.4909 - val_accuracy: 0.3399\n",
            "Epoch 21/25\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5302 - accuracy: 0.3439\n",
            "Epoch 00021: loss did not improve from 1.52937\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5301 - accuracy: 0.3436 - val_loss: 1.4938 - val_accuracy: 0.3399\n",
            "Epoch 22/25\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5303 - accuracy: 0.3432\n",
            "Epoch 00022: loss did not improve from 1.52937\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5298 - accuracy: 0.3436 - val_loss: 1.4922 - val_accuracy: 0.3399\n",
            "Epoch 23/25\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.5295 - accuracy: 0.3436\n",
            "Epoch 00023: loss did not improve from 1.52937\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5295 - accuracy: 0.3436 - val_loss: 1.4925 - val_accuracy: 0.3399\n",
            "Epoch 24/25\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.5305 - accuracy: 0.3432\n",
            "Epoch 00024: loss did not improve from 1.52937\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5303 - accuracy: 0.3436 - val_loss: 1.4978 - val_accuracy: 0.3399\n",
            "Epoch 25/25\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5299 - accuracy: 0.3436\n",
            "Epoch 00025: loss did not improve from 1.52937\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5297 - accuracy: 0.3436 - val_loss: 1.4975 - val_accuracy: 0.3399\n",
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
            "Epoch 1/25\n",
            "  2/595 [..............................] - ETA: 33s - loss: 1.9792 - accuracy: 0.2200WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0152s vs `on_train_batch_end` time: 0.1007s). Check your callbacks.\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5536 - accuracy: 0.3335\n",
            "Epoch 00001: loss improved from inf to 1.55335, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f59c8bc5828>_Batch25_LR0.01_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5533 - accuracy: 0.3338 - val_loss: 1.4947 - val_accuracy: 0.3323\n",
            "Epoch 2/25\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5327 - accuracy: 0.3410\n",
            "Epoch 00002: loss improved from 1.55335 to 1.53266, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f59c8bc5828>_Batch25_LR0.01_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5327 - accuracy: 0.3411 - val_loss: 1.5036 - val_accuracy: 0.3399\n",
            "Epoch 3/25\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5295 - accuracy: 0.3437\n",
            "Epoch 00003: loss improved from 1.53266 to 1.52941, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f59c8bc5828>_Batch25_LR0.01_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5294 - accuracy: 0.3436 - val_loss: 1.4982 - val_accuracy: 0.3399\n",
            "Epoch 4/25\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5288 - accuracy: 0.3439\n",
            "Epoch 00004: loss improved from 1.52941 to 1.52909, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f59c8bc5828>_Batch25_LR0.01_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5291 - accuracy: 0.3436 - val_loss: 1.4991 - val_accuracy: 0.3399\n",
            "Epoch 5/25\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5289 - accuracy: 0.3434\n",
            "Epoch 00005: loss improved from 1.52909 to 1.52907, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f59c8bc5828>_Batch25_LR0.01_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5291 - accuracy: 0.3436 - val_loss: 1.4947 - val_accuracy: 0.3399\n",
            "Epoch 6/25\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.5291 - accuracy: 0.3436\n",
            "Epoch 00006: loss did not improve from 1.52907\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5291 - accuracy: 0.3436 - val_loss: 1.4954 - val_accuracy: 0.3399\n",
            "Epoch 7/25\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.5288 - accuracy: 0.3436\n",
            "Epoch 00007: loss improved from 1.52907 to 1.52881, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f59c8bc5828>_Batch25_LR0.01_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5288 - accuracy: 0.3436 - val_loss: 1.4999 - val_accuracy: 0.3399\n",
            "Epoch 8/25\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5288 - accuracy: 0.3437\n",
            "Epoch 00008: loss did not improve from 1.52881\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5289 - accuracy: 0.3436 - val_loss: 1.4978 - val_accuracy: 0.3399\n",
            "Epoch 9/25\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5293 - accuracy: 0.3436\n",
            "Epoch 00009: loss did not improve from 1.52881\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5292 - accuracy: 0.3436 - val_loss: 1.5000 - val_accuracy: 0.3399\n",
            "Epoch 10/25\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.5291 - accuracy: 0.3436\n",
            "Epoch 00010: loss did not improve from 1.52881\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5291 - accuracy: 0.3436 - val_loss: 1.4912 - val_accuracy: 0.3399\n",
            "Epoch 11/25\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.5289 - accuracy: 0.3436\n",
            "Epoch 00011: loss did not improve from 1.52881\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5289 - accuracy: 0.3436 - val_loss: 1.4949 - val_accuracy: 0.3399\n",
            "Epoch 12/25\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5291 - accuracy: 0.3432\n",
            "Epoch 00012: loss did not improve from 1.52881\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5290 - accuracy: 0.3436 - val_loss: 1.4972 - val_accuracy: 0.3399\n",
            "Epoch 13/25\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.5291 - accuracy: 0.3434\n",
            "Epoch 00013: loss did not improve from 1.52881\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5290 - accuracy: 0.3436 - val_loss: 1.4924 - val_accuracy: 0.3399\n",
            "Epoch 14/25\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.5294 - accuracy: 0.3435\n",
            "Epoch 00014: loss did not improve from 1.52881\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5293 - accuracy: 0.3436 - val_loss: 1.4974 - val_accuracy: 0.3399\n",
            "Epoch 15/25\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5289 - accuracy: 0.3438\n",
            "Epoch 00015: loss did not improve from 1.52881\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5288 - accuracy: 0.3436 - val_loss: 1.5003 - val_accuracy: 0.3399\n",
            "Epoch 16/25\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5285 - accuracy: 0.3440\n",
            "Epoch 00016: loss improved from 1.52881 to 1.52861, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f59c8bc5828>_Batch25_LR0.01_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5286 - accuracy: 0.3436 - val_loss: 1.4977 - val_accuracy: 0.3399\n",
            "Epoch 17/25\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5289 - accuracy: 0.3435\n",
            "Epoch 00017: loss did not improve from 1.52861\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5292 - accuracy: 0.3436 - val_loss: 1.4991 - val_accuracy: 0.3399\n",
            "Epoch 18/25\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5294 - accuracy: 0.3436\n",
            "Epoch 00018: loss did not improve from 1.52861\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5293 - accuracy: 0.3436 - val_loss: 1.4997 - val_accuracy: 0.3399\n",
            "Epoch 19/25\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5286 - accuracy: 0.3438\n",
            "Epoch 00019: loss did not improve from 1.52861\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5289 - accuracy: 0.3436 - val_loss: 1.4967 - val_accuracy: 0.3399\n",
            "Epoch 20/25\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5289 - accuracy: 0.3437\n",
            "Epoch 00020: loss did not improve from 1.52861\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5291 - accuracy: 0.3436 - val_loss: 1.5008 - val_accuracy: 0.3399\n",
            "Epoch 21/25\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5289 - accuracy: 0.3438\n",
            "Epoch 00021: loss did not improve from 1.52861\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5290 - accuracy: 0.3436 - val_loss: 1.4944 - val_accuracy: 0.3399\n",
            "Epoch 22/25\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5288 - accuracy: 0.3437\n",
            "Epoch 00022: loss did not improve from 1.52861\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5289 - accuracy: 0.3436 - val_loss: 1.4968 - val_accuracy: 0.3399\n",
            "Epoch 23/25\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5291 - accuracy: 0.3436\n",
            "Epoch 00023: loss did not improve from 1.52861\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5289 - accuracy: 0.3436 - val_loss: 1.4975 - val_accuracy: 0.3399\n",
            "Epoch 24/25\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.5289 - accuracy: 0.3436\n",
            "Epoch 00024: loss did not improve from 1.52861\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5289 - accuracy: 0.3436 - val_loss: 1.4962 - val_accuracy: 0.3399\n",
            "Epoch 25/25\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5288 - accuracy: 0.3440\n",
            "Epoch 00025: loss did not improve from 1.52861\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5287 - accuracy: 0.3436 - val_loss: 1.4947 - val_accuracy: 0.3399\n",
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
            "Epoch 1/25\n",
            "  2/595 [..............................] - ETA: 34s - loss: 2.0010 - accuracy: 0.1400WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0165s vs `on_train_batch_end` time: 0.0975s). Check your callbacks.\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.5469 - accuracy: 0.3292\n",
            "Epoch 00001: loss improved from inf to 1.54695, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adagrad.Adagrad object at 0x7f59c8ff0e80>_Batch25_LR0.01_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5469 - accuracy: 0.3292 - val_loss: 1.5165 - val_accuracy: 0.3399\n",
            "Epoch 2/25\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5330 - accuracy: 0.3392\n",
            "Epoch 00002: loss improved from 1.54695 to 1.53307, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adagrad.Adagrad object at 0x7f59c8ff0e80>_Batch25_LR0.01_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5331 - accuracy: 0.3391 - val_loss: 1.5040 - val_accuracy: 0.3399\n",
            "Epoch 3/25\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5280 - accuracy: 0.3436\n",
            "Epoch 00003: loss improved from 1.53307 to 1.52826, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adagrad.Adagrad object at 0x7f59c8ff0e80>_Batch25_LR0.01_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5283 - accuracy: 0.3430 - val_loss: 1.5046 - val_accuracy: 0.3399\n",
            "Epoch 4/25\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.5257 - accuracy: 0.3422\n",
            "Epoch 00004: loss improved from 1.52826 to 1.52569, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adagrad.Adagrad object at 0x7f59c8ff0e80>_Batch25_LR0.01_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5257 - accuracy: 0.3422 - val_loss: 1.5107 - val_accuracy: 0.3399\n",
            "Epoch 5/25\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.5236 - accuracy: 0.3441\n",
            "Epoch 00005: loss improved from 1.52569 to 1.52361, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adagrad.Adagrad object at 0x7f59c8ff0e80>_Batch25_LR0.01_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5236 - accuracy: 0.3441 - val_loss: 1.4989 - val_accuracy: 0.3399\n",
            "Epoch 6/25\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5225 - accuracy: 0.3432\n",
            "Epoch 00006: loss improved from 1.52361 to 1.52262, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adagrad.Adagrad object at 0x7f59c8ff0e80>_Batch25_LR0.01_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5226 - accuracy: 0.3433 - val_loss: 1.5001 - val_accuracy: 0.3399\n",
            "Epoch 7/25\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5218 - accuracy: 0.3446\n",
            "Epoch 00007: loss improved from 1.52262 to 1.52158, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adagrad.Adagrad object at 0x7f59c8ff0e80>_Batch25_LR0.01_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5216 - accuracy: 0.3443 - val_loss: 1.4977 - val_accuracy: 0.3399\n",
            "Epoch 8/25\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.5198 - accuracy: 0.3424\n",
            "Epoch 00008: loss improved from 1.52158 to 1.51971, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adagrad.Adagrad object at 0x7f59c8ff0e80>_Batch25_LR0.01_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5197 - accuracy: 0.3428 - val_loss: 1.5040 - val_accuracy: 0.3399\n",
            "Epoch 9/25\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.5189 - accuracy: 0.3431\n",
            "Epoch 00009: loss improved from 1.51971 to 1.51921, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adagrad.Adagrad object at 0x7f59c8ff0e80>_Batch25_LR0.01_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5192 - accuracy: 0.3434 - val_loss: 1.4987 - val_accuracy: 0.3399\n",
            "Epoch 10/25\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.5179 - accuracy: 0.3433\n",
            "Epoch 00010: loss improved from 1.51921 to 1.51790, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adagrad.Adagrad object at 0x7f59c8ff0e80>_Batch25_LR0.01_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5179 - accuracy: 0.3433 - val_loss: 1.4942 - val_accuracy: 0.3399\n",
            "Epoch 11/25\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5169 - accuracy: 0.3432\n",
            "Epoch 00011: loss improved from 1.51790 to 1.51679, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adagrad.Adagrad object at 0x7f59c8ff0e80>_Batch25_LR0.01_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5168 - accuracy: 0.3434 - val_loss: 1.4967 - val_accuracy: 0.3399\n",
            "Epoch 12/25\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.5161 - accuracy: 0.3459\n",
            "Epoch 00012: loss improved from 1.51679 to 1.51613, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adagrad.Adagrad object at 0x7f59c8ff0e80>_Batch25_LR0.01_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5161 - accuracy: 0.3459 - val_loss: 1.4969 - val_accuracy: 0.3399\n",
            "Epoch 13/25\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5153 - accuracy: 0.3431\n",
            "Epoch 00013: loss improved from 1.51613 to 1.51543, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adagrad.Adagrad object at 0x7f59c8ff0e80>_Batch25_LR0.01_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5154 - accuracy: 0.3430 - val_loss: 1.4968 - val_accuracy: 0.3399\n",
            "Epoch 14/25\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5139 - accuracy: 0.3450\n",
            "Epoch 00014: loss improved from 1.51543 to 1.51426, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adagrad.Adagrad object at 0x7f59c8ff0e80>_Batch25_LR0.01_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5143 - accuracy: 0.3448 - val_loss: 1.4954 - val_accuracy: 0.3399\n",
            "Epoch 15/25\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5142 - accuracy: 0.3452\n",
            "Epoch 00015: loss did not improve from 1.51426\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5143 - accuracy: 0.3451 - val_loss: 1.4909 - val_accuracy: 0.3399\n",
            "Epoch 16/25\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5143 - accuracy: 0.3444\n",
            "Epoch 00016: loss improved from 1.51426 to 1.51413, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adagrad.Adagrad object at 0x7f59c8ff0e80>_Batch25_LR0.01_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5141 - accuracy: 0.3443 - val_loss: 1.4950 - val_accuracy: 0.3399\n",
            "Epoch 17/25\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5128 - accuracy: 0.3468\n",
            "Epoch 00017: loss improved from 1.51413 to 1.51348, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adagrad.Adagrad object at 0x7f59c8ff0e80>_Batch25_LR0.01_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5135 - accuracy: 0.3463 - val_loss: 1.4960 - val_accuracy: 0.3396\n",
            "Epoch 18/25\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.5123 - accuracy: 0.3435\n",
            "Epoch 00018: loss improved from 1.51348 to 1.51233, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adagrad.Adagrad object at 0x7f59c8ff0e80>_Batch25_LR0.01_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5123 - accuracy: 0.3435 - val_loss: 1.4957 - val_accuracy: 0.3399\n",
            "Epoch 19/25\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5118 - accuracy: 0.3470\n",
            "Epoch 00019: loss improved from 1.51233 to 1.51209, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adagrad.Adagrad object at 0x7f59c8ff0e80>_Batch25_LR0.01_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5121 - accuracy: 0.3469 - val_loss: 1.4970 - val_accuracy: 0.3361\n",
            "Epoch 20/25\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5123 - accuracy: 0.3465\n",
            "Epoch 00020: loss did not improve from 1.51209\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5122 - accuracy: 0.3466 - val_loss: 1.4968 - val_accuracy: 0.3399\n",
            "Epoch 21/25\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5118 - accuracy: 0.3465\n",
            "Epoch 00021: loss improved from 1.51209 to 1.51166, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adagrad.Adagrad object at 0x7f59c8ff0e80>_Batch25_LR0.01_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5117 - accuracy: 0.3469 - val_loss: 1.4954 - val_accuracy: 0.3399\n",
            "Epoch 22/25\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5104 - accuracy: 0.3455\n",
            "Epoch 00022: loss improved from 1.51166 to 1.51054, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adagrad.Adagrad object at 0x7f59c8ff0e80>_Batch25_LR0.01_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5105 - accuracy: 0.3456 - val_loss: 1.4963 - val_accuracy: 0.3372\n",
            "Epoch 23/25\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5105 - accuracy: 0.3460\n",
            "Epoch 00023: loss did not improve from 1.51054\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5107 - accuracy: 0.3459 - val_loss: 1.4945 - val_accuracy: 0.3348\n",
            "Epoch 24/25\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5088 - accuracy: 0.3446\n",
            "Epoch 00024: loss improved from 1.51054 to 1.50878, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adagrad.Adagrad object at 0x7f59c8ff0e80>_Batch25_LR0.01_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5088 - accuracy: 0.3447 - val_loss: 1.4971 - val_accuracy: 0.3369\n",
            "Epoch 25/25\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.5100 - accuracy: 0.3434\n",
            "Epoch 00025: loss did not improve from 1.50878\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5100 - accuracy: 0.3434 - val_loss: 1.4946 - val_accuracy: 0.3358\n",
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
            "Epoch 1/25\n",
            "  2/595 [..............................] - ETA: 43s - loss: 2.1160 - accuracy: 0.2600WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0177s vs `on_train_batch_end` time: 0.1277s). Check your callbacks.\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5451 - accuracy: 0.3324\n",
            "Epoch 00001: loss improved from inf to 1.54523, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adamax.Adamax object at 0x7f59c8bcde48>_Batch25_LR0.01_Epochs25.hdf5\n",
            "595/595 [==============================] - 9s 14ms/step - loss: 1.5452 - accuracy: 0.3324 - val_loss: 1.5013 - val_accuracy: 0.3399\n",
            "Epoch 2/25\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5313 - accuracy: 0.3397\n",
            "Epoch 00002: loss improved from 1.54523 to 1.53107, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adamax.Adamax object at 0x7f59c8bcde48>_Batch25_LR0.01_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5311 - accuracy: 0.3400 - val_loss: 1.4999 - val_accuracy: 0.3399\n",
            "Epoch 3/25\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.5271 - accuracy: 0.3432\n",
            "Epoch 00003: loss improved from 1.53107 to 1.52712, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adamax.Adamax object at 0x7f59c8bcde48>_Batch25_LR0.01_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5271 - accuracy: 0.3432 - val_loss: 1.4883 - val_accuracy: 0.3399\n",
            "Epoch 4/25\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.5238 - accuracy: 0.3425\n",
            "Epoch 00004: loss improved from 1.52712 to 1.52378, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adamax.Adamax object at 0x7f59c8bcde48>_Batch25_LR0.01_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5238 - accuracy: 0.3425 - val_loss: 1.4862 - val_accuracy: 0.3372\n",
            "Epoch 5/25\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5213 - accuracy: 0.3443\n",
            "Epoch 00005: loss improved from 1.52378 to 1.52140, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adamax.Adamax object at 0x7f59c8bcde48>_Batch25_LR0.01_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5214 - accuracy: 0.3441 - val_loss: 1.4954 - val_accuracy: 0.3372\n",
            "Epoch 6/25\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5191 - accuracy: 0.3420\n",
            "Epoch 00006: loss improved from 1.52140 to 1.51938, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adamax.Adamax object at 0x7f59c8bcde48>_Batch25_LR0.01_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5194 - accuracy: 0.3418 - val_loss: 1.4909 - val_accuracy: 0.3372\n",
            "Epoch 7/25\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.5154 - accuracy: 0.3408\n",
            "Epoch 00007: loss improved from 1.51938 to 1.51537, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adamax.Adamax object at 0x7f59c8bcde48>_Batch25_LR0.01_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5154 - accuracy: 0.3408 - val_loss: 1.4971 - val_accuracy: 0.3380\n",
            "Epoch 8/25\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5163 - accuracy: 0.3457\n",
            "Epoch 00008: loss did not improve from 1.51537\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5169 - accuracy: 0.3457 - val_loss: 1.5035 - val_accuracy: 0.3380\n",
            "Epoch 9/25\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.5146 - accuracy: 0.3439\n",
            "Epoch 00009: loss improved from 1.51537 to 1.51445, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adamax.Adamax object at 0x7f59c8bcde48>_Batch25_LR0.01_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5144 - accuracy: 0.3443 - val_loss: 1.5064 - val_accuracy: 0.3396\n",
            "Epoch 10/25\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5111 - accuracy: 0.3425\n",
            "Epoch 00010: loss improved from 1.51445 to 1.51114, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adamax.Adamax object at 0x7f59c8bcde48>_Batch25_LR0.01_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5111 - accuracy: 0.3427 - val_loss: 1.5258 - val_accuracy: 0.3334\n",
            "Epoch 11/25\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5105 - accuracy: 0.3425\n",
            "Epoch 00011: loss improved from 1.51114 to 1.51018, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adamax.Adamax object at 0x7f59c8bcde48>_Batch25_LR0.01_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5102 - accuracy: 0.3428 - val_loss: 1.4969 - val_accuracy: 0.3219\n",
            "Epoch 12/25\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5077 - accuracy: 0.3461\n",
            "Epoch 00012: loss improved from 1.51018 to 1.50776, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adamax.Adamax object at 0x7f59c8bcde48>_Batch25_LR0.01_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5078 - accuracy: 0.3461 - val_loss: 1.4837 - val_accuracy: 0.3399\n",
            "Epoch 13/25\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5055 - accuracy: 0.3414\n",
            "Epoch 00013: loss improved from 1.50776 to 1.50534, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adamax.Adamax object at 0x7f59c8bcde48>_Batch25_LR0.01_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5053 - accuracy: 0.3412 - val_loss: 1.4943 - val_accuracy: 0.3186\n",
            "Epoch 14/25\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5063 - accuracy: 0.3429\n",
            "Epoch 00014: loss did not improve from 1.50534\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5060 - accuracy: 0.3428 - val_loss: 1.4863 - val_accuracy: 0.3259\n",
            "Epoch 15/25\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5054 - accuracy: 0.3469\n",
            "Epoch 00015: loss did not improve from 1.50534\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5059 - accuracy: 0.3471 - val_loss: 1.4820 - val_accuracy: 0.3332\n",
            "Epoch 16/25\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5019 - accuracy: 0.3453\n",
            "Epoch 00016: loss improved from 1.50534 to 1.50112, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adamax.Adamax object at 0x7f59c8bcde48>_Batch25_LR0.01_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5011 - accuracy: 0.3459 - val_loss: 1.4956 - val_accuracy: 0.3332\n",
            "Epoch 17/25\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.5033 - accuracy: 0.3467\n",
            "Epoch 00017: loss did not improve from 1.50112\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5033 - accuracy: 0.3467 - val_loss: 1.4913 - val_accuracy: 0.3393\n",
            "Epoch 18/25\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.4995 - accuracy: 0.3488\n",
            "Epoch 00018: loss improved from 1.50112 to 1.49953, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adamax.Adamax object at 0x7f59c8bcde48>_Batch25_LR0.01_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.4995 - accuracy: 0.3488 - val_loss: 1.4847 - val_accuracy: 0.3307\n",
            "Epoch 19/25\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.4992 - accuracy: 0.3455\n",
            "Epoch 00019: loss did not improve from 1.49953\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.4996 - accuracy: 0.3453 - val_loss: 1.4983 - val_accuracy: 0.3057\n",
            "Epoch 20/25\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.4983 - accuracy: 0.3489\n",
            "Epoch 00020: loss improved from 1.49953 to 1.49826, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adamax.Adamax object at 0x7f59c8bcde48>_Batch25_LR0.01_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.4983 - accuracy: 0.3488 - val_loss: 1.4890 - val_accuracy: 0.3280\n",
            "Epoch 21/25\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.4995 - accuracy: 0.3503\n",
            "Epoch 00021: loss did not improve from 1.49826\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.4990 - accuracy: 0.3504 - val_loss: 1.4883 - val_accuracy: 0.3280\n",
            "Epoch 22/25\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.4998 - accuracy: 0.3504\n",
            "Epoch 00022: loss did not improve from 1.49826\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.4998 - accuracy: 0.3504 - val_loss: 1.4912 - val_accuracy: 0.3272\n",
            "Epoch 23/25\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.4991 - accuracy: 0.3470\n",
            "Epoch 00023: loss did not improve from 1.49826\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.4993 - accuracy: 0.3469 - val_loss: 1.4950 - val_accuracy: 0.3114\n",
            "Epoch 24/25\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.4965 - accuracy: 0.3500\n",
            "Epoch 00024: loss improved from 1.49826 to 1.49655, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adamax.Adamax object at 0x7f59c8bcde48>_Batch25_LR0.01_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.4965 - accuracy: 0.3500 - val_loss: 1.4787 - val_accuracy: 0.3272\n",
            "Epoch 25/25\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.4977 - accuracy: 0.3492\n",
            "Epoch 00025: loss did not improve from 1.49655\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.4978 - accuracy: 0.3492 - val_loss: 1.5022 - val_accuracy: 0.3253\n",
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
            "Epoch 1/25\n",
            "  2/595 [..............................] - ETA: 44s - loss: 2.1071 - accuracy: 0.1400WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0151s vs `on_train_batch_end` time: 0.1349s). Check your callbacks.\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.5449 - accuracy: 0.3418\n",
            "Epoch 00001: loss improved from inf to 1.54494, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.ftrl.Ftrl object at 0x7f59c45e9fd0>_Batch25_LR0.01_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5449 - accuracy: 0.3418 - val_loss: 1.4955 - val_accuracy: 0.3399\n",
            "Epoch 2/25\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5317 - accuracy: 0.3436\n",
            "Epoch 00002: loss improved from 1.54494 to 1.53172, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.ftrl.Ftrl object at 0x7f59c45e9fd0>_Batch25_LR0.01_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5317 - accuracy: 0.3436 - val_loss: 1.4993 - val_accuracy: 0.3399\n",
            "Epoch 3/25\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5292 - accuracy: 0.3438\n",
            "Epoch 00003: loss improved from 1.53172 to 1.52936, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.ftrl.Ftrl object at 0x7f59c45e9fd0>_Batch25_LR0.01_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5294 - accuracy: 0.3436 - val_loss: 1.5000 - val_accuracy: 0.3399\n",
            "Epoch 4/25\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5290 - accuracy: 0.3432\n",
            "Epoch 00004: loss improved from 1.52936 to 1.52877, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.ftrl.Ftrl object at 0x7f59c45e9fd0>_Batch25_LR0.01_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5288 - accuracy: 0.3436 - val_loss: 1.4933 - val_accuracy: 0.3399\n",
            "Epoch 5/25\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.5294 - accuracy: 0.3435\n",
            "Epoch 00005: loss did not improve from 1.52877\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5296 - accuracy: 0.3436 - val_loss: 1.4981 - val_accuracy: 0.3399\n",
            "Epoch 6/25\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5287 - accuracy: 0.3438\n",
            "Epoch 00006: loss improved from 1.52877 to 1.52867, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.ftrl.Ftrl object at 0x7f59c45e9fd0>_Batch25_LR0.01_Epochs25.hdf5\n",
            "595/595 [==============================] - 9s 14ms/step - loss: 1.5287 - accuracy: 0.3436 - val_loss: 1.4990 - val_accuracy: 0.3399\n",
            "Epoch 7/25\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.5292 - accuracy: 0.3435\n",
            "Epoch 00007: loss did not improve from 1.52867\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5287 - accuracy: 0.3436 - val_loss: 1.4936 - val_accuracy: 0.3399\n",
            "Epoch 8/25\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.5287 - accuracy: 0.3436\n",
            "Epoch 00008: loss did not improve from 1.52867\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5287 - accuracy: 0.3436 - val_loss: 1.5011 - val_accuracy: 0.3399\n",
            "Epoch 9/25\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5286 - accuracy: 0.3434\n",
            "Epoch 00009: loss improved from 1.52867 to 1.52862, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.ftrl.Ftrl object at 0x7f59c45e9fd0>_Batch25_LR0.01_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5286 - accuracy: 0.3436 - val_loss: 1.5033 - val_accuracy: 0.3399\n",
            "Epoch 10/25\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5274 - accuracy: 0.3436\n",
            "Epoch 00010: loss improved from 1.52862 to 1.52737, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.ftrl.Ftrl object at 0x7f59c45e9fd0>_Batch25_LR0.01_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5274 - accuracy: 0.3436 - val_loss: 1.5012 - val_accuracy: 0.3399\n",
            "Epoch 11/25\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5278 - accuracy: 0.3436\n",
            "Epoch 00011: loss did not improve from 1.52737\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5276 - accuracy: 0.3436 - val_loss: 1.4964 - val_accuracy: 0.3399\n",
            "Epoch 12/25\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5262 - accuracy: 0.3437\n",
            "Epoch 00012: loss improved from 1.52737 to 1.52624, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.ftrl.Ftrl object at 0x7f59c45e9fd0>_Batch25_LR0.01_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5262 - accuracy: 0.3436 - val_loss: 1.4975 - val_accuracy: 0.3399\n",
            "Epoch 13/25\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5259 - accuracy: 0.3437\n",
            "Epoch 00013: loss improved from 1.52624 to 1.52591, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.ftrl.Ftrl object at 0x7f59c45e9fd0>_Batch25_LR0.01_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5259 - accuracy: 0.3436 - val_loss: 1.5051 - val_accuracy: 0.3399\n",
            "Epoch 14/25\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5253 - accuracy: 0.3434\n",
            "Epoch 00014: loss improved from 1.52591 to 1.52517, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.ftrl.Ftrl object at 0x7f59c45e9fd0>_Batch25_LR0.01_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5252 - accuracy: 0.3436 - val_loss: 1.4986 - val_accuracy: 0.3399\n",
            "Epoch 15/25\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.5258 - accuracy: 0.3429\n",
            "Epoch 00015: loss did not improve from 1.52517\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5252 - accuracy: 0.3436 - val_loss: 1.4991 - val_accuracy: 0.3399\n",
            "Epoch 16/25\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5249 - accuracy: 0.3436\n",
            "Epoch 00016: loss improved from 1.52517 to 1.52484, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.ftrl.Ftrl object at 0x7f59c45e9fd0>_Batch25_LR0.01_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5248 - accuracy: 0.3436 - val_loss: 1.4971 - val_accuracy: 0.3399\n",
            "Epoch 17/25\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5242 - accuracy: 0.3440\n",
            "Epoch 00017: loss improved from 1.52484 to 1.52430, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.ftrl.Ftrl object at 0x7f59c45e9fd0>_Batch25_LR0.01_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5243 - accuracy: 0.3439 - val_loss: 1.5003 - val_accuracy: 0.3399\n",
            "Epoch 18/25\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5237 - accuracy: 0.3437\n",
            "Epoch 00018: loss improved from 1.52430 to 1.52390, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.ftrl.Ftrl object at 0x7f59c45e9fd0>_Batch25_LR0.01_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5239 - accuracy: 0.3434 - val_loss: 1.5019 - val_accuracy: 0.3399\n",
            "Epoch 19/25\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5235 - accuracy: 0.3436\n",
            "Epoch 00019: loss improved from 1.52390 to 1.52346, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.ftrl.Ftrl object at 0x7f59c45e9fd0>_Batch25_LR0.01_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5235 - accuracy: 0.3435 - val_loss: 1.5084 - val_accuracy: 0.3399\n",
            "Epoch 20/25\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.5232 - accuracy: 0.3433\n",
            "Epoch 00020: loss improved from 1.52346 to 1.52335, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.ftrl.Ftrl object at 0x7f59c45e9fd0>_Batch25_LR0.01_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5233 - accuracy: 0.3436 - val_loss: 1.5022 - val_accuracy: 0.3399\n",
            "Epoch 21/25\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5231 - accuracy: 0.3435\n",
            "Epoch 00021: loss improved from 1.52335 to 1.52314, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.ftrl.Ftrl object at 0x7f59c45e9fd0>_Batch25_LR0.01_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5231 - accuracy: 0.3437 - val_loss: 1.5025 - val_accuracy: 0.3399\n",
            "Epoch 22/25\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.5234 - accuracy: 0.3435\n",
            "Epoch 00022: loss improved from 1.52314 to 1.52314, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.ftrl.Ftrl object at 0x7f59c45e9fd0>_Batch25_LR0.01_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5231 - accuracy: 0.3436 - val_loss: 1.5015 - val_accuracy: 0.3399\n",
            "Epoch 23/25\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5216 - accuracy: 0.3439\n",
            "Epoch 00023: loss improved from 1.52314 to 1.52189, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.ftrl.Ftrl object at 0x7f59c45e9fd0>_Batch25_LR0.01_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5219 - accuracy: 0.3436 - val_loss: 1.5018 - val_accuracy: 0.3399\n",
            "Epoch 24/25\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.5214 - accuracy: 0.3433\n",
            "Epoch 00024: loss improved from 1.52189 to 1.52137, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.ftrl.Ftrl object at 0x7f59c45e9fd0>_Batch25_LR0.01_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5214 - accuracy: 0.3433 - val_loss: 1.4987 - val_accuracy: 0.3399\n",
            "Epoch 25/25\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.5217 - accuracy: 0.3437\n",
            "Epoch 00025: loss did not improve from 1.52137\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5219 - accuracy: 0.3434 - val_loss: 1.4994 - val_accuracy: 0.3399\n",
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
            "Epoch 1/25\n",
            "  2/595 [..............................] - ETA: 32s - loss: 2.4005 - accuracy: 0.1800WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0155s vs `on_train_batch_end` time: 0.0944s). Check your callbacks.\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5523 - accuracy: 0.3327\n",
            "Epoch 00001: loss improved from inf to 1.55214, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f59c422fa58>_Batch25_LR0.1_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5521 - accuracy: 0.3328 - val_loss: 1.4925 - val_accuracy: 0.3399\n",
            "Epoch 2/25\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5367 - accuracy: 0.3376\n",
            "Epoch 00002: loss improved from 1.55214 to 1.53694, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f59c422fa58>_Batch25_LR0.1_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5369 - accuracy: 0.3373 - val_loss: 1.5014 - val_accuracy: 0.3399\n",
            "Epoch 3/25\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.5341 - accuracy: 0.3391\n",
            "Epoch 00003: loss improved from 1.53694 to 1.53411, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f59c422fa58>_Batch25_LR0.1_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5341 - accuracy: 0.3391 - val_loss: 1.5084 - val_accuracy: 0.3399\n",
            "Epoch 4/25\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.5334 - accuracy: 0.3396\n",
            "Epoch 00004: loss improved from 1.53411 to 1.53265, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f59c422fa58>_Batch25_LR0.1_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5327 - accuracy: 0.3397 - val_loss: 1.4998 - val_accuracy: 0.3022\n",
            "Epoch 5/25\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.5321 - accuracy: 0.3426\n",
            "Epoch 00005: loss improved from 1.53265 to 1.53207, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f59c422fa58>_Batch25_LR0.1_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5321 - accuracy: 0.3423 - val_loss: 1.4992 - val_accuracy: 0.3399\n",
            "Epoch 6/25\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.5302 - accuracy: 0.3426\n",
            "Epoch 00006: loss improved from 1.53207 to 1.53019, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f59c422fa58>_Batch25_LR0.1_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5302 - accuracy: 0.3428 - val_loss: 1.5037 - val_accuracy: 0.3399\n",
            "Epoch 7/25\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5271 - accuracy: 0.3424\n",
            "Epoch 00007: loss improved from 1.53019 to 1.52725, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f59c422fa58>_Batch25_LR0.1_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5273 - accuracy: 0.3421 - val_loss: 1.5022 - val_accuracy: 0.3399\n",
            "Epoch 8/25\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5264 - accuracy: 0.3433\n",
            "Epoch 00008: loss improved from 1.52725 to 1.52629, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f59c422fa58>_Batch25_LR0.1_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5263 - accuracy: 0.3431 - val_loss: 1.4939 - val_accuracy: 0.3399\n",
            "Epoch 9/25\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5257 - accuracy: 0.3422\n",
            "Epoch 00009: loss improved from 1.52629 to 1.52526, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f59c422fa58>_Batch25_LR0.1_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5253 - accuracy: 0.3426 - val_loss: 1.4927 - val_accuracy: 0.3399\n",
            "Epoch 10/25\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5230 - accuracy: 0.3430\n",
            "Epoch 00010: loss improved from 1.52526 to 1.52312, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f59c422fa58>_Batch25_LR0.1_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5231 - accuracy: 0.3428 - val_loss: 1.4917 - val_accuracy: 0.3399\n",
            "Epoch 11/25\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.5228 - accuracy: 0.3435\n",
            "Epoch 00011: loss improved from 1.52312 to 1.52273, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f59c422fa58>_Batch25_LR0.1_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5227 - accuracy: 0.3434 - val_loss: 1.5113 - val_accuracy: 0.3375\n",
            "Epoch 12/25\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5224 - accuracy: 0.3436\n",
            "Epoch 00012: loss improved from 1.52273 to 1.52245, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f59c422fa58>_Batch25_LR0.1_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5225 - accuracy: 0.3438 - val_loss: 1.4849 - val_accuracy: 0.3372\n",
            "Epoch 13/25\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5213 - accuracy: 0.3436\n",
            "Epoch 00013: loss improved from 1.52245 to 1.52138, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f59c422fa58>_Batch25_LR0.1_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5214 - accuracy: 0.3436 - val_loss: 1.4899 - val_accuracy: 0.3399\n",
            "Epoch 14/25\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5225 - accuracy: 0.3437\n",
            "Epoch 00014: loss did not improve from 1.52138\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5225 - accuracy: 0.3438 - val_loss: 1.5007 - val_accuracy: 0.3372\n",
            "Epoch 15/25\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.5214 - accuracy: 0.3436\n",
            "Epoch 00015: loss improved from 1.52138 to 1.52136, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f59c422fa58>_Batch25_LR0.1_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5214 - accuracy: 0.3436 - val_loss: 1.4860 - val_accuracy: 0.3372\n",
            "Epoch 16/25\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5194 - accuracy: 0.3440\n",
            "Epoch 00016: loss improved from 1.52136 to 1.51948, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f59c422fa58>_Batch25_LR0.1_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5195 - accuracy: 0.3439 - val_loss: 1.4967 - val_accuracy: 0.3391\n",
            "Epoch 17/25\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5200 - accuracy: 0.3429\n",
            "Epoch 00017: loss did not improve from 1.51948\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5202 - accuracy: 0.3430 - val_loss: 1.5008 - val_accuracy: 0.3372\n",
            "Epoch 18/25\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5203 - accuracy: 0.3435\n",
            "Epoch 00018: loss did not improve from 1.51948\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5198 - accuracy: 0.3441 - val_loss: 1.4962 - val_accuracy: 0.3399\n",
            "Epoch 19/25\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5209 - accuracy: 0.3438\n",
            "Epoch 00019: loss did not improve from 1.51948\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5206 - accuracy: 0.3438 - val_loss: 1.4920 - val_accuracy: 0.3399\n",
            "Epoch 20/25\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5191 - accuracy: 0.3434\n",
            "Epoch 00020: loss improved from 1.51948 to 1.51934, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f59c422fa58>_Batch25_LR0.1_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5193 - accuracy: 0.3434 - val_loss: 1.5000 - val_accuracy: 0.3372\n",
            "Epoch 21/25\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5197 - accuracy: 0.3425\n",
            "Epoch 00021: loss did not improve from 1.51934\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5196 - accuracy: 0.3430 - val_loss: 1.5052 - val_accuracy: 0.3240\n",
            "Epoch 22/25\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5190 - accuracy: 0.3434\n",
            "Epoch 00022: loss improved from 1.51934 to 1.51900, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f59c422fa58>_Batch25_LR0.1_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5190 - accuracy: 0.3436 - val_loss: 1.5070 - val_accuracy: 0.3375\n",
            "Epoch 23/25\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5182 - accuracy: 0.3441\n",
            "Epoch 00023: loss improved from 1.51900 to 1.51820, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f59c422fa58>_Batch25_LR0.1_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5182 - accuracy: 0.3439 - val_loss: 1.5042 - val_accuracy: 0.3291\n",
            "Epoch 24/25\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5170 - accuracy: 0.3444\n",
            "Epoch 00024: loss improved from 1.51820 to 1.51781, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f59c422fa58>_Batch25_LR0.1_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5178 - accuracy: 0.3442 - val_loss: 1.4884 - val_accuracy: 0.3372\n",
            "Epoch 25/25\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.5179 - accuracy: 0.3443\n",
            "Epoch 00025: loss did not improve from 1.51781\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5179 - accuracy: 0.3443 - val_loss: 1.5057 - val_accuracy: 0.3375\n",
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
            "Epoch 1/25\n",
            "  2/595 [..............................] - ETA: 2:01 - loss: 46.0505 - accuracy: 0.2800WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0155s vs `on_train_batch_end` time: 0.3925s). Check your callbacks.\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.8809 - accuracy: 0.3316\n",
            "Epoch 00001: loss improved from inf to 1.87838, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.rmsprop.RMSprop object at 0x7f59beee5978>_Batch25_LR0.1_Epochs25.hdf5\n",
            "595/595 [==============================] - 9s 15ms/step - loss: 1.8784 - accuracy: 0.3318 - val_loss: 1.4994 - val_accuracy: 0.3399\n",
            "Epoch 2/25\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5389 - accuracy: 0.3349\n",
            "Epoch 00002: loss improved from 1.87838 to 1.53868, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.rmsprop.RMSprop object at 0x7f59beee5978>_Batch25_LR0.1_Epochs25.hdf5\n",
            "595/595 [==============================] - 9s 14ms/step - loss: 1.5387 - accuracy: 0.3350 - val_loss: 1.5049 - val_accuracy: 0.3399\n",
            "Epoch 3/25\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5392 - accuracy: 0.3371\n",
            "Epoch 00003: loss did not improve from 1.53868\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5393 - accuracy: 0.3371 - val_loss: 1.5011 - val_accuracy: 0.3399\n",
            "Epoch 4/25\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.5402 - accuracy: 0.3350\n",
            "Epoch 00004: loss did not improve from 1.53868\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5402 - accuracy: 0.3350 - val_loss: 1.4915 - val_accuracy: 0.3399\n",
            "Epoch 5/25\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.5401 - accuracy: 0.3361\n",
            "Epoch 00005: loss did not improve from 1.53868\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5401 - accuracy: 0.3361 - val_loss: 1.5057 - val_accuracy: 0.3399\n",
            "Epoch 6/25\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5379 - accuracy: 0.3353\n",
            "Epoch 00006: loss improved from 1.53868 to 1.53809, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.rmsprop.RMSprop object at 0x7f59beee5978>_Batch25_LR0.1_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5381 - accuracy: 0.3352 - val_loss: 1.5420 - val_accuracy: 0.3399\n",
            "Epoch 7/25\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5385 - accuracy: 0.3364\n",
            "Epoch 00007: loss did not improve from 1.53809\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5385 - accuracy: 0.3364 - val_loss: 1.4961 - val_accuracy: 0.3399\n",
            "Epoch 8/25\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5392 - accuracy: 0.3366\n",
            "Epoch 00008: loss did not improve from 1.53809\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5393 - accuracy: 0.3365 - val_loss: 1.5192 - val_accuracy: 0.3022\n",
            "Epoch 9/25\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5373 - accuracy: 0.3354\n",
            "Epoch 00009: loss improved from 1.53809 to 1.53736, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.rmsprop.RMSprop object at 0x7f59beee5978>_Batch25_LR0.1_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5374 - accuracy: 0.3352 - val_loss: 1.5028 - val_accuracy: 0.3022\n",
            "Epoch 10/25\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.5391 - accuracy: 0.3346\n",
            "Epoch 00010: loss did not improve from 1.53736\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5391 - accuracy: 0.3346 - val_loss: 1.4968 - val_accuracy: 0.3399\n",
            "Epoch 11/25\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.5378 - accuracy: 0.3357\n",
            "Epoch 00011: loss did not improve from 1.53736\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5377 - accuracy: 0.3360 - val_loss: 1.4977 - val_accuracy: 0.3399\n",
            "Epoch 12/25\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5391 - accuracy: 0.3364\n",
            "Epoch 00012: loss did not improve from 1.53736\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5391 - accuracy: 0.3365 - val_loss: 1.5129 - val_accuracy: 0.3399\n",
            "Epoch 13/25\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.5384 - accuracy: 0.3349\n",
            "Epoch 00013: loss did not improve from 1.53736\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5385 - accuracy: 0.3350 - val_loss: 1.4928 - val_accuracy: 0.3399\n",
            "Epoch 14/25\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.5377 - accuracy: 0.3335\n",
            "Epoch 00014: loss did not improve from 1.53736\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5377 - accuracy: 0.3330 - val_loss: 1.4991 - val_accuracy: 0.3022\n",
            "Epoch 15/25\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5387 - accuracy: 0.3345\n",
            "Epoch 00015: loss did not improve from 1.53736\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5386 - accuracy: 0.3346 - val_loss: 1.5230 - val_accuracy: 0.3399\n",
            "Epoch 16/25\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5385 - accuracy: 0.3353\n",
            "Epoch 00016: loss did not improve from 1.53736\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5388 - accuracy: 0.3351 - val_loss: 1.5194 - val_accuracy: 0.3399\n",
            "Epoch 17/25\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5385 - accuracy: 0.3355\n",
            "Epoch 00017: loss did not improve from 1.53736\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5390 - accuracy: 0.3350 - val_loss: 1.5304 - val_accuracy: 0.3399\n",
            "Epoch 18/25\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5398 - accuracy: 0.3348\n",
            "Epoch 00018: loss did not improve from 1.53736\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5399 - accuracy: 0.3348 - val_loss: 1.4900 - val_accuracy: 0.3399\n",
            "Epoch 19/25\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.5393 - accuracy: 0.3346\n",
            "Epoch 00019: loss did not improve from 1.53736\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5399 - accuracy: 0.3346 - val_loss: 1.4903 - val_accuracy: 0.3399\n",
            "Epoch 20/25\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.5390 - accuracy: 0.3349\n",
            "Epoch 00020: loss did not improve from 1.53736\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5390 - accuracy: 0.3349 - val_loss: 1.5239 - val_accuracy: 0.3399\n",
            "Epoch 21/25\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.5383 - accuracy: 0.3354\n",
            "Epoch 00021: loss did not improve from 1.53736\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5385 - accuracy: 0.3358 - val_loss: 1.5022 - val_accuracy: 0.3399\n",
            "Epoch 22/25\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.5387 - accuracy: 0.3363\n",
            "Epoch 00022: loss did not improve from 1.53736\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5385 - accuracy: 0.3367 - val_loss: 1.4938 - val_accuracy: 0.3399\n",
            "Epoch 23/25\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5402 - accuracy: 0.3379\n",
            "Epoch 00023: loss did not improve from 1.53736\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5400 - accuracy: 0.3381 - val_loss: 1.5240 - val_accuracy: 0.3399\n",
            "Epoch 24/25\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5399 - accuracy: 0.3337\n",
            "Epoch 00024: loss did not improve from 1.53736\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5397 - accuracy: 0.3336 - val_loss: 1.4899 - val_accuracy: 0.3399\n",
            "Epoch 25/25\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5388 - accuracy: 0.3331\n",
            "Epoch 00025: loss did not improve from 1.53736\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5388 - accuracy: 0.3331 - val_loss: 1.5058 - val_accuracy: 0.3399\n",
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
            "Epoch 1/25\n",
            "  2/595 [..............................] - ETA: 34s - loss: 9.3191 - accuracy: 0.2000WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0168s vs `on_train_batch_end` time: 0.0999s). Check your callbacks.\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.6430 - accuracy: 0.3368\n",
            "Epoch 00001: loss improved from inf to 1.64303, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f59bed616d8>_Batch25_LR0.1_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.6430 - accuracy: 0.3368 - val_loss: 1.5229 - val_accuracy: 0.3399\n",
            "Epoch 2/25\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5355 - accuracy: 0.3358\n",
            "Epoch 00002: loss improved from 1.64303 to 1.53606, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f59bed616d8>_Batch25_LR0.1_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5361 - accuracy: 0.3355 - val_loss: 1.5037 - val_accuracy: 0.3399\n",
            "Epoch 3/25\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5376 - accuracy: 0.3331\n",
            "Epoch 00003: loss did not improve from 1.53606\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5375 - accuracy: 0.3332 - val_loss: 1.4957 - val_accuracy: 0.3399\n",
            "Epoch 4/25\n",
            "589/595 [============================>.] - ETA: 0s - loss: 1.5373 - accuracy: 0.3360\n",
            "Epoch 00004: loss did not improve from 1.53606\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5374 - accuracy: 0.3365 - val_loss: 1.5086 - val_accuracy: 0.3399\n",
            "Epoch 5/25\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5375 - accuracy: 0.3362\n",
            "Epoch 00005: loss did not improve from 1.53606\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5375 - accuracy: 0.3363 - val_loss: 1.4962 - val_accuracy: 0.3399\n",
            "Epoch 6/25\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.5397 - accuracy: 0.3355\n",
            "Epoch 00006: loss did not improve from 1.53606\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5397 - accuracy: 0.3355 - val_loss: 1.5431 - val_accuracy: 0.3399\n",
            "Epoch 7/25\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.5372 - accuracy: 0.3420\n",
            "Epoch 00007: loss did not improve from 1.53606\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5372 - accuracy: 0.3420 - val_loss: 1.4936 - val_accuracy: 0.3399\n",
            "Epoch 8/25\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5368 - accuracy: 0.3328\n",
            "Epoch 00008: loss did not improve from 1.53606\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5368 - accuracy: 0.3331 - val_loss: 1.4984 - val_accuracy: 0.3399\n",
            "Epoch 9/25\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5370 - accuracy: 0.3326\n",
            "Epoch 00009: loss did not improve from 1.53606\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5372 - accuracy: 0.3324 - val_loss: 1.4971 - val_accuracy: 0.3399\n",
            "Epoch 10/25\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5377 - accuracy: 0.3383\n",
            "Epoch 00010: loss did not improve from 1.53606\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5379 - accuracy: 0.3381 - val_loss: 1.5014 - val_accuracy: 0.3399\n",
            "Epoch 11/25\n",
            "589/595 [============================>.] - ETA: 0s - loss: 1.5373 - accuracy: 0.3362\n",
            "Epoch 00011: loss did not improve from 1.53606\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5371 - accuracy: 0.3364 - val_loss: 1.5107 - val_accuracy: 0.3399\n",
            "Epoch 12/25\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5362 - accuracy: 0.3387\n",
            "Epoch 00012: loss did not improve from 1.53606\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5361 - accuracy: 0.3389 - val_loss: 1.5404 - val_accuracy: 0.3399\n",
            "Epoch 13/25\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5357 - accuracy: 0.3388\n",
            "Epoch 00013: loss improved from 1.53606 to 1.53600, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f59bed616d8>_Batch25_LR0.1_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5360 - accuracy: 0.3385 - val_loss: 1.5036 - val_accuracy: 0.3399\n",
            "Epoch 14/25\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.5370 - accuracy: 0.3368\n",
            "Epoch 00014: loss did not improve from 1.53600\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5371 - accuracy: 0.3369 - val_loss: 1.5017 - val_accuracy: 0.3399\n",
            "Epoch 15/25\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5364 - accuracy: 0.3370\n",
            "Epoch 00015: loss did not improve from 1.53600\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5365 - accuracy: 0.3368 - val_loss: 1.4927 - val_accuracy: 0.3399\n",
            "Epoch 16/25\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5391 - accuracy: 0.3318\n",
            "Epoch 00016: loss did not improve from 1.53600\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5396 - accuracy: 0.3315 - val_loss: 1.5038 - val_accuracy: 0.3399\n",
            "Epoch 17/25\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5377 - accuracy: 0.3361\n",
            "Epoch 00017: loss did not improve from 1.53600\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5375 - accuracy: 0.3365 - val_loss: 1.5100 - val_accuracy: 0.3399\n",
            "Epoch 18/25\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5371 - accuracy: 0.3326\n",
            "Epoch 00018: loss did not improve from 1.53600\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5374 - accuracy: 0.3326 - val_loss: 1.5012 - val_accuracy: 0.3399\n",
            "Epoch 19/25\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5364 - accuracy: 0.3367\n",
            "Epoch 00019: loss did not improve from 1.53600\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5364 - accuracy: 0.3367 - val_loss: 1.4994 - val_accuracy: 0.3022\n",
            "Epoch 20/25\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5381 - accuracy: 0.3343\n",
            "Epoch 00020: loss did not improve from 1.53600\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5381 - accuracy: 0.3344 - val_loss: 1.4956 - val_accuracy: 0.3399\n",
            "Epoch 21/25\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5366 - accuracy: 0.3376\n",
            "Epoch 00021: loss did not improve from 1.53600\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5368 - accuracy: 0.3378 - val_loss: 1.5114 - val_accuracy: 0.3399\n",
            "Epoch 22/25\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5379 - accuracy: 0.3362\n",
            "Epoch 00022: loss did not improve from 1.53600\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5378 - accuracy: 0.3363 - val_loss: 1.4975 - val_accuracy: 0.3399\n",
            "Epoch 23/25\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.5374 - accuracy: 0.3362\n",
            "Epoch 00023: loss did not improve from 1.53600\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5374 - accuracy: 0.3362 - val_loss: 1.4947 - val_accuracy: 0.3399\n",
            "Epoch 24/25\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5363 - accuracy: 0.3410\n",
            "Epoch 00024: loss did not improve from 1.53600\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5367 - accuracy: 0.3406 - val_loss: 1.5083 - val_accuracy: 0.3399\n",
            "Epoch 25/25\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.5376 - accuracy: 0.3354\n",
            "Epoch 00025: loss did not improve from 1.53600\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5375 - accuracy: 0.3353 - val_loss: 1.4928 - val_accuracy: 0.3399\n",
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
            "Epoch 1/25\n",
            "  2/595 [..............................] - ETA: 33s - loss: 3.6261 - accuracy: 0.0600   WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0144s vs `on_train_batch_end` time: 0.0973s). Check your callbacks.\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.5532 - accuracy: 0.3406\n",
            "Epoch 00001: loss improved from inf to 1.55321, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adagrad.Adagrad object at 0x7f59bf042630>_Batch25_LR0.1_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5532 - accuracy: 0.3406 - val_loss: 1.4941 - val_accuracy: 0.3399\n",
            "Epoch 2/25\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.5320 - accuracy: 0.3419\n",
            "Epoch 00002: loss improved from 1.55321 to 1.53175, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adagrad.Adagrad object at 0x7f59bf042630>_Batch25_LR0.1_Epochs25.hdf5\n",
            "595/595 [==============================] - 9s 14ms/step - loss: 1.5317 - accuracy: 0.3417 - val_loss: 1.4948 - val_accuracy: 0.3399\n",
            "Epoch 3/25\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5314 - accuracy: 0.3430\n",
            "Epoch 00003: loss improved from 1.53175 to 1.53146, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adagrad.Adagrad object at 0x7f59bf042630>_Batch25_LR0.1_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5315 - accuracy: 0.3429 - val_loss: 1.4928 - val_accuracy: 0.3399\n",
            "Epoch 4/25\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.5296 - accuracy: 0.3435\n",
            "Epoch 00004: loss improved from 1.53146 to 1.52946, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adagrad.Adagrad object at 0x7f59bf042630>_Batch25_LR0.1_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5295 - accuracy: 0.3434 - val_loss: 1.5025 - val_accuracy: 0.3399\n",
            "Epoch 5/25\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.5285 - accuracy: 0.3436\n",
            "Epoch 00005: loss improved from 1.52946 to 1.52846, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adagrad.Adagrad object at 0x7f59bf042630>_Batch25_LR0.1_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5285 - accuracy: 0.3436 - val_loss: 1.5008 - val_accuracy: 0.3399\n",
            "Epoch 6/25\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5273 - accuracy: 0.3438\n",
            "Epoch 00006: loss improved from 1.52846 to 1.52747, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adagrad.Adagrad object at 0x7f59bf042630>_Batch25_LR0.1_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5275 - accuracy: 0.3436 - val_loss: 1.4985 - val_accuracy: 0.3399\n",
            "Epoch 7/25\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.5280 - accuracy: 0.3428\n",
            "Epoch 00007: loss improved from 1.52747 to 1.52710, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adagrad.Adagrad object at 0x7f59bf042630>_Batch25_LR0.1_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5271 - accuracy: 0.3436 - val_loss: 1.4995 - val_accuracy: 0.3399\n",
            "Epoch 8/25\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5249 - accuracy: 0.3439\n",
            "Epoch 00008: loss improved from 1.52710 to 1.52520, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adagrad.Adagrad object at 0x7f59bf042630>_Batch25_LR0.1_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5252 - accuracy: 0.3437 - val_loss: 1.5021 - val_accuracy: 0.3399\n",
            "Epoch 9/25\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.5242 - accuracy: 0.3437\n",
            "Epoch 00009: loss improved from 1.52520 to 1.52397, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adagrad.Adagrad object at 0x7f59bf042630>_Batch25_LR0.1_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5240 - accuracy: 0.3435 - val_loss: 1.4984 - val_accuracy: 0.3399\n",
            "Epoch 10/25\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5233 - accuracy: 0.3432\n",
            "Epoch 00010: loss improved from 1.52397 to 1.52304, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adagrad.Adagrad object at 0x7f59bf042630>_Batch25_LR0.1_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5230 - accuracy: 0.3436 - val_loss: 1.5028 - val_accuracy: 0.3399\n",
            "Epoch 11/25\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.5223 - accuracy: 0.3436\n",
            "Epoch 00011: loss improved from 1.52304 to 1.52224, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adagrad.Adagrad object at 0x7f59bf042630>_Batch25_LR0.1_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5222 - accuracy: 0.3435 - val_loss: 1.4970 - val_accuracy: 0.3399\n",
            "Epoch 12/25\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5214 - accuracy: 0.3432\n",
            "Epoch 00012: loss improved from 1.52224 to 1.52139, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adagrad.Adagrad object at 0x7f59bf042630>_Batch25_LR0.1_Epochs25.hdf5\n",
            "595/595 [==============================] - 9s 15ms/step - loss: 1.5214 - accuracy: 0.3432 - val_loss: 1.5031 - val_accuracy: 0.3399\n",
            "Epoch 13/25\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.5191 - accuracy: 0.3440\n",
            "Epoch 00013: loss improved from 1.52139 to 1.51914, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adagrad.Adagrad object at 0x7f59bf042630>_Batch25_LR0.1_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5191 - accuracy: 0.3436 - val_loss: 1.4992 - val_accuracy: 0.3399\n",
            "Epoch 14/25\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5184 - accuracy: 0.3431\n",
            "Epoch 00014: loss improved from 1.51914 to 1.51822, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adagrad.Adagrad object at 0x7f59bf042630>_Batch25_LR0.1_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5182 - accuracy: 0.3432 - val_loss: 1.4970 - val_accuracy: 0.3399\n",
            "Epoch 15/25\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5184 - accuracy: 0.3427\n",
            "Epoch 00015: loss did not improve from 1.51822\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5185 - accuracy: 0.3429 - val_loss: 1.4963 - val_accuracy: 0.3399\n",
            "Epoch 16/25\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5157 - accuracy: 0.3436\n",
            "Epoch 00016: loss improved from 1.51822 to 1.51597, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adagrad.Adagrad object at 0x7f59bf042630>_Batch25_LR0.1_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5160 - accuracy: 0.3434 - val_loss: 1.4996 - val_accuracy: 0.3399\n",
            "Epoch 17/25\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5161 - accuracy: 0.3435\n",
            "Epoch 00017: loss improved from 1.51597 to 1.51586, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adagrad.Adagrad object at 0x7f59bf042630>_Batch25_LR0.1_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5159 - accuracy: 0.3436 - val_loss: 1.5035 - val_accuracy: 0.3399\n",
            "Epoch 18/25\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5164 - accuracy: 0.3436\n",
            "Epoch 00018: loss did not improve from 1.51586\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5167 - accuracy: 0.3432 - val_loss: 1.4981 - val_accuracy: 0.3399\n",
            "Epoch 19/25\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5156 - accuracy: 0.3439\n",
            "Epoch 00019: loss did not improve from 1.51586\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5159 - accuracy: 0.3436 - val_loss: 1.4948 - val_accuracy: 0.3262\n",
            "Epoch 20/25\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5148 - accuracy: 0.3444\n",
            "Epoch 00020: loss improved from 1.51586 to 1.51493, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adagrad.Adagrad object at 0x7f59bf042630>_Batch25_LR0.1_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5149 - accuracy: 0.3444 - val_loss: 1.5132 - val_accuracy: 0.3410\n",
            "Epoch 21/25\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5143 - accuracy: 0.3437\n",
            "Epoch 00021: loss improved from 1.51493 to 1.51429, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adagrad.Adagrad object at 0x7f59bf042630>_Batch25_LR0.1_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5143 - accuracy: 0.3437 - val_loss: 1.5013 - val_accuracy: 0.3399\n",
            "Epoch 22/25\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5138 - accuracy: 0.3447\n",
            "Epoch 00022: loss improved from 1.51429 to 1.51322, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adagrad.Adagrad object at 0x7f59bf042630>_Batch25_LR0.1_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5132 - accuracy: 0.3451 - val_loss: 1.5113 - val_accuracy: 0.3402\n",
            "Epoch 23/25\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.5127 - accuracy: 0.3459\n",
            "Epoch 00023: loss improved from 1.51322 to 1.51262, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adagrad.Adagrad object at 0x7f59bf042630>_Batch25_LR0.1_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5126 - accuracy: 0.3461 - val_loss: 1.5067 - val_accuracy: 0.3399\n",
            "Epoch 24/25\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5126 - accuracy: 0.3453\n",
            "Epoch 00024: loss improved from 1.51262 to 1.51238, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adagrad.Adagrad object at 0x7f59bf042630>_Batch25_LR0.1_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5124 - accuracy: 0.3455 - val_loss: 1.5047 - val_accuracy: 0.3385\n",
            "Epoch 25/25\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5120 - accuracy: 0.3444\n",
            "Epoch 00025: loss improved from 1.51238 to 1.51176, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adagrad.Adagrad object at 0x7f59bf042630>_Batch25_LR0.1_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5118 - accuracy: 0.3444 - val_loss: 1.5084 - val_accuracy: 0.3410\n",
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
            "Epoch 1/25\n",
            "  2/595 [..............................] - ETA: 51s - loss: 14.8021 - accuracy: 0.1600WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0148s vs `on_train_batch_end` time: 0.1619s). Check your callbacks.\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.6659 - accuracy: 0.3361\n",
            "Epoch 00001: loss improved from inf to 1.66466, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adamax.Adamax object at 0x7f59c45e9fd0>_Batch25_LR0.1_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.6647 - accuracy: 0.3366 - val_loss: 1.4979 - val_accuracy: 0.3399\n",
            "Epoch 2/25\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5303 - accuracy: 0.3437\n",
            "Epoch 00002: loss improved from 1.66466 to 1.53024, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adamax.Adamax object at 0x7f59c45e9fd0>_Batch25_LR0.1_Epochs25.hdf5\n",
            "595/595 [==============================] - 9s 14ms/step - loss: 1.5302 - accuracy: 0.3436 - val_loss: 1.4979 - val_accuracy: 0.3399\n",
            "Epoch 3/25\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5308 - accuracy: 0.3396\n",
            "Epoch 00003: loss did not improve from 1.53024\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5309 - accuracy: 0.3395 - val_loss: 1.5025 - val_accuracy: 0.3399\n",
            "Epoch 4/25\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5315 - accuracy: 0.3407\n",
            "Epoch 00004: loss did not improve from 1.53024\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5316 - accuracy: 0.3408 - val_loss: 1.4961 - val_accuracy: 0.3399\n",
            "Epoch 5/25\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5309 - accuracy: 0.3401\n",
            "Epoch 00005: loss did not improve from 1.53024\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5307 - accuracy: 0.3404 - val_loss: 1.4961 - val_accuracy: 0.3399\n",
            "Epoch 6/25\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5313 - accuracy: 0.3434\n",
            "Epoch 00006: loss did not improve from 1.53024\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5312 - accuracy: 0.3436 - val_loss: 1.4991 - val_accuracy: 0.3399\n",
            "Epoch 7/25\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.5309 - accuracy: 0.3436\n",
            "Epoch 00007: loss did not improve from 1.53024\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5309 - accuracy: 0.3436 - val_loss: 1.4947 - val_accuracy: 0.3399\n",
            "Epoch 8/25\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5312 - accuracy: 0.3439\n",
            "Epoch 00008: loss did not improve from 1.53024\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5311 - accuracy: 0.3436 - val_loss: 1.5021 - val_accuracy: 0.3399\n",
            "Epoch 9/25\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5319 - accuracy: 0.3423\n",
            "Epoch 00009: loss did not improve from 1.53024\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5317 - accuracy: 0.3422 - val_loss: 1.4939 - val_accuracy: 0.3399\n",
            "Epoch 10/25\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.5324 - accuracy: 0.3436\n",
            "Epoch 00010: loss did not improve from 1.53024\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5324 - accuracy: 0.3436 - val_loss: 1.4992 - val_accuracy: 0.3399\n",
            "Epoch 11/25\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5306 - accuracy: 0.3435\n",
            "Epoch 00011: loss did not improve from 1.53024\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5307 - accuracy: 0.3436 - val_loss: 1.5000 - val_accuracy: 0.3399\n",
            "Epoch 12/25\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.5319 - accuracy: 0.3423\n",
            "Epoch 00012: loss did not improve from 1.53024\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5319 - accuracy: 0.3427 - val_loss: 1.4903 - val_accuracy: 0.3399\n",
            "Epoch 13/25\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.5309 - accuracy: 0.3426\n",
            "Epoch 00013: loss did not improve from 1.53024\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5314 - accuracy: 0.3424 - val_loss: 1.5094 - val_accuracy: 0.3399\n",
            "Epoch 14/25\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5320 - accuracy: 0.3433\n",
            "Epoch 00014: loss did not improve from 1.53024\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5319 - accuracy: 0.3436 - val_loss: 1.4959 - val_accuracy: 0.3399\n",
            "Epoch 15/25\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5322 - accuracy: 0.3422\n",
            "Epoch 00015: loss did not improve from 1.53024\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5320 - accuracy: 0.3422 - val_loss: 1.5034 - val_accuracy: 0.3399\n",
            "Epoch 16/25\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.5321 - accuracy: 0.3426\n",
            "Epoch 00016: loss did not improve from 1.53024\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5317 - accuracy: 0.3428 - val_loss: 1.4961 - val_accuracy: 0.3399\n",
            "Epoch 17/25\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5315 - accuracy: 0.3438\n",
            "Epoch 00017: loss did not improve from 1.53024\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5316 - accuracy: 0.3436 - val_loss: 1.5029 - val_accuracy: 0.3399\n",
            "Epoch 18/25\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5317 - accuracy: 0.3421\n",
            "Epoch 00018: loss did not improve from 1.53024\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5315 - accuracy: 0.3424 - val_loss: 1.4906 - val_accuracy: 0.3399\n",
            "Epoch 19/25\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.5315 - accuracy: 0.3428\n",
            "Epoch 00019: loss did not improve from 1.53024\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5316 - accuracy: 0.3424 - val_loss: 1.4917 - val_accuracy: 0.3399\n",
            "Epoch 20/25\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.5312 - accuracy: 0.3430\n",
            "Epoch 00020: loss did not improve from 1.53024\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5313 - accuracy: 0.3430 - val_loss: 1.4960 - val_accuracy: 0.3399\n",
            "Epoch 21/25\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5320 - accuracy: 0.3373\n",
            "Epoch 00021: loss did not improve from 1.53024\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5319 - accuracy: 0.3375 - val_loss: 1.5046 - val_accuracy: 0.3399\n",
            "Epoch 22/25\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5317 - accuracy: 0.3393\n",
            "Epoch 00022: loss did not improve from 1.53024\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5317 - accuracy: 0.3397 - val_loss: 1.5141 - val_accuracy: 0.3399\n",
            "Epoch 23/25\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5317 - accuracy: 0.3436\n",
            "Epoch 00023: loss did not improve from 1.53024\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5316 - accuracy: 0.3436 - val_loss: 1.4998 - val_accuracy: 0.3399\n",
            "Epoch 24/25\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5311 - accuracy: 0.3417\n",
            "Epoch 00024: loss did not improve from 1.53024\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5309 - accuracy: 0.3418 - val_loss: 1.5043 - val_accuracy: 0.3399\n",
            "Epoch 25/25\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5315 - accuracy: 0.3433\n",
            "Epoch 00025: loss did not improve from 1.53024\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5313 - accuracy: 0.3436 - val_loss: 1.5071 - val_accuracy: 0.3399\n",
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
            "Epoch 1/25\n",
            "  2/595 [..............................] - ETA: 34s - loss: 2.0317 - accuracy: 0.2000WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0144s vs `on_train_batch_end` time: 0.1007s). Check your callbacks.\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5444 - accuracy: 0.3372\n",
            "Epoch 00001: loss improved from inf to 1.54445, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.ftrl.Ftrl object at 0x7f59bebb3f60>_Batch25_LR0.1_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5445 - accuracy: 0.3372 - val_loss: 1.4944 - val_accuracy: 0.3399\n",
            "Epoch 2/25\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5341 - accuracy: 0.3412\n",
            "Epoch 00002: loss improved from 1.54445 to 1.53384, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.ftrl.Ftrl object at 0x7f59bebb3f60>_Batch25_LR0.1_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5338 - accuracy: 0.3414 - val_loss: 1.4945 - val_accuracy: 0.3399\n",
            "Epoch 3/25\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5304 - accuracy: 0.3422\n",
            "Epoch 00003: loss improved from 1.53384 to 1.53029, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.ftrl.Ftrl object at 0x7f59bebb3f60>_Batch25_LR0.1_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5303 - accuracy: 0.3423 - val_loss: 1.4987 - val_accuracy: 0.3399\n",
            "Epoch 4/25\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.5280 - accuracy: 0.3436\n",
            "Epoch 00004: loss improved from 1.53029 to 1.52804, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.ftrl.Ftrl object at 0x7f59bebb3f60>_Batch25_LR0.1_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5280 - accuracy: 0.3436 - val_loss: 1.5025 - val_accuracy: 0.3399\n",
            "Epoch 5/25\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5285 - accuracy: 0.3435\n",
            "Epoch 00005: loss did not improve from 1.52804\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5285 - accuracy: 0.3435 - val_loss: 1.4985 - val_accuracy: 0.3399\n",
            "Epoch 6/25\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.5259 - accuracy: 0.3437\n",
            "Epoch 00006: loss improved from 1.52804 to 1.52648, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.ftrl.Ftrl object at 0x7f59bebb3f60>_Batch25_LR0.1_Epochs25.hdf5\n",
            "595/595 [==============================] - 9s 15ms/step - loss: 1.5265 - accuracy: 0.3436 - val_loss: 1.5180 - val_accuracy: 0.3399\n",
            "Epoch 7/25\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5252 - accuracy: 0.3436\n",
            "Epoch 00007: loss improved from 1.52648 to 1.52508, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.ftrl.Ftrl object at 0x7f59bebb3f60>_Batch25_LR0.1_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5251 - accuracy: 0.3436 - val_loss: 1.5060 - val_accuracy: 0.3399\n",
            "Epoch 8/25\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5244 - accuracy: 0.3435\n",
            "Epoch 00008: loss improved from 1.52508 to 1.52425, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.ftrl.Ftrl object at 0x7f59bebb3f60>_Batch25_LR0.1_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5243 - accuracy: 0.3436 - val_loss: 1.4930 - val_accuracy: 0.3399\n",
            "Epoch 9/25\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5230 - accuracy: 0.3437\n",
            "Epoch 00009: loss improved from 1.52425 to 1.52300, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.ftrl.Ftrl object at 0x7f59bebb3f60>_Batch25_LR0.1_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5230 - accuracy: 0.3436 - val_loss: 1.4994 - val_accuracy: 0.3399\n",
            "Epoch 10/25\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.5208 - accuracy: 0.3431\n",
            "Epoch 00010: loss improved from 1.52300 to 1.52039, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.ftrl.Ftrl object at 0x7f59bebb3f60>_Batch25_LR0.1_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5204 - accuracy: 0.3434 - val_loss: 1.4962 - val_accuracy: 0.3399\n",
            "Epoch 11/25\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5208 - accuracy: 0.3438\n",
            "Epoch 00011: loss did not improve from 1.52039\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5206 - accuracy: 0.3442 - val_loss: 1.4993 - val_accuracy: 0.3399\n",
            "Epoch 12/25\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5202 - accuracy: 0.3434\n",
            "Epoch 00012: loss improved from 1.52039 to 1.52011, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.ftrl.Ftrl object at 0x7f59bebb3f60>_Batch25_LR0.1_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5201 - accuracy: 0.3435 - val_loss: 1.5054 - val_accuracy: 0.3399\n",
            "Epoch 13/25\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5193 - accuracy: 0.3434\n",
            "Epoch 00013: loss improved from 1.52011 to 1.51957, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.ftrl.Ftrl object at 0x7f59bebb3f60>_Batch25_LR0.1_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5196 - accuracy: 0.3432 - val_loss: 1.5074 - val_accuracy: 0.3399\n",
            "Epoch 14/25\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5184 - accuracy: 0.3439\n",
            "Epoch 00014: loss improved from 1.51957 to 1.51878, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.ftrl.Ftrl object at 0x7f59bebb3f60>_Batch25_LR0.1_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5188 - accuracy: 0.3440 - val_loss: 1.5065 - val_accuracy: 0.3399\n",
            "Epoch 15/25\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5180 - accuracy: 0.3433\n",
            "Epoch 00015: loss improved from 1.51878 to 1.51763, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.ftrl.Ftrl object at 0x7f59bebb3f60>_Batch25_LR0.1_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5176 - accuracy: 0.3439 - val_loss: 1.5077 - val_accuracy: 0.3399\n",
            "Epoch 16/25\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.5166 - accuracy: 0.3436\n",
            "Epoch 00016: loss improved from 1.51763 to 1.51656, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.ftrl.Ftrl object at 0x7f59bebb3f60>_Batch25_LR0.1_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5166 - accuracy: 0.3439 - val_loss: 1.5037 - val_accuracy: 0.3399\n",
            "Epoch 17/25\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5146 - accuracy: 0.3444\n",
            "Epoch 00017: loss improved from 1.51656 to 1.51479, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.ftrl.Ftrl object at 0x7f59bebb3f60>_Batch25_LR0.1_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5148 - accuracy: 0.3443 - val_loss: 1.5019 - val_accuracy: 0.3399\n",
            "Epoch 18/25\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5148 - accuracy: 0.3447\n",
            "Epoch 00018: loss improved from 1.51479 to 1.51468, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.ftrl.Ftrl object at 0x7f59bebb3f60>_Batch25_LR0.1_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5147 - accuracy: 0.3448 - val_loss: 1.5070 - val_accuracy: 0.3399\n",
            "Epoch 19/25\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5148 - accuracy: 0.3456\n",
            "Epoch 00019: loss did not improve from 1.51468\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5147 - accuracy: 0.3455 - val_loss: 1.4981 - val_accuracy: 0.3399\n",
            "Epoch 20/25\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5139 - accuracy: 0.3437\n",
            "Epoch 00020: loss improved from 1.51468 to 1.51410, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.ftrl.Ftrl object at 0x7f59bebb3f60>_Batch25_LR0.1_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5141 - accuracy: 0.3434 - val_loss: 1.5058 - val_accuracy: 0.3399\n",
            "Epoch 21/25\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5142 - accuracy: 0.3446\n",
            "Epoch 00021: loss improved from 1.51410 to 1.51384, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.ftrl.Ftrl object at 0x7f59bebb3f60>_Batch25_LR0.1_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5138 - accuracy: 0.3447 - val_loss: 1.5191 - val_accuracy: 0.3348\n",
            "Epoch 22/25\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5117 - accuracy: 0.3433\n",
            "Epoch 00022: loss improved from 1.51384 to 1.51178, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.ftrl.Ftrl object at 0x7f59bebb3f60>_Batch25_LR0.1_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5118 - accuracy: 0.3432 - val_loss: 1.5181 - val_accuracy: 0.3407\n",
            "Epoch 23/25\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.5117 - accuracy: 0.3442\n",
            "Epoch 00023: loss did not improve from 1.51178\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5123 - accuracy: 0.3439 - val_loss: 1.5160 - val_accuracy: 0.3407\n",
            "Epoch 24/25\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.5112 - accuracy: 0.3427\n",
            "Epoch 00024: loss improved from 1.51178 to 1.51093, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.ftrl.Ftrl object at 0x7f59bebb3f60>_Batch25_LR0.1_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5109 - accuracy: 0.3432 - val_loss: 1.5018 - val_accuracy: 0.3399\n",
            "Epoch 25/25\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.5110 - accuracy: 0.3419\n",
            "Epoch 00025: loss did not improve from 1.51093\n",
            "595/595 [==============================] - 7s 12ms/step - loss: 1.5110 - accuracy: 0.3419 - val_loss: 1.4985 - val_accuracy: 0.3399\n",
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
            "Epoch 1/50\n",
            "  2/595 [..............................] - ETA: 42s - loss: 2.3351 - accuracy: 0.0600WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0154s vs `on_train_batch_end` time: 0.1259s). Check your callbacks.\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.6065 - accuracy: 0.3098\n",
            "Epoch 00001: loss improved from inf to 1.60648, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f59b97fbf28>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.6065 - accuracy: 0.3095 - val_loss: 1.5196 - val_accuracy: 0.3399\n",
            "Epoch 2/50\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.5625 - accuracy: 0.3265\n",
            "Epoch 00002: loss improved from 1.60648 to 1.56251, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f59b97fbf28>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5625 - accuracy: 0.3265 - val_loss: 1.5098 - val_accuracy: 0.3399\n",
            "Epoch 3/50\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5525 - accuracy: 0.3343\n",
            "Epoch 00003: loss improved from 1.56251 to 1.55234, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f59b97fbf28>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5523 - accuracy: 0.3342 - val_loss: 1.5095 - val_accuracy: 0.3399\n",
            "Epoch 4/50\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5487 - accuracy: 0.3321\n",
            "Epoch 00004: loss improved from 1.55234 to 1.54854, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f59b97fbf28>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5485 - accuracy: 0.3321 - val_loss: 1.5078 - val_accuracy: 0.3399\n",
            "Epoch 5/50\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5428 - accuracy: 0.3359\n",
            "Epoch 00005: loss improved from 1.54854 to 1.54251, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f59b97fbf28>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5425 - accuracy: 0.3362 - val_loss: 1.5071 - val_accuracy: 0.3399\n",
            "Epoch 6/50\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5403 - accuracy: 0.3355\n",
            "Epoch 00006: loss improved from 1.54251 to 1.54046, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f59b97fbf28>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5405 - accuracy: 0.3355 - val_loss: 1.5058 - val_accuracy: 0.3399\n",
            "Epoch 7/50\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5391 - accuracy: 0.3374\n",
            "Epoch 00007: loss improved from 1.54046 to 1.53896, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f59b97fbf28>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5390 - accuracy: 0.3376 - val_loss: 1.5050 - val_accuracy: 0.3399\n",
            "Epoch 8/50\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5379 - accuracy: 0.3426\n",
            "Epoch 00008: loss improved from 1.53896 to 1.53789, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f59b97fbf28>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5379 - accuracy: 0.3425 - val_loss: 1.5012 - val_accuracy: 0.3399\n",
            "Epoch 9/50\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5368 - accuracy: 0.3389\n",
            "Epoch 00009: loss improved from 1.53789 to 1.53665, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f59b97fbf28>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5366 - accuracy: 0.3389 - val_loss: 1.5054 - val_accuracy: 0.3399\n",
            "Epoch 10/50\n",
            "589/595 [============================>.] - ETA: 0s - loss: 1.5375 - accuracy: 0.3357\n",
            "Epoch 00010: loss did not improve from 1.53665\n",
            "595/595 [==============================] - 7s 12ms/step - loss: 1.5369 - accuracy: 0.3362 - val_loss: 1.5060 - val_accuracy: 0.3399\n",
            "Epoch 11/50\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5350 - accuracy: 0.3386\n",
            "Epoch 00011: loss improved from 1.53665 to 1.53507, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f59b97fbf28>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5351 - accuracy: 0.3385 - val_loss: 1.5057 - val_accuracy: 0.3399\n",
            "Epoch 12/50\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5320 - accuracy: 0.3431\n",
            "Epoch 00012: loss improved from 1.53507 to 1.53211, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f59b97fbf28>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5321 - accuracy: 0.3430 - val_loss: 1.5056 - val_accuracy: 0.3399\n",
            "Epoch 13/50\n",
            "589/595 [============================>.] - ETA: 0s - loss: 1.5311 - accuracy: 0.3410\n",
            "Epoch 00013: loss improved from 1.53211 to 1.53190, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f59b97fbf28>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5319 - accuracy: 0.3402 - val_loss: 1.5020 - val_accuracy: 0.3399\n",
            "Epoch 14/50\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5342 - accuracy: 0.3392\n",
            "Epoch 00014: loss did not improve from 1.53190\n",
            "595/595 [==============================] - 7s 12ms/step - loss: 1.5340 - accuracy: 0.3391 - val_loss: 1.5023 - val_accuracy: 0.3399\n",
            "Epoch 15/50\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.5316 - accuracy: 0.3408\n",
            "Epoch 00015: loss improved from 1.53190 to 1.53089, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f59b97fbf28>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5309 - accuracy: 0.3409 - val_loss: 1.4996 - val_accuracy: 0.3399\n",
            "Epoch 16/50\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5297 - accuracy: 0.3418\n",
            "Epoch 00016: loss improved from 1.53089 to 1.53003, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f59b97fbf28>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5300 - accuracy: 0.3412 - val_loss: 1.5029 - val_accuracy: 0.3399\n",
            "Epoch 17/50\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5313 - accuracy: 0.3418\n",
            "Epoch 00017: loss did not improve from 1.53003\n",
            "595/595 [==============================] - 7s 13ms/step - loss: 1.5312 - accuracy: 0.3418 - val_loss: 1.5032 - val_accuracy: 0.3399\n",
            "Epoch 18/50\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.5303 - accuracy: 0.3443\n",
            "Epoch 00018: loss did not improve from 1.53003\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5303 - accuracy: 0.3443 - val_loss: 1.5041 - val_accuracy: 0.3399\n",
            "Epoch 19/50\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5298 - accuracy: 0.3418\n",
            "Epoch 00019: loss improved from 1.53003 to 1.52972, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f59b97fbf28>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5297 - accuracy: 0.3420 - val_loss: 1.4993 - val_accuracy: 0.3399\n",
            "Epoch 20/50\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.5287 - accuracy: 0.3397\n",
            "Epoch 00020: loss improved from 1.52972 to 1.52872, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f59b97fbf28>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5287 - accuracy: 0.3397 - val_loss: 1.5041 - val_accuracy: 0.3399\n",
            "Epoch 21/50\n",
            "589/595 [============================>.] - ETA: 0s - loss: 1.5303 - accuracy: 0.3444\n",
            "Epoch 00021: loss did not improve from 1.52872\n",
            "595/595 [==============================] - 7s 12ms/step - loss: 1.5300 - accuracy: 0.3442 - val_loss: 1.4988 - val_accuracy: 0.3399\n",
            "Epoch 22/50\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5288 - accuracy: 0.3429\n",
            "Epoch 00022: loss did not improve from 1.52872\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5288 - accuracy: 0.3430 - val_loss: 1.4982 - val_accuracy: 0.3399\n",
            "Epoch 23/50\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5276 - accuracy: 0.3426\n",
            "Epoch 00023: loss improved from 1.52872 to 1.52765, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f59b97fbf28>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5277 - accuracy: 0.3424 - val_loss: 1.5030 - val_accuracy: 0.3399\n",
            "Epoch 24/50\n",
            "589/595 [============================>.] - ETA: 0s - loss: 1.5279 - accuracy: 0.3428\n",
            "Epoch 00024: loss did not improve from 1.52765\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5277 - accuracy: 0.3433 - val_loss: 1.5051 - val_accuracy: 0.3399\n",
            "Epoch 25/50\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5270 - accuracy: 0.3421\n",
            "Epoch 00025: loss improved from 1.52765 to 1.52694, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f59b97fbf28>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5269 - accuracy: 0.3420 - val_loss: 1.5011 - val_accuracy: 0.3399\n",
            "Epoch 26/50\n",
            "589/595 [============================>.] - ETA: 0s - loss: 1.5272 - accuracy: 0.3429\n",
            "Epoch 00026: loss did not improve from 1.52694\n",
            "595/595 [==============================] - 7s 13ms/step - loss: 1.5273 - accuracy: 0.3431 - val_loss: 1.5014 - val_accuracy: 0.3399\n",
            "Epoch 27/50\n",
            "589/595 [============================>.] - ETA: 0s - loss: 1.5279 - accuracy: 0.3432\n",
            "Epoch 00027: loss did not improve from 1.52694\n",
            "595/595 [==============================] - 7s 12ms/step - loss: 1.5280 - accuracy: 0.3434 - val_loss: 1.5021 - val_accuracy: 0.3399\n",
            "Epoch 28/50\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.5270 - accuracy: 0.3426\n",
            "Epoch 00028: loss did not improve from 1.52694\n",
            "595/595 [==============================] - 7s 12ms/step - loss: 1.5271 - accuracy: 0.3429 - val_loss: 1.5011 - val_accuracy: 0.3399\n",
            "Epoch 29/50\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5270 - accuracy: 0.3436\n",
            "Epoch 00029: loss improved from 1.52694 to 1.52684, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f59b97fbf28>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5268 - accuracy: 0.3432 - val_loss: 1.4991 - val_accuracy: 0.3399\n",
            "Epoch 30/50\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5266 - accuracy: 0.3425\n",
            "Epoch 00030: loss did not improve from 1.52684\n",
            "595/595 [==============================] - 7s 13ms/step - loss: 1.5268 - accuracy: 0.3420 - val_loss: 1.4988 - val_accuracy: 0.3399\n",
            "Epoch 31/50\n",
            "589/595 [============================>.] - ETA: 0s - loss: 1.5278 - accuracy: 0.3429\n",
            "Epoch 00031: loss did not improve from 1.52684\n",
            "595/595 [==============================] - 7s 13ms/step - loss: 1.5274 - accuracy: 0.3432 - val_loss: 1.5006 - val_accuracy: 0.3399\n",
            "Epoch 32/50\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.5273 - accuracy: 0.3425\n",
            "Epoch 00032: loss did not improve from 1.52684\n",
            "595/595 [==============================] - 7s 12ms/step - loss: 1.5276 - accuracy: 0.3428 - val_loss: 1.5044 - val_accuracy: 0.3399\n",
            "Epoch 33/50\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5244 - accuracy: 0.3437\n",
            "Epoch 00033: loss improved from 1.52684 to 1.52508, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f59b97fbf28>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5251 - accuracy: 0.3437 - val_loss: 1.4996 - val_accuracy: 0.3399\n",
            "Epoch 34/50\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.5272 - accuracy: 0.3422\n",
            "Epoch 00034: loss did not improve from 1.52508\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5274 - accuracy: 0.3420 - val_loss: 1.5001 - val_accuracy: 0.3399\n",
            "Epoch 35/50\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5261 - accuracy: 0.3426\n",
            "Epoch 00035: loss did not improve from 1.52508\n",
            "595/595 [==============================] - 7s 12ms/step - loss: 1.5266 - accuracy: 0.3425 - val_loss: 1.5043 - val_accuracy: 0.3399\n",
            "Epoch 36/50\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.5258 - accuracy: 0.3434\n",
            "Epoch 00036: loss did not improve from 1.52508\n",
            "595/595 [==============================] - 7s 13ms/step - loss: 1.5258 - accuracy: 0.3434 - val_loss: 1.5010 - val_accuracy: 0.3399\n",
            "Epoch 37/50\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5246 - accuracy: 0.3442\n",
            "Epoch 00037: loss improved from 1.52508 to 1.52491, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f59b97fbf28>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5249 - accuracy: 0.3437 - val_loss: 1.4982 - val_accuracy: 0.3399\n",
            "Epoch 38/50\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.5248 - accuracy: 0.3418\n",
            "Epoch 00038: loss improved from 1.52491 to 1.52474, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f59b97fbf28>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5247 - accuracy: 0.3417 - val_loss: 1.4975 - val_accuracy: 0.3399\n",
            "Epoch 39/50\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5234 - accuracy: 0.3438\n",
            "Epoch 00039: loss improved from 1.52474 to 1.52315, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f59b97fbf28>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5231 - accuracy: 0.3440 - val_loss: 1.4962 - val_accuracy: 0.3399\n",
            "Epoch 40/50\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5248 - accuracy: 0.3428\n",
            "Epoch 00040: loss did not improve from 1.52315\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5247 - accuracy: 0.3427 - val_loss: 1.4970 - val_accuracy: 0.3399\n",
            "Epoch 41/50\n",
            "589/595 [============================>.] - ETA: 0s - loss: 1.5243 - accuracy: 0.3432\n",
            "Epoch 00041: loss did not improve from 1.52315\n",
            "595/595 [==============================] - 7s 12ms/step - loss: 1.5243 - accuracy: 0.3435 - val_loss: 1.4986 - val_accuracy: 0.3399\n",
            "Epoch 42/50\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5257 - accuracy: 0.3423\n",
            "Epoch 00042: loss did not improve from 1.52315\n",
            "595/595 [==============================] - 7s 12ms/step - loss: 1.5256 - accuracy: 0.3423 - val_loss: 1.5012 - val_accuracy: 0.3399\n",
            "Epoch 43/50\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5237 - accuracy: 0.3427\n",
            "Epoch 00043: loss did not improve from 1.52315\n",
            "595/595 [==============================] - 7s 12ms/step - loss: 1.5237 - accuracy: 0.3426 - val_loss: 1.5006 - val_accuracy: 0.3399\n",
            "Epoch 44/50\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5234 - accuracy: 0.3437\n",
            "Epoch 00044: loss did not improve from 1.52315\n",
            "595/595 [==============================] - 7s 12ms/step - loss: 1.5233 - accuracy: 0.3436 - val_loss: 1.5021 - val_accuracy: 0.3399\n",
            "Epoch 45/50\n",
            "589/595 [============================>.] - ETA: 0s - loss: 1.5238 - accuracy: 0.3434\n",
            "Epoch 00045: loss did not improve from 1.52315\n",
            "595/595 [==============================] - 7s 12ms/step - loss: 1.5235 - accuracy: 0.3439 - val_loss: 1.5009 - val_accuracy: 0.3399\n",
            "Epoch 46/50\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.5232 - accuracy: 0.3449\n",
            "Epoch 00046: loss did not improve from 1.52315\n",
            "595/595 [==============================] - 7s 12ms/step - loss: 1.5233 - accuracy: 0.3449 - val_loss: 1.5000 - val_accuracy: 0.3399\n",
            "Epoch 47/50\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5229 - accuracy: 0.3438\n",
            "Epoch 00047: loss improved from 1.52315 to 1.52275, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f59b97fbf28>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5228 - accuracy: 0.3439 - val_loss: 1.4996 - val_accuracy: 0.3399\n",
            "Epoch 48/50\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5220 - accuracy: 0.3438\n",
            "Epoch 00048: loss improved from 1.52275 to 1.52268, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f59b97fbf28>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5227 - accuracy: 0.3436 - val_loss: 1.4985 - val_accuracy: 0.3399\n",
            "Epoch 49/50\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5223 - accuracy: 0.3432\n",
            "Epoch 00049: loss did not improve from 1.52268\n",
            "595/595 [==============================] - 7s 13ms/step - loss: 1.5228 - accuracy: 0.3430 - val_loss: 1.5006 - val_accuracy: 0.3399\n",
            "Epoch 50/50\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5221 - accuracy: 0.3433\n",
            "Epoch 00050: loss improved from 1.52268 to 1.52242, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f59b97fbf28>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 10s 17ms/step - loss: 1.5224 - accuracy: 0.3432 - val_loss: 1.5030 - val_accuracy: 0.3399\n",
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
            "Epoch 1/50\n",
            "  2/595 [..............................] - ETA: 52s - loss: 2.1366 - accuracy: 0.1600WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0158s vs `on_train_batch_end` time: 0.1613s). Check your callbacks.\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5460 - accuracy: 0.3316\n",
            "Epoch 00001: loss improved from inf to 1.54612, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.rmsprop.RMSprop object at 0x7f59b9879470>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5461 - accuracy: 0.3313 - val_loss: 1.5033 - val_accuracy: 0.3399\n",
            "Epoch 2/50\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5286 - accuracy: 0.3405\n",
            "Epoch 00002: loss improved from 1.54612 to 1.52856, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.rmsprop.RMSprop object at 0x7f59b9879470>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5286 - accuracy: 0.3404 - val_loss: 1.4959 - val_accuracy: 0.3399\n",
            "Epoch 3/50\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5234 - accuracy: 0.3431\n",
            "Epoch 00003: loss improved from 1.52856 to 1.52350, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.rmsprop.RMSprop object at 0x7f59b9879470>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5235 - accuracy: 0.3432 - val_loss: 1.4909 - val_accuracy: 0.3399\n",
            "Epoch 4/50\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5203 - accuracy: 0.3426\n",
            "Epoch 00004: loss improved from 1.52350 to 1.52067, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.rmsprop.RMSprop object at 0x7f59b9879470>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5207 - accuracy: 0.3426 - val_loss: 1.4865 - val_accuracy: 0.3399\n",
            "Epoch 5/50\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5175 - accuracy: 0.3423\n",
            "Epoch 00005: loss improved from 1.52067 to 1.51755, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.rmsprop.RMSprop object at 0x7f59b9879470>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5176 - accuracy: 0.3423 - val_loss: 1.5161 - val_accuracy: 0.3372\n",
            "Epoch 6/50\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.5139 - accuracy: 0.3458\n",
            "Epoch 00006: loss improved from 1.51755 to 1.51391, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.rmsprop.RMSprop object at 0x7f59b9879470>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5139 - accuracy: 0.3458 - val_loss: 1.4960 - val_accuracy: 0.3399\n",
            "Epoch 7/50\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5138 - accuracy: 0.3434\n",
            "Epoch 00007: loss improved from 1.51391 to 1.51385, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.rmsprop.RMSprop object at 0x7f59b9879470>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5139 - accuracy: 0.3434 - val_loss: 1.4899 - val_accuracy: 0.3372\n",
            "Epoch 8/50\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5102 - accuracy: 0.3466\n",
            "Epoch 00008: loss improved from 1.51385 to 1.51018, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.rmsprop.RMSprop object at 0x7f59b9879470>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5102 - accuracy: 0.3462 - val_loss: 1.4943 - val_accuracy: 0.3321\n",
            "Epoch 9/50\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.5088 - accuracy: 0.3460\n",
            "Epoch 00009: loss improved from 1.51018 to 1.50911, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.rmsprop.RMSprop object at 0x7f59b9879470>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5091 - accuracy: 0.3460 - val_loss: 1.5073 - val_accuracy: 0.3348\n",
            "Epoch 10/50\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.5083 - accuracy: 0.3461\n",
            "Epoch 00010: loss improved from 1.50911 to 1.50829, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.rmsprop.RMSprop object at 0x7f59b9879470>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5083 - accuracy: 0.3461 - val_loss: 1.4933 - val_accuracy: 0.3337\n",
            "Epoch 11/50\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5053 - accuracy: 0.3492\n",
            "Epoch 00011: loss improved from 1.50829 to 1.50524, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.rmsprop.RMSprop object at 0x7f59b9879470>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5052 - accuracy: 0.3491 - val_loss: 1.4981 - val_accuracy: 0.3138\n",
            "Epoch 12/50\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5022 - accuracy: 0.3454\n",
            "Epoch 00012: loss improved from 1.50524 to 1.50208, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.rmsprop.RMSprop object at 0x7f59b9879470>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5021 - accuracy: 0.3453 - val_loss: 1.5023 - val_accuracy: 0.3270\n",
            "Epoch 13/50\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5034 - accuracy: 0.3448\n",
            "Epoch 00013: loss did not improve from 1.50208\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5036 - accuracy: 0.3448 - val_loss: 1.5023 - val_accuracy: 0.3407\n",
            "Epoch 14/50\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5023 - accuracy: 0.3484\n",
            "Epoch 00014: loss did not improve from 1.50208\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5023 - accuracy: 0.3484 - val_loss: 1.5032 - val_accuracy: 0.3321\n",
            "Epoch 15/50\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.5005 - accuracy: 0.3484\n",
            "Epoch 00015: loss improved from 1.50208 to 1.50048, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.rmsprop.RMSprop object at 0x7f59b9879470>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5005 - accuracy: 0.3484 - val_loss: 1.4884 - val_accuracy: 0.3305\n",
            "Epoch 16/50\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.4993 - accuracy: 0.3491\n",
            "Epoch 00016: loss improved from 1.50048 to 1.49936, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.rmsprop.RMSprop object at 0x7f59b9879470>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.4994 - accuracy: 0.3487 - val_loss: 1.5012 - val_accuracy: 0.3122\n",
            "Epoch 17/50\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.4980 - accuracy: 0.3478\n",
            "Epoch 00017: loss improved from 1.49936 to 1.49815, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.rmsprop.RMSprop object at 0x7f59b9879470>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.4982 - accuracy: 0.3477 - val_loss: 1.4997 - val_accuracy: 0.3256\n",
            "Epoch 18/50\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.4998 - accuracy: 0.3485\n",
            "Epoch 00018: loss did not improve from 1.49815\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.4998 - accuracy: 0.3485 - val_loss: 1.5157 - val_accuracy: 0.3248\n",
            "Epoch 19/50\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.4945 - accuracy: 0.3569\n",
            "Epoch 00019: loss improved from 1.49815 to 1.49437, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.rmsprop.RMSprop object at 0x7f59b9879470>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.4944 - accuracy: 0.3569 - val_loss: 1.5051 - val_accuracy: 0.3307\n",
            "Epoch 20/50\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.4955 - accuracy: 0.3519\n",
            "Epoch 00020: loss did not improve from 1.49437\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.4956 - accuracy: 0.3519 - val_loss: 1.5244 - val_accuracy: 0.3237\n",
            "Epoch 21/50\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.4950 - accuracy: 0.3540\n",
            "Epoch 00021: loss did not improve from 1.49437\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.4952 - accuracy: 0.3543 - val_loss: 1.5356 - val_accuracy: 0.3353\n",
            "Epoch 22/50\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.4942 - accuracy: 0.3526\n",
            "Epoch 00022: loss improved from 1.49437 to 1.49422, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.rmsprop.RMSprop object at 0x7f59b9879470>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.4942 - accuracy: 0.3525 - val_loss: 1.5225 - val_accuracy: 0.3178\n",
            "Epoch 23/50\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.4904 - accuracy: 0.3538\n",
            "Epoch 00023: loss improved from 1.49422 to 1.49062, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.rmsprop.RMSprop object at 0x7f59b9879470>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.4906 - accuracy: 0.3536 - val_loss: 1.5259 - val_accuracy: 0.3369\n",
            "Epoch 24/50\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.4913 - accuracy: 0.3539\n",
            "Epoch 00024: loss did not improve from 1.49062\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.4910 - accuracy: 0.3543 - val_loss: 1.5472 - val_accuracy: 0.3127\n",
            "Epoch 25/50\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.4916 - accuracy: 0.3600\n",
            "Epoch 00025: loss did not improve from 1.49062\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.4917 - accuracy: 0.3600 - val_loss: 1.5451 - val_accuracy: 0.3202\n",
            "Epoch 26/50\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.4902 - accuracy: 0.3572\n",
            "Epoch 00026: loss improved from 1.49062 to 1.49008, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.rmsprop.RMSprop object at 0x7f59b9879470>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.4901 - accuracy: 0.3574 - val_loss: 1.5259 - val_accuracy: 0.3227\n",
            "Epoch 27/50\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.4898 - accuracy: 0.3540\n",
            "Epoch 00027: loss improved from 1.49008 to 1.48953, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.rmsprop.RMSprop object at 0x7f59b9879470>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.4895 - accuracy: 0.3545 - val_loss: 1.5468 - val_accuracy: 0.3229\n",
            "Epoch 28/50\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.4896 - accuracy: 0.3556\n",
            "Epoch 00028: loss did not improve from 1.48953\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.4897 - accuracy: 0.3555 - val_loss: 1.5496 - val_accuracy: 0.3205\n",
            "Epoch 29/50\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.4889 - accuracy: 0.3593\n",
            "Epoch 00029: loss improved from 1.48953 to 1.48903, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.rmsprop.RMSprop object at 0x7f59b9879470>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.4890 - accuracy: 0.3591 - val_loss: 1.5435 - val_accuracy: 0.3450\n",
            "Epoch 30/50\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.4868 - accuracy: 0.3560\n",
            "Epoch 00030: loss improved from 1.48903 to 1.48654, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.rmsprop.RMSprop object at 0x7f59b9879470>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.4865 - accuracy: 0.3559 - val_loss: 1.5485 - val_accuracy: 0.3248\n",
            "Epoch 31/50\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.4846 - accuracy: 0.3618\n",
            "Epoch 00031: loss improved from 1.48654 to 1.48472, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.rmsprop.RMSprop object at 0x7f59b9879470>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.4847 - accuracy: 0.3615 - val_loss: 1.5656 - val_accuracy: 0.3399\n",
            "Epoch 32/50\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.4828 - accuracy: 0.3589\n",
            "Epoch 00032: loss improved from 1.48472 to 1.48311, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.rmsprop.RMSprop object at 0x7f59b9879470>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.4831 - accuracy: 0.3588 - val_loss: 1.5578 - val_accuracy: 0.3315\n",
            "Epoch 33/50\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.4839 - accuracy: 0.3646\n",
            "Epoch 00033: loss did not improve from 1.48311\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.4841 - accuracy: 0.3644 - val_loss: 1.5585 - val_accuracy: 0.3402\n",
            "Epoch 34/50\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.4831 - accuracy: 0.3621\n",
            "Epoch 00034: loss improved from 1.48311 to 1.48309, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.rmsprop.RMSprop object at 0x7f59b9879470>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.4831 - accuracy: 0.3621 - val_loss: 1.5644 - val_accuracy: 0.3380\n",
            "Epoch 35/50\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.4863 - accuracy: 0.3641\n",
            "Epoch 00035: loss did not improve from 1.48309\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.4861 - accuracy: 0.3642 - val_loss: 1.5996 - val_accuracy: 0.3186\n",
            "Epoch 36/50\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.4834 - accuracy: 0.3628\n",
            "Epoch 00036: loss did not improve from 1.48309\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.4841 - accuracy: 0.3626 - val_loss: 1.6068 - val_accuracy: 0.3219\n",
            "Epoch 37/50\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.4822 - accuracy: 0.3614\n",
            "Epoch 00037: loss improved from 1.48309 to 1.48163, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.rmsprop.RMSprop object at 0x7f59b9879470>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.4816 - accuracy: 0.3617 - val_loss: 1.5950 - val_accuracy: 0.3219\n",
            "Epoch 38/50\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.4813 - accuracy: 0.3644\n",
            "Epoch 00038: loss improved from 1.48163 to 1.48112, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.rmsprop.RMSprop object at 0x7f59b9879470>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.4811 - accuracy: 0.3644 - val_loss: 1.6051 - val_accuracy: 0.3318\n",
            "Epoch 39/50\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.4818 - accuracy: 0.3610\n",
            "Epoch 00039: loss did not improve from 1.48112\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.4817 - accuracy: 0.3611 - val_loss: 1.6593 - val_accuracy: 0.3130\n",
            "Epoch 40/50\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.4805 - accuracy: 0.3683\n",
            "Epoch 00040: loss improved from 1.48112 to 1.48046, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.rmsprop.RMSprop object at 0x7f59b9879470>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.4805 - accuracy: 0.3683 - val_loss: 1.6470 - val_accuracy: 0.3149\n",
            "Epoch 41/50\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.4799 - accuracy: 0.3651\n",
            "Epoch 00041: loss improved from 1.48046 to 1.48022, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.rmsprop.RMSprop object at 0x7f59b9879470>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.4802 - accuracy: 0.3651 - val_loss: 1.6367 - val_accuracy: 0.3170\n",
            "Epoch 42/50\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.4770 - accuracy: 0.3632\n",
            "Epoch 00042: loss improved from 1.48022 to 1.47775, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.rmsprop.RMSprop object at 0x7f59b9879470>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.4778 - accuracy: 0.3634 - val_loss: 1.6119 - val_accuracy: 0.3248\n",
            "Epoch 43/50\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.4770 - accuracy: 0.3607\n",
            "Epoch 00043: loss improved from 1.47775 to 1.47708, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.rmsprop.RMSprop object at 0x7f59b9879470>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.4771 - accuracy: 0.3605 - val_loss: 1.6690 - val_accuracy: 0.3315\n",
            "Epoch 44/50\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.4766 - accuracy: 0.3664\n",
            "Epoch 00044: loss improved from 1.47708 to 1.47662, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.rmsprop.RMSprop object at 0x7f59b9879470>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.4766 - accuracy: 0.3662 - val_loss: 1.6762 - val_accuracy: 0.3235\n",
            "Epoch 45/50\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.4753 - accuracy: 0.3686\n",
            "Epoch 00045: loss improved from 1.47662 to 1.47584, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.rmsprop.RMSprop object at 0x7f59b9879470>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.4758 - accuracy: 0.3681 - val_loss: 1.6771 - val_accuracy: 0.3087\n",
            "Epoch 46/50\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.4750 - accuracy: 0.3685\n",
            "Epoch 00046: loss improved from 1.47584 to 1.47517, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.rmsprop.RMSprop object at 0x7f59b9879470>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.4752 - accuracy: 0.3683 - val_loss: 1.7080 - val_accuracy: 0.3251\n",
            "Epoch 47/50\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.4763 - accuracy: 0.3665\n",
            "Epoch 00047: loss did not improve from 1.47517\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.4763 - accuracy: 0.3665 - val_loss: 1.6730 - val_accuracy: 0.3235\n",
            "Epoch 48/50\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.4746 - accuracy: 0.3641\n",
            "Epoch 00048: loss improved from 1.47517 to 1.47514, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.rmsprop.RMSprop object at 0x7f59b9879470>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.4751 - accuracy: 0.3642 - val_loss: 1.7208 - val_accuracy: 0.3232\n",
            "Epoch 49/50\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.4778 - accuracy: 0.3703\n",
            "Epoch 00049: loss did not improve from 1.47514\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.4778 - accuracy: 0.3703 - val_loss: 1.7109 - val_accuracy: 0.3127\n",
            "Epoch 50/50\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.4761 - accuracy: 0.3666\n",
            "Epoch 00050: loss did not improve from 1.47514\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.4761 - accuracy: 0.3666 - val_loss: 1.7159 - val_accuracy: 0.3184\n",
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
            "Epoch 1/50\n",
            "  2/595 [..............................] - ETA: 34s - loss: 2.1142 - accuracy: 0.2200WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0160s vs `on_train_batch_end` time: 0.1006s). Check your callbacks.\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5489 - accuracy: 0.3312\n",
            "Epoch 00001: loss improved from inf to 1.54839, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f59c41d7fd0>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5484 - accuracy: 0.3314 - val_loss: 1.4996 - val_accuracy: 0.3399\n",
            "Epoch 2/50\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.5272 - accuracy: 0.3434\n",
            "Epoch 00002: loss improved from 1.54839 to 1.52717, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f59c41d7fd0>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5272 - accuracy: 0.3434 - val_loss: 1.4991 - val_accuracy: 0.3399\n",
            "Epoch 3/50\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.5214 - accuracy: 0.3435\n",
            "Epoch 00003: loss improved from 1.52717 to 1.52144, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f59c41d7fd0>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5214 - accuracy: 0.3435 - val_loss: 1.4975 - val_accuracy: 0.3372\n",
            "Epoch 4/50\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.5188 - accuracy: 0.3401\n",
            "Epoch 00004: loss improved from 1.52144 to 1.51883, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f59c41d7fd0>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5188 - accuracy: 0.3401 - val_loss: 1.4923 - val_accuracy: 0.3372\n",
            "Epoch 5/50\n",
            "589/595 [============================>.] - ETA: 0s - loss: 1.5171 - accuracy: 0.3447\n",
            "Epoch 00005: loss improved from 1.51883 to 1.51661, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f59c41d7fd0>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5166 - accuracy: 0.3451 - val_loss: 1.4929 - val_accuracy: 0.3372\n",
            "Epoch 6/50\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5143 - accuracy: 0.3436\n",
            "Epoch 00006: loss improved from 1.51661 to 1.51436, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f59c41d7fd0>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5144 - accuracy: 0.3436 - val_loss: 1.5010 - val_accuracy: 0.3372\n",
            "Epoch 7/50\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.5124 - accuracy: 0.3463\n",
            "Epoch 00007: loss improved from 1.51436 to 1.51254, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f59c41d7fd0>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5125 - accuracy: 0.3461 - val_loss: 1.4939 - val_accuracy: 0.3367\n",
            "Epoch 8/50\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.5105 - accuracy: 0.3396\n",
            "Epoch 00008: loss improved from 1.51254 to 1.51046, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f59c41d7fd0>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5105 - accuracy: 0.3396 - val_loss: 1.4925 - val_accuracy: 0.3372\n",
            "Epoch 9/50\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5080 - accuracy: 0.3465\n",
            "Epoch 00009: loss improved from 1.51046 to 1.50846, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f59c41d7fd0>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5085 - accuracy: 0.3463 - val_loss: 1.4923 - val_accuracy: 0.3372\n",
            "Epoch 10/50\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5069 - accuracy: 0.3414\n",
            "Epoch 00010: loss improved from 1.50846 to 1.50677, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f59c41d7fd0>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5068 - accuracy: 0.3416 - val_loss: 1.4932 - val_accuracy: 0.3245\n",
            "Epoch 11/50\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5065 - accuracy: 0.3462\n",
            "Epoch 00011: loss improved from 1.50677 to 1.50629, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f59c41d7fd0>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5063 - accuracy: 0.3464 - val_loss: 1.4920 - val_accuracy: 0.3259\n",
            "Epoch 12/50\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5030 - accuracy: 0.3505\n",
            "Epoch 00012: loss improved from 1.50629 to 1.50314, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f59c41d7fd0>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5031 - accuracy: 0.3504 - val_loss: 1.4959 - val_accuracy: 0.3332\n",
            "Epoch 13/50\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5007 - accuracy: 0.3443\n",
            "Epoch 00013: loss improved from 1.50314 to 1.50113, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f59c41d7fd0>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5011 - accuracy: 0.3440 - val_loss: 1.5004 - val_accuracy: 0.3364\n",
            "Epoch 14/50\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.4992 - accuracy: 0.3470\n",
            "Epoch 00014: loss improved from 1.50113 to 1.49960, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f59c41d7fd0>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.4996 - accuracy: 0.3468 - val_loss: 1.5030 - val_accuracy: 0.3358\n",
            "Epoch 15/50\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5011 - accuracy: 0.3430\n",
            "Epoch 00015: loss did not improve from 1.49960\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5015 - accuracy: 0.3429 - val_loss: 1.4969 - val_accuracy: 0.3297\n",
            "Epoch 16/50\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.4987 - accuracy: 0.3448\n",
            "Epoch 00016: loss improved from 1.49960 to 1.49812, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f59c41d7fd0>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.4981 - accuracy: 0.3448 - val_loss: 1.5037 - val_accuracy: 0.3385\n",
            "Epoch 17/50\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.4949 - accuracy: 0.3480\n",
            "Epoch 00017: loss improved from 1.49812 to 1.49487, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f59c41d7fd0>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.4949 - accuracy: 0.3480 - val_loss: 1.5027 - val_accuracy: 0.3092\n",
            "Epoch 18/50\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.4927 - accuracy: 0.3526\n",
            "Epoch 00018: loss improved from 1.49487 to 1.49322, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f59c41d7fd0>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.4932 - accuracy: 0.3522 - val_loss: 1.4928 - val_accuracy: 0.3189\n",
            "Epoch 19/50\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.4925 - accuracy: 0.3500\n",
            "Epoch 00019: loss improved from 1.49322 to 1.49267, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f59c41d7fd0>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.4927 - accuracy: 0.3497 - val_loss: 1.5023 - val_accuracy: 0.3270\n",
            "Epoch 20/50\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.4903 - accuracy: 0.3536\n",
            "Epoch 00020: loss improved from 1.49267 to 1.49038, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f59c41d7fd0>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.4904 - accuracy: 0.3535 - val_loss: 1.4957 - val_accuracy: 0.3143\n",
            "Epoch 21/50\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.4893 - accuracy: 0.3513\n",
            "Epoch 00021: loss improved from 1.49038 to 1.48934, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f59c41d7fd0>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.4893 - accuracy: 0.3513 - val_loss: 1.4833 - val_accuracy: 0.3302\n",
            "Epoch 22/50\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.4881 - accuracy: 0.3514\n",
            "Epoch 00022: loss improved from 1.48934 to 1.48806, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f59c41d7fd0>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.4881 - accuracy: 0.3514 - val_loss: 1.4888 - val_accuracy: 0.3348\n",
            "Epoch 23/50\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.4865 - accuracy: 0.3543\n",
            "Epoch 00023: loss improved from 1.48806 to 1.48650, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f59c41d7fd0>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.4865 - accuracy: 0.3543 - val_loss: 1.5094 - val_accuracy: 0.3108\n",
            "Epoch 24/50\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.4876 - accuracy: 0.3522\n",
            "Epoch 00024: loss did not improve from 1.48650\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.4876 - accuracy: 0.3521 - val_loss: 1.5056 - val_accuracy: 0.3345\n",
            "Epoch 25/50\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.4864 - accuracy: 0.3555\n",
            "Epoch 00025: loss did not improve from 1.48650\n",
            "595/595 [==============================] - 7s 13ms/step - loss: 1.4868 - accuracy: 0.3554 - val_loss: 1.4891 - val_accuracy: 0.3393\n",
            "Epoch 26/50\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.4838 - accuracy: 0.3561\n",
            "Epoch 00026: loss improved from 1.48650 to 1.48367, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f59c41d7fd0>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.4837 - accuracy: 0.3566 - val_loss: 1.4946 - val_accuracy: 0.3089\n",
            "Epoch 27/50\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.4830 - accuracy: 0.3530\n",
            "Epoch 00027: loss improved from 1.48367 to 1.48297, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f59c41d7fd0>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.4830 - accuracy: 0.3532 - val_loss: 1.5004 - val_accuracy: 0.3272\n",
            "Epoch 28/50\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.4812 - accuracy: 0.3550\n",
            "Epoch 00028: loss improved from 1.48297 to 1.48077, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f59c41d7fd0>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.4808 - accuracy: 0.3549 - val_loss: 1.5266 - val_accuracy: 0.3221\n",
            "Epoch 29/50\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.4809 - accuracy: 0.3589\n",
            "Epoch 00029: loss improved from 1.48077 to 1.48072, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f59c41d7fd0>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.4807 - accuracy: 0.3591 - val_loss: 1.5171 - val_accuracy: 0.3216\n",
            "Epoch 30/50\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.4800 - accuracy: 0.3616\n",
            "Epoch 00030: loss improved from 1.48072 to 1.47967, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f59c41d7fd0>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.4797 - accuracy: 0.3616 - val_loss: 1.5120 - val_accuracy: 0.3391\n",
            "Epoch 31/50\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.4767 - accuracy: 0.3636\n",
            "Epoch 00031: loss improved from 1.47967 to 1.47647, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f59c41d7fd0>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.4765 - accuracy: 0.3636 - val_loss: 1.5117 - val_accuracy: 0.3353\n",
            "Epoch 32/50\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.4758 - accuracy: 0.3671\n",
            "Epoch 00032: loss improved from 1.47647 to 1.47612, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f59c41d7fd0>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.4761 - accuracy: 0.3668 - val_loss: 1.5309 - val_accuracy: 0.3157\n",
            "Epoch 33/50\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.4751 - accuracy: 0.3640\n",
            "Epoch 00033: loss improved from 1.47612 to 1.47511, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f59c41d7fd0>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.4751 - accuracy: 0.3638 - val_loss: 1.5156 - val_accuracy: 0.3399\n",
            "Epoch 34/50\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.4754 - accuracy: 0.3634\n",
            "Epoch 00034: loss did not improve from 1.47511\n",
            "595/595 [==============================] - 7s 13ms/step - loss: 1.4759 - accuracy: 0.3634 - val_loss: 1.5284 - val_accuracy: 0.3235\n",
            "Epoch 35/50\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.4707 - accuracy: 0.3696\n",
            "Epoch 00035: loss improved from 1.47511 to 1.47077, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f59c41d7fd0>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.4708 - accuracy: 0.3696 - val_loss: 1.5433 - val_accuracy: 0.3229\n",
            "Epoch 36/50\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.4745 - accuracy: 0.3657\n",
            "Epoch 00036: loss did not improve from 1.47077\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.4745 - accuracy: 0.3657 - val_loss: 1.5182 - val_accuracy: 0.3345\n",
            "Epoch 37/50\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.4733 - accuracy: 0.3620\n",
            "Epoch 00037: loss did not improve from 1.47077\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.4731 - accuracy: 0.3622 - val_loss: 1.5358 - val_accuracy: 0.3216\n",
            "Epoch 38/50\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.4728 - accuracy: 0.3642\n",
            "Epoch 00038: loss did not improve from 1.47077\n",
            "595/595 [==============================] - 7s 13ms/step - loss: 1.4729 - accuracy: 0.3644 - val_loss: 1.5747 - val_accuracy: 0.3159\n",
            "Epoch 39/50\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.4717 - accuracy: 0.3657\n",
            "Epoch 00039: loss did not improve from 1.47077\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.4717 - accuracy: 0.3657 - val_loss: 1.5523 - val_accuracy: 0.3264\n",
            "Epoch 40/50\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.4688 - accuracy: 0.3667\n",
            "Epoch 00040: loss improved from 1.47077 to 1.46867, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f59c41d7fd0>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.4687 - accuracy: 0.3667 - val_loss: 1.5319 - val_accuracy: 0.3385\n",
            "Epoch 41/50\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.4670 - accuracy: 0.3681\n",
            "Epoch 00041: loss improved from 1.46867 to 1.46697, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f59c41d7fd0>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.4670 - accuracy: 0.3681 - val_loss: 1.5752 - val_accuracy: 0.3275\n",
            "Epoch 42/50\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.4677 - accuracy: 0.3696\n",
            "Epoch 00042: loss did not improve from 1.46697\n",
            "595/595 [==============================] - 7s 13ms/step - loss: 1.4678 - accuracy: 0.3693 - val_loss: 1.5681 - val_accuracy: 0.3469\n",
            "Epoch 43/50\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.4671 - accuracy: 0.3643\n",
            "Epoch 00043: loss did not improve from 1.46697\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.4671 - accuracy: 0.3646 - val_loss: 1.5685 - val_accuracy: 0.3297\n",
            "Epoch 44/50\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.4643 - accuracy: 0.3661\n",
            "Epoch 00044: loss improved from 1.46697 to 1.46454, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f59c41d7fd0>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.4645 - accuracy: 0.3659 - val_loss: 1.5665 - val_accuracy: 0.3367\n",
            "Epoch 45/50\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.4648 - accuracy: 0.3704\n",
            "Epoch 00045: loss did not improve from 1.46454\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.4648 - accuracy: 0.3700 - val_loss: 1.5553 - val_accuracy: 0.3393\n",
            "Epoch 46/50\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.4630 - accuracy: 0.3654\n",
            "Epoch 00046: loss improved from 1.46454 to 1.46255, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f59c41d7fd0>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.4626 - accuracy: 0.3657 - val_loss: 1.5682 - val_accuracy: 0.3213\n",
            "Epoch 47/50\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.4625 - accuracy: 0.3659\n",
            "Epoch 00047: loss did not improve from 1.46255\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.4626 - accuracy: 0.3656 - val_loss: 1.5984 - val_accuracy: 0.3377\n",
            "Epoch 48/50\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.4617 - accuracy: 0.3690\n",
            "Epoch 00048: loss improved from 1.46255 to 1.46157, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f59c41d7fd0>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.4616 - accuracy: 0.3692 - val_loss: 1.6402 - val_accuracy: 0.3270\n",
            "Epoch 49/50\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.4582 - accuracy: 0.3743\n",
            "Epoch 00049: loss improved from 1.46157 to 1.45834, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f59c41d7fd0>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.4583 - accuracy: 0.3743 - val_loss: 1.5950 - val_accuracy: 0.3375\n",
            "Epoch 50/50\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.4600 - accuracy: 0.3749\n",
            "Epoch 00050: loss did not improve from 1.45834\n",
            "595/595 [==============================] - 7s 13ms/step - loss: 1.4599 - accuracy: 0.3752 - val_loss: 1.5681 - val_accuracy: 0.3469\n",
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
            "Epoch 1/50\n",
            "  2/595 [..............................] - ETA: 31s - loss: 2.3347 - accuracy: 0.1200WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0151s vs `on_train_batch_end` time: 0.0912s). Check your callbacks.\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.6021 - accuracy: 0.3112\n",
            "Epoch 00001: loss improved from inf to 1.60236, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adagrad.Adagrad object at 0x7f59b9872be0>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.6024 - accuracy: 0.3110 - val_loss: 1.5213 - val_accuracy: 0.3399\n",
            "Epoch 2/50\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5546 - accuracy: 0.3298\n",
            "Epoch 00002: loss improved from 1.60236 to 1.55455, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adagrad.Adagrad object at 0x7f59b9872be0>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5545 - accuracy: 0.3297 - val_loss: 1.5087 - val_accuracy: 0.3399\n",
            "Epoch 3/50\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5539 - accuracy: 0.3283\n",
            "Epoch 00003: loss improved from 1.55455 to 1.55411, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adagrad.Adagrad object at 0x7f59b9872be0>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5541 - accuracy: 0.3282 - val_loss: 1.5104 - val_accuracy: 0.3399\n",
            "Epoch 4/50\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5501 - accuracy: 0.3276\n",
            "Epoch 00004: loss improved from 1.55411 to 1.54999, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adagrad.Adagrad object at 0x7f59b9872be0>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5500 - accuracy: 0.3276 - val_loss: 1.5069 - val_accuracy: 0.3399\n",
            "Epoch 5/50\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.5471 - accuracy: 0.3255\n",
            "Epoch 00005: loss improved from 1.54999 to 1.54706, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adagrad.Adagrad object at 0x7f59b9872be0>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5471 - accuracy: 0.3255 - val_loss: 1.5065 - val_accuracy: 0.3399\n",
            "Epoch 6/50\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5439 - accuracy: 0.3280\n",
            "Epoch 00006: loss improved from 1.54706 to 1.54387, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adagrad.Adagrad object at 0x7f59b9872be0>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5439 - accuracy: 0.3277 - val_loss: 1.5065 - val_accuracy: 0.3399\n",
            "Epoch 7/50\n",
            "589/595 [============================>.] - ETA: 0s - loss: 1.5399 - accuracy: 0.3330\n",
            "Epoch 00007: loss improved from 1.54387 to 1.54005, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adagrad.Adagrad object at 0x7f59b9872be0>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5400 - accuracy: 0.3325 - val_loss: 1.5049 - val_accuracy: 0.3399\n",
            "Epoch 8/50\n",
            "589/595 [============================>.] - ETA: 0s - loss: 1.5431 - accuracy: 0.3358\n",
            "Epoch 00008: loss did not improve from 1.54005\n",
            "595/595 [==============================] - 7s 13ms/step - loss: 1.5432 - accuracy: 0.3357 - val_loss: 1.5042 - val_accuracy: 0.3399\n",
            "Epoch 9/50\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5408 - accuracy: 0.3307\n",
            "Epoch 00009: loss did not improve from 1.54005\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5407 - accuracy: 0.3307 - val_loss: 1.5043 - val_accuracy: 0.3399\n",
            "Epoch 10/50\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.5409 - accuracy: 0.3351\n",
            "Epoch 00010: loss did not improve from 1.54005\n",
            "595/595 [==============================] - 7s 12ms/step - loss: 1.5407 - accuracy: 0.3350 - val_loss: 1.5040 - val_accuracy: 0.3399\n",
            "Epoch 11/50\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5352 - accuracy: 0.3389\n",
            "Epoch 00011: loss improved from 1.54005 to 1.53548, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adagrad.Adagrad object at 0x7f59b9872be0>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5355 - accuracy: 0.3385 - val_loss: 1.5036 - val_accuracy: 0.3399\n",
            "Epoch 12/50\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5358 - accuracy: 0.3374\n",
            "Epoch 00012: loss did not improve from 1.53548\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5361 - accuracy: 0.3370 - val_loss: 1.5042 - val_accuracy: 0.3399\n",
            "Epoch 13/50\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5358 - accuracy: 0.3363\n",
            "Epoch 00013: loss did not improve from 1.53548\n",
            "595/595 [==============================] - 7s 13ms/step - loss: 1.5358 - accuracy: 0.3363 - val_loss: 1.5036 - val_accuracy: 0.3399\n",
            "Epoch 14/50\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.5356 - accuracy: 0.3385\n",
            "Epoch 00014: loss improved from 1.53548 to 1.53489, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adagrad.Adagrad object at 0x7f59b9872be0>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5349 - accuracy: 0.3391 - val_loss: 1.5046 - val_accuracy: 0.3399\n",
            "Epoch 15/50\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.5333 - accuracy: 0.3394\n",
            "Epoch 00015: loss improved from 1.53489 to 1.53339, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adagrad.Adagrad object at 0x7f59b9872be0>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5334 - accuracy: 0.3395 - val_loss: 1.5042 - val_accuracy: 0.3399\n",
            "Epoch 16/50\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.5339 - accuracy: 0.3346\n",
            "Epoch 00016: loss did not improve from 1.53339\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5339 - accuracy: 0.3346 - val_loss: 1.5036 - val_accuracy: 0.3399\n",
            "Epoch 17/50\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5356 - accuracy: 0.3394\n",
            "Epoch 00017: loss did not improve from 1.53339\n",
            "595/595 [==============================] - 7s 12ms/step - loss: 1.5357 - accuracy: 0.3392 - val_loss: 1.5040 - val_accuracy: 0.3399\n",
            "Epoch 18/50\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5341 - accuracy: 0.3347\n",
            "Epoch 00018: loss did not improve from 1.53339\n",
            "595/595 [==============================] - 7s 12ms/step - loss: 1.5341 - accuracy: 0.3348 - val_loss: 1.5035 - val_accuracy: 0.3399\n",
            "Epoch 19/50\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5329 - accuracy: 0.3395\n",
            "Epoch 00019: loss improved from 1.53339 to 1.53306, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adagrad.Adagrad object at 0x7f59b9872be0>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5331 - accuracy: 0.3394 - val_loss: 1.5033 - val_accuracy: 0.3399\n",
            "Epoch 20/50\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5326 - accuracy: 0.3364\n",
            "Epoch 00020: loss improved from 1.53306 to 1.53253, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adagrad.Adagrad object at 0x7f59b9872be0>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5325 - accuracy: 0.3364 - val_loss: 1.5032 - val_accuracy: 0.3399\n",
            "Epoch 21/50\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5326 - accuracy: 0.3389\n",
            "Epoch 00021: loss did not improve from 1.53253\n",
            "595/595 [==============================] - 7s 12ms/step - loss: 1.5328 - accuracy: 0.3388 - val_loss: 1.5030 - val_accuracy: 0.3399\n",
            "Epoch 22/50\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5315 - accuracy: 0.3381\n",
            "Epoch 00022: loss improved from 1.53253 to 1.53179, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adagrad.Adagrad object at 0x7f59b9872be0>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5318 - accuracy: 0.3380 - val_loss: 1.5043 - val_accuracy: 0.3399\n",
            "Epoch 23/50\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5311 - accuracy: 0.3376\n",
            "Epoch 00023: loss improved from 1.53179 to 1.53145, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adagrad.Adagrad object at 0x7f59b9872be0>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5315 - accuracy: 0.3377 - val_loss: 1.5030 - val_accuracy: 0.3399\n",
            "Epoch 24/50\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5312 - accuracy: 0.3387\n",
            "Epoch 00024: loss improved from 1.53145 to 1.53125, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adagrad.Adagrad object at 0x7f59b9872be0>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5313 - accuracy: 0.3387 - val_loss: 1.5038 - val_accuracy: 0.3399\n",
            "Epoch 25/50\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5319 - accuracy: 0.3410\n",
            "Epoch 00025: loss did not improve from 1.53125\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5319 - accuracy: 0.3413 - val_loss: 1.5033 - val_accuracy: 0.3399\n",
            "Epoch 26/50\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.5325 - accuracy: 0.3401\n",
            "Epoch 00026: loss did not improve from 1.53125\n",
            "595/595 [==============================] - 7s 13ms/step - loss: 1.5325 - accuracy: 0.3401 - val_loss: 1.5032 - val_accuracy: 0.3399\n",
            "Epoch 27/50\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5306 - accuracy: 0.3423\n",
            "Epoch 00027: loss improved from 1.53125 to 1.53047, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adagrad.Adagrad object at 0x7f59b9872be0>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5305 - accuracy: 0.3423 - val_loss: 1.5033 - val_accuracy: 0.3399\n",
            "Epoch 28/50\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.5298 - accuracy: 0.3420\n",
            "Epoch 00028: loss improved from 1.53047 to 1.52983, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adagrad.Adagrad object at 0x7f59b9872be0>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5298 - accuracy: 0.3420 - val_loss: 1.5035 - val_accuracy: 0.3399\n",
            "Epoch 29/50\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.5269 - accuracy: 0.3410\n",
            "Epoch 00029: loss improved from 1.52983 to 1.52705, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adagrad.Adagrad object at 0x7f59b9872be0>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5271 - accuracy: 0.3411 - val_loss: 1.5023 - val_accuracy: 0.3399\n",
            "Epoch 30/50\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.5293 - accuracy: 0.3418\n",
            "Epoch 00030: loss did not improve from 1.52705\n",
            "595/595 [==============================] - 7s 13ms/step - loss: 1.5293 - accuracy: 0.3418 - val_loss: 1.5026 - val_accuracy: 0.3399\n",
            "Epoch 31/50\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5297 - accuracy: 0.3398\n",
            "Epoch 00031: loss did not improve from 1.52705\n",
            "595/595 [==============================] - 7s 12ms/step - loss: 1.5295 - accuracy: 0.3402 - val_loss: 1.5033 - val_accuracy: 0.3399\n",
            "Epoch 32/50\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5292 - accuracy: 0.3431\n",
            "Epoch 00032: loss did not improve from 1.52705\n",
            "595/595 [==============================] - 7s 13ms/step - loss: 1.5288 - accuracy: 0.3432 - val_loss: 1.5029 - val_accuracy: 0.3399\n",
            "Epoch 33/50\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5287 - accuracy: 0.3434\n",
            "Epoch 00033: loss did not improve from 1.52705\n",
            "595/595 [==============================] - 7s 12ms/step - loss: 1.5285 - accuracy: 0.3432 - val_loss: 1.5023 - val_accuracy: 0.3399\n",
            "Epoch 34/50\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5272 - accuracy: 0.3427\n",
            "Epoch 00034: loss did not improve from 1.52705\n",
            "595/595 [==============================] - 7s 12ms/step - loss: 1.5272 - accuracy: 0.3424 - val_loss: 1.5025 - val_accuracy: 0.3399\n",
            "Epoch 35/50\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.5280 - accuracy: 0.3401\n",
            "Epoch 00035: loss did not improve from 1.52705\n",
            "595/595 [==============================] - 7s 12ms/step - loss: 1.5284 - accuracy: 0.3400 - val_loss: 1.5022 - val_accuracy: 0.3399\n",
            "Epoch 36/50\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5302 - accuracy: 0.3397\n",
            "Epoch 00036: loss did not improve from 1.52705\n",
            "595/595 [==============================] - 7s 12ms/step - loss: 1.5301 - accuracy: 0.3396 - val_loss: 1.5025 - val_accuracy: 0.3399\n",
            "Epoch 37/50\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5285 - accuracy: 0.3422\n",
            "Epoch 00037: loss did not improve from 1.52705\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5285 - accuracy: 0.3421 - val_loss: 1.5020 - val_accuracy: 0.3399\n",
            "Epoch 38/50\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5280 - accuracy: 0.3402\n",
            "Epoch 00038: loss did not improve from 1.52705\n",
            "595/595 [==============================] - 7s 12ms/step - loss: 1.5276 - accuracy: 0.3405 - val_loss: 1.5015 - val_accuracy: 0.3399\n",
            "Epoch 39/50\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5276 - accuracy: 0.3431\n",
            "Epoch 00039: loss improved from 1.52705 to 1.52695, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adagrad.Adagrad object at 0x7f59b9872be0>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5269 - accuracy: 0.3436 - val_loss: 1.5016 - val_accuracy: 0.3399\n",
            "Epoch 40/50\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.5274 - accuracy: 0.3432\n",
            "Epoch 00040: loss did not improve from 1.52695\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5273 - accuracy: 0.3428 - val_loss: 1.5012 - val_accuracy: 0.3399\n",
            "Epoch 41/50\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.5291 - accuracy: 0.3409\n",
            "Epoch 00041: loss did not improve from 1.52695\n",
            "595/595 [==============================] - 7s 13ms/step - loss: 1.5291 - accuracy: 0.3411 - val_loss: 1.5016 - val_accuracy: 0.3399\n",
            "Epoch 42/50\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.5260 - accuracy: 0.3444\n",
            "Epoch 00042: loss improved from 1.52695 to 1.52564, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adagrad.Adagrad object at 0x7f59b9872be0>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5256 - accuracy: 0.3449 - val_loss: 1.5019 - val_accuracy: 0.3399\n",
            "Epoch 43/50\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5266 - accuracy: 0.3411\n",
            "Epoch 00043: loss did not improve from 1.52564\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5269 - accuracy: 0.3410 - val_loss: 1.5016 - val_accuracy: 0.3399\n",
            "Epoch 44/50\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5261 - accuracy: 0.3432\n",
            "Epoch 00044: loss did not improve from 1.52564\n",
            "595/595 [==============================] - 7s 13ms/step - loss: 1.5259 - accuracy: 0.3432 - val_loss: 1.5015 - val_accuracy: 0.3399\n",
            "Epoch 45/50\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.5254 - accuracy: 0.3432\n",
            "Epoch 00045: loss did not improve from 1.52564\n",
            "595/595 [==============================] - 7s 13ms/step - loss: 1.5259 - accuracy: 0.3426 - val_loss: 1.5016 - val_accuracy: 0.3399\n",
            "Epoch 46/50\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5272 - accuracy: 0.3396\n",
            "Epoch 00046: loss did not improve from 1.52564\n",
            "595/595 [==============================] - 7s 12ms/step - loss: 1.5270 - accuracy: 0.3399 - val_loss: 1.5017 - val_accuracy: 0.3399\n",
            "Epoch 47/50\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5263 - accuracy: 0.3411\n",
            "Epoch 00047: loss did not improve from 1.52564\n",
            "595/595 [==============================] - 7s 13ms/step - loss: 1.5265 - accuracy: 0.3410 - val_loss: 1.5016 - val_accuracy: 0.3399\n",
            "Epoch 48/50\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.5267 - accuracy: 0.3412\n",
            "Epoch 00048: loss did not improve from 1.52564\n",
            "595/595 [==============================] - 7s 13ms/step - loss: 1.5260 - accuracy: 0.3418 - val_loss: 1.5014 - val_accuracy: 0.3399\n",
            "Epoch 49/50\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5263 - accuracy: 0.3438\n",
            "Epoch 00049: loss did not improve from 1.52564\n",
            "595/595 [==============================] - 7s 12ms/step - loss: 1.5262 - accuracy: 0.3436 - val_loss: 1.5012 - val_accuracy: 0.3399\n",
            "Epoch 50/50\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5254 - accuracy: 0.3403\n",
            "Epoch 00050: loss improved from 1.52564 to 1.52495, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adagrad.Adagrad object at 0x7f59b9872be0>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5250 - accuracy: 0.3404 - val_loss: 1.5010 - val_accuracy: 0.3399\n",
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
            "Epoch 1/50\n",
            "  2/595 [..............................] - ETA: 33s - loss: 2.0763 - accuracy: 0.1400WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0143s vs `on_train_batch_end` time: 0.0976s). Check your callbacks.\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5553 - accuracy: 0.3291\n",
            "Epoch 00001: loss improved from inf to 1.55545, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adamax.Adamax object at 0x7f59b980be10>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5554 - accuracy: 0.3291 - val_loss: 1.5041 - val_accuracy: 0.3399\n",
            "Epoch 2/50\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.5335 - accuracy: 0.3389\n",
            "Epoch 00002: loss improved from 1.55545 to 1.53351, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adamax.Adamax object at 0x7f59b980be10>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5335 - accuracy: 0.3389 - val_loss: 1.5014 - val_accuracy: 0.3399\n",
            "Epoch 3/50\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.5284 - accuracy: 0.3379\n",
            "Epoch 00003: loss improved from 1.53351 to 1.52836, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adamax.Adamax object at 0x7f59b980be10>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5284 - accuracy: 0.3379 - val_loss: 1.5152 - val_accuracy: 0.3399\n",
            "Epoch 4/50\n",
            "589/595 [============================>.] - ETA: 0s - loss: 1.5258 - accuracy: 0.3400\n",
            "Epoch 00004: loss improved from 1.52836 to 1.52622, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adamax.Adamax object at 0x7f59b980be10>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5262 - accuracy: 0.3392 - val_loss: 1.5033 - val_accuracy: 0.3399\n",
            "Epoch 5/50\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.5199 - accuracy: 0.3424\n",
            "Epoch 00005: loss improved from 1.52622 to 1.51986, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adamax.Adamax object at 0x7f59b980be10>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5199 - accuracy: 0.3424 - val_loss: 1.4914 - val_accuracy: 0.3399\n",
            "Epoch 6/50\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5199 - accuracy: 0.3426\n",
            "Epoch 00006: loss improved from 1.51986 to 1.51983, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adamax.Adamax object at 0x7f59b980be10>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5198 - accuracy: 0.3427 - val_loss: 1.5032 - val_accuracy: 0.3399\n",
            "Epoch 7/50\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5173 - accuracy: 0.3423\n",
            "Epoch 00007: loss improved from 1.51983 to 1.51749, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adamax.Adamax object at 0x7f59b980be10>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5175 - accuracy: 0.3425 - val_loss: 1.4980 - val_accuracy: 0.3399\n",
            "Epoch 8/50\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5161 - accuracy: 0.3434\n",
            "Epoch 00008: loss improved from 1.51749 to 1.51585, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adamax.Adamax object at 0x7f59b980be10>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5159 - accuracy: 0.3432 - val_loss: 1.4876 - val_accuracy: 0.3407\n",
            "Epoch 9/50\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5131 - accuracy: 0.3417\n",
            "Epoch 00009: loss improved from 1.51585 to 1.51286, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adamax.Adamax object at 0x7f59b980be10>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5129 - accuracy: 0.3418 - val_loss: 1.4944 - val_accuracy: 0.3372\n",
            "Epoch 10/50\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5123 - accuracy: 0.3444\n",
            "Epoch 00010: loss improved from 1.51286 to 1.51260, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adamax.Adamax object at 0x7f59b980be10>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5126 - accuracy: 0.3442 - val_loss: 1.4970 - val_accuracy: 0.3372\n",
            "Epoch 11/50\n",
            "589/595 [============================>.] - ETA: 0s - loss: 1.5111 - accuracy: 0.3473\n",
            "Epoch 00011: loss improved from 1.51260 to 1.51063, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adamax.Adamax object at 0x7f59b980be10>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5106 - accuracy: 0.3476 - val_loss: 1.4914 - val_accuracy: 0.3205\n",
            "Epoch 12/50\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.5104 - accuracy: 0.3449\n",
            "Epoch 00012: loss improved from 1.51063 to 1.51050, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adamax.Adamax object at 0x7f59b980be10>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5105 - accuracy: 0.3449 - val_loss: 1.4974 - val_accuracy: 0.3372\n",
            "Epoch 13/50\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5096 - accuracy: 0.3428\n",
            "Epoch 00013: loss improved from 1.51050 to 1.50957, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adamax.Adamax object at 0x7f59b980be10>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5096 - accuracy: 0.3428 - val_loss: 1.4949 - val_accuracy: 0.3372\n",
            "Epoch 14/50\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5062 - accuracy: 0.3465\n",
            "Epoch 00014: loss improved from 1.50957 to 1.50615, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adamax.Adamax object at 0x7f59b980be10>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5061 - accuracy: 0.3464 - val_loss: 1.5026 - val_accuracy: 0.3372\n",
            "Epoch 15/50\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.5055 - accuracy: 0.3471\n",
            "Epoch 00015: loss improved from 1.50615 to 1.50546, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adamax.Adamax object at 0x7f59b980be10>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5055 - accuracy: 0.3471 - val_loss: 1.4993 - val_accuracy: 0.3372\n",
            "Epoch 16/50\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5056 - accuracy: 0.3492\n",
            "Epoch 00016: loss did not improve from 1.50546\n",
            "595/595 [==============================] - 7s 13ms/step - loss: 1.5056 - accuracy: 0.3495 - val_loss: 1.4980 - val_accuracy: 0.3399\n",
            "Epoch 17/50\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.5024 - accuracy: 0.3489\n",
            "Epoch 00017: loss improved from 1.50546 to 1.50244, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adamax.Adamax object at 0x7f59b980be10>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5024 - accuracy: 0.3489 - val_loss: 1.4937 - val_accuracy: 0.3367\n",
            "Epoch 18/50\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5023 - accuracy: 0.3465\n",
            "Epoch 00018: loss did not improve from 1.50244\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5026 - accuracy: 0.3464 - val_loss: 1.4954 - val_accuracy: 0.3205\n",
            "Epoch 19/50\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.4989 - accuracy: 0.3542\n",
            "Epoch 00019: loss improved from 1.50244 to 1.49865, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adamax.Adamax object at 0x7f59b980be10>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.4986 - accuracy: 0.3544 - val_loss: 1.5002 - val_accuracy: 0.3367\n",
            "Epoch 20/50\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.4982 - accuracy: 0.3518\n",
            "Epoch 00020: loss improved from 1.49865 to 1.49848, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adamax.Adamax object at 0x7f59b980be10>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.4985 - accuracy: 0.3519 - val_loss: 1.4951 - val_accuracy: 0.3227\n",
            "Epoch 21/50\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.4991 - accuracy: 0.3511\n",
            "Epoch 00021: loss did not improve from 1.49848\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.4988 - accuracy: 0.3510 - val_loss: 1.4979 - val_accuracy: 0.3143\n",
            "Epoch 22/50\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.4967 - accuracy: 0.3492\n",
            "Epoch 00022: loss improved from 1.49848 to 1.49667, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adamax.Adamax object at 0x7f59b980be10>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.4967 - accuracy: 0.3492 - val_loss: 1.4914 - val_accuracy: 0.3243\n",
            "Epoch 23/50\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.4951 - accuracy: 0.3516\n",
            "Epoch 00023: loss improved from 1.49667 to 1.49511, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adamax.Adamax object at 0x7f59b980be10>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.4951 - accuracy: 0.3516 - val_loss: 1.4950 - val_accuracy: 0.3065\n",
            "Epoch 24/50\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.4959 - accuracy: 0.3535\n",
            "Epoch 00024: loss did not improve from 1.49511\n",
            "595/595 [==============================] - 7s 13ms/step - loss: 1.4961 - accuracy: 0.3535 - val_loss: 1.4951 - val_accuracy: 0.3369\n",
            "Epoch 25/50\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.4945 - accuracy: 0.3521\n",
            "Epoch 00025: loss improved from 1.49511 to 1.49430, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adamax.Adamax object at 0x7f59b980be10>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.4943 - accuracy: 0.3523 - val_loss: 1.4991 - val_accuracy: 0.3358\n",
            "Epoch 26/50\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.4938 - accuracy: 0.3506\n",
            "Epoch 00026: loss improved from 1.49430 to 1.49425, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adamax.Adamax object at 0x7f59b980be10>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.4943 - accuracy: 0.3504 - val_loss: 1.4865 - val_accuracy: 0.3326\n",
            "Epoch 27/50\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.4939 - accuracy: 0.3527\n",
            "Epoch 00027: loss improved from 1.49425 to 1.49390, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adamax.Adamax object at 0x7f59b980be10>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.4939 - accuracy: 0.3527 - val_loss: 1.4908 - val_accuracy: 0.3213\n",
            "Epoch 28/50\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.4909 - accuracy: 0.3533\n",
            "Epoch 00028: loss improved from 1.49390 to 1.49083, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adamax.Adamax object at 0x7f59b980be10>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.4908 - accuracy: 0.3532 - val_loss: 1.4918 - val_accuracy: 0.3135\n",
            "Epoch 29/50\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.4891 - accuracy: 0.3523\n",
            "Epoch 00029: loss improved from 1.49083 to 1.48911, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adamax.Adamax object at 0x7f59b980be10>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.4891 - accuracy: 0.3522 - val_loss: 1.4934 - val_accuracy: 0.3175\n",
            "Epoch 30/50\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.4910 - accuracy: 0.3548\n",
            "Epoch 00030: loss did not improve from 1.48911\n",
            "595/595 [==============================] - 7s 13ms/step - loss: 1.4905 - accuracy: 0.3550 - val_loss: 1.5024 - val_accuracy: 0.3219\n",
            "Epoch 31/50\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.4887 - accuracy: 0.3515\n",
            "Epoch 00031: loss improved from 1.48911 to 1.48862, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adamax.Adamax object at 0x7f59b980be10>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.4886 - accuracy: 0.3515 - val_loss: 1.5028 - val_accuracy: 0.3165\n",
            "Epoch 32/50\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.4880 - accuracy: 0.3565\n",
            "Epoch 00032: loss improved from 1.48862 to 1.48799, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adamax.Adamax object at 0x7f59b980be10>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.4880 - accuracy: 0.3565 - val_loss: 1.4962 - val_accuracy: 0.3358\n",
            "Epoch 33/50\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.4857 - accuracy: 0.3543\n",
            "Epoch 00033: loss improved from 1.48799 to 1.48555, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adamax.Adamax object at 0x7f59b980be10>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.4856 - accuracy: 0.3542 - val_loss: 1.4972 - val_accuracy: 0.3116\n",
            "Epoch 34/50\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.4866 - accuracy: 0.3557\n",
            "Epoch 00034: loss did not improve from 1.48555\n",
            "595/595 [==============================] - 7s 13ms/step - loss: 1.4864 - accuracy: 0.3560 - val_loss: 1.4973 - val_accuracy: 0.3326\n",
            "Epoch 35/50\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.4833 - accuracy: 0.3616\n",
            "Epoch 00035: loss improved from 1.48555 to 1.48334, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adamax.Adamax object at 0x7f59b980be10>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.4833 - accuracy: 0.3617 - val_loss: 1.4953 - val_accuracy: 0.3423\n",
            "Epoch 36/50\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.4840 - accuracy: 0.3608\n",
            "Epoch 00036: loss did not improve from 1.48334\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.4839 - accuracy: 0.3613 - val_loss: 1.5000 - val_accuracy: 0.3189\n",
            "Epoch 37/50\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.4825 - accuracy: 0.3614\n",
            "Epoch 00037: loss improved from 1.48334 to 1.48232, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adamax.Adamax object at 0x7f59b980be10>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.4823 - accuracy: 0.3617 - val_loss: 1.5010 - val_accuracy: 0.3348\n",
            "Epoch 38/50\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.4834 - accuracy: 0.3639\n",
            "Epoch 00038: loss did not improve from 1.48232\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.4836 - accuracy: 0.3639 - val_loss: 1.5014 - val_accuracy: 0.3291\n",
            "Epoch 39/50\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.4818 - accuracy: 0.3583\n",
            "Epoch 00039: loss improved from 1.48232 to 1.48190, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adamax.Adamax object at 0x7f59b980be10>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.4819 - accuracy: 0.3586 - val_loss: 1.4979 - val_accuracy: 0.3079\n",
            "Epoch 40/50\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.4813 - accuracy: 0.3637\n",
            "Epoch 00040: loss improved from 1.48190 to 1.48129, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adamax.Adamax object at 0x7f59b980be10>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.4813 - accuracy: 0.3637 - val_loss: 1.5017 - val_accuracy: 0.3140\n",
            "Epoch 41/50\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.4789 - accuracy: 0.3597\n",
            "Epoch 00041: loss improved from 1.48129 to 1.47887, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adamax.Adamax object at 0x7f59b980be10>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.4789 - accuracy: 0.3597 - val_loss: 1.5018 - val_accuracy: 0.3259\n",
            "Epoch 42/50\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.4804 - accuracy: 0.3587\n",
            "Epoch 00042: loss did not improve from 1.47887\n",
            "595/595 [==============================] - 7s 13ms/step - loss: 1.4802 - accuracy: 0.3587 - val_loss: 1.4961 - val_accuracy: 0.3227\n",
            "Epoch 43/50\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.4788 - accuracy: 0.3599\n",
            "Epoch 00043: loss improved from 1.47887 to 1.47880, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adamax.Adamax object at 0x7f59b980be10>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.4788 - accuracy: 0.3600 - val_loss: 1.4985 - val_accuracy: 0.3221\n",
            "Epoch 44/50\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.4790 - accuracy: 0.3605\n",
            "Epoch 00044: loss did not improve from 1.47880\n",
            "595/595 [==============================] - 7s 12ms/step - loss: 1.4792 - accuracy: 0.3603 - val_loss: 1.5130 - val_accuracy: 0.3278\n",
            "Epoch 45/50\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.4783 - accuracy: 0.3635\n",
            "Epoch 00045: loss improved from 1.47880 to 1.47768, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adamax.Adamax object at 0x7f59b980be10>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.4777 - accuracy: 0.3638 - val_loss: 1.4963 - val_accuracy: 0.3310\n",
            "Epoch 46/50\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.4771 - accuracy: 0.3639\n",
            "Epoch 00046: loss improved from 1.47768 to 1.47717, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adamax.Adamax object at 0x7f59b980be10>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.4772 - accuracy: 0.3637 - val_loss: 1.4970 - val_accuracy: 0.3264\n",
            "Epoch 47/50\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.4787 - accuracy: 0.3628\n",
            "Epoch 00047: loss did not improve from 1.47717\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.4781 - accuracy: 0.3634 - val_loss: 1.5040 - val_accuracy: 0.3213\n",
            "Epoch 48/50\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.4762 - accuracy: 0.3661\n",
            "Epoch 00048: loss improved from 1.47717 to 1.47640, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adamax.Adamax object at 0x7f59b980be10>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.4764 - accuracy: 0.3661 - val_loss: 1.4992 - val_accuracy: 0.3208\n",
            "Epoch 49/50\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.4754 - accuracy: 0.3651\n",
            "Epoch 00049: loss improved from 1.47640 to 1.47536, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adamax.Adamax object at 0x7f59b980be10>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.4754 - accuracy: 0.3650 - val_loss: 1.5013 - val_accuracy: 0.3251\n",
            "Epoch 50/50\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.4726 - accuracy: 0.3676\n",
            "Epoch 00050: loss improved from 1.47536 to 1.47272, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adamax.Adamax object at 0x7f59b980be10>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.4727 - accuracy: 0.3675 - val_loss: 1.4998 - val_accuracy: 0.3367\n",
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
            "Epoch 1/50\n",
            "  2/595 [..............................] - ETA: 33s - loss: 2.1722 - accuracy: 0.0400   WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0142s vs `on_train_batch_end` time: 0.0968s). Check your callbacks.\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.6596 - accuracy: 0.3032\n",
            "Epoch 00001: loss improved from inf to 1.65915, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.ftrl.Ftrl object at 0x7f59b2f7e080>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.6591 - accuracy: 0.3035 - val_loss: 1.5447 - val_accuracy: 0.3399\n",
            "Epoch 2/50\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5574 - accuracy: 0.3436\n",
            "Epoch 00002: loss improved from 1.65915 to 1.55745, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.ftrl.Ftrl object at 0x7f59b2f7e080>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5574 - accuracy: 0.3435 - val_loss: 1.5183 - val_accuracy: 0.3399\n",
            "Epoch 3/50\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5435 - accuracy: 0.3435\n",
            "Epoch 00003: loss improved from 1.55745 to 1.54326, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.ftrl.Ftrl object at 0x7f59b2f7e080>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5433 - accuracy: 0.3437 - val_loss: 1.5096 - val_accuracy: 0.3399\n",
            "Epoch 4/50\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5368 - accuracy: 0.3437\n",
            "Epoch 00004: loss improved from 1.54326 to 1.53677, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.ftrl.Ftrl object at 0x7f59b2f7e080>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5368 - accuracy: 0.3437 - val_loss: 1.5056 - val_accuracy: 0.3399\n",
            "Epoch 5/50\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.5348 - accuracy: 0.3443\n",
            "Epoch 00005: loss improved from 1.53677 to 1.53525, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.ftrl.Ftrl object at 0x7f59b2f7e080>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5352 - accuracy: 0.3438 - val_loss: 1.5030 - val_accuracy: 0.3399\n",
            "Epoch 6/50\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5325 - accuracy: 0.3437\n",
            "Epoch 00006: loss improved from 1.53525 to 1.53254, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.ftrl.Ftrl object at 0x7f59b2f7e080>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5325 - accuracy: 0.3433 - val_loss: 1.5013 - val_accuracy: 0.3399\n",
            "Epoch 7/50\n",
            "589/595 [============================>.] - ETA: 0s - loss: 1.5307 - accuracy: 0.3439\n",
            "Epoch 00007: loss improved from 1.53254 to 1.53099, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.ftrl.Ftrl object at 0x7f59b2f7e080>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5310 - accuracy: 0.3436 - val_loss: 1.5011 - val_accuracy: 0.3399\n",
            "Epoch 8/50\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5310 - accuracy: 0.3438\n",
            "Epoch 00008: loss did not improve from 1.53099\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5310 - accuracy: 0.3437 - val_loss: 1.5000 - val_accuracy: 0.3399\n",
            "Epoch 9/50\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5302 - accuracy: 0.3437\n",
            "Epoch 00009: loss improved from 1.53099 to 1.53016, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.ftrl.Ftrl object at 0x7f59b2f7e080>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5302 - accuracy: 0.3436 - val_loss: 1.4995 - val_accuracy: 0.3399\n",
            "Epoch 10/50\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5298 - accuracy: 0.3436\n",
            "Epoch 00010: loss improved from 1.53016 to 1.52974, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.ftrl.Ftrl object at 0x7f59b2f7e080>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5297 - accuracy: 0.3436 - val_loss: 1.4991 - val_accuracy: 0.3399\n",
            "Epoch 11/50\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.5308 - accuracy: 0.3437\n",
            "Epoch 00011: loss did not improve from 1.52974\n",
            "595/595 [==============================] - 7s 13ms/step - loss: 1.5307 - accuracy: 0.3434 - val_loss: 1.4995 - val_accuracy: 0.3399\n",
            "Epoch 12/50\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.5300 - accuracy: 0.3431\n",
            "Epoch 00012: loss did not improve from 1.52974\n",
            "595/595 [==============================] - 7s 13ms/step - loss: 1.5301 - accuracy: 0.3434 - val_loss: 1.4995 - val_accuracy: 0.3399\n",
            "Epoch 13/50\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.5300 - accuracy: 0.3436\n",
            "Epoch 00013: loss did not improve from 1.52974\n",
            "595/595 [==============================] - 7s 12ms/step - loss: 1.5300 - accuracy: 0.3436 - val_loss: 1.4989 - val_accuracy: 0.3399\n",
            "Epoch 14/50\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5299 - accuracy: 0.3436\n",
            "Epoch 00014: loss improved from 1.52974 to 1.52948, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.ftrl.Ftrl object at 0x7f59b2f7e080>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5295 - accuracy: 0.3440 - val_loss: 1.4983 - val_accuracy: 0.3399\n",
            "Epoch 15/50\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5290 - accuracy: 0.3439\n",
            "Epoch 00015: loss improved from 1.52948 to 1.52879, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.ftrl.Ftrl object at 0x7f59b2f7e080>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5288 - accuracy: 0.3440 - val_loss: 1.4984 - val_accuracy: 0.3399\n",
            "Epoch 16/50\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5290 - accuracy: 0.3436\n",
            "Epoch 00016: loss did not improve from 1.52879\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5292 - accuracy: 0.3434 - val_loss: 1.4985 - val_accuracy: 0.3399\n",
            "Epoch 17/50\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5282 - accuracy: 0.3437\n",
            "Epoch 00017: loss improved from 1.52879 to 1.52853, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.ftrl.Ftrl object at 0x7f59b2f7e080>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5285 - accuracy: 0.3435 - val_loss: 1.4985 - val_accuracy: 0.3399\n",
            "Epoch 18/50\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5290 - accuracy: 0.3438\n",
            "Epoch 00018: loss did not improve from 1.52853\n",
            "595/595 [==============================] - 7s 13ms/step - loss: 1.5293 - accuracy: 0.3436 - val_loss: 1.4980 - val_accuracy: 0.3399\n",
            "Epoch 19/50\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5290 - accuracy: 0.3436\n",
            "Epoch 00019: loss did not improve from 1.52853\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5289 - accuracy: 0.3435 - val_loss: 1.4983 - val_accuracy: 0.3399\n",
            "Epoch 20/50\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5285 - accuracy: 0.3437\n",
            "Epoch 00020: loss did not improve from 1.52853\n",
            "595/595 [==============================] - 7s 12ms/step - loss: 1.5286 - accuracy: 0.3437 - val_loss: 1.4985 - val_accuracy: 0.3399\n",
            "Epoch 21/50\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5292 - accuracy: 0.3436\n",
            "Epoch 00021: loss did not improve from 1.52853\n",
            "595/595 [==============================] - 7s 12ms/step - loss: 1.5294 - accuracy: 0.3432 - val_loss: 1.4985 - val_accuracy: 0.3399\n",
            "Epoch 22/50\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5300 - accuracy: 0.3435\n",
            "Epoch 00022: loss did not improve from 1.52853\n",
            "595/595 [==============================] - 7s 13ms/step - loss: 1.5300 - accuracy: 0.3433 - val_loss: 1.4987 - val_accuracy: 0.3399\n",
            "Epoch 23/50\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5278 - accuracy: 0.3438\n",
            "Epoch 00023: loss improved from 1.52853 to 1.52793, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.ftrl.Ftrl object at 0x7f59b2f7e080>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5279 - accuracy: 0.3436 - val_loss: 1.4985 - val_accuracy: 0.3399\n",
            "Epoch 24/50\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.5281 - accuracy: 0.3440\n",
            "Epoch 00024: loss did not improve from 1.52793\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5281 - accuracy: 0.3440 - val_loss: 1.4986 - val_accuracy: 0.3399\n",
            "Epoch 25/50\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5285 - accuracy: 0.3434\n",
            "Epoch 00025: loss did not improve from 1.52793\n",
            "595/595 [==============================] - 7s 12ms/step - loss: 1.5284 - accuracy: 0.3434 - val_loss: 1.4983 - val_accuracy: 0.3399\n",
            "Epoch 26/50\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.5282 - accuracy: 0.3436\n",
            "Epoch 00026: loss did not improve from 1.52793\n",
            "595/595 [==============================] - 7s 12ms/step - loss: 1.5282 - accuracy: 0.3436 - val_loss: 1.4984 - val_accuracy: 0.3399\n",
            "Epoch 27/50\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5283 - accuracy: 0.3437\n",
            "Epoch 00027: loss did not improve from 1.52793\n",
            "595/595 [==============================] - 7s 12ms/step - loss: 1.5281 - accuracy: 0.3436 - val_loss: 1.4982 - val_accuracy: 0.3399\n",
            "Epoch 28/50\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.5298 - accuracy: 0.3432\n",
            "Epoch 00028: loss did not improve from 1.52793\n",
            "595/595 [==============================] - 7s 12ms/step - loss: 1.5295 - accuracy: 0.3435 - val_loss: 1.4982 - val_accuracy: 0.3399\n",
            "Epoch 29/50\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5284 - accuracy: 0.3436\n",
            "Epoch 00029: loss did not improve from 1.52793\n",
            "595/595 [==============================] - 7s 12ms/step - loss: 1.5286 - accuracy: 0.3436 - val_loss: 1.4984 - val_accuracy: 0.3399\n",
            "Epoch 30/50\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5290 - accuracy: 0.3438\n",
            "Epoch 00030: loss did not improve from 1.52793\n",
            "595/595 [==============================] - 7s 12ms/step - loss: 1.5286 - accuracy: 0.3436 - val_loss: 1.4982 - val_accuracy: 0.3399\n",
            "Epoch 31/50\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.5286 - accuracy: 0.3427\n",
            "Epoch 00031: loss did not improve from 1.52793\n",
            "595/595 [==============================] - 7s 12ms/step - loss: 1.5283 - accuracy: 0.3436 - val_loss: 1.4988 - val_accuracy: 0.3399\n",
            "Epoch 32/50\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5283 - accuracy: 0.3439\n",
            "Epoch 00032: loss did not improve from 1.52793\n",
            "595/595 [==============================] - 7s 13ms/step - loss: 1.5285 - accuracy: 0.3435 - val_loss: 1.4985 - val_accuracy: 0.3399\n",
            "Epoch 33/50\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5294 - accuracy: 0.3427\n",
            "Epoch 00033: loss did not improve from 1.52793\n",
            "595/595 [==============================] - 7s 12ms/step - loss: 1.5288 - accuracy: 0.3434 - val_loss: 1.4987 - val_accuracy: 0.3399\n",
            "Epoch 34/50\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5284 - accuracy: 0.3436\n",
            "Epoch 00034: loss did not improve from 1.52793\n",
            "595/595 [==============================] - 7s 13ms/step - loss: 1.5285 - accuracy: 0.3435 - val_loss: 1.4988 - val_accuracy: 0.3399\n",
            "Epoch 35/50\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5286 - accuracy: 0.3436\n",
            "Epoch 00035: loss did not improve from 1.52793\n",
            "595/595 [==============================] - 7s 12ms/step - loss: 1.5286 - accuracy: 0.3436 - val_loss: 1.4989 - val_accuracy: 0.3399\n",
            "Epoch 36/50\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5276 - accuracy: 0.3435\n",
            "Epoch 00036: loss improved from 1.52793 to 1.52740, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.ftrl.Ftrl object at 0x7f59b2f7e080>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5274 - accuracy: 0.3437 - val_loss: 1.4987 - val_accuracy: 0.3399\n",
            "Epoch 37/50\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5273 - accuracy: 0.3434\n",
            "Epoch 00037: loss improved from 1.52740 to 1.52715, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.ftrl.Ftrl object at 0x7f59b2f7e080>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5272 - accuracy: 0.3435 - val_loss: 1.4988 - val_accuracy: 0.3399\n",
            "Epoch 38/50\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5272 - accuracy: 0.3445\n",
            "Epoch 00038: loss did not improve from 1.52715\n",
            "595/595 [==============================] - 7s 12ms/step - loss: 1.5278 - accuracy: 0.3439 - val_loss: 1.4990 - val_accuracy: 0.3399\n",
            "Epoch 39/50\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5288 - accuracy: 0.3437\n",
            "Epoch 00039: loss did not improve from 1.52715\n",
            "595/595 [==============================] - 7s 12ms/step - loss: 1.5288 - accuracy: 0.3438 - val_loss: 1.4988 - val_accuracy: 0.3399\n",
            "Epoch 40/50\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5273 - accuracy: 0.3437\n",
            "Epoch 00040: loss did not improve from 1.52715\n",
            "595/595 [==============================] - 7s 12ms/step - loss: 1.5274 - accuracy: 0.3435 - val_loss: 1.4991 - val_accuracy: 0.3399\n",
            "Epoch 41/50\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5273 - accuracy: 0.3440\n",
            "Epoch 00041: loss did not improve from 1.52715\n",
            "595/595 [==============================] - 7s 12ms/step - loss: 1.5274 - accuracy: 0.3436 - val_loss: 1.4991 - val_accuracy: 0.3399\n",
            "Epoch 42/50\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5276 - accuracy: 0.3437\n",
            "Epoch 00042: loss did not improve from 1.52715\n",
            "595/595 [==============================] - 7s 12ms/step - loss: 1.5274 - accuracy: 0.3436 - val_loss: 1.4991 - val_accuracy: 0.3399\n",
            "Epoch 43/50\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.5283 - accuracy: 0.3434\n",
            "Epoch 00043: loss did not improve from 1.52715\n",
            "595/595 [==============================] - 7s 12ms/step - loss: 1.5280 - accuracy: 0.3437 - val_loss: 1.4989 - val_accuracy: 0.3399\n",
            "Epoch 44/50\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.5275 - accuracy: 0.3434\n",
            "Epoch 00044: loss did not improve from 1.52715\n",
            "595/595 [==============================] - 7s 13ms/step - loss: 1.5275 - accuracy: 0.3434 - val_loss: 1.4990 - val_accuracy: 0.3399\n",
            "Epoch 45/50\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5273 - accuracy: 0.3440\n",
            "Epoch 00045: loss did not improve from 1.52715\n",
            "595/595 [==============================] - 7s 13ms/step - loss: 1.5276 - accuracy: 0.3436 - val_loss: 1.4991 - val_accuracy: 0.3399\n",
            "Epoch 46/50\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.5278 - accuracy: 0.3436\n",
            "Epoch 00046: loss did not improve from 1.52715\n",
            "595/595 [==============================] - 7s 12ms/step - loss: 1.5280 - accuracy: 0.3435 - val_loss: 1.4990 - val_accuracy: 0.3399\n",
            "Epoch 47/50\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5281 - accuracy: 0.3434\n",
            "Epoch 00047: loss did not improve from 1.52715\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5278 - accuracy: 0.3436 - val_loss: 1.4992 - val_accuracy: 0.3399\n",
            "Epoch 48/50\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5279 - accuracy: 0.3436\n",
            "Epoch 00048: loss did not improve from 1.52715\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5279 - accuracy: 0.3436 - val_loss: 1.4991 - val_accuracy: 0.3399\n",
            "Epoch 49/50\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5276 - accuracy: 0.3437\n",
            "Epoch 00049: loss did not improve from 1.52715\n",
            "595/595 [==============================] - 7s 12ms/step - loss: 1.5276 - accuracy: 0.3437 - val_loss: 1.4992 - val_accuracy: 0.3399\n",
            "Epoch 50/50\n",
            "589/595 [============================>.] - ETA: 0s - loss: 1.5268 - accuracy: 0.3440\n",
            "Epoch 00050: loss did not improve from 1.52715\n",
            "595/595 [==============================] - 7s 12ms/step - loss: 1.5273 - accuracy: 0.3436 - val_loss: 1.4991 - val_accuracy: 0.3399\n",
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
            "Epoch 1/50\n",
            "  2/595 [..............................] - ETA: 31s - loss: 2.1827 - accuracy: 0.1000WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0144s vs `on_train_batch_end` time: 0.0926s). Check your callbacks.\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5557 - accuracy: 0.3273\n",
            "Epoch 00001: loss improved from inf to 1.55562, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f59b0bdc0b8>_Batch25_LR0.01_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5556 - accuracy: 0.3274 - val_loss: 1.5055 - val_accuracy: 0.3399\n",
            "Epoch 2/50\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.5370 - accuracy: 0.3372\n",
            "Epoch 00002: loss improved from 1.55562 to 1.53693, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f59b0bdc0b8>_Batch25_LR0.01_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5369 - accuracy: 0.3377 - val_loss: 1.5081 - val_accuracy: 0.3375\n",
            "Epoch 3/50\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.5326 - accuracy: 0.3397\n",
            "Epoch 00003: loss improved from 1.53693 to 1.53259, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f59b0bdc0b8>_Batch25_LR0.01_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5326 - accuracy: 0.3397 - val_loss: 1.5073 - val_accuracy: 0.3399\n",
            "Epoch 4/50\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5301 - accuracy: 0.3424\n",
            "Epoch 00004: loss improved from 1.53259 to 1.52969, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f59b0bdc0b8>_Batch25_LR0.01_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5297 - accuracy: 0.3424 - val_loss: 1.5038 - val_accuracy: 0.3399\n",
            "Epoch 5/50\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5298 - accuracy: 0.3429\n",
            "Epoch 00005: loss improved from 1.52969 to 1.52939, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f59b0bdc0b8>_Batch25_LR0.01_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5294 - accuracy: 0.3431 - val_loss: 1.5054 - val_accuracy: 0.3399\n",
            "Epoch 6/50\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5284 - accuracy: 0.3415\n",
            "Epoch 00006: loss improved from 1.52939 to 1.52830, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f59b0bdc0b8>_Batch25_LR0.01_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5283 - accuracy: 0.3413 - val_loss: 1.5034 - val_accuracy: 0.3399\n",
            "Epoch 7/50\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5273 - accuracy: 0.3422\n",
            "Epoch 00007: loss improved from 1.52830 to 1.52718, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f59b0bdc0b8>_Batch25_LR0.01_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5272 - accuracy: 0.3422 - val_loss: 1.5039 - val_accuracy: 0.3399\n",
            "Epoch 8/50\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5266 - accuracy: 0.3432\n",
            "Epoch 00008: loss improved from 1.52718 to 1.52615, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f59b0bdc0b8>_Batch25_LR0.01_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5262 - accuracy: 0.3433 - val_loss: 1.4998 - val_accuracy: 0.3399\n",
            "Epoch 9/50\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5259 - accuracy: 0.3419\n",
            "Epoch 00009: loss improved from 1.52615 to 1.52535, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f59b0bdc0b8>_Batch25_LR0.01_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5254 - accuracy: 0.3424 - val_loss: 1.4999 - val_accuracy: 0.3399\n",
            "Epoch 10/50\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.5241 - accuracy: 0.3438\n",
            "Epoch 00010: loss improved from 1.52535 to 1.52465, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f59b0bdc0b8>_Batch25_LR0.01_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5247 - accuracy: 0.3439 - val_loss: 1.5060 - val_accuracy: 0.3399\n",
            "Epoch 11/50\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5229 - accuracy: 0.3438\n",
            "Epoch 00011: loss improved from 1.52465 to 1.52328, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f59b0bdc0b8>_Batch25_LR0.01_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5233 - accuracy: 0.3433 - val_loss: 1.4973 - val_accuracy: 0.3399\n",
            "Epoch 12/50\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5247 - accuracy: 0.3429\n",
            "Epoch 00012: loss did not improve from 1.52328\n",
            "595/595 [==============================] - 7s 12ms/step - loss: 1.5243 - accuracy: 0.3432 - val_loss: 1.4952 - val_accuracy: 0.3399\n",
            "Epoch 13/50\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5233 - accuracy: 0.3427\n",
            "Epoch 00013: loss improved from 1.52328 to 1.52327, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f59b0bdc0b8>_Batch25_LR0.01_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5233 - accuracy: 0.3429 - val_loss: 1.5010 - val_accuracy: 0.3399\n",
            "Epoch 14/50\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5223 - accuracy: 0.3444\n",
            "Epoch 00014: loss improved from 1.52327 to 1.52237, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f59b0bdc0b8>_Batch25_LR0.01_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5224 - accuracy: 0.3444 - val_loss: 1.4972 - val_accuracy: 0.3399\n",
            "Epoch 15/50\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5215 - accuracy: 0.3441\n",
            "Epoch 00015: loss improved from 1.52237 to 1.52176, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f59b0bdc0b8>_Batch25_LR0.01_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5218 - accuracy: 0.3439 - val_loss: 1.4977 - val_accuracy: 0.3399\n",
            "Epoch 16/50\n",
            "589/595 [============================>.] - ETA: 0s - loss: 1.5216 - accuracy: 0.3443\n",
            "Epoch 00016: loss improved from 1.52176 to 1.52147, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f59b0bdc0b8>_Batch25_LR0.01_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5215 - accuracy: 0.3439 - val_loss: 1.4912 - val_accuracy: 0.3369\n",
            "Epoch 17/50\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5215 - accuracy: 0.3429\n",
            "Epoch 00017: loss improved from 1.52147 to 1.52134, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f59b0bdc0b8>_Batch25_LR0.01_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5213 - accuracy: 0.3428 - val_loss: 1.4945 - val_accuracy: 0.3399\n",
            "Epoch 18/50\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.5203 - accuracy: 0.3422\n",
            "Epoch 00018: loss improved from 1.52134 to 1.52010, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f59b0bdc0b8>_Batch25_LR0.01_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5201 - accuracy: 0.3424 - val_loss: 1.4996 - val_accuracy: 0.3399\n",
            "Epoch 19/50\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5197 - accuracy: 0.3429\n",
            "Epoch 00019: loss improved from 1.52010 to 1.51972, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f59b0bdc0b8>_Batch25_LR0.01_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5197 - accuracy: 0.3429 - val_loss: 1.4925 - val_accuracy: 0.3399\n",
            "Epoch 20/50\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5208 - accuracy: 0.3427\n",
            "Epoch 00020: loss did not improve from 1.51972\n",
            "595/595 [==============================] - 7s 12ms/step - loss: 1.5207 - accuracy: 0.3426 - val_loss: 1.5184 - val_accuracy: 0.3399\n",
            "Epoch 21/50\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.5184 - accuracy: 0.3430\n",
            "Epoch 00021: loss improved from 1.51972 to 1.51848, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f59b0bdc0b8>_Batch25_LR0.01_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5185 - accuracy: 0.3434 - val_loss: 1.5268 - val_accuracy: 0.3399\n",
            "Epoch 22/50\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5204 - accuracy: 0.3429\n",
            "Epoch 00022: loss did not improve from 1.51848\n",
            "595/595 [==============================] - 7s 12ms/step - loss: 1.5198 - accuracy: 0.3433 - val_loss: 1.4964 - val_accuracy: 0.3399\n",
            "Epoch 23/50\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5192 - accuracy: 0.3428\n",
            "Epoch 00023: loss did not improve from 1.51848\n",
            "595/595 [==============================] - 7s 13ms/step - loss: 1.5188 - accuracy: 0.3432 - val_loss: 1.5109 - val_accuracy: 0.3399\n",
            "Epoch 24/50\n",
            "589/595 [============================>.] - ETA: 0s - loss: 1.5180 - accuracy: 0.3442\n",
            "Epoch 00024: loss improved from 1.51848 to 1.51811, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f59b0bdc0b8>_Batch25_LR0.01_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5181 - accuracy: 0.3441 - val_loss: 1.5014 - val_accuracy: 0.3399\n",
            "Epoch 25/50\n",
            "589/595 [============================>.] - ETA: 0s - loss: 1.5170 - accuracy: 0.3432\n",
            "Epoch 00025: loss improved from 1.51811 to 1.51700, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f59b0bdc0b8>_Batch25_LR0.01_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5170 - accuracy: 0.3430 - val_loss: 1.4958 - val_accuracy: 0.3399\n",
            "Epoch 26/50\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5170 - accuracy: 0.3428\n",
            "Epoch 00026: loss improved from 1.51700 to 1.51689, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f59b0bdc0b8>_Batch25_LR0.01_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5169 - accuracy: 0.3428 - val_loss: 1.4952 - val_accuracy: 0.3399\n",
            "Epoch 27/50\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5143 - accuracy: 0.3433\n",
            "Epoch 00027: loss improved from 1.51689 to 1.51445, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f59b0bdc0b8>_Batch25_LR0.01_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5144 - accuracy: 0.3432 - val_loss: 1.5125 - val_accuracy: 0.3399\n",
            "Epoch 28/50\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5167 - accuracy: 0.3424\n",
            "Epoch 00028: loss did not improve from 1.51445\n",
            "595/595 [==============================] - 7s 13ms/step - loss: 1.5169 - accuracy: 0.3424 - val_loss: 1.5033 - val_accuracy: 0.3399\n",
            "Epoch 29/50\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5160 - accuracy: 0.3436\n",
            "Epoch 00029: loss did not improve from 1.51445\n",
            "595/595 [==============================] - 7s 13ms/step - loss: 1.5158 - accuracy: 0.3434 - val_loss: 1.5016 - val_accuracy: 0.3399\n",
            "Epoch 30/50\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5159 - accuracy: 0.3429\n",
            "Epoch 00030: loss did not improve from 1.51445\n",
            "595/595 [==============================] - 7s 13ms/step - loss: 1.5157 - accuracy: 0.3431 - val_loss: 1.4935 - val_accuracy: 0.3399\n",
            "Epoch 31/50\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.5144 - accuracy: 0.3415\n",
            "Epoch 00031: loss improved from 1.51445 to 1.51441, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f59b0bdc0b8>_Batch25_LR0.01_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5144 - accuracy: 0.3415 - val_loss: 1.5011 - val_accuracy: 0.3399\n",
            "Epoch 32/50\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5143 - accuracy: 0.3418\n",
            "Epoch 00032: loss improved from 1.51441 to 1.51427, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f59b0bdc0b8>_Batch25_LR0.01_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5143 - accuracy: 0.3418 - val_loss: 1.5067 - val_accuracy: 0.3399\n",
            "Epoch 33/50\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5144 - accuracy: 0.3447\n",
            "Epoch 00033: loss did not improve from 1.51427\n",
            "595/595 [==============================] - 7s 12ms/step - loss: 1.5144 - accuracy: 0.3447 - val_loss: 1.5019 - val_accuracy: 0.3399\n",
            "Epoch 34/50\n",
            "589/595 [============================>.] - ETA: 0s - loss: 1.5133 - accuracy: 0.3423\n",
            "Epoch 00034: loss improved from 1.51427 to 1.51376, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f59b0bdc0b8>_Batch25_LR0.01_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5138 - accuracy: 0.3422 - val_loss: 1.4979 - val_accuracy: 0.3399\n",
            "Epoch 35/50\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5135 - accuracy: 0.3448\n",
            "Epoch 00035: loss improved from 1.51376 to 1.51330, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f59b0bdc0b8>_Batch25_LR0.01_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5133 - accuracy: 0.3449 - val_loss: 1.4936 - val_accuracy: 0.3399\n",
            "Epoch 36/50\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.5127 - accuracy: 0.3447\n",
            "Epoch 00036: loss improved from 1.51330 to 1.51267, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f59b0bdc0b8>_Batch25_LR0.01_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5127 - accuracy: 0.3447 - val_loss: 1.4984 - val_accuracy: 0.3399\n",
            "Epoch 37/50\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.5128 - accuracy: 0.3458\n",
            "Epoch 00037: loss improved from 1.51267 to 1.51266, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f59b0bdc0b8>_Batch25_LR0.01_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5127 - accuracy: 0.3456 - val_loss: 1.4895 - val_accuracy: 0.3399\n",
            "Epoch 38/50\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5113 - accuracy: 0.3430\n",
            "Epoch 00038: loss improved from 1.51266 to 1.51157, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f59b0bdc0b8>_Batch25_LR0.01_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5116 - accuracy: 0.3428 - val_loss: 1.4925 - val_accuracy: 0.3393\n",
            "Epoch 39/50\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5118 - accuracy: 0.3432\n",
            "Epoch 00039: loss did not improve from 1.51157\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5117 - accuracy: 0.3436 - val_loss: 1.5276 - val_accuracy: 0.3340\n",
            "Epoch 40/50\n",
            "589/595 [============================>.] - ETA: 0s - loss: 1.5109 - accuracy: 0.3451\n",
            "Epoch 00040: loss improved from 1.51157 to 1.51091, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f59b0bdc0b8>_Batch25_LR0.01_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5109 - accuracy: 0.3450 - val_loss: 1.4986 - val_accuracy: 0.3399\n",
            "Epoch 41/50\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5118 - accuracy: 0.3463\n",
            "Epoch 00041: loss did not improve from 1.51091\n",
            "595/595 [==============================] - 7s 12ms/step - loss: 1.5117 - accuracy: 0.3464 - val_loss: 1.5011 - val_accuracy: 0.3399\n",
            "Epoch 42/50\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5109 - accuracy: 0.3439\n",
            "Epoch 00042: loss did not improve from 1.51091\n",
            "595/595 [==============================] - 7s 12ms/step - loss: 1.5110 - accuracy: 0.3437 - val_loss: 1.5008 - val_accuracy: 0.3372\n",
            "Epoch 43/50\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5107 - accuracy: 0.3448\n",
            "Epoch 00043: loss improved from 1.51091 to 1.51050, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f59b0bdc0b8>_Batch25_LR0.01_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5105 - accuracy: 0.3449 - val_loss: 1.5099 - val_accuracy: 0.3399\n",
            "Epoch 44/50\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.5113 - accuracy: 0.3439\n",
            "Epoch 00044: loss did not improve from 1.51050\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5113 - accuracy: 0.3439 - val_loss: 1.5022 - val_accuracy: 0.3372\n",
            "Epoch 45/50\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5101 - accuracy: 0.3443\n",
            "Epoch 00045: loss improved from 1.51050 to 1.51000, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f59b0bdc0b8>_Batch25_LR0.01_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5100 - accuracy: 0.3443 - val_loss: 1.5027 - val_accuracy: 0.3372\n",
            "Epoch 46/50\n",
            "589/595 [============================>.] - ETA: 0s - loss: 1.5095 - accuracy: 0.3438\n",
            "Epoch 00046: loss improved from 1.51000 to 1.50932, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f59b0bdc0b8>_Batch25_LR0.01_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5093 - accuracy: 0.3445 - val_loss: 1.5084 - val_accuracy: 0.3375\n",
            "Epoch 47/50\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5095 - accuracy: 0.3437\n",
            "Epoch 00047: loss did not improve from 1.50932\n",
            "595/595 [==============================] - 7s 12ms/step - loss: 1.5094 - accuracy: 0.3436 - val_loss: 1.4981 - val_accuracy: 0.3245\n",
            "Epoch 48/50\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5093 - accuracy: 0.3433\n",
            "Epoch 00048: loss did not improve from 1.50932\n",
            "595/595 [==============================] - 7s 13ms/step - loss: 1.5098 - accuracy: 0.3432 - val_loss: 1.5083 - val_accuracy: 0.3375\n",
            "Epoch 49/50\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5088 - accuracy: 0.3468\n",
            "Epoch 00049: loss improved from 1.50932 to 1.50881, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f59b0bdc0b8>_Batch25_LR0.01_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5088 - accuracy: 0.3468 - val_loss: 1.5093 - val_accuracy: 0.3372\n",
            "Epoch 50/50\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5091 - accuracy: 0.3438\n",
            "Epoch 00050: loss did not improve from 1.50881\n",
            "595/595 [==============================] - 7s 12ms/step - loss: 1.5090 - accuracy: 0.3439 - val_loss: 1.4919 - val_accuracy: 0.3372\n",
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
            "Epoch 1/50\n",
            "  2/595 [..............................] - ETA: 42s - loss: 3.1488 - accuracy: 0.2200   WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0150s vs `on_train_batch_end` time: 0.1315s). Check your callbacks.\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5630 - accuracy: 0.3333\n",
            "Epoch 00001: loss improved from inf to 1.56282, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.rmsprop.RMSprop object at 0x7f59b0bce5c0>_Batch25_LR0.01_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5628 - accuracy: 0.3334 - val_loss: 1.5065 - val_accuracy: 0.3399\n",
            "Epoch 2/50\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5301 - accuracy: 0.3427\n",
            "Epoch 00002: loss improved from 1.56282 to 1.53018, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.rmsprop.RMSprop object at 0x7f59b0bce5c0>_Batch25_LR0.01_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5302 - accuracy: 0.3426 - val_loss: 1.4938 - val_accuracy: 0.3399\n",
            "Epoch 3/50\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5294 - accuracy: 0.3434\n",
            "Epoch 00003: loss improved from 1.53018 to 1.52923, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.rmsprop.RMSprop object at 0x7f59b0bce5c0>_Batch25_LR0.01_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5292 - accuracy: 0.3436 - val_loss: 1.4991 - val_accuracy: 0.3399\n",
            "Epoch 4/50\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5298 - accuracy: 0.3438\n",
            "Epoch 00004: loss did not improve from 1.52923\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5298 - accuracy: 0.3436 - val_loss: 1.4925 - val_accuracy: 0.3399\n",
            "Epoch 5/50\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5290 - accuracy: 0.3438\n",
            "Epoch 00005: loss did not improve from 1.52923\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5296 - accuracy: 0.3436 - val_loss: 1.4962 - val_accuracy: 0.3399\n",
            "Epoch 6/50\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5298 - accuracy: 0.3436\n",
            "Epoch 00006: loss did not improve from 1.52923\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5298 - accuracy: 0.3436 - val_loss: 1.4986 - val_accuracy: 0.3399\n",
            "Epoch 7/50\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5300 - accuracy: 0.3433\n",
            "Epoch 00007: loss did not improve from 1.52923\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5298 - accuracy: 0.3436 - val_loss: 1.4970 - val_accuracy: 0.3399\n",
            "Epoch 8/50\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5296 - accuracy: 0.3436\n",
            "Epoch 00008: loss did not improve from 1.52923\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5297 - accuracy: 0.3436 - val_loss: 1.5001 - val_accuracy: 0.3399\n",
            "Epoch 9/50\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5298 - accuracy: 0.3439\n",
            "Epoch 00009: loss did not improve from 1.52923\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5295 - accuracy: 0.3436 - val_loss: 1.4960 - val_accuracy: 0.3399\n",
            "Epoch 10/50\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5302 - accuracy: 0.3438\n",
            "Epoch 00010: loss did not improve from 1.52923\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5303 - accuracy: 0.3436 - val_loss: 1.4957 - val_accuracy: 0.3399\n",
            "Epoch 11/50\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5304 - accuracy: 0.3437\n",
            "Epoch 00011: loss did not improve from 1.52923\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5304 - accuracy: 0.3436 - val_loss: 1.4980 - val_accuracy: 0.3399\n",
            "Epoch 12/50\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5302 - accuracy: 0.3442\n",
            "Epoch 00012: loss did not improve from 1.52923\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5304 - accuracy: 0.3436 - val_loss: 1.4941 - val_accuracy: 0.3399\n",
            "Epoch 13/50\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.5294 - accuracy: 0.3441\n",
            "Epoch 00013: loss did not improve from 1.52923\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5296 - accuracy: 0.3436 - val_loss: 1.4961 - val_accuracy: 0.3399\n",
            "Epoch 14/50\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.5298 - accuracy: 0.3433\n",
            "Epoch 00014: loss did not improve from 1.52923\n",
            "595/595 [==============================] - 7s 13ms/step - loss: 1.5297 - accuracy: 0.3436 - val_loss: 1.4979 - val_accuracy: 0.3399\n",
            "Epoch 15/50\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5294 - accuracy: 0.3438\n",
            "Epoch 00015: loss did not improve from 1.52923\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5294 - accuracy: 0.3436 - val_loss: 1.4927 - val_accuracy: 0.3399\n",
            "Epoch 16/50\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5297 - accuracy: 0.3436\n",
            "Epoch 00016: loss did not improve from 1.52923\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5296 - accuracy: 0.3436 - val_loss: 1.4921 - val_accuracy: 0.3399\n",
            "Epoch 17/50\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5303 - accuracy: 0.3434\n",
            "Epoch 00017: loss did not improve from 1.52923\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5299 - accuracy: 0.3436 - val_loss: 1.4927 - val_accuracy: 0.3399\n",
            "Epoch 18/50\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5300 - accuracy: 0.3431\n",
            "Epoch 00018: loss did not improve from 1.52923\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5296 - accuracy: 0.3436 - val_loss: 1.4993 - val_accuracy: 0.3399\n",
            "Epoch 19/50\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5305 - accuracy: 0.3434\n",
            "Epoch 00019: loss did not improve from 1.52923\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5304 - accuracy: 0.3436 - val_loss: 1.4951 - val_accuracy: 0.3399\n",
            "Epoch 20/50\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5301 - accuracy: 0.3436\n",
            "Epoch 00020: loss did not improve from 1.52923\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5301 - accuracy: 0.3436 - val_loss: 1.4947 - val_accuracy: 0.3399\n",
            "Epoch 21/50\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5299 - accuracy: 0.3438\n",
            "Epoch 00021: loss did not improve from 1.52923\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5301 - accuracy: 0.3436 - val_loss: 1.4925 - val_accuracy: 0.3399\n",
            "Epoch 22/50\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5298 - accuracy: 0.3436\n",
            "Epoch 00022: loss did not improve from 1.52923\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5298 - accuracy: 0.3436 - val_loss: 1.4903 - val_accuracy: 0.3399\n",
            "Epoch 23/50\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5303 - accuracy: 0.3435\n",
            "Epoch 00023: loss did not improve from 1.52923\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5300 - accuracy: 0.3436 - val_loss: 1.4915 - val_accuracy: 0.3399\n",
            "Epoch 24/50\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5299 - accuracy: 0.3438\n",
            "Epoch 00024: loss did not improve from 1.52923\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5300 - accuracy: 0.3436 - val_loss: 1.4985 - val_accuracy: 0.3399\n",
            "Epoch 25/50\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5293 - accuracy: 0.3438\n",
            "Epoch 00025: loss did not improve from 1.52923\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5299 - accuracy: 0.3436 - val_loss: 1.4931 - val_accuracy: 0.3399\n",
            "Epoch 26/50\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5296 - accuracy: 0.3440\n",
            "Epoch 00026: loss did not improve from 1.52923\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5299 - accuracy: 0.3436 - val_loss: 1.4970 - val_accuracy: 0.3399\n",
            "Epoch 27/50\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5297 - accuracy: 0.3436\n",
            "Epoch 00027: loss did not improve from 1.52923\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5297 - accuracy: 0.3436 - val_loss: 1.4943 - val_accuracy: 0.3399\n",
            "Epoch 28/50\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5292 - accuracy: 0.3438\n",
            "Epoch 00028: loss did not improve from 1.52923\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5295 - accuracy: 0.3436 - val_loss: 1.4958 - val_accuracy: 0.3399\n",
            "Epoch 29/50\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5293 - accuracy: 0.3437\n",
            "Epoch 00029: loss did not improve from 1.52923\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5293 - accuracy: 0.3436 - val_loss: 1.4972 - val_accuracy: 0.3399\n",
            "Epoch 30/50\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5299 - accuracy: 0.3434\n",
            "Epoch 00030: loss did not improve from 1.52923\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5301 - accuracy: 0.3436 - val_loss: 1.4938 - val_accuracy: 0.3399\n",
            "Epoch 31/50\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5301 - accuracy: 0.3437\n",
            "Epoch 00031: loss did not improve from 1.52923\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5301 - accuracy: 0.3436 - val_loss: 1.4920 - val_accuracy: 0.3399\n",
            "Epoch 32/50\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5304 - accuracy: 0.3436\n",
            "Epoch 00032: loss did not improve from 1.52923\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5303 - accuracy: 0.3436 - val_loss: 1.4928 - val_accuracy: 0.3399\n",
            "Epoch 33/50\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5304 - accuracy: 0.3435\n",
            "Epoch 00033: loss did not improve from 1.52923\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5306 - accuracy: 0.3436 - val_loss: 1.4918 - val_accuracy: 0.3399\n",
            "Epoch 34/50\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5302 - accuracy: 0.3435\n",
            "Epoch 00034: loss did not improve from 1.52923\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5300 - accuracy: 0.3436 - val_loss: 1.4975 - val_accuracy: 0.3399\n",
            "Epoch 35/50\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5301 - accuracy: 0.3438\n",
            "Epoch 00035: loss did not improve from 1.52923\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5300 - accuracy: 0.3436 - val_loss: 1.4917 - val_accuracy: 0.3399\n",
            "Epoch 36/50\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5308 - accuracy: 0.3437\n",
            "Epoch 00036: loss did not improve from 1.52923\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5308 - accuracy: 0.3436 - val_loss: 1.4920 - val_accuracy: 0.3399\n",
            "Epoch 37/50\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5305 - accuracy: 0.3432\n",
            "Epoch 00037: loss did not improve from 1.52923\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5301 - accuracy: 0.3436 - val_loss: 1.4959 - val_accuracy: 0.3399\n",
            "Epoch 38/50\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5304 - accuracy: 0.3437\n",
            "Epoch 00038: loss did not improve from 1.52923\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5303 - accuracy: 0.3436 - val_loss: 1.4928 - val_accuracy: 0.3399\n",
            "Epoch 39/50\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5304 - accuracy: 0.3436\n",
            "Epoch 00039: loss did not improve from 1.52923\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5302 - accuracy: 0.3436 - val_loss: 1.4941 - val_accuracy: 0.3399\n",
            "Epoch 40/50\n",
            "589/595 [============================>.] - ETA: 0s - loss: 1.5302 - accuracy: 0.3435\n",
            "Epoch 00040: loss did not improve from 1.52923\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5299 - accuracy: 0.3436 - val_loss: 1.4980 - val_accuracy: 0.3399\n",
            "Epoch 41/50\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5302 - accuracy: 0.3436\n",
            "Epoch 00041: loss did not improve from 1.52923\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5302 - accuracy: 0.3436 - val_loss: 1.4994 - val_accuracy: 0.3399\n",
            "Epoch 42/50\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5299 - accuracy: 0.3431\n",
            "Epoch 00042: loss did not improve from 1.52923\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5299 - accuracy: 0.3436 - val_loss: 1.5012 - val_accuracy: 0.3399\n",
            "Epoch 43/50\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.5302 - accuracy: 0.3436\n",
            "Epoch 00043: loss did not improve from 1.52923\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5302 - accuracy: 0.3436 - val_loss: 1.4931 - val_accuracy: 0.3399\n",
            "Epoch 44/50\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5298 - accuracy: 0.3436\n",
            "Epoch 00044: loss did not improve from 1.52923\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5297 - accuracy: 0.3436 - val_loss: 1.4945 - val_accuracy: 0.3399\n",
            "Epoch 45/50\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5296 - accuracy: 0.3439\n",
            "Epoch 00045: loss did not improve from 1.52923\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5298 - accuracy: 0.3436 - val_loss: 1.4929 - val_accuracy: 0.3399\n",
            "Epoch 46/50\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.5299 - accuracy: 0.3436\n",
            "Epoch 00046: loss did not improve from 1.52923\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5299 - accuracy: 0.3436 - val_loss: 1.4917 - val_accuracy: 0.3399\n",
            "Epoch 47/50\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.5293 - accuracy: 0.3439\n",
            "Epoch 00047: loss did not improve from 1.52923\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5297 - accuracy: 0.3436 - val_loss: 1.4967 - val_accuracy: 0.3399\n",
            "Epoch 48/50\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5295 - accuracy: 0.3438\n",
            "Epoch 00048: loss did not improve from 1.52923\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5296 - accuracy: 0.3436 - val_loss: 1.4945 - val_accuracy: 0.3399\n",
            "Epoch 49/50\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5293 - accuracy: 0.3439\n",
            "Epoch 00049: loss did not improve from 1.52923\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5300 - accuracy: 0.3436 - val_loss: 1.4967 - val_accuracy: 0.3399\n",
            "Epoch 50/50\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5298 - accuracy: 0.3437\n",
            "Epoch 00050: loss did not improve from 1.52923\n",
            "595/595 [==============================] - 7s 13ms/step - loss: 1.5299 - accuracy: 0.3436 - val_loss: 1.4948 - val_accuracy: 0.3399\n",
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
            "Epoch 1/50\n",
            "  2/595 [..............................] - ETA: 39s - loss: 1.9916 - accuracy: 0.2000WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0142s vs `on_train_batch_end` time: 0.1169s). Check your callbacks.\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5508 - accuracy: 0.3316\n",
            "Epoch 00001: loss improved from inf to 1.55038, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f59b0a1cf60>_Batch25_LR0.01_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5504 - accuracy: 0.3317 - val_loss: 1.4961 - val_accuracy: 0.3399\n",
            "Epoch 2/50\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.5343 - accuracy: 0.3438\n",
            "Epoch 00002: loss improved from 1.55038 to 1.53427, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f59b0a1cf60>_Batch25_LR0.01_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5343 - accuracy: 0.3438 - val_loss: 1.4989 - val_accuracy: 0.3399\n",
            "Epoch 3/50\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.5289 - accuracy: 0.3436\n",
            "Epoch 00003: loss improved from 1.53427 to 1.52891, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f59b0a1cf60>_Batch25_LR0.01_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5289 - accuracy: 0.3436 - val_loss: 1.4961 - val_accuracy: 0.3399\n",
            "Epoch 4/50\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5287 - accuracy: 0.3441\n",
            "Epoch 00004: loss improved from 1.52891 to 1.52884, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f59b0a1cf60>_Batch25_LR0.01_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5288 - accuracy: 0.3436 - val_loss: 1.5011 - val_accuracy: 0.3399\n",
            "Epoch 5/50\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.5290 - accuracy: 0.3436\n",
            "Epoch 00005: loss did not improve from 1.52884\n",
            "595/595 [==============================] - 7s 13ms/step - loss: 1.5290 - accuracy: 0.3436 - val_loss: 1.4964 - val_accuracy: 0.3399\n",
            "Epoch 6/50\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.5292 - accuracy: 0.3438\n",
            "Epoch 00006: loss did not improve from 1.52884\n",
            "595/595 [==============================] - 7s 12ms/step - loss: 1.5292 - accuracy: 0.3436 - val_loss: 1.4989 - val_accuracy: 0.3399\n",
            "Epoch 7/50\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.5288 - accuracy: 0.3436\n",
            "Epoch 00007: loss improved from 1.52884 to 1.52880, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f59b0a1cf60>_Batch25_LR0.01_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5288 - accuracy: 0.3436 - val_loss: 1.5025 - val_accuracy: 0.3399\n",
            "Epoch 8/50\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5289 - accuracy: 0.3437\n",
            "Epoch 00008: loss did not improve from 1.52880\n",
            "595/595 [==============================] - 7s 13ms/step - loss: 1.5291 - accuracy: 0.3436 - val_loss: 1.4910 - val_accuracy: 0.3399\n",
            "Epoch 9/50\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.5286 - accuracy: 0.3435\n",
            "Epoch 00009: loss did not improve from 1.52880\n",
            "595/595 [==============================] - 7s 13ms/step - loss: 1.5290 - accuracy: 0.3436 - val_loss: 1.4977 - val_accuracy: 0.3399\n",
            "Epoch 10/50\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.5291 - accuracy: 0.3437\n",
            "Epoch 00010: loss did not improve from 1.52880\n",
            "595/595 [==============================] - 7s 13ms/step - loss: 1.5289 - accuracy: 0.3436 - val_loss: 1.4922 - val_accuracy: 0.3399\n",
            "Epoch 11/50\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5292 - accuracy: 0.3436\n",
            "Epoch 00011: loss did not improve from 1.52880\n",
            "595/595 [==============================] - 7s 12ms/step - loss: 1.5292 - accuracy: 0.3436 - val_loss: 1.4986 - val_accuracy: 0.3399\n",
            "Epoch 12/50\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5293 - accuracy: 0.3437\n",
            "Epoch 00012: loss did not improve from 1.52880\n",
            "595/595 [==============================] - 7s 12ms/step - loss: 1.5291 - accuracy: 0.3436 - val_loss: 1.4996 - val_accuracy: 0.3399\n",
            "Epoch 13/50\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5291 - accuracy: 0.3437\n",
            "Epoch 00013: loss did not improve from 1.52880\n",
            "595/595 [==============================] - 7s 12ms/step - loss: 1.5290 - accuracy: 0.3436 - val_loss: 1.4966 - val_accuracy: 0.3399\n",
            "Epoch 14/50\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5293 - accuracy: 0.3434\n",
            "Epoch 00014: loss did not improve from 1.52880\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5291 - accuracy: 0.3436 - val_loss: 1.4958 - val_accuracy: 0.3399\n",
            "Epoch 15/50\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5289 - accuracy: 0.3435\n",
            "Epoch 00015: loss did not improve from 1.52880\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5288 - accuracy: 0.3436 - val_loss: 1.4971 - val_accuracy: 0.3399\n",
            "Epoch 16/50\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5291 - accuracy: 0.3436\n",
            "Epoch 00016: loss did not improve from 1.52880\n",
            "595/595 [==============================] - 7s 13ms/step - loss: 1.5290 - accuracy: 0.3436 - val_loss: 1.4943 - val_accuracy: 0.3399\n",
            "Epoch 17/50\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5289 - accuracy: 0.3441\n",
            "Epoch 00017: loss did not improve from 1.52880\n",
            "595/595 [==============================] - 7s 13ms/step - loss: 1.5289 - accuracy: 0.3436 - val_loss: 1.4955 - val_accuracy: 0.3399\n",
            "Epoch 18/50\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5292 - accuracy: 0.3436\n",
            "Epoch 00018: loss did not improve from 1.52880\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5290 - accuracy: 0.3436 - val_loss: 1.4960 - val_accuracy: 0.3399\n",
            "Epoch 19/50\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5291 - accuracy: 0.3435\n",
            "Epoch 00019: loss did not improve from 1.52880\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5290 - accuracy: 0.3436 - val_loss: 1.4981 - val_accuracy: 0.3399\n",
            "Epoch 20/50\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5292 - accuracy: 0.3436\n",
            "Epoch 00020: loss did not improve from 1.52880\n",
            "595/595 [==============================] - 7s 13ms/step - loss: 1.5290 - accuracy: 0.3436 - val_loss: 1.4983 - val_accuracy: 0.3399\n",
            "Epoch 21/50\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5293 - accuracy: 0.3436\n",
            "Epoch 00021: loss did not improve from 1.52880\n",
            "595/595 [==============================] - 7s 12ms/step - loss: 1.5293 - accuracy: 0.3436 - val_loss: 1.5002 - val_accuracy: 0.3399\n",
            "Epoch 22/50\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.5289 - accuracy: 0.3436\n",
            "Epoch 00022: loss did not improve from 1.52880\n",
            "595/595 [==============================] - 7s 12ms/step - loss: 1.5289 - accuracy: 0.3436 - val_loss: 1.4922 - val_accuracy: 0.3399\n",
            "Epoch 23/50\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5291 - accuracy: 0.3437\n",
            "Epoch 00023: loss did not improve from 1.52880\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5290 - accuracy: 0.3436 - val_loss: 1.4981 - val_accuracy: 0.3399\n",
            "Epoch 24/50\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5288 - accuracy: 0.3439\n",
            "Epoch 00024: loss did not improve from 1.52880\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5290 - accuracy: 0.3436 - val_loss: 1.4967 - val_accuracy: 0.3399\n",
            "Epoch 25/50\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5286 - accuracy: 0.3438\n",
            "Epoch 00025: loss improved from 1.52880 to 1.52876, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f59b0a1cf60>_Batch25_LR0.01_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5288 - accuracy: 0.3436 - val_loss: 1.5009 - val_accuracy: 0.3399\n",
            "Epoch 26/50\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5295 - accuracy: 0.3436\n",
            "Epoch 00026: loss did not improve from 1.52876\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5293 - accuracy: 0.3436 - val_loss: 1.4980 - val_accuracy: 0.3399\n",
            "Epoch 27/50\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5291 - accuracy: 0.3437\n",
            "Epoch 00027: loss did not improve from 1.52876\n",
            "595/595 [==============================] - 7s 13ms/step - loss: 1.5292 - accuracy: 0.3436 - val_loss: 1.4943 - val_accuracy: 0.3399\n",
            "Epoch 28/50\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5290 - accuracy: 0.3435\n",
            "Epoch 00028: loss did not improve from 1.52876\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5289 - accuracy: 0.3436 - val_loss: 1.4981 - val_accuracy: 0.3399\n",
            "Epoch 29/50\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5278 - accuracy: 0.3438\n",
            "Epoch 00029: loss improved from 1.52876 to 1.52829, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f59b0a1cf60>_Batch25_LR0.01_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5283 - accuracy: 0.3436 - val_loss: 1.5012 - val_accuracy: 0.3399\n",
            "Epoch 30/50\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5290 - accuracy: 0.3440\n",
            "Epoch 00030: loss did not improve from 1.52829\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5294 - accuracy: 0.3436 - val_loss: 1.4978 - val_accuracy: 0.3399\n",
            "Epoch 31/50\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5287 - accuracy: 0.3434\n",
            "Epoch 00031: loss did not improve from 1.52829\n",
            "595/595 [==============================] - 7s 13ms/step - loss: 1.5285 - accuracy: 0.3436 - val_loss: 1.4966 - val_accuracy: 0.3399\n",
            "Epoch 32/50\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.5291 - accuracy: 0.3437\n",
            "Epoch 00032: loss did not improve from 1.52829\n",
            "595/595 [==============================] - 7s 12ms/step - loss: 1.5290 - accuracy: 0.3436 - val_loss: 1.5004 - val_accuracy: 0.3399\n",
            "Epoch 33/50\n",
            "589/595 [============================>.] - ETA: 0s - loss: 1.5294 - accuracy: 0.3436\n",
            "Epoch 00033: loss did not improve from 1.52829\n",
            "595/595 [==============================] - 7s 12ms/step - loss: 1.5291 - accuracy: 0.3436 - val_loss: 1.4977 - val_accuracy: 0.3399\n",
            "Epoch 34/50\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5293 - accuracy: 0.3436\n",
            "Epoch 00034: loss did not improve from 1.52829\n",
            "595/595 [==============================] - 7s 12ms/step - loss: 1.5291 - accuracy: 0.3436 - val_loss: 1.4964 - val_accuracy: 0.3399\n",
            "Epoch 35/50\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.5293 - accuracy: 0.3436\n",
            "Epoch 00035: loss did not improve from 1.52829\n",
            "595/595 [==============================] - 7s 12ms/step - loss: 1.5293 - accuracy: 0.3436 - val_loss: 1.4951 - val_accuracy: 0.3399\n",
            "Epoch 36/50\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5285 - accuracy: 0.3444\n",
            "Epoch 00036: loss did not improve from 1.52829\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5290 - accuracy: 0.3436 - val_loss: 1.4942 - val_accuracy: 0.3399\n",
            "Epoch 37/50\n",
            "589/595 [============================>.] - ETA: 0s - loss: 1.5298 - accuracy: 0.3429\n",
            "Epoch 00037: loss did not improve from 1.52829\n",
            "595/595 [==============================] - 7s 12ms/step - loss: 1.5288 - accuracy: 0.3436 - val_loss: 1.5000 - val_accuracy: 0.3399\n",
            "Epoch 38/50\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5291 - accuracy: 0.3438\n",
            "Epoch 00038: loss did not improve from 1.52829\n",
            "595/595 [==============================] - 7s 12ms/step - loss: 1.5290 - accuracy: 0.3436 - val_loss: 1.4989 - val_accuracy: 0.3399\n",
            "Epoch 39/50\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5291 - accuracy: 0.3437\n",
            "Epoch 00039: loss did not improve from 1.52829\n",
            "595/595 [==============================] - 7s 12ms/step - loss: 1.5291 - accuracy: 0.3436 - val_loss: 1.4964 - val_accuracy: 0.3399\n",
            "Epoch 40/50\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5289 - accuracy: 0.3434\n",
            "Epoch 00040: loss did not improve from 1.52829\n",
            "595/595 [==============================] - 7s 13ms/step - loss: 1.5287 - accuracy: 0.3436 - val_loss: 1.4915 - val_accuracy: 0.3399\n",
            "Epoch 41/50\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5294 - accuracy: 0.3432\n",
            "Epoch 00041: loss did not improve from 1.52829\n",
            "595/595 [==============================] - 7s 13ms/step - loss: 1.5291 - accuracy: 0.3436 - val_loss: 1.4953 - val_accuracy: 0.3399\n",
            "Epoch 42/50\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5290 - accuracy: 0.3434\n",
            "Epoch 00042: loss did not improve from 1.52829\n",
            "595/595 [==============================] - 7s 13ms/step - loss: 1.5289 - accuracy: 0.3436 - val_loss: 1.4994 - val_accuracy: 0.3399\n",
            "Epoch 43/50\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.5291 - accuracy: 0.3436\n",
            "Epoch 00043: loss did not improve from 1.52829\n",
            "595/595 [==============================] - 7s 12ms/step - loss: 1.5291 - accuracy: 0.3436 - val_loss: 1.4933 - val_accuracy: 0.3399\n",
            "Epoch 44/50\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5286 - accuracy: 0.3435\n",
            "Epoch 00044: loss did not improve from 1.52829\n",
            "595/595 [==============================] - 7s 13ms/step - loss: 1.5285 - accuracy: 0.3436 - val_loss: 1.4920 - val_accuracy: 0.3399\n",
            "Epoch 45/50\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.5293 - accuracy: 0.3433\n",
            "Epoch 00045: loss did not improve from 1.52829\n",
            "595/595 [==============================] - 7s 12ms/step - loss: 1.5290 - accuracy: 0.3436 - val_loss: 1.5013 - val_accuracy: 0.3399\n",
            "Epoch 46/50\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5292 - accuracy: 0.3437\n",
            "Epoch 00046: loss did not improve from 1.52829\n",
            "595/595 [==============================] - 7s 12ms/step - loss: 1.5288 - accuracy: 0.3436 - val_loss: 1.4958 - val_accuracy: 0.3399\n",
            "Epoch 47/50\n",
            "589/595 [============================>.] - ETA: 0s - loss: 1.5295 - accuracy: 0.3432\n",
            "Epoch 00047: loss did not improve from 1.52829\n",
            "595/595 [==============================] - 7s 13ms/step - loss: 1.5290 - accuracy: 0.3436 - val_loss: 1.4961 - val_accuracy: 0.3399\n",
            "Epoch 48/50\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5288 - accuracy: 0.3439\n",
            "Epoch 00048: loss did not improve from 1.52829\n",
            "595/595 [==============================] - 7s 12ms/step - loss: 1.5290 - accuracy: 0.3436 - val_loss: 1.4963 - val_accuracy: 0.3399\n",
            "Epoch 49/50\n",
            "589/595 [============================>.] - ETA: 0s - loss: 1.5292 - accuracy: 0.3432\n",
            "Epoch 00049: loss did not improve from 1.52829\n",
            "595/595 [==============================] - 7s 12ms/step - loss: 1.5291 - accuracy: 0.3436 - val_loss: 1.4929 - val_accuracy: 0.3399\n",
            "Epoch 50/50\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5294 - accuracy: 0.3435\n",
            "Epoch 00050: loss did not improve from 1.52829\n",
            "595/595 [==============================] - 7s 12ms/step - loss: 1.5292 - accuracy: 0.3436 - val_loss: 1.4950 - val_accuracy: 0.3399\n",
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
            "Epoch 1/50\n",
            "  2/595 [..............................] - ETA: 36s - loss: 2.0207 - accuracy: 0.1600WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0135s vs `on_train_batch_end` time: 0.1105s). Check your callbacks.\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5496 - accuracy: 0.3290\n",
            "Epoch 00001: loss improved from inf to 1.55009, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adagrad.Adagrad object at 0x7f59b0a265c0>_Batch25_LR0.01_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5501 - accuracy: 0.3284 - val_loss: 1.5087 - val_accuracy: 0.3399\n",
            "Epoch 2/50\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5323 - accuracy: 0.3405\n",
            "Epoch 00002: loss improved from 1.55009 to 1.53219, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adagrad.Adagrad object at 0x7f59b0a265c0>_Batch25_LR0.01_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5322 - accuracy: 0.3405 - val_loss: 1.4982 - val_accuracy: 0.3399\n",
            "Epoch 3/50\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5279 - accuracy: 0.3443\n",
            "Epoch 00003: loss improved from 1.53219 to 1.52812, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adagrad.Adagrad object at 0x7f59b0a265c0>_Batch25_LR0.01_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5281 - accuracy: 0.3443 - val_loss: 1.5020 - val_accuracy: 0.3399\n",
            "Epoch 4/50\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5245 - accuracy: 0.3429\n",
            "Epoch 00004: loss improved from 1.52812 to 1.52459, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adagrad.Adagrad object at 0x7f59b0a265c0>_Batch25_LR0.01_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5246 - accuracy: 0.3429 - val_loss: 1.4988 - val_accuracy: 0.3399\n",
            "Epoch 5/50\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5230 - accuracy: 0.3443\n",
            "Epoch 00005: loss improved from 1.52459 to 1.52275, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adagrad.Adagrad object at 0x7f59b0a265c0>_Batch25_LR0.01_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5228 - accuracy: 0.3442 - val_loss: 1.4977 - val_accuracy: 0.3399\n",
            "Epoch 6/50\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5240 - accuracy: 0.3432\n",
            "Epoch 00006: loss did not improve from 1.52275\n",
            "595/595 [==============================] - 7s 13ms/step - loss: 1.5240 - accuracy: 0.3431 - val_loss: 1.5002 - val_accuracy: 0.3399\n",
            "Epoch 7/50\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5237 - accuracy: 0.3427\n",
            "Epoch 00007: loss did not improve from 1.52275\n",
            "595/595 [==============================] - 7s 12ms/step - loss: 1.5234 - accuracy: 0.3430 - val_loss: 1.5044 - val_accuracy: 0.3399\n",
            "Epoch 8/50\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5201 - accuracy: 0.3441\n",
            "Epoch 00008: loss improved from 1.52275 to 1.52077, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adagrad.Adagrad object at 0x7f59b0a265c0>_Batch25_LR0.01_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5208 - accuracy: 0.3438 - val_loss: 1.5031 - val_accuracy: 0.3399\n",
            "Epoch 9/50\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5207 - accuracy: 0.3435\n",
            "Epoch 00009: loss improved from 1.52077 to 1.52043, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adagrad.Adagrad object at 0x7f59b0a265c0>_Batch25_LR0.01_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5204 - accuracy: 0.3437 - val_loss: 1.4988 - val_accuracy: 0.3399\n",
            "Epoch 10/50\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5195 - accuracy: 0.3438\n",
            "Epoch 00010: loss improved from 1.52043 to 1.51951, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adagrad.Adagrad object at 0x7f59b0a265c0>_Batch25_LR0.01_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5195 - accuracy: 0.3439 - val_loss: 1.5025 - val_accuracy: 0.3399\n",
            "Epoch 11/50\n",
            "288/595 [=============>................] - ETA: 2s - loss: 1.5143 - accuracy: 0.3432"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l8lTHN_5ZpzC",
        "outputId": "f7f916b7-daa2-43bc-bb9e-fd19fd5f3de2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import json\n",
        "\n",
        "# as requested in comment\n",
        "with open('result_training.txt', 'w') as file:\n",
        "     file.write(json.dumps(Tabres)) # use `json.loads` to do the reverse"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fUVybHIT4aA8"
      },
      "source": [
        "Loss vs accuracy\n",
        "\n",
        "https://kharshit.github.io/blog/2018/12/07/loss-vs-accuracy\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fxXLslzA4V8j"
      },
      "source": [
        "https://kharshit.github.io/blog/2018/12/07/loss-vs-accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fo3lfsYbZpcB"
      },
      "source": [
        "####Improve the Model with GAN - NOT FUNCTIONAL SO FAR"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Qltq67Ua9ac"
      },
      "source": [
        "https://keras.io/guides/customizing_what_happens_in_fit/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d9lf5VWAKZ2m"
      },
      "source": [
        "from tensorflow.keras import layers\n",
        "\n",
        "# Create the discriminator\n",
        "discriminator = keras.Sequential(\n",
        "    [\n",
        "        keras.Input(shape=(28, 28, 1)),\n",
        "        layers.Conv2D(64, (3, 3), strides=(2, 2), padding=\"same\"),\n",
        "        layers.LeakyReLU(alpha=0.2),\n",
        "        layers.Conv2D(128, (3, 3), strides=(2, 2), padding=\"same\"),\n",
        "        layers.LeakyReLU(alpha=0.2),\n",
        "        layers.GlobalMaxPooling2D(),\n",
        "        layers.Dense(1),\n",
        "    ],\n",
        "    name=\"discriminator\",\n",
        ")\n",
        "\n",
        "# Create the generator\n",
        "latent_dim = 128\n",
        "generator = keras.Sequential(\n",
        "    [\n",
        "        keras.Input(shape=(latent_dim,)),\n",
        "        # We want to generate 128 coefficients to reshape into a 7x7x128 map\n",
        "        layers.Dense(7 * 7 * 128),\n",
        "        layers.LeakyReLU(alpha=0.2),\n",
        "        layers.Reshape((7, 7, 128)),\n",
        "        layers.Conv2DTranspose(128, (4, 4), strides=(2, 2), padding=\"same\"),\n",
        "        layers.LeakyReLU(alpha=0.2),\n",
        "        layers.Conv2DTranspose(128, (4, 4), strides=(2, 2), padding=\"same\"),\n",
        "        layers.LeakyReLU(alpha=0.2),\n",
        "        layers.Conv2D(1, (7, 7), padding=\"same\", activation=\"sigmoid\"),\n",
        "    ],\n",
        "    name=\"generator\",\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R4wpRPC5bTua"
      },
      "source": [
        "class GAN(keras.Model):\n",
        "    def __init__(self, discriminator, generator, latent_dim):\n",
        "        super(GAN, self).__init__()\n",
        "        self.discriminator = discriminator\n",
        "        self.generator = generator\n",
        "        self.latent_dim = latent_dim\n",
        "\n",
        "    def compile(self, d_optimizer, g_optimizer, loss_fn):\n",
        "        super(GAN, self).compile()\n",
        "        self.d_optimizer = d_optimizer\n",
        "        self.g_optimizer = g_optimizer\n",
        "        self.loss_fn = loss_fn\n",
        "\n",
        "    def train_step(self, real_images):\n",
        "        if isinstance(real_images, tuple):\n",
        "            real_images = real_images[0]\n",
        "        # Sample random points in the latent space\n",
        "        batch_size = tf.shape(real_images)[0]\n",
        "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
        "\n",
        "        # Decode them to fake images\n",
        "        generated_images = self.generator(random_latent_vectors)\n",
        "\n",
        "        # Combine them with real images\n",
        "        combined_images = tf.concat([generated_images, real_images], axis=0)\n",
        "\n",
        "        # Assemble labels discriminating real from fake images\n",
        "        labels = tf.concat(\n",
        "            [tf.ones((batch_size, 1)), tf.zeros((batch_size, 1))], axis=0\n",
        "        )\n",
        "        # Add random noise to the labels - important trick!\n",
        "        labels += 0.05 * tf.random.uniform(tf.shape(labels))\n",
        "\n",
        "        # Train the discriminator\n",
        "        with tf.GradientTape() as tape:\n",
        "            predictions = self.discriminator(combined_images)\n",
        "            d_loss = self.loss_fn(labels, predictions)\n",
        "        grads = tape.gradient(d_loss, self.discriminator.trainable_weights)\n",
        "        self.d_optimizer.apply_gradients(\n",
        "            zip(grads, self.discriminator.trainable_weights)\n",
        "        )\n",
        "\n",
        "        # Sample random points in the latent space\n",
        "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
        "\n",
        "        # Assemble labels that say \"all real images\"\n",
        "        misleading_labels = tf.zeros((batch_size, 1))\n",
        "\n",
        "        # Train the generator (note that we should *not* update the weights\n",
        "        # of the discriminator)!\n",
        "        with tf.GradientTape() as tape:\n",
        "            predictions = self.discriminator(self.generator(random_latent_vectors))\n",
        "            g_loss = self.loss_fn(misleading_labels, predictions)\n",
        "        grads = tape.gradient(g_loss, self.generator.trainable_weights)\n",
        "        self.g_optimizer.apply_gradients(zip(grads, self.generator.trainable_weights))\n",
        "        return {\"d_loss\": d_loss, \"g_loss\": g_loss}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vOtZ0dD7bYhp"
      },
      "source": [
        "# Prepare the dataset. We use both the training & test MNIST digits.\n",
        "batch_size = 64\n",
        "(x_train, _), (x_test, _) = keras.datasets.mnist.load_data()\n",
        "all_digits = np.concatenate([x_train, x_test])\n",
        "all_digits = all_digits.astype(\"float32\") / 255.0\n",
        "all_digits = np.reshape(all_digits, (-1, 28, 28, 1))\n",
        "dataset = tf.data.Dataset.from_tensor_slices(all_digits)\n",
        "dataset = dataset.shuffle(buffer_size=1024).batch(batch_size)\n",
        "\n",
        "gan = GAN(discriminator=discriminator, generator=generator, latent_dim=latent_dim)\n",
        "gan.compile(\n",
        "    d_optimizer=keras.optimizers.Adam(learning_rate=0.0003),\n",
        "    g_optimizer=keras.optimizers.Adam(learning_rate=0.0003),\n",
        "    loss_fn=keras.losses.BinaryCrossentropy(from_logits=True),\n",
        ")\n",
        "\n",
        "# To limit execution time, we only train on 100 batches. You can train on\n",
        "# the entire dataset. You will need about 20 epochs to get nice results.\n",
        "gan.fit(dataset.take(100), epochs=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cez5uqWb4SSc"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oyRJzUx_5DSF"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R2_xo8psY5Iy"
      },
      "source": [
        "##Step 3: Evaluate the VGGsp500 model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PXXUYNbwZ4TX"
      },
      "source": [
        "This part will evaluate the model with the testing dataset that we generated in first step. We show the accuracy, the confusion matrix and the classification report"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mUSUr5CdwPPz"
      },
      "source": [
        "To improve learning after playing with all the parameters \n",
        "\n",
        "- get more dataset on stock and indice\n",
        "\n",
        "- work on vgg untrained \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eKbQ6tBAhlw8",
        "outputId": "84b52850-8f5a-4e5d-eeb5-0cc8628b0cf6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 251
        }
      },
      "source": [
        "print(\"name of the best model for each set of parameters\")\n",
        "bestmodel=\"best_model\"+vggsp500loss+\"_\"+vggsp500optimizer_name+\"_Batch\"+str(batch_size)+\"_LR\"+str(initial_learning_rate)+\".hdf5\"\n",
        "print(bestmodel)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "name of the best model for each set of parameters\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-379eac6dc040>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"name of the best model for each set of parameters\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mbestmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"best_model\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mvggsp500loss\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"_\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mvggsp500optimizer_name\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"_Batch\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"_LR\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minitial_learning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\".hdf5\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbestmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'vggsp500loss' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "23kG1lJE5KqF",
        "outputId": "7a92151c-ae13-4338-e07c-a1b8a5387089",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        }
      },
      "source": [
        "%cd /content/drive/My Drive/A_transfertTFMVggSP500\n",
        "%cd model \n",
        "%cp best_modelcategorical_crossentropy_adam_Batch100_LR0.1_0.99.hdf5 /content/DL_Tools_For_Finance/model/\n",
        "%ls -l"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/A_transfertTFMVggSP500\n",
            "/content/drive/My Drive/A_transfertTFMVggSP500/model\n",
            "cp: cannot stat 'best_modelcategorical_crossentropy_adam_Batch100_LR0.1_0.99.hdf5': No such file or directory\n",
            "total 465803\n",
            "-rw------- 1 root root 59726904 Sep  5 06:04  best_modelcategorical_crossentropy_Adagrad_Batch32_LR0.01_0.98.hdf5\n",
            "-rw------- 1 root root 59456824 Sep  1 00:22 'best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adagrad.Adagrad object at 0x7f59b9654fd0>_Batch25_LR0.001_Epochs100vggforsp500.h5'\n",
            "-rw------- 1 root root 59726928 Sep  1 00:35 'best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adamax.Adamax object at 0x7f59b9808a20>_Batch25_LR0.001_Epochs100.hdf5'\n",
            "-rw------- 1 root root 59726928 Sep  1 00:35 'best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adamax.Adamax object at 0x7f59b9808a20>_Batch25_LR0.001_Epochs100vggforsp500.h5'\n",
            "-rw------- 1 root root 59725496 Sep  1 00:44 'best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.ftrl.Ftrl object at 0x7f599f76e400>_Batch25_LR0.001_Epochs100.hdf5'\n",
            "-rw------- 1 root root 59726920 Aug 31 08:06  final_modelcategorical_crossentropy_adam_32.h5\n",
            "-rw------- 1 root root 59161736 Sep  5 06:10  initial_weights.h5\n",
            "-rw------- 1 root root 59726904 Sep  5 06:04  vggforsp500.h5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HAQnQZoR6rD6",
        "outputId": "bd4d4adc-5751-4c3d-81a0-e3489531edda",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%cd /content/DL_Tools_For_Finance/\n",
        "trained_model_path='/content/DL_Tools_For_Finance/model/final_modelcategorical_crossentropy_adam_32.h5'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/DL_Tools_For_Finance\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yxMUW3tCMVBM",
        "outputId": "31913564-b148-4977-e387-9e10ea68b556",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 598
        }
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from keras.utils import np_utils\n",
        "from keras.models import load_model\n",
        "from keras import backend as K\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "'''\n",
        "Here we have a trained model model/vggforsp500.h5 and datas for testing \n",
        "datas/X_test_image.csv\n",
        "datas/Y_test_StateClass_image.csv\n",
        "datas/Y_test_FutPredict_image.csv\n",
        "\n",
        "'''\n",
        "\n",
        "##\n",
        "'''\n",
        "UTILITY FUNCTIONS\n",
        "to put in another file\n",
        "'''\n",
        "\n",
        "def change_X_df__nparray_image(df_X_train_image_flattened ):\n",
        "  '''\n",
        "  setup_input_NN_image returns a dataframe of flaten image for x train and xtest\n",
        "  then this function will change each date into a nparray list of images with 32, 32, 3 size \n",
        "  '''\n",
        "  X_train_image=df_X_train_image_flattened\n",
        "  nb_train=len(X_train_image.index)\n",
        "  \n",
        "  x_train=np.zeros((nb_train,32,32,3))\n",
        "  for i in range(nb_train):\n",
        "    tmp=np.array(X_train_image.iloc[i])\n",
        "    tmp=tmp.reshape(32,32,3)\n",
        "    x_train[i]=tmp\n",
        "  return x_train\n",
        "##\n",
        "\n",
        "\n",
        "\n",
        "#recuperation of testing datas and organising it \n",
        "X_test_image=pd.read_csv('datas/X_test_image.csv')\n",
        "Y_test_StateClass_image=pd.read_csv('datas/Y_test_StateClass_image.csv')\n",
        "Y_test_FutPredict_image=pd.read_csv('datas/Y_test_FutPredict_image.csv')\n",
        "\n",
        "#setting up the index to Date\n",
        "X_test_image=X_test_image.set_index(\"Date\")\n",
        "Y_test_StateClass_image=Y_test_StateClass_image.set_index(\"Date\")\n",
        "Y_test_FutPredict_image=Y_test_FutPredict_image.set_index(\"Date\")\n",
        "\n",
        "#modify dataset to np array for input to NN\n",
        "x_test=change_X_df__nparray_image(X_test_image)\n",
        "y_test_state=np.array(Y_test_StateClass_image)\n",
        "y_test_value=np.array(Y_test_FutPredict_image)\n",
        "\n",
        "##Setting up xtest and ytest\n",
        "#Here we focus on predicting the future state Y_train_StateClass_image\n",
        "nb_test=len(X_test_image.index)\n",
        "x_test=np.zeros((nb_test,32,32,3))\n",
        "for i in range(nb_test):\n",
        "  tmp=np.array(X_test_image.iloc[i])\n",
        "  tmp=tmp.reshape(32,32,3)\n",
        "  x_test[i]=tmp\n",
        "\n",
        "y_test=np.array(Y_test_StateClass_image)\n",
        "#y_test=np.array(Y_test_FutPredict_image)\n",
        "\n",
        "#In our example we need to y into categorical as it has 6 categories\n",
        "nb_classes=6\n",
        "y_test_m = np_utils.to_categorical(y_test, nb_classes)\n",
        "############\n",
        "#recuperation of model\n",
        "vggsp500model = load_model(trained_model_path)\n",
        "\n",
        "#Evaluate the model on the test data\n",
        "score  = vggsp500model.evaluate(x_test, y_test_m)\n",
        "\n",
        "\n",
        "Y_pred = vggsp500model.predict(x_test)\n",
        "y_pred = np.argmax(Y_pred, axis=1)\n",
        "y= np.argmax(y_test_m,axis=1)\n",
        "\n",
        "\n",
        "print('Confusion Matrix')\n",
        "\n",
        "target_state = ['SS', 'SN', 'N','NB','BB','Error']\n",
        "\n",
        "def statetostring(x):\n",
        "  return target_state[int(x)]\n",
        "\n",
        "sY_pred=[statetostring(i) for i in y_pred]\n",
        "sY_real=[statetostring(i) for i in y]\n",
        "\n",
        "#matrice  de confusion\n",
        "mat=confusion_matrix(sY_real, sY_pred, normalize='true', labels=target_state)\n",
        "df_confmat=pd.DataFrame(mat,index=target_state, columns=target_state)\n",
        "\n",
        "#matrice  de confusion\n",
        "mat2=confusion_matrix(sY_real, sY_pred,  labels=target_state)\n",
        "df_confmat2=pd.DataFrame(mat2,index=target_state, columns=target_state)\n",
        "\n",
        "#Accuracy on test data\n",
        "print('Accuracy on the Test Images: ', score[1])\n",
        "#matrice  de confusion\n",
        "print(df_confmat)\n",
        "\n",
        "#matrice  de confusion\n",
        "print(df_confmat2)\n",
        "\n",
        "# Classification report\n",
        "print('classification report')\n",
        "print(classification_report(sY_real, sY_pred, target_names=target_state))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "146/146 [==============================] - 1s 5ms/step - loss: 1.4815 - accuracy: 0.3833\n",
            "Confusion Matrix\n",
            "Accuracy on the Test Images:  0.3833225667476654\n",
            "             SS        SN         N   NB        BB  Error\n",
            "SS     0.051780  0.000000  0.800971  0.0  0.147249    0.0\n",
            "SN     0.013834  0.023715  0.806324  0.0  0.156126    0.0\n",
            "N      0.007975  0.006135  0.712270  0.0  0.273620    0.0\n",
            "NB     0.007496  0.000000  0.659670  0.0  0.332834    0.0\n",
            "BB     0.004136  0.000000  0.521092  0.0  0.474773    0.0\n",
            "Error  0.000000  0.000000  0.454545  0.0  0.545455    0.0\n",
            "       SS  SN     N  NB   BB  Error\n",
            "SS     32   0   495   0   91      0\n",
            "SN      7  12   408   0   79      0\n",
            "N      13  10  1161   0  446      0\n",
            "NB      5   0   440   0  222      0\n",
            "BB      5   0   630   0  574      0\n",
            "Error   0   0     5   0    6      0\n",
            "classification report\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          SS       0.40      0.47      0.44      1209\n",
            "          SN       0.00      0.00      0.00        11\n",
            "           N       0.37      0.71      0.49      1630\n",
            "          NB       0.00      0.00      0.00       667\n",
            "          BB       0.55      0.02      0.05       506\n",
            "       Error       0.52      0.05      0.09       618\n",
            "\n",
            "    accuracy                           0.38      4641\n",
            "   macro avg       0.31      0.21      0.18      4641\n",
            "weighted avg       0.36      0.38      0.30      4641\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TIz78gePZY76"
      },
      "source": [
        "##Step 4: Guess future market state from random image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ulkwESoxZm1z"
      },
      "source": [
        "Take an image of an historical graph from a market webpage like investing.com and save it to the ImageM/ folder with name image1.PNG or you can change the value of image_path to the link you need.\n",
        "\n",
        "This execution tell us which market state in the future is the best representative."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yvqRijItZML8",
        "outputId": "c5063777-565a-44f7-82c8-2a53cb84637b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        }
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from keras.utils import np_utils\n",
        "from keras.models import load_model\n",
        "from keras import backend as K\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "#path for trained model\n",
        "trained_model_path='model/vggforsp500.h5'\n",
        "\n",
        "#path for the image taken by the user\n",
        "#image_path='ImageM/image1.PNG'\n",
        "image_path =input(\"Enter the path of the image to check next state:\\n\")\n",
        "\n",
        "#Load the image and resize it to 32x32 and taking off the transparency\n",
        "load_img_rz = np.array(Image.open(image_path).resize((32,32)))\n",
        "#Image.fromarray(load_img_rz).save('/content/drive/My Drive/_sample_data/ImageM/image1.PNG')\n",
        "image=load_img_rz[:,:,:3]/255\n",
        "print(\"After resizing:\",image.shape)\n",
        "\n",
        "#petite astuce pour ne pas avoir d erreur avec les types list, tensors,  nparray et dataframe\n",
        "doubleimage=np.array([image,image])\n",
        "############\n",
        "#recuperation of the model\n",
        "vggsp500model = load_model(trained_model_path)\n",
        "Y_pred = vggsp500model.predict(doubleimage)\n",
        "y_pred = np.argmax(Y_pred, axis=1)\n",
        "\n",
        "target_state = ['SS', 'SN', 'N','NB','BB','Error']\n",
        "df_result=pd.DataFrame((Y_pred))\n",
        "\n",
        "df_result.columns=target_state\n",
        "df_result.index=[image_path, image_path+'1']\n",
        "print (\"for \",image_path, \"the best result is \", target_state[int(y_pred[0])] )\n",
        "\n",
        "df_result"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Enter the path of the image to check next state:\n",
            "/content/DL_Tools_For_Finance/ImageM/image2.PNG\n",
            "After resizing: (32, 32, 3)\n",
            "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f3286689ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "for  /content/DL_Tools_For_Finance/ImageM/image2.PNG the best result is  SS\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>SS</th>\n",
              "      <th>SN</th>\n",
              "      <th>N</th>\n",
              "      <th>NB</th>\n",
              "      <th>BB</th>\n",
              "      <th>Error</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>/content/DL_Tools_For_Finance/ImageM/image2.PNG</th>\n",
              "      <td>0.31159</td>\n",
              "      <td>0.119561</td>\n",
              "      <td>0.263224</td>\n",
              "      <td>0.073631</td>\n",
              "      <td>0.231359</td>\n",
              "      <td>0.000636</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>/content/DL_Tools_For_Finance/ImageM/image2.PNG1</th>\n",
              "      <td>0.31159</td>\n",
              "      <td>0.119561</td>\n",
              "      <td>0.263224</td>\n",
              "      <td>0.073631</td>\n",
              "      <td>0.231359</td>\n",
              "      <td>0.000636</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                       SS  ...     Error\n",
              "/content/DL_Tools_For_Finance/ImageM/image2.PNG   0.31159  ...  0.000636\n",
              "/content/DL_Tools_For_Finance/ImageM/image2.PNG1  0.31159  ...  0.000636\n",
              "\n",
              "[2 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GBOG00tVbHS7",
        "outputId": "686ec072-2661-4b03-c1a8-d4ea42ed5233",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 285
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.imshow(image)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f007032f1d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gXmOXke1hlip"
      },
      "source": [
        "Give the results for all the PNG file of a directory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z0aDBnGTh6yi",
        "outputId": "2a0aa2d4-3708-496b-9b3a-ecfcab9b7387",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 662
        }
      },
      "source": [
        "import os\n",
        "\n",
        "path = '/content/DL_Tools_For_Finance/ImageM/'\n",
        "\n",
        "#files = os.listdir(path)\n",
        "#l_file=[]\n",
        "#for f in files:\n",
        "#    l_file.append(f)\n",
        "\n",
        "l_file=glob.glob('**/*.PNG', recursive=True)\n",
        "l_image_input_NN=[]\n",
        "\n",
        "for x_image in l_file:  \n",
        "  #Load the image and resize it to 32x32 and taking off the transparency\n",
        "  load_img_rz = np.array(Image.open(x_image).resize((32,32)))\n",
        "\n",
        "  #Image.fromarray(load_img_rz).save('/content/drive/My Drive/_sample_data/ImageM/image1.PNG')\n",
        "  image=load_img_rz[:,:,:3]/255\n",
        "  print(\"After resizing:\",image.shape)\n",
        "\n",
        "  l_image_input_NN.append(image)\n",
        "\n",
        "############\n",
        "#recuperation of the model\n",
        "vggsp500model = load_model(trained_model_path)\n",
        "Y_pred = vggsp500model.predict(np.array(l_image_input_NN))\n",
        "y_pred = np.argmax(Y_pred, axis=1)\n",
        "\n",
        "target_state = ['SS', 'SN', 'N','NB','BB','Error']\n",
        "df_result=pd.DataFrame((Y_pred))\n",
        "\n",
        "df_result.columns=target_state\n",
        "df_result.index=l_file\n",
        "\n",
        "df_decision=pd.DataFrame([target_state[i] for i in  y_pred],index=l_file, columns=['Decision'])\n",
        "df_result=pd.concat([df_result,df_decision],axis=1)\n",
        "(df_result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "After resizing: (32, 32, 3)\n",
            "After resizing: (32, 32, 3)\n",
            "After resizing: (32, 32, 3)\n",
            "After resizing: (32, 32, 3)\n",
            "After resizing: (32, 32, 3)\n",
            "After resizing: (32, 32, 3)\n",
            "After resizing: (32, 32, 3)\n",
            "After resizing: (32, 32, 3)\n",
            "After resizing: (32, 32, 3)\n",
            "After resizing: (32, 32, 3)\n",
            "After resizing: (32, 32, 3)\n",
            "After resizing: (32, 32, 3)\n",
            "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f32888d3488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>SS</th>\n",
              "      <th>SN</th>\n",
              "      <th>N</th>\n",
              "      <th>NB</th>\n",
              "      <th>BB</th>\n",
              "      <th>Error</th>\n",
              "      <th>Decision</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>ImageM/image3.PNG</th>\n",
              "      <td>0.301182</td>\n",
              "      <td>0.121881</td>\n",
              "      <td>0.264708</td>\n",
              "      <td>0.073682</td>\n",
              "      <td>0.237916</td>\n",
              "      <td>0.000632</td>\n",
              "      <td>SS</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ImageM/image2.PNG</th>\n",
              "      <td>0.311591</td>\n",
              "      <td>0.119561</td>\n",
              "      <td>0.263224</td>\n",
              "      <td>0.073630</td>\n",
              "      <td>0.231358</td>\n",
              "      <td>0.000636</td>\n",
              "      <td>SS</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ImageM/image1.PNG</th>\n",
              "      <td>0.209848</td>\n",
              "      <td>0.190190</td>\n",
              "      <td>0.379863</td>\n",
              "      <td>0.045567</td>\n",
              "      <td>0.174114</td>\n",
              "      <td>0.000418</td>\n",
              "      <td>N</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ImageM/image4.PNG</th>\n",
              "      <td>0.342570</td>\n",
              "      <td>0.142008</td>\n",
              "      <td>0.243726</td>\n",
              "      <td>0.072087</td>\n",
              "      <td>0.198942</td>\n",
              "      <td>0.000667</td>\n",
              "      <td>SS</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ImageM/image5.PNG</th>\n",
              "      <td>0.194147</td>\n",
              "      <td>0.321485</td>\n",
              "      <td>0.290173</td>\n",
              "      <td>0.048638</td>\n",
              "      <td>0.144945</td>\n",
              "      <td>0.000611</td>\n",
              "      <td>SN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ImageM/graphsp500tocrop.PNG</th>\n",
              "      <td>0.001264</td>\n",
              "      <td>0.057363</td>\n",
              "      <td>0.833274</td>\n",
              "      <td>0.032646</td>\n",
              "      <td>0.075410</td>\n",
              "      <td>0.000044</td>\n",
              "      <td>N</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ImageM/ImageM/image3.PNG</th>\n",
              "      <td>0.301182</td>\n",
              "      <td>0.121881</td>\n",
              "      <td>0.264708</td>\n",
              "      <td>0.073682</td>\n",
              "      <td>0.237916</td>\n",
              "      <td>0.000632</td>\n",
              "      <td>SS</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ImageM/ImageM/image2.PNG</th>\n",
              "      <td>0.311591</td>\n",
              "      <td>0.119561</td>\n",
              "      <td>0.263224</td>\n",
              "      <td>0.073630</td>\n",
              "      <td>0.231358</td>\n",
              "      <td>0.000636</td>\n",
              "      <td>SS</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ImageM/ImageM/image1.PNG</th>\n",
              "      <td>0.209848</td>\n",
              "      <td>0.190190</td>\n",
              "      <td>0.379863</td>\n",
              "      <td>0.045567</td>\n",
              "      <td>0.174114</td>\n",
              "      <td>0.000418</td>\n",
              "      <td>N</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ImageM/ImageM/image4.PNG</th>\n",
              "      <td>0.342570</td>\n",
              "      <td>0.142008</td>\n",
              "      <td>0.243726</td>\n",
              "      <td>0.072087</td>\n",
              "      <td>0.198942</td>\n",
              "      <td>0.000667</td>\n",
              "      <td>SS</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ImageM/ImageM/image5.PNG</th>\n",
              "      <td>0.194147</td>\n",
              "      <td>0.321485</td>\n",
              "      <td>0.290173</td>\n",
              "      <td>0.048638</td>\n",
              "      <td>0.144945</td>\n",
              "      <td>0.000611</td>\n",
              "      <td>SN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ImageM/ImageM/graphsp500tocrop.PNG</th>\n",
              "      <td>0.001264</td>\n",
              "      <td>0.057363</td>\n",
              "      <td>0.833274</td>\n",
              "      <td>0.032646</td>\n",
              "      <td>0.075410</td>\n",
              "      <td>0.000044</td>\n",
              "      <td>N</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                          SS        SN  ...     Error  Decision\n",
              "ImageM/image3.PNG                   0.301182  0.121881  ...  0.000632        SS\n",
              "ImageM/image2.PNG                   0.311591  0.119561  ...  0.000636        SS\n",
              "ImageM/image1.PNG                   0.209848  0.190190  ...  0.000418         N\n",
              "ImageM/image4.PNG                   0.342570  0.142008  ...  0.000667        SS\n",
              "ImageM/image5.PNG                   0.194147  0.321485  ...  0.000611        SN\n",
              "ImageM/graphsp500tocrop.PNG         0.001264  0.057363  ...  0.000044         N\n",
              "ImageM/ImageM/image3.PNG            0.301182  0.121881  ...  0.000632        SS\n",
              "ImageM/ImageM/image2.PNG            0.311591  0.119561  ...  0.000636        SS\n",
              "ImageM/ImageM/image1.PNG            0.209848  0.190190  ...  0.000418         N\n",
              "ImageM/ImageM/image4.PNG            0.342570  0.142008  ...  0.000667        SS\n",
              "ImageM/ImageM/image5.PNG            0.194147  0.321485  ...  0.000611        SN\n",
              "ImageM/ImageM/graphsp500tocrop.PNG  0.001264  0.057363  ...  0.000044         N\n",
              "\n",
              "[12 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w8Rqf47ewbmk",
        "outputId": "539f26c0-046f-495b-b3f6-b7e630e6ba9c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Enter the path of the image to check next state:/content/DL_Tools_For_Finance/ImageM/image2.PNG\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QeXi5PdYxJ3r",
        "outputId": "40bc8e51-f752-4868-d0a1-372f17cb57d9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "direction"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/DL_Tools_For_Finance/ImageM/image2.PNG'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uE2TBYhJxQK7"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}