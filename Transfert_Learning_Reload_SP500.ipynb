{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Transfert_Learning_Vgg16forSP500.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/imiled/DL_Tools_For_Finance/blob/master/Transfert_Learning_Reload_SP500.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QGkfWRfEIyvy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "66b54387-b8ba-4c1d-d461-ca5011e92580"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SmqFDg5Dxk7m",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "b663c387-80f5-44b2-da72-281b28bdaef4"
      },
      "source": [
        "%pwd"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J597ZlrUNtzb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "ce30630a-0ce9-4444-e2e4-bdd38ddc911d"
      },
      "source": [
        "%ls"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[0m\u001b[01;34mDL_Tools_For_Finance\u001b[0m/  \u001b[01;34mdrive\u001b[0m/  \u001b[01;34msample_data\u001b[0m/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cAHcIW__zC2D",
        "colab_type": "text"
      },
      "source": [
        "Downloading the project from github"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dWRUvmdLI64s",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "15e99794-75cb-4a8b-ec67-65212fad6529"
      },
      "source": [
        "!git clone https://github.com/imiled/DL_Tools_For_Finance.git\n",
        "#!git clone https://korakot:$password@github.com/korakot/myrepo\n",
        "\n",
        " "
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'DL_Tools_For_Finance'...\n",
            "remote: Enumerating objects: 887, done.\u001b[K\n",
            "remote: Total 887 (delta 0), reused 0 (delta 0), pack-reused 887\u001b[K\n",
            "Receiving objects: 100% (887/887), 206.84 MiB | 32.93 MiB/s, done.\n",
            "Resolving deltas: 100% (491/491), done.\n",
            "Checking out files: 100% (59/59), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1LgIdJgy1U3_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "a5308861-4c23-4bcc-e72a-d41461ca95cc"
      },
      "source": [
        "%cd /content/DL_Tools_For_Finance  \n",
        "!git init\n",
        "!git status\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/DL_Tools_For_Finance\n",
            "Reinitialized existing Git repository in /content/DL_Tools_For_Finance/.git/\n",
            "On branch master\n",
            "Your branch is up to date with 'origin/master'.\n",
            "\n",
            "nothing to commit, working tree clean\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QDgwDAvbxhHX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!git log"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n8y-NAfI2Bpp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!ls > hello.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F1se_dxs2DyU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "409c88d5-763e-4a5e-9488-5a01cf257a6a"
      },
      "source": [
        "!cat hello.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "datas\n",
            "hello.txt\n",
            "ImageM\n",
            "LICENSE\n",
            "MainNotebook\n",
            "MANIFEST.in\n",
            "model\n",
            "Notebooks\n",
            "README.md\n",
            "requirements.txt\n",
            "setup.cfg\n",
            "setup.py\n",
            "step1_generate_dataset_IndexImage.py\n",
            "step2_loadingtrainingdatas_vgg_transfert_modelandtraining.py\n",
            "step3_evaluate_vggsp500_model.py\n",
            "step4_guess_future_marketstate_from_image.py\n",
            "tests\n",
            "TFM Imiled 2019-2020_Deep Learning application to Finance.docx\n",
            "trainer\n",
            "Transfert_Learning_Vgg16forSP500.ipynb\n",
            "versioneer.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4_dnJgtZMHHv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329
        },
        "outputId": "8b32395b-b120-4dd2-a534-14042eb32c74"
      },
      "source": [
        "%cd /content/DL_Tools_For_Finance  \n",
        "!git add .\n",
        "!ls > hello.txt\n",
        "!git add hello.txt\n",
        "!git commit -m \"reorganisation of files\"\n",
        "!git push origin master          # push to github\n",
        "\n",
        "!git config user.email 'miledismael@gmail.com'\n",
        "!git config user.name 'imiled'\n",
        "\n",
        "username=input(\"what is you github username?\\n\")\n",
        "from getpass import getpass\n",
        "password = getpass('Password:')\n",
        "!git remote add origin https://$username:$password@github.com/$username/reponame.git\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/DL_Tools_For_Finance\n",
            "\n",
            "*** Please tell me who you are.\n",
            "\n",
            "Run\n",
            "\n",
            "  git config --global user.email \"you@example.com\"\n",
            "  git config --global user.name \"Your Name\"\n",
            "\n",
            "to set your account's default identity.\n",
            "Omit --global to set the identity only in this repository.\n",
            "\n",
            "fatal: unable to auto-detect email address (got 'root@4544a85cfe70.(none)')\n",
            "fatal: could not read Username for 'https://github.com': No such device or address\n",
            "what is you github username?\n",
            "imiled\n",
            "Password:··········\n",
            "fatal: remote origin already exists.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nr7TjeHXy80v",
        "colab_type": "text"
      },
      "source": [
        "Saving dataset from drive to local"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3TiNDoAbMJx2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "outputId": "2461143e-2974-4951-d714-b6fd52a07124"
      },
      "source": [
        "%cd /content/drive/My Drive/A_transfertTFMVggSP500\n",
        "%cp DatasetVggSP500.zip /content/DL_Tools_For_Finance/datas\n",
        "%cd /content/DL_Tools_For_Finance/datas\n",
        "!unzip DatasetVggSP500.zip"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/A_transfertTFMVggSP500\n",
            "/content/DL_Tools_For_Finance/datas\n",
            "Archive:  DatasetVggSP500.zip\n",
            "replace X_test_image.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: A\n",
            "  inflating: X_test_image.csv        \n",
            "  inflating: X_train_image.csv       \n",
            "  inflating: Y_test_FutPredict_image.csv  \n",
            "  inflating: Y_test_StateClass_image.csv  \n",
            "  inflating: Y_train_FutPredict_image.csv  \n",
            "  inflating: Y_train_StateClass_image.csv  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mq_ZXp4zzQ7l",
        "colab_type": "text"
      },
      "source": [
        "Saving latest model from drive to local"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iSxPytYKNjKH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "be7754fe-5473-4898-a83b-df7e6c673fe3"
      },
      "source": [
        "%cd /content/drive/My Drive/A_transfertTFMVggSP500 \n",
        "%cp model/final_modelcategorical_crossentropy_adam_32.h5 /content/DL_Tools_For_Finance/model\n",
        "%cp vggforsp500.h5 /content/DL_Tools_For_Finance/model"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Errno 2] No such file or directory: '/content/drive/My Drive/A_transfertTFMVggSP500'\n",
            "/content/DL_Tools_For_Finance/datas\n",
            "cp: cannot stat 'model/final_modelcategorical_crossentropy_adam_32.h5': No such file or directory\n",
            "cp: cannot stat 'vggforsp500.h5': No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VehZVTphSljL",
        "colab_type": "text"
      },
      "source": [
        "###Requirement instalation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EEpibDu_Ovpv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "a6126643-3642-4bb0-efd2-67ebb4044a4f"
      },
      "source": [
        "%cd /content/DL_Tools_For_Finance\n",
        "!pip3 install -r requirements.txt  "
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Errno 2] No such file or directory: '/content/DL_Tools_For_Finance'\n",
            "/content/drive/My Drive/A_transfertTFMVggSP500\n",
            "\u001b[31mERROR: Could not open requirements file: [Errno 2] No such file or directory: 'requirements.txt'\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dTwxPiI4zLz3",
        "colab_type": "text"
      },
      "source": [
        "##Part with all steps through python command##"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_SEuIXmtTG26",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "35fef0a0-efce-4166-8dd0-77856a90d251"
      },
      "source": [
        "%cd /content/DL_Tools_For_Finance\n",
        "!python3 step1_generate_dataset_IndexImage.py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Errno 2] No such file or directory: '/content/DL_Tools_For_Finance'\n",
            "/content\n",
            "python3: can't open file 'step1_generate_dataset_IndexImage.py': [Errno 2] No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jKbZEi02Ti5r",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "fd6b1d56-610a-4156-cc40-48381fc4c2b7"
      },
      "source": [
        "%cd /content/DL_Tools_For_Finance\n",
        "!python3 step2_loadingtrainingdatas_vgg_transfert_modelandtraining.py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/DL_Tools_For_Finance\n",
            "2020-08-31 14:00:07.536779: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-08-31 14:00:23.283485: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1\n",
            "2020-08-31 14:00:23.354418: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-08-31 14:00:23.355305: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \n",
            "pciBusID: 0000:00:04.0 name: Tesla T4 computeCapability: 7.5\n",
            "coreClock: 1.59GHz coreCount: 40 deviceMemorySize: 14.73GiB deviceMemoryBandwidth: 298.08GiB/s\n",
            "2020-08-31 14:00:23.355378: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-08-31 14:00:23.618262: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\n",
            "2020-08-31 14:00:23.772071: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10\n",
            "2020-08-31 14:00:23.799494: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10\n",
            "2020-08-31 14:00:24.091903: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10\n",
            "2020-08-31 14:00:24.121391: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10\n",
            "2020-08-31 14:00:24.631937: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7\n",
            "2020-08-31 14:00:24.632162: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-08-31 14:00:24.632916: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-08-31 14:00:24.633516: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\n",
            "2020-08-31 14:00:24.633867: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX512F\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2020-08-31 14:00:24.655218: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2000125000 Hz\n",
            "2020-08-31 14:00:24.655444: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x1b70f40 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
            "2020-08-31 14:00:24.655479: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
            "2020-08-31 14:00:24.794351: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-08-31 14:00:24.795101: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x1b70d80 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "2020-08-31 14:00:24.795132: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
            "2020-08-31 14:00:24.796328: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-08-31 14:00:24.796900: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \n",
            "pciBusID: 0000:00:04.0 name: Tesla T4 computeCapability: 7.5\n",
            "coreClock: 1.59GHz coreCount: 40 deviceMemorySize: 14.73GiB deviceMemoryBandwidth: 298.08GiB/s\n",
            "2020-08-31 14:00:24.796955: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-08-31 14:00:24.796995: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\n",
            "2020-08-31 14:00:24.797016: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10\n",
            "2020-08-31 14:00:24.797040: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10\n",
            "2020-08-31 14:00:24.797060: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10\n",
            "2020-08-31 14:00:24.797078: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10\n",
            "2020-08-31 14:00:24.797098: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7\n",
            "2020-08-31 14:00:24.797174: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-08-31 14:00:24.797845: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-08-31 14:00:24.798390: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\n",
            "2020-08-31 14:00:24.804305: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-08-31 14:00:28.654055: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2020-08-31 14:00:28.654111: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0 \n",
            "2020-08-31 14:00:28.654124: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N \n",
            "2020-08-31 14:00:28.661735: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-08-31 14:00:28.662361: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-08-31 14:00:28.662893: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2020-08-31 14:00:28.662938: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 13962 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "58892288/58889256 [==============================] - 1s 0us/step\n",
            "Model: \"vgg16\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 32, 32, 3)]       0         \n",
            "_________________________________________________________________\n",
            "block1_conv1 (Conv2D)        (None, 32, 32, 64)        1792      \n",
            "_________________________________________________________________\n",
            "block1_conv2 (Conv2D)        (None, 32, 32, 64)        36928     \n",
            "_________________________________________________________________\n",
            "block1_pool (MaxPooling2D)   (None, 16, 16, 64)        0         \n",
            "_________________________________________________________________\n",
            "block2_conv1 (Conv2D)        (None, 16, 16, 128)       73856     \n",
            "_________________________________________________________________\n",
            "block2_conv2 (Conv2D)        (None, 16, 16, 128)       147584    \n",
            "_________________________________________________________________\n",
            "block2_pool (MaxPooling2D)   (None, 8, 8, 128)         0         \n",
            "_________________________________________________________________\n",
            "block3_conv1 (Conv2D)        (None, 8, 8, 256)         295168    \n",
            "_________________________________________________________________\n",
            "block3_conv2 (Conv2D)        (None, 8, 8, 256)         590080    \n",
            "_________________________________________________________________\n",
            "block3_conv3 (Conv2D)        (None, 8, 8, 256)         590080    \n",
            "_________________________________________________________________\n",
            "block3_pool (MaxPooling2D)   (None, 4, 4, 256)         0         \n",
            "_________________________________________________________________\n",
            "block4_conv1 (Conv2D)        (None, 4, 4, 512)         1180160   \n",
            "_________________________________________________________________\n",
            "block4_conv2 (Conv2D)        (None, 4, 4, 512)         2359808   \n",
            "_________________________________________________________________\n",
            "block4_conv3 (Conv2D)        (None, 4, 4, 512)         2359808   \n",
            "_________________________________________________________________\n",
            "block4_pool (MaxPooling2D)   (None, 2, 2, 512)         0         \n",
            "_________________________________________________________________\n",
            "block5_conv1 (Conv2D)        (None, 2, 2, 512)         2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv2 (Conv2D)        (None, 2, 2, 512)         2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv3 (Conv2D)        (None, 2, 2, 512)         2359808   \n",
            "_________________________________________________________________\n",
            "block5_pool (MaxPooling2D)   (None, 1, 1, 512)         0         \n",
            "=================================================================\n",
            "Total params: 14,714,688\n",
            "Trainable params: 0\n",
            "Non-trainable params: 14,714,688\n",
            "_________________________________________________________________\n",
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
            "Epoch 1/50\n",
            "2020-08-31 14:00:30.921687: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\n",
            "2020-08-31 14:00:32.390571: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7\n",
            "149/149 [==============================] - ETA: 0s - loss: 2.0473 - accuracy: 0.3377\n",
            "Epoch 00001: loss improved from inf to 2.04734, saving model to model/best_modelcategorical_crossentropy_adam_Batch100_LR0.1_0.99.hdf5\n",
            "149/149 [==============================] - 3s 21ms/step - loss: 2.0473 - accuracy: 0.3377 - val_loss: 1.4949 - val_accuracy: 0.3399\n",
            "Epoch 2/50\n",
            "145/149 [============================>.] - ETA: 0s - loss: 1.5323 - accuracy: 0.3433\n",
            "Epoch 00002: loss improved from 2.04734 to 1.53141, saving model to model/best_modelcategorical_crossentropy_adam_Batch100_LR0.1_0.99.hdf5\n",
            "149/149 [==============================] - 3s 17ms/step - loss: 1.5314 - accuracy: 0.3436 - val_loss: 1.4906 - val_accuracy: 0.3399\n",
            "Epoch 3/50\n",
            "149/149 [==============================] - ETA: 0s - loss: 1.5308 - accuracy: 0.3436\n",
            "Epoch 00003: loss improved from 1.53141 to 1.53084, saving model to model/best_modelcategorical_crossentropy_adam_Batch100_LR0.1_0.99.hdf5\n",
            "149/149 [==============================] - 3s 17ms/step - loss: 1.5308 - accuracy: 0.3436 - val_loss: 1.4926 - val_accuracy: 0.3399\n",
            "Epoch 4/50\n",
            "149/149 [==============================] - ETA: 0s - loss: 1.5320 - accuracy: 0.3436\n",
            "Epoch 00004: loss did not improve from 1.53084\n",
            "149/149 [==============================] - 2s 16ms/step - loss: 1.5320 - accuracy: 0.3436 - val_loss: 1.4935 - val_accuracy: 0.3399\n",
            "Epoch 5/50\n",
            "145/149 [============================>.] - ETA: 0s - loss: 1.5321 - accuracy: 0.3389\n",
            "Epoch 00005: loss did not improve from 1.53084\n",
            "149/149 [==============================] - 2s 16ms/step - loss: 1.5314 - accuracy: 0.3394 - val_loss: 1.5005 - val_accuracy: 0.3399\n",
            "Epoch 6/50\n",
            "145/149 [============================>.] - ETA: 0s - loss: 1.5315 - accuracy: 0.3385\n",
            "Epoch 00006: loss did not improve from 1.53084\n",
            "149/149 [==============================] - 2s 16ms/step - loss: 1.5321 - accuracy: 0.3381 - val_loss: 1.5235 - val_accuracy: 0.3399\n",
            "Epoch 7/50\n",
            "149/149 [==============================] - ETA: 0s - loss: 1.5318 - accuracy: 0.3414\n",
            "Epoch 00007: loss did not improve from 1.53084\n",
            "149/149 [==============================] - 2s 16ms/step - loss: 1.5318 - accuracy: 0.3414 - val_loss: 1.5054 - val_accuracy: 0.3399\n",
            "Epoch 8/50\n",
            "149/149 [==============================] - ETA: 0s - loss: 1.5345 - accuracy: 0.3360\n",
            "Epoch 00008: loss did not improve from 1.53084\n",
            "149/149 [==============================] - 2s 17ms/step - loss: 1.5345 - accuracy: 0.3360 - val_loss: 1.4982 - val_accuracy: 0.3399\n",
            "Epoch 9/50\n",
            "145/149 [============================>.] - ETA: 0s - loss: 1.5337 - accuracy: 0.3428\n",
            "Epoch 00009: loss did not improve from 1.53084\n",
            "149/149 [==============================] - 2s 17ms/step - loss: 1.5329 - accuracy: 0.3436 - val_loss: 1.5099 - val_accuracy: 0.3399\n",
            "Epoch 10/50\n",
            "149/149 [==============================] - ETA: 0s - loss: 1.5336 - accuracy: 0.3358\n",
            "Epoch 00010: loss did not improve from 1.53084\n",
            "149/149 [==============================] - 2s 16ms/step - loss: 1.5336 - accuracy: 0.3358 - val_loss: 1.4912 - val_accuracy: 0.3399\n",
            "Epoch 11/50\n",
            "145/149 [============================>.] - ETA: 0s - loss: 1.5348 - accuracy: 0.3426\n",
            "Epoch 00011: loss did not improve from 1.53084\n",
            "149/149 [==============================] - 2s 16ms/step - loss: 1.5333 - accuracy: 0.3436 - val_loss: 1.4965 - val_accuracy: 0.3399\n",
            "Epoch 12/50\n",
            "145/149 [============================>.] - ETA: 0s - loss: 1.5330 - accuracy: 0.3417\n",
            "Epoch 00012: loss did not improve from 1.53084\n",
            "149/149 [==============================] - 2s 16ms/step - loss: 1.5326 - accuracy: 0.3422 - val_loss: 1.5040 - val_accuracy: 0.3399\n",
            "Epoch 13/50\n",
            "145/149 [============================>.] - ETA: 0s - loss: 1.5341 - accuracy: 0.3383\n",
            "Epoch 00013: loss did not improve from 1.53084\n",
            "149/149 [==============================] - 2s 16ms/step - loss: 1.5341 - accuracy: 0.3381 - val_loss: 1.4915 - val_accuracy: 0.3399\n",
            "Epoch 14/50\n",
            "145/149 [============================>.] - ETA: 0s - loss: 1.5342 - accuracy: 0.3352\n",
            "Epoch 00014: loss did not improve from 1.53084\n",
            "149/149 [==============================] - 2s 16ms/step - loss: 1.5330 - accuracy: 0.3366 - val_loss: 1.5078 - val_accuracy: 0.3399\n",
            "Epoch 15/50\n",
            "149/149 [==============================] - ETA: 0s - loss: 1.5326 - accuracy: 0.3412\n",
            "Epoch 00015: loss did not improve from 1.53084\n",
            "149/149 [==============================] - 2s 16ms/step - loss: 1.5326 - accuracy: 0.3412 - val_loss: 1.5212 - val_accuracy: 0.3399\n",
            "Epoch 16/50\n",
            "149/149 [==============================] - ETA: 0s - loss: 1.5344 - accuracy: 0.3402\n",
            "Epoch 00016: loss did not improve from 1.53084\n",
            "149/149 [==============================] - 2s 16ms/step - loss: 1.5344 - accuracy: 0.3402 - val_loss: 1.5016 - val_accuracy: 0.3399\n",
            "Epoch 17/50\n",
            "149/149 [==============================] - ETA: 0s - loss: 1.5320 - accuracy: 0.3436\n",
            "Epoch 00017: loss did not improve from 1.53084\n",
            "149/149 [==============================] - 2s 17ms/step - loss: 1.5320 - accuracy: 0.3436 - val_loss: 1.4908 - val_accuracy: 0.3399\n",
            "Epoch 18/50\n",
            "145/149 [============================>.] - ETA: 0s - loss: 1.5335 - accuracy: 0.3357\n",
            "Epoch 00018: loss did not improve from 1.53084\n",
            "149/149 [==============================] - 2s 16ms/step - loss: 1.5332 - accuracy: 0.3360 - val_loss: 1.5041 - val_accuracy: 0.3399\n",
            "Epoch 19/50\n",
            "149/149 [==============================] - ETA: 0s - loss: 1.5333 - accuracy: 0.3388\n",
            "Epoch 00019: loss did not improve from 1.53084\n",
            "149/149 [==============================] - 2s 16ms/step - loss: 1.5333 - accuracy: 0.3388 - val_loss: 1.4987 - val_accuracy: 0.3399\n",
            "Epoch 20/50\n",
            "145/149 [============================>.] - ETA: 0s - loss: 1.5306 - accuracy: 0.3443\n",
            "Epoch 00020: loss improved from 1.53084 to 1.53068, saving model to model/best_modelcategorical_crossentropy_adam_Batch100_LR0.1_0.99.hdf5\n",
            "149/149 [==============================] - 3s 17ms/step - loss: 1.5307 - accuracy: 0.3436 - val_loss: 1.4947 - val_accuracy: 0.3399\n",
            "Epoch 21/50\n",
            "149/149 [==============================] - ETA: 0s - loss: 1.5315 - accuracy: 0.3436\n",
            "Epoch 00021: loss did not improve from 1.53068\n",
            "149/149 [==============================] - 3s 17ms/step - loss: 1.5315 - accuracy: 0.3436 - val_loss: 1.4899 - val_accuracy: 0.3399\n",
            "Epoch 22/50\n",
            "145/149 [============================>.] - ETA: 0s - loss: 1.5312 - accuracy: 0.3430\n",
            "Epoch 00022: loss did not improve from 1.53068\n",
            "149/149 [==============================] - 2s 17ms/step - loss: 1.5323 - accuracy: 0.3421 - val_loss: 1.5026 - val_accuracy: 0.3399\n",
            "Epoch 23/50\n",
            "145/149 [============================>.] - ETA: 0s - loss: 1.5311 - accuracy: 0.3438\n",
            "Epoch 00023: loss did not improve from 1.53068\n",
            "149/149 [==============================] - 2s 17ms/step - loss: 1.5313 - accuracy: 0.3436 - val_loss: 1.5046 - val_accuracy: 0.3399\n",
            "Epoch 24/50\n",
            "149/149 [==============================] - ETA: 0s - loss: 1.5319 - accuracy: 0.3413\n",
            "Epoch 00024: loss did not improve from 1.53068\n",
            "149/149 [==============================] - 2s 17ms/step - loss: 1.5319 - accuracy: 0.3413 - val_loss: 1.5038 - val_accuracy: 0.3399\n",
            "Epoch 25/50\n",
            "145/149 [============================>.] - ETA: 0s - loss: 1.5322 - accuracy: 0.3435\n",
            "Epoch 00025: loss did not improve from 1.53068\n",
            "149/149 [==============================] - 2s 17ms/step - loss: 1.5322 - accuracy: 0.3436 - val_loss: 1.5049 - val_accuracy: 0.3399\n",
            "Epoch 26/50\n",
            "149/149 [==============================] - ETA: 0s - loss: 1.5319 - accuracy: 0.3436\n",
            "Epoch 00026: loss did not improve from 1.53068\n",
            "149/149 [==============================] - 2s 17ms/step - loss: 1.5319 - accuracy: 0.3436 - val_loss: 1.4944 - val_accuracy: 0.3399\n",
            "Epoch 27/50\n",
            "149/149 [==============================] - ETA: 0s - loss: 1.5332 - accuracy: 0.3436\n",
            "Epoch 00027: loss did not improve from 1.53068\n",
            "149/149 [==============================] - 2s 17ms/step - loss: 1.5332 - accuracy: 0.3436 - val_loss: 1.5045 - val_accuracy: 0.3399\n",
            "Epoch 28/50\n",
            "145/149 [============================>.] - ETA: 0s - loss: 1.5328 - accuracy: 0.3399\n",
            "Epoch 00028: loss did not improve from 1.53068\n",
            "149/149 [==============================] - 2s 17ms/step - loss: 1.5326 - accuracy: 0.3402 - val_loss: 1.4924 - val_accuracy: 0.3399\n",
            "Epoch 29/50\n",
            "149/149 [==============================] - ETA: 0s - loss: 1.5317 - accuracy: 0.3436\n",
            "Epoch 00029: loss did not improve from 1.53068\n",
            "149/149 [==============================] - 2s 17ms/step - loss: 1.5317 - accuracy: 0.3436 - val_loss: 1.4979 - val_accuracy: 0.3399\n",
            "Epoch 30/50\n",
            "145/149 [============================>.] - ETA: 0s - loss: 1.5314 - accuracy: 0.3441\n",
            "Epoch 00030: loss did not improve from 1.53068\n",
            "149/149 [==============================] - 2s 17ms/step - loss: 1.5317 - accuracy: 0.3436 - val_loss: 1.4978 - val_accuracy: 0.3399\n",
            "Epoch 31/50\n",
            "149/149 [==============================] - ETA: 0s - loss: 1.5328 - accuracy: 0.3383\n",
            "Epoch 00031: loss did not improve from 1.53068\n",
            "149/149 [==============================] - 2s 17ms/step - loss: 1.5328 - accuracy: 0.3383 - val_loss: 1.5141 - val_accuracy: 0.3399\n",
            "Epoch 32/50\n",
            "149/149 [==============================] - ETA: 0s - loss: 1.5315 - accuracy: 0.3410\n",
            "Epoch 00032: loss did not improve from 1.53068\n",
            "149/149 [==============================] - 2s 17ms/step - loss: 1.5315 - accuracy: 0.3410 - val_loss: 1.4968 - val_accuracy: 0.3399\n",
            "Epoch 33/50\n",
            "149/149 [==============================] - ETA: 0s - loss: 1.5342 - accuracy: 0.3372\n",
            "Epoch 00033: loss did not improve from 1.53068\n",
            "149/149 [==============================] - 2s 17ms/step - loss: 1.5342 - accuracy: 0.3372 - val_loss: 1.4964 - val_accuracy: 0.3399\n",
            "Epoch 34/50\n",
            "149/149 [==============================] - ETA: 0s - loss: 1.5337 - accuracy: 0.3420\n",
            "Epoch 00034: loss did not improve from 1.53068\n",
            "149/149 [==============================] - 3s 17ms/step - loss: 1.5337 - accuracy: 0.3420 - val_loss: 1.4939 - val_accuracy: 0.3399\n",
            "Epoch 35/50\n",
            "149/149 [==============================] - ETA: 0s - loss: 1.5328 - accuracy: 0.3436\n",
            "Epoch 00035: loss did not improve from 1.53068\n",
            "149/149 [==============================] - 2s 17ms/step - loss: 1.5328 - accuracy: 0.3436 - val_loss: 1.4953 - val_accuracy: 0.3399\n",
            "Epoch 36/50\n",
            "145/149 [============================>.] - ETA: 0s - loss: 1.5336 - accuracy: 0.3441\n",
            "Epoch 00036: loss did not improve from 1.53068\n",
            "149/149 [==============================] - 3s 17ms/step - loss: 1.5341 - accuracy: 0.3436 - val_loss: 1.5010 - val_accuracy: 0.3399\n",
            "Epoch 37/50\n",
            "149/149 [==============================] - ETA: 0s - loss: 1.5328 - accuracy: 0.3399\n",
            "Epoch 00037: loss did not improve from 1.53068\n",
            "149/149 [==============================] - 3s 17ms/step - loss: 1.5328 - accuracy: 0.3399 - val_loss: 1.4952 - val_accuracy: 0.3399\n",
            "Epoch 38/50\n",
            "149/149 [==============================] - ETA: 0s - loss: 1.5319 - accuracy: 0.3415\n",
            "Epoch 00038: loss did not improve from 1.53068\n",
            "149/149 [==============================] - 2s 17ms/step - loss: 1.5319 - accuracy: 0.3415 - val_loss: 1.5012 - val_accuracy: 0.3399\n",
            "Epoch 39/50\n",
            "149/149 [==============================] - ETA: 0s - loss: 1.5335 - accuracy: 0.3417\n",
            "Epoch 00039: loss did not improve from 1.53068\n",
            "149/149 [==============================] - 3s 17ms/step - loss: 1.5335 - accuracy: 0.3417 - val_loss: 1.4931 - val_accuracy: 0.3399\n",
            "Epoch 40/50\n",
            "149/149 [==============================] - ETA: 0s - loss: 1.5328 - accuracy: 0.3427\n",
            "Epoch 00040: loss did not improve from 1.53068\n",
            "149/149 [==============================] - 3s 17ms/step - loss: 1.5328 - accuracy: 0.3427 - val_loss: 1.5152 - val_accuracy: 0.3399\n",
            "Epoch 41/50\n",
            "149/149 [==============================] - ETA: 0s - loss: 1.5332 - accuracy: 0.3414\n",
            "Epoch 00041: loss did not improve from 1.53068\n",
            "149/149 [==============================] - 3s 17ms/step - loss: 1.5332 - accuracy: 0.3414 - val_loss: 1.4931 - val_accuracy: 0.3399\n",
            "Epoch 42/50\n",
            "149/149 [==============================] - ETA: 0s - loss: 1.5312 - accuracy: 0.3436\n",
            "Epoch 00042: loss did not improve from 1.53068\n",
            "149/149 [==============================] - 3s 17ms/step - loss: 1.5312 - accuracy: 0.3436 - val_loss: 1.5019 - val_accuracy: 0.3399\n",
            "Epoch 43/50\n",
            "149/149 [==============================] - ETA: 0s - loss: 1.5320 - accuracy: 0.3397\n",
            "Epoch 00043: loss did not improve from 1.53068\n",
            "149/149 [==============================] - 3s 17ms/step - loss: 1.5320 - accuracy: 0.3397 - val_loss: 1.5109 - val_accuracy: 0.3399\n",
            "Epoch 44/50\n",
            "149/149 [==============================] - ETA: 0s - loss: 1.5329 - accuracy: 0.3436\n",
            "Epoch 00044: loss did not improve from 1.53068\n",
            "149/149 [==============================] - 3s 17ms/step - loss: 1.5329 - accuracy: 0.3436 - val_loss: 1.5056 - val_accuracy: 0.3399\n",
            "Epoch 45/50\n",
            "149/149 [==============================] - ETA: 0s - loss: 1.5321 - accuracy: 0.3436\n",
            "Epoch 00045: loss did not improve from 1.53068\n",
            "149/149 [==============================] - 3s 17ms/step - loss: 1.5321 - accuracy: 0.3436 - val_loss: 1.5010 - val_accuracy: 0.3399\n",
            "Epoch 46/50\n",
            "149/149 [==============================] - ETA: 0s - loss: 1.5316 - accuracy: 0.3409\n",
            "Epoch 00046: loss did not improve from 1.53068\n",
            "149/149 [==============================] - 3s 17ms/step - loss: 1.5316 - accuracy: 0.3409 - val_loss: 1.4912 - val_accuracy: 0.3399\n",
            "Epoch 47/50\n",
            "149/149 [==============================] - ETA: 0s - loss: 1.5313 - accuracy: 0.3436\n",
            "Epoch 00047: loss did not improve from 1.53068\n",
            "149/149 [==============================] - 3s 17ms/step - loss: 1.5313 - accuracy: 0.3436 - val_loss: 1.4899 - val_accuracy: 0.3399\n",
            "Epoch 48/50\n",
            "145/149 [============================>.] - ETA: 0s - loss: 1.5329 - accuracy: 0.3438\n",
            "Epoch 00048: loss did not improve from 1.53068\n",
            "149/149 [==============================] - 3s 17ms/step - loss: 1.5331 - accuracy: 0.3436 - val_loss: 1.5025 - val_accuracy: 0.3399\n",
            "Epoch 49/50\n",
            "145/149 [============================>.] - ETA: 0s - loss: 1.5322 - accuracy: 0.3412\n",
            "Epoch 00049: loss did not improve from 1.53068\n",
            "149/149 [==============================] - 3s 17ms/step - loss: 1.5326 - accuracy: 0.3408 - val_loss: 1.5109 - val_accuracy: 0.3399\n",
            "Epoch 50/50\n",
            "149/149 [==============================] - ETA: 0s - loss: 1.5326 - accuracy: 0.3436\n",
            "Epoch 00050: loss did not improve from 1.53068\n",
            "149/149 [==============================] - 3s 17ms/step - loss: 1.5326 - accuracy: 0.3436 - val_loss: 1.4915 - val_accuracy: 0.3399\n",
            "Traceback (most recent call last):\n",
            "  File \"step2_loadingtrainingdatas_vgg_transfert_modelandtraining.py\", line 160, in <module>\n",
            "    transfer_model.save(\"model/\"+datetime.now().strftime(\"%Y%m%d-%H%M%S\")+\"vggforsp500.h5\")\n",
            "NameError: name 'datetime' is not defined\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CjSc-KxlVc0L",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b639276e-8b87-4388-99ea-a75aba35a86f"
      },
      "source": [
        "%cd /content/DL_Tools_For_Finance\n",
        "!python3 step3_evaluate_vggsp500_model.py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/DL_Tools_For_Finance\n",
            "2020-08-31 14:02:47.233647: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-08-31 14:02:51.669914: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1\n",
            "2020-08-31 14:02:51.704030: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-08-31 14:02:51.704632: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \n",
            "pciBusID: 0000:00:04.0 name: Tesla T4 computeCapability: 7.5\n",
            "coreClock: 1.59GHz coreCount: 40 deviceMemorySize: 14.73GiB deviceMemoryBandwidth: 298.08GiB/s\n",
            "2020-08-31 14:02:51.704688: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-08-31 14:02:51.706171: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\n",
            "2020-08-31 14:02:51.707708: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10\n",
            "2020-08-31 14:02:51.708035: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10\n",
            "2020-08-31 14:02:51.709428: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10\n",
            "2020-08-31 14:02:51.710185: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10\n",
            "2020-08-31 14:02:51.713048: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7\n",
            "2020-08-31 14:02:51.713172: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-08-31 14:02:51.713766: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-08-31 14:02:51.714261: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\n",
            "2020-08-31 14:02:51.714718: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX512F\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2020-08-31 14:02:51.719389: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2000125000 Hz\n",
            "2020-08-31 14:02:51.719605: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x28f6bc0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
            "2020-08-31 14:02:51.719633: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
            "2020-08-31 14:02:51.807725: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-08-31 14:02:51.808365: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x28f6a00 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "2020-08-31 14:02:51.808401: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
            "2020-08-31 14:02:51.808620: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-08-31 14:02:51.809133: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \n",
            "pciBusID: 0000:00:04.0 name: Tesla T4 computeCapability: 7.5\n",
            "coreClock: 1.59GHz coreCount: 40 deviceMemorySize: 14.73GiB deviceMemoryBandwidth: 298.08GiB/s\n",
            "2020-08-31 14:02:51.809188: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-08-31 14:02:51.809222: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\n",
            "2020-08-31 14:02:51.809244: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10\n",
            "2020-08-31 14:02:51.809264: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10\n",
            "2020-08-31 14:02:51.809287: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10\n",
            "2020-08-31 14:02:51.809306: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10\n",
            "2020-08-31 14:02:51.809326: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7\n",
            "2020-08-31 14:02:51.809405: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-08-31 14:02:51.810022: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-08-31 14:02:51.810533: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\n",
            "2020-08-31 14:02:51.810597: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-08-31 14:02:52.339278: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2020-08-31 14:02:52.339335: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0 \n",
            "2020-08-31 14:02:52.339348: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N \n",
            "2020-08-31 14:02:52.339561: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-08-31 14:02:52.340218: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-08-31 14:02:52.340755: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2020-08-31 14:02:52.340796: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 13962 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)\n",
            "2020-08-31 14:02:52.849852: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\n",
            "2020-08-31 14:02:53.190922: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7\n",
            "146/146 [==============================] - 1s 9ms/step - loss: 2.0681 - accuracy: 0.2512\n",
            "Confusion Matrix\n",
            "Accuracy on the Test Images:  0.25123897194862366\n",
            "             SS  ...  Error\n",
            "SS     0.152104  ...    0.0\n",
            "SN     0.175889  ...    0.0\n",
            "N      0.157669  ...    0.0\n",
            "NB     0.157421  ...    0.0\n",
            "BB     0.150538  ...    0.0\n",
            "Error  0.000000  ...    0.0\n",
            "\n",
            "[6 rows x 6 columns]\n",
            "classification report\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          SS       0.26      0.47      0.33      1209\n",
            "          SN       0.00      0.00      0.00        11\n",
            "           N       0.36      0.26      0.30      1630\n",
            "          NB       0.14      0.10      0.12       667\n",
            "          BB       0.52      0.02      0.05       506\n",
            "       Error       0.13      0.15      0.14       618\n",
            "\n",
            "    accuracy                           0.25      4641\n",
            "   macro avg       0.23      0.17      0.16      4641\n",
            "weighted avg       0.29      0.25      0.23      4641\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MfSMBNcGxMF2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 988
        },
        "outputId": "c3008cb3-6bfe-4259-ce16-c9adcf367400"
      },
      "source": [
        "%cd /content/DL_Tools_For_Finance\n",
        "!python3 step4_guess_future_marketstate_from_image.py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/DL_Tools_For_Finance\n",
            "2020-08-31 14:03:08.385690: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "After resizing: (32, 32, 3)\n",
            "2020-08-31 14:03:09.535186: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1\n",
            "2020-08-31 14:03:09.574599: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-08-31 14:03:09.575181: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \n",
            "pciBusID: 0000:00:04.0 name: Tesla T4 computeCapability: 7.5\n",
            "coreClock: 1.59GHz coreCount: 40 deviceMemorySize: 14.73GiB deviceMemoryBandwidth: 298.08GiB/s\n",
            "2020-08-31 14:03:09.575244: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-08-31 14:03:09.576673: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\n",
            "2020-08-31 14:03:09.585531: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10\n",
            "2020-08-31 14:03:09.585899: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10\n",
            "2020-08-31 14:03:09.591997: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10\n",
            "2020-08-31 14:03:09.592918: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10\n",
            "2020-08-31 14:03:09.602832: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7\n",
            "2020-08-31 14:03:09.602955: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-08-31 14:03:09.603582: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-08-31 14:03:09.604090: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\n",
            "2020-08-31 14:03:09.604397: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX512F\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2020-08-31 14:03:09.610479: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2000125000 Hz\n",
            "2020-08-31 14:03:09.610705: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x1354bc0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
            "2020-08-31 14:03:09.610739: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
            "2020-08-31 14:03:09.722558: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-08-31 14:03:09.723230: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x1354d80 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "2020-08-31 14:03:09.723264: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
            "2020-08-31 14:03:09.723464: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-08-31 14:03:09.724073: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \n",
            "pciBusID: 0000:00:04.0 name: Tesla T4 computeCapability: 7.5\n",
            "coreClock: 1.59GHz coreCount: 40 deviceMemorySize: 14.73GiB deviceMemoryBandwidth: 298.08GiB/s\n",
            "2020-08-31 14:03:09.724132: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-08-31 14:03:09.724172: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\n",
            "2020-08-31 14:03:09.724199: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10\n",
            "2020-08-31 14:03:09.724224: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10\n",
            "2020-08-31 14:03:09.724253: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10\n",
            "2020-08-31 14:03:09.724275: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10\n",
            "2020-08-31 14:03:09.724305: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7\n",
            "2020-08-31 14:03:09.724386: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-08-31 14:03:09.725011: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-08-31 14:03:09.725535: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\n",
            "2020-08-31 14:03:09.725602: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-08-31 14:03:10.322458: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2020-08-31 14:03:10.322529: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0 \n",
            "2020-08-31 14:03:10.322543: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N \n",
            "2020-08-31 14:03:10.322768: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-08-31 14:03:10.323702: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-08-31 14:03:10.324428: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2020-08-31 14:03:10.324484: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 13962 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)\n",
            "2020-08-31 14:03:10.764422: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\n",
            "2020-08-31 14:03:11.077927: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7\n",
            "for  ImageM/image1.PNG the best result is  N\n",
            "                          SS  ...     Error\n",
            "ImageM/image1.PNG   0.209848  ...  0.000418\n",
            "ImageM/image1.PNG1  0.209848  ...  0.000418\n",
            "\n",
            "[2 rows x 6 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1n0vJA_lL-Xv",
        "colab_type": "text"
      },
      "source": [
        "##Step 1 : Generate Dataset of the Image and the Future maket state"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7jbsgi1Qa6Rg",
        "colab_type": "text"
      },
      "source": [
        "In this part we are generating the training and testing dataset.\n",
        "First we download the historical prices of the sp500 from 1927 to 31 July 2020 and built the image of 15 days historical graph also we get the 5 days future price evolution of the sp500. \n",
        "From the future price evolution, we calculate a future state which can be splitted in 6 classes :\n",
        "\n",
        "Sell-Sell | Sell- Neutral | Neutral | Neutral -Buy | Buy -Buy (and the Error class)\n",
        "\n",
        "The objective is to get the following files which represent a dataframe in the data/ repertory:\n",
        "\n",
        "X_train_image.csv , X_test_image.csv a 3072 column time serie dataframe  of the image (32 x 32 x3) of the sp500 closing price \n",
        "\n",
        "Y_test_StateClass.csv, Y_train_StateClass.csv a 1 column time serie dataframe of the future state value betwwen -1 to 4\n",
        "\n",
        "We generate also the following files but we won´t use it in this project - more fore RNN & price prediction - Y_test_FutPredict.csv Y_train_FutPredict.csv\n",
        "\n",
        "the testing and training time serie dataset are shuffled by the date of reference with a split number of 0.8\n",
        "\n",
        "Please note that: \n",
        "1. We can increase the dataset taking into account the evolution very liquid stocks or other indices as long as we have very high the liquidity and number of participants \n",
        "2. The calculation of the dataset can take more than 6 hours of calulation as the code is not optimized so far, we can quickly implement parallel computing and rapid image setup instead of using matplotlib library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_7hN1p8cJY_3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import bs4 as bs\n",
        "import requests\n",
        "import yfinance as yf\n",
        "import fix_yahoo_finance as yf\n",
        "import datetime\n",
        "import io\n",
        "import cv2\n",
        "import skimage\n",
        "import datetime\n",
        "from PIL import Image\n",
        "from pandas_datareader import data as pdr\n",
        "from skimage import measure\n",
        "from skimage.measure import block_reduce\n",
        "from datetime import datetime\n",
        "\n",
        "'''\n",
        "Functions to be used for data generation \n",
        "'''\n",
        "\n",
        "def get_img_from_fig(fig, dpi=180):\n",
        "   # get_img_from_fig is function which returns an image as numpy array from figure\n",
        "    buf = io.BytesIO()\n",
        "    fig.savefig(buf, format=\"png\", dpi=dpi)\n",
        "    buf.seek(0)\n",
        "    img_arr = np.frombuffer(buf.getvalue(), dtype=np.uint8)\n",
        "    buf.close()\n",
        "    img = cv2.imdecode(img_arr, 1)\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    return img\n",
        "\n",
        "def build_image(stockindex, idate=10, pastlag=10, futlag=3,nb_dates=1000):\n",
        "  # Build image from a table stockindex price list \n",
        "  #return a (32,32,3) np.array representing the image in color\n",
        "  #ising idate as a starting point\n",
        "  #paslag futlag number of days to consider for translate\n",
        "  sp500close=stockindex\n",
        "  nb_days=nb_dates\n",
        "\n",
        "  x_datas=[]\n",
        "  x_datas=np.zeros((32,32,3))\n",
        "  i=idate\n",
        "  \n",
        "  fig=plt.figure()\n",
        "  ax=fig.add_subplot(111)\n",
        "  ax.plot(sp500close[(i-pastlag):i])\n",
        "  plot_img_np = get_img_from_fig(fig)\n",
        "  x_tmp= skimage.measure.block_reduce(plot_img_np[90:620,140:970], (18,28,1), np.mean)\n",
        "  (x_datas[1:-1])[:,1:-1][:]=x_tmp\n",
        "  fig.clear()\n",
        "  plt.close(fig)\n",
        "    \n",
        "  x_datas=x_datas/255\n",
        "  return x_datas\n",
        "  \n",
        "'''\n",
        "MAIN FUNCTION OF CLASSIFICATION \n",
        "build y state y fut \n",
        "and x  \n",
        "'''\n",
        "def class_shortterm_returnfut(x, yfut, indexforpast,tpastlag):\n",
        "  '''\n",
        "  #this function is use to classifiy the future state based on the position of future value with the past range \n",
        "  #Put the value from the 2 boxes (max min) or (open close) on the time range  and check if it is within\n",
        "  #go down go up or exit the box\n",
        "  #the fucntion return 5 state depending on the future value position on the boxes and one state for error cases\n",
        "  '''\n",
        "\n",
        "  xpast_min=np.min(x[(indexforpast-tpastlag):indexforpast])\n",
        "  xpast_max=np.max(x[(indexforpast-tpastlag):indexforpast])\n",
        "  x_open=x[int(indexforpast-tpastlag)]\n",
        "  x_close=x[indexforpast]\n",
        "  \n",
        "  if (yfut < xpast_min ): return 0\n",
        "  elif  (yfut < min(x_open,x_close)): return 1\n",
        "  elif  (yfut < max(x_open,x_close)): return 2\n",
        "  elif  (yfut < xpast_max): return 3\n",
        "  elif  (yfut > xpast_max): return 4\n",
        "  else  : return -1\n",
        "\n",
        "def main_class_shortterm_returnfut(iterable):\n",
        "  return class_shortterm_returnfut(sp500close, iterable, pastlag,futlag)\n",
        "\n",
        "def normalise_df_image(xdf):\n",
        "  #normalisation to 0,1 range of the equity index\n",
        "  df_tmp=xdf\n",
        "  maxval=np.max(df_tmp)\n",
        "  df_tmp=df_tmp/maxval\n",
        "  return df_tmp, maxval\n",
        "\n",
        "def build_image(stockindex, idate=10, pastlag=10, futlag=3):\n",
        "  #another version of returning image from a data frame index\n",
        "  #using the pastlag as range for the graph\n",
        "  #ising idate as a starting point\n",
        "  #return a (32,32,3) np array\n",
        "\n",
        "  #number of days to consider for translate\n",
        "  sp500close=stockindex\n",
        "  x_datas=[]\n",
        "  x_datas=np.zeros((32,32,3))\n",
        "  i=idate\n",
        "  \n",
        "  fig=plt.figure()\n",
        "  ax=fig.add_subplot(111)\n",
        "  ax.plot(sp500close[(i-pastlag):i])\n",
        "  plot_img_np = get_img_from_fig(fig)\n",
        "  x_tmp= skimage.measure.block_reduce(plot_img_np[90:620,140:970], (18,28,1), np.mean)\n",
        "  (x_datas[1:-1])[:,1:-1][:]=x_tmp\n",
        "  fig.clear()\n",
        "  plt.close(fig)\n",
        "    \n",
        "  x_datas=x_datas/255\n",
        "  return x_datas\n",
        "\n",
        "def build_image_optimfig(fig, stockindex, idate=10, pastlag=10, futlag=3):\n",
        "  '''\n",
        "  #version of returning image from a data frame index\n",
        "  #using the pastlag as range for the graph\n",
        "  #ising idate as a starting point\n",
        "  #return a (32,32,3) np array\n",
        "  #this one is optimisng the use of ram \n",
        "  '''\n",
        "\n",
        "  #number of days to consider for translate\n",
        "  sp500close=stockindex\n",
        "  x_datas=[]\n",
        "  x_datas=np.zeros((32,32,3))\n",
        "  i=idate\n",
        "  \n",
        "  plt.plot(sp500close[(i-pastlag):i])\n",
        "  plot_img_np = get_img_from_fig(fig)\n",
        "  x_tmp= skimage.measure.block_reduce(plot_img_np[90:620,140:970], (18,28,1), np.mean)\n",
        "  (x_datas[1:-1])[:,1:-1][:]=x_tmp\n",
        "    \n",
        "  x_datas=x_datas/255\n",
        "  return x_datas     \n",
        "\n",
        "def build_image_df(xdf, past_step,fut_step) :\n",
        "  '''\n",
        "  returning a dictionary of time series dataframes to be used in setup_input_NN_image so a to generate \n",
        "  Input X Result Y_StateClass, Y_FutPredict\n",
        "  pastlag as range for the graph\n",
        "  fut _step the future value lag in time to predict or to check the financial state of the market \n",
        "  #times series to get information from the stock index value\n",
        "  'stock_value':the time serie of the index normalised on the whole period\n",
        "  'moving_average':  time serie of the rolling moving average value of the index for past step image\n",
        "  \"max\": time serie of the rolling max  value of the index for past step image\n",
        "  \"min\": time serie of the rolling  min value of the index for past step image\n",
        "  'volatility':  time serie of the rolling  vol value of the index for past step image\n",
        "          \n",
        "  'df_x_image': is a time series of flattened (1, ) calculed from images (32, 32, 3) list \n",
        "  #I had to flatten it because panda does not create table with this format\n",
        "  'market_state': future markket state to be predicted time lag is futlag\n",
        "  'future_value': future value of stock price to predict  time lag is futlag\n",
        "  'future_volatility':  time serie of the future volatility of the index time lag is futlag\n",
        "  '''\n",
        "\n",
        "  df_stockvaluecorrected=xdf\n",
        "  df_stockvaluecorrected, _ = normalise_df_image(df_stockvaluecorrected)\n",
        "  df_pctchge = df_stockvaluecorrected.pct_change(periods=past_step)\n",
        "  df_movave = df_stockvaluecorrected.rolling(window=past_step).mean()\n",
        "  df_volaty = np.sqrt(252)*df_pctchge.rolling(window=past_step).std()\n",
        "  df_max =df_stockvaluecorrected.rolling(window=past_step).max()\n",
        "  df_min =df_stockvaluecorrected.rolling(window=past_step).min()\n",
        "  df_Fut_value =df_stockvaluecorrected.shift(periods=-fut_step)\n",
        "  df_Fut_value.name='future_value'\n",
        "  df_Fut_volaty =df_volaty.shift(periods=-fut_step)\n",
        "  \n",
        "  df_market_state=pd.DataFrame(index=df_stockvaluecorrected.index,columns=['market_state'],dtype=np.float64)\n",
        "  \n",
        "  tmpimage=build_image(df_stockvaluecorrected,past_step+1,pastlag=past_step,futlag=fut_step)\n",
        "  flatten_image=np.reshape(tmpimage,(1,-1))\n",
        "  colname_d_x_image_flattened = ['Image Col'+str(j) for j in range(flatten_image.shape[1])]\n",
        "\n",
        "  np_x_image=np.zeros((len(df_stockvaluecorrected.index),flatten_image.shape[1]))\n",
        "  \n",
        "  for i in range(len(df_stockvaluecorrected.index)):\n",
        "        yfut=df_Fut_value.iloc[i]\n",
        "        df_market_state.iloc[i]=class_shortterm_returnfut(df_stockvaluecorrected,yfut, i,tpastlag=past_step)\n",
        "        print(\"loop 1 market state :\", \"step \",i,\"market state fut\", df_market_state.iloc[i],\" future value\",df_Fut_value.iloc[i] )\n",
        "  df_market_state.index=df_Fut_value.index\n",
        "\n",
        "  fig=plt.figure()\n",
        "  '''\n",
        "  for i in range(len(df_stockvaluecorrected.index)):\n",
        "        try:\n",
        "          tmpimage=build_image_optimfig(fig, df_stockvaluecorrected,i,pastlag=past_step,futlag=fut_step)\n",
        "          np_x_image[i,:]=np.reshape(tmpimage,(1,-1))\n",
        "          print(\"loop 2 image :\", \"step \",i,\"market state fut\", df_market_state.iloc[i],\" future value\",df_Fut_value.iloc[i] )\n",
        "        except:\n",
        "           print(\"error at index\", i)\n",
        "  '''           \n",
        "  \n",
        "  def build_image_optimfig_simplified(i_index):\n",
        "    return build_image_optimfig(fig, df_stockvaluecorrected,i_index,pastlag=past_step,futlag=fut_step)\n",
        "\n",
        "  def quick_build_image_from_index(indexstart, index_end, np_x_image):\n",
        "        if (indexstart==index_end):\n",
        "            tmpimage=build_image_optimfig_simplified(indexstart)\n",
        "            np_x_image[indexstart,:]=np.reshape(tmpimage,(1,-1))\n",
        "            print(\"loop 2 image :\", \"step \",indexstart)\n",
        "        else :\n",
        "            i_split=indexstart+(index_end-indexstart)//2\n",
        "            quick_build_image_from_index(indexstart, i_split,np_x_image)\n",
        "            quick_build_image_from_index(i_split+1, index_end,np_x_image)\n",
        "\n",
        "  quick_build_image_from_index(0, len(df_stockvaluecorrected.index)-1, np_x_image)\n",
        "\n",
        "  df_x_image=pd.DataFrame(data=np_x_image,columns=colname_d_x_image_flattened, index=df_stockvaluecorrected.index)\n",
        "  fig.clear\n",
        "  plt.close(fig)\n",
        "\n",
        "\n",
        "  df_data= {\n",
        "          'stock_value': df_stockvaluecorrected, \n",
        "          'moving_average': df_movave, \n",
        "          \"max\": df_max, \n",
        "          \"min\": df_max,\n",
        "          'volatility': df_volaty,\n",
        "          'future_volatility': df_Fut_volaty,\n",
        "          \n",
        "          'df_x_image':df_x_image,\n",
        "          'market_state':df_market_state,\n",
        "          'future_value': df_Fut_value,\n",
        "\n",
        "          }\n",
        "\n",
        "  return df_data\n",
        "\n",
        "def build_image_clean(stockindex_ohlcv, ret_image_size=(32,32,3), idate=10, pastlag=32):\n",
        "  '''\n",
        "  TO BE COMPLETED\n",
        "  NOT USED NOW\n",
        "  \n",
        "  change one date into an array (32,32,3)\n",
        "  Each absciss pixel is one day\n",
        "  in ordinate the min value of ohlc shall be 0 (volume is tabled on the third image) \n",
        "  in ordinate the max value of ohlc shall be  (volume is tabled on the third image) \n",
        "  1st image: 32 x32\n",
        "    based on each day we place the open and close point\n",
        "    in ordinate int (255 * price /max ohlc)\n",
        "    with value of  255 for close and 127 for open\n",
        "  2nd image: 32 x32\n",
        "    based on each day we place the high low point \n",
        "    in ordinate int (255 * price /max ohlc)\n",
        "    with 64 for high and 32 for low\n",
        "  3rd image: 32 x32\n",
        "    each column value is a equal to int 255* volume of day / volume max period)\n",
        "  '''\n",
        "  #number of days to consider for translate\n",
        "  tsindexstock=stockindex_ohlcv.iloc[(idate-pastlag):idate]\n",
        "  valmax=np.max(np.array(tsindexstock[tsindexstock.columns[:-1]]))\n",
        "  valmin=np.min(np.array(tsindexstock[tsindexstock.columns[:-1]]))\n",
        "  vol=tsindexstock[tsindexstock.columns[-1]]\n",
        "  \n",
        "  x_datas=np.zeros(ret_image_size)\n",
        "  \n",
        "  return x_datas\n",
        "  \n",
        "def setup_input_NN_image(xdf, past_step=25,fut_step=5, split=0.8):\n",
        "  '''\n",
        "  this function the time serie of the index price \n",
        "  and generate the random dataset with split value from the whole time serie\n",
        "  X is a time serie of the flattened 32, 32 ,3 image list\n",
        "  Y_StateClass is a time serie of future state to predict with a classification made with class_shortterm_returnfut\n",
        "  Y_FutPredict is the time serie of stocke index shifted in time to be predicted\n",
        "  we randomize the dates and retun 2 set of dataframes\n",
        "  '''\n",
        "  xdf_data=build_image_df(xdf,past_step,fut_step)\n",
        "  \n",
        "  tmp_data=pd.concat([xdf_data['market_state'],xdf_data['future_value'],xdf_data['df_x_image']],axis=1)\n",
        "  tmp_data=tmp_data.dropna()\n",
        "\n",
        "  Y_StateClass= tmp_data['market_state']\n",
        "  Y_FutPredict= tmp_data['future_value']  \n",
        "  X=tmp_data.drop(columns=['market_state','future_value'])\n",
        "\n",
        "  nb_dates=len(Y_StateClass.index)\n",
        "  rng = np.random.default_rng()\n",
        "  list_shuffle = np.arange(nb_dates)\n",
        "  rng.shuffle(list_shuffle)\n",
        "  split_index=int(split*nb_dates)\n",
        "    \n",
        "  train_split=list_shuffle[:split_index]\n",
        "  test_split=list_shuffle[(split_index+1):]\n",
        "\n",
        "  X_train=(X.iloc[train_split])\n",
        "  Y_train_StateClass=(Y_StateClass.iloc[train_split])\n",
        "  Y_train_FutPredict=(Y_FutPredict.iloc[train_split])\n",
        "\n",
        "  X_test=(X.iloc[test_split])\n",
        "  Y_test_StateClass=(Y_StateClass.iloc[test_split])\n",
        "  Y_test_FutPredict=(Y_FutPredict.iloc[test_split])\n",
        "\n",
        "  return (X_train, Y_train_StateClass, Y_train_FutPredict), (X_test, Y_test_StateClass, Y_test_FutPredict)\n",
        "\n",
        "def change_X_df__nparray_image(df_X_train_image_flattened ):\n",
        "  '''\n",
        "  setup_input_NN_image returns a dataframe of flaten image for x train and xtest\n",
        "  then this function will change each date into a nparray list of images with 32, 32, 3 size \n",
        "  '''\n",
        "  X_train_image=df_X_train_image_flattened\n",
        "  nb_train=len(X_train_image.index)\n",
        "  \n",
        "  x_train=np.zeros((nb_train,32,32,3))\n",
        "  for i in range(nb_train):\n",
        "    tmp=np.array(X_train_image.iloc[i])\n",
        "    tmp=tmp.reshape(32,32,3)\n",
        "    x_train[i]=tmp\n",
        "  return x_train\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0mY5OLZIymc_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OyzAFCoi0hFD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "25597ce8-97f7-406e-ecd0-676cb0f01784"
      },
      "source": [
        "#https://www.machinelearningplus.com/python/parallel-processing-python/\n",
        "import multiprocessing as mp\n",
        "print(\"Number of processors: \", mp.cpu_count())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of processors:  2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AZLYbyEmwBwF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "95b65ba0-3fc2-4ff4-b4ed-5cdb9fed4dda"
      },
      "source": [
        "\n",
        "'''\n",
        "COMMAND NOW FOR THE DATSET GENERATION\n",
        "'''\n",
        "\n",
        "#Recuperation from yahoo of sp500 large history\n",
        "start = datetime(1920,1,1)\n",
        "end = datetime(2020,7,31)\n",
        "yf.pdr_override() # <== that's all it takes :-)\n",
        "sp500 = pdr.get_data_yahoo('^GSPC', \n",
        "                           start,\n",
        "                             end)\n",
        "\n",
        "#generate the dataset it can take 6 - 8 hours\n",
        "#Need to be optimzed with more time\n",
        "testsp500=(sp500['Close'])[21002:]\n",
        "(X_train_image, Y_train_StateClass_image, Y_train_FutPredict_image) , (X_test_image, Y_test_StateClass_image, Y_test_FutPredict_image) = setup_input_NN_image(testsp500)\n",
        "\n",
        "#copy the datafrae dataset in csv format to be used after\n",
        "#dateTimeObj = datetime.now()\n",
        "#timeStr = dateTimeObj.strftime(\"%Y_%m_%d_%H_%M_%S_%f\")\n",
        "\n",
        "X_train_image.to_csv('datas/X_train_image4.csv')\n",
        "Y_train_StateClass_image.to_csv('datas/Y_train_StateClass_image4.csv')\n",
        "Y_train_FutPredict_image.to_csv('datas/Y_train_FutPredict_image4.csv')\n",
        "\n",
        "X_test_image.to_csv('datas/X_test_image4.csv')\n",
        "Y_test_StateClass_image.to_csv('datas/Y_test_StateClass_image4.csv')\n",
        "Y_test_FutPredict_image.to_csv('datas/Y_test_FutPredict_image4.csv')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[*********************100%***********************]  1 of 1 downloaded\n",
            "loop 1 market state : step  0 market state fut market_state    1.0\n",
            "Name: 2011-08-17 00:00:00, dtype: float64  future value 0.3477695938104987\n",
            "loop 1 market state : step  1 market state fut market_state    2.0\n",
            "Name: 2011-08-18 00:00:00, dtype: float64  future value 0.3423563792362787\n",
            "loop 1 market state : step  2 market state fut market_state    2.0\n",
            "Name: 2011-08-19 00:00:00, dtype: float64  future value 0.347533358846557\n",
            "loop 1 market state : step  3 market state fut market_state    2.0\n",
            "Name: 2011-08-22 00:00:00, dtype: float64  future value 0.3573616027114679\n",
            "loop 1 market state : step  4 market state fut market_state    2.0\n",
            "Name: 2011-08-23 00:00:00, dtype: float64  future value 0.3582003393540254\n",
            "loop 1 market state : step  5 market state fut market_state    2.0\n",
            "Name: 2011-08-24 00:00:00, dtype: float64  future value 0.35996339508775826\n",
            "loop 1 market state : step  6 market state fut market_state    2.0\n",
            "Name: 2011-08-25 00:00:00, dtype: float64  future value 0.35569011380406396\n",
            "loop 1 market state : step  7 market state fut market_state    1.0\n",
            "Name: 2011-08-26 00:00:00, dtype: float64  future value 0.3466975783637354\n",
            "loop 1 market state : step  8 market state fut market_state    1.0\n",
            "Name: 2011-08-29 00:00:00, dtype: float64  future value 0.3441194346746909\n",
            "loop 1 market state : step  9 market state fut market_state    1.0\n",
            "Name: 2011-08-30 00:00:00, dtype: float64  future value 0.3539772395463194\n",
            "loop 1 market state : step  10 market state fut market_state    1.0\n",
            "Name: 2011-08-31 00:00:00, dtype: float64  future value 0.3502207694052642\n",
            "loop 1 market state : step  11 market state fut market_state    1.0\n",
            "Name: 2011-09-01 00:00:00, dtype: float64  future value 0.3408679513326519\n",
            "loop 1 market state : step  12 market state fut market_state    1.0\n",
            "Name: 2011-09-02 00:00:00, dtype: float64  future value 0.3432423411950886\n",
            "loop 1 market state : step  13 market state fut market_state    2.0\n",
            "Name: 2011-09-06 00:00:00, dtype: float64  future value 0.3463727327332008\n",
            "loop 1 market state : step  14 market state fut market_state    1.0\n",
            "Name: 2011-09-07 00:00:00, dtype: float64  future value 0.35104176968004763\n",
            "loop 1 market state : step  15 market state fut market_state    2.0\n",
            "Name: 2011-09-08 00:00:00, dtype: float64  future value 0.3570751502424183\n",
            "loop 1 market state : step  16 market state fut market_state    2.0\n",
            "Name: 2011-09-09 00:00:00, dtype: float64  future value 0.3591128701306975\n",
            "loop 1 market state : step  17 market state fut market_state    2.0\n",
            "Name: 2011-09-12 00:00:00, dtype: float64  future value 0.3555926349535839\n",
            "loop 1 market state : step  18 market state fut market_state    2.0\n",
            "Name: 2011-09-13 00:00:00, dtype: float64  future value 0.3550019936477106\n",
            "loop 1 market state : step  19 market state fut market_state    1.0\n",
            "Name: 2011-09-14 00:00:00, dtype: float64  future value 0.3445683279735676\n",
            "loop 1 market state : step  20 market state fut market_state    1.0\n",
            "Name: 2011-09-15 00:00:00, dtype: float64  future value 0.33358241415503637\n",
            "loop 1 market state : step  21 market state fut market_state    1.0\n",
            "Name: 2011-09-16 00:00:00, dtype: float64  future value 0.33561126556410786\n",
            "loop 1 market state : step  22 market state fut market_state    1.0\n",
            "Name: 2011-09-19 00:00:00, dtype: float64  future value 0.34344313886196054\n",
            "loop 1 market state : step  23 market state fut market_state    1.0\n",
            "Name: 2011-09-20 00:00:00, dtype: float64  future value 0.34711399052527825\n",
            "loop 1 market state : step  24 market state fut market_state    1.0\n",
            "Name: 2011-09-21 00:00:00, dtype: float64  future value 0.33993180819317426\n",
            "loop 1 market state : step  25 market state fut market_state    2.0\n",
            "Name: 2011-09-22 00:00:00, dtype: float64  future value 0.34269009275537976\n",
            "loop 1 market state : step  26 market state fut market_state    1.0\n",
            "Name: 2011-09-23 00:00:00, dtype: float64  future value 0.3341317061396888\n",
            "loop 1 market state : step  27 market state fut market_state    0.0\n",
            "Name: 2011-09-26 00:00:00, dtype: float64  future value 0.3246253154211364\n",
            "loop 1 market state : step  28 market state fut market_state    2.0\n",
            "Name: 2011-09-27 00:00:00, dtype: float64  future value 0.3319256333974313\n",
            "loop 1 market state : step  29 market state fut market_state    1.0\n",
            "Name: 2011-09-28 00:00:00, dtype: float64  future value 0.3378556951434101\n",
            "loop 1 market state : step  30 market state fut market_state    2.0\n",
            "Name: 2011-09-29 00:00:00, dtype: float64  future value 0.3440396924873056\n",
            "loop 1 market state : step  31 market state fut market_state    2.0\n",
            "Name: 2011-09-30 00:00:00, dtype: float64  future value 0.34123119012467157\n",
            "loop 1 market state : step  32 market state fut market_state    3.0\n",
            "Name: 2011-10-03 00:00:00, dtype: float64  future value 0.35287569941727875\n",
            "loop 1 market state : step  33 market state fut market_state    2.0\n",
            "Name: 2011-10-04 00:00:00, dtype: float64  future value 0.35306766492938324\n",
            "loop 1 market state : step  34 market state fut market_state    2.0\n",
            "Name: 2011-10-05 00:00:00, dtype: float64  future value 0.3565258582577659\n",
            "loop 1 market state : step  35 market state fut market_state    2.0\n",
            "Name: 2011-10-06 00:00:00, dtype: float64  future value 0.3554656671546256\n",
            "loop 1 market state : step  36 market state fut market_state    4.0\n",
            "Name: 2011-10-07 00:00:00, dtype: float64  future value 0.36164375217904926\n",
            "loop 1 market state : step  37 market state fut market_state    3.0\n",
            "Name: 2011-10-10 00:00:00, dtype: float64  future value 0.35463875485569096\n",
            "loop 1 market state : step  38 market state fut market_state    4.0\n",
            "Name: 2011-10-11 00:00:00, dtype: float64  future value 0.3618800231721106\n",
            "loop 1 market state : step  39 market state fut market_state    3.0\n",
            "Name: 2011-10-12 00:00:00, dtype: float64  future value 0.3573025530515926\n",
            "loop 1 market state : step  40 market state fut market_state    3.0\n",
            "Name: 2011-10-13 00:00:00, dtype: float64  future value 0.35892977280248\n",
            "loop 1 market state : step  41 market state fut market_state    4.0\n",
            "Name: 2011-10-14 00:00:00, dtype: float64  future value 0.365680798498802\n",
            "loop 1 market state : step  42 market state fut market_state    4.0\n",
            "Name: 2011-10-17 00:00:00, dtype: float64  future value 0.3703881922826936\n",
            "loop 1 market state : step  43 market state fut market_state    4.0\n",
            "Name: 2011-10-18 00:00:00, dtype: float64  future value 0.3629638629624968\n",
            "loop 1 market state : step  44 market state fut market_state    4.0\n",
            "Name: 2011-10-19 00:00:00, dtype: float64  future value 0.3667882509473144\n",
            "loop 1 market state : step  45 market state fut market_state    4.0\n",
            "Name: 2011-10-20 00:00:00, dtype: float64  future value 0.3793659475149839\n",
            "loop 1 market state : step  46 market state fut market_state    4.0\n",
            "Name: 2011-10-21 00:00:00, dtype: float64  future value 0.37951360784145227\n",
            "loop 1 market state : step  47 market state fut market_state    2.0\n",
            "Name: 2011-10-24 00:00:00, dtype: float64  future value 0.37012538879621043\n",
            "loop 1 market state : step  48 market state fut market_state    2.0\n",
            "Name: 2011-10-25 00:00:00, dtype: float64  future value 0.3597832536239561\n",
            "loop 1 market state : step  49 market state fut market_state    2.0\n",
            "Name: 2011-10-26 00:00:00, dtype: float64  future value 0.3655774433579698\n",
            "loop 1 market state : step  50 market state fut market_state    2.0\n",
            "Name: 2011-10-27 00:00:00, dtype: float64  future value 0.37244364853874684\n",
            "loop 1 market state : step  51 market state fut market_state    2.0\n",
            "Name: 2011-10-28 00:00:00, dtype: float64  future value 0.37010469597337986\n",
            "loop 1 market state : step  52 market state fut market_state    3.0\n",
            "Name: 2011-10-31 00:00:00, dtype: float64  future value 0.3724347803548598\n",
            "loop 1 market state : step  53 market state fut market_state    3.0\n",
            "Name: 2011-11-01 00:00:00, dtype: float64  future value 0.37680554048903414\n",
            "loop 1 market state : step  54 market state fut market_state    2.0\n",
            "Name: 2011-11-02 00:00:00, dtype: float64  future value 0.36297860743673593\n",
            "loop 1 market state : step  55 market state fut market_state    2.0\n",
            "Name: 2011-11-03 00:00:00, dtype: float64  future value 0.3661089989748481\n",
            "loop 1 market state : step  56 market state fut market_state    3.0\n",
            "Name: 2011-11-04 00:00:00, dtype: float64  future value 0.3732410001262844\n",
            "loop 1 market state : step  57 market state fut market_state    2.0\n",
            "Name: 2011-11-07 00:00:00, dtype: float64  future value 0.3696764954973337\n",
            "loop 1 market state : step  58 market state fut market_state    2.0\n",
            "Name: 2011-11-08 00:00:00, dtype: float64  future value 0.3714572878941612\n",
            "loop 1 market state : step  59 market state fut market_state    3.0\n",
            "Name: 2011-11-09 00:00:00, dtype: float64  future value 0.365285078864769\n",
            "loop 1 market state : step  60 market state fut market_state    2.0\n",
            "Name: 2011-11-10 00:00:00, dtype: float64  future value 0.3591483071324466\n",
            "loop 1 market state : step  61 market state fut market_state    2.0\n",
            "Name: 2011-11-11 00:00:00, dtype: float64  future value 0.35900655883012944\n",
            "loop 1 market state : step  62 market state fut market_state    0.0\n",
            "Name: 2011-11-14 00:00:00, dtype: float64  future value 0.35231162663394694\n",
            "loop 1 market state : step  63 market state fut market_state    0.0\n",
            "Name: 2011-11-15 00:00:00, dtype: float64  future value 0.35085276003235844\n",
            "loop 1 market state : step  64 market state fut market_state    0.0\n",
            "Name: 2011-11-16 00:00:00, dtype: float64  future value 0.34310059289277145\n",
            "loop 1 market state : step  65 market state fut market_state    0.0\n",
            "Name: 2011-11-17 00:00:00, dtype: float64  future value 0.34217919393221236\n",
            "loop 1 market state : step  66 market state fut market_state    0.0\n",
            "Name: 2011-11-18 00:00:00, dtype: float64  future value 0.3521846591303092\n",
            "loop 1 market state : step  67 market state fut market_state    0.0\n",
            "Name: 2011-11-21 00:00:00, dtype: float64  future value 0.35296427375943146\n",
            "loop 1 market state : step  68 market state fut market_state    3.0\n",
            "Name: 2011-11-22 00:00:00, dtype: float64  future value 0.36825302986837466\n",
            "loop 1 market state : step  69 market state fut market_state    3.0\n",
            "Name: 2011-11-23 00:00:00, dtype: float64  future value 0.3675501652377822\n",
            "loop 1 market state : step  70 market state fut market_state    3.0\n",
            "Name: 2011-11-25 00:00:00, dtype: float64  future value 0.36746159060030886\n",
            "loop 1 market state : step  71 market state fut market_state    3.0\n",
            "Name: 2011-11-28 00:00:00, dtype: float64  future value 0.37124167339949027\n",
            "loop 1 market state : step  72 market state fut market_state    3.0\n",
            "Name: 2011-11-29 00:00:00, dtype: float64  future value 0.371652173536882\n",
            "loop 1 market state : step  73 market state fut market_state    3.0\n",
            "Name: 2011-11-30 00:00:00, dtype: float64  future value 0.37240229951284654\n",
            "loop 1 market state : step  74 market state fut market_state    1.0\n",
            "Name: 2011-12-01 00:00:00, dtype: float64  future value 0.3645290408646534\n",
            "loop 1 market state : step  75 market state fut market_state    2.0\n",
            "Name: 2011-12-02 00:00:00, dtype: float64  future value 0.37068351293563023\n",
            "loop 1 market state : step  76 market state fut market_state    1.0\n",
            "Name: 2011-12-05 00:00:00, dtype: float64  future value 0.3651551191722758\n",
            "loop 1 market state : step  77 market state fut market_state    1.0\n",
            "Name: 2011-12-06 00:00:00, dtype: float64  future value 0.3619833780176221\n",
            "loop 1 market state : step  78 market state fut market_state    1.0\n",
            "Name: 2011-12-07 00:00:00, dtype: float64  future value 0.3578754576943712\n",
            "loop 1 market state : step  79 market state fut market_state    1.0\n",
            "Name: 2011-12-08 00:00:00, dtype: float64  future value 0.3590360838077274\n",
            "loop 1 market state : step  80 market state fut market_state    1.0\n",
            "Name: 2011-12-09 00:00:00, dtype: float64  future value 0.3601907976016119\n",
            "loop 1 market state : step  81 market state fut market_state    1.0\n",
            "Name: 2011-12-12 00:00:00, dtype: float64  future value 0.3559647419294906\n",
            "loop 1 market state : step  82 market state fut market_state    2.0\n",
            "Name: 2011-12-13 00:00:00, dtype: float64  future value 0.3665815409609707\n",
            "loop 1 market state : step  83 market state fut market_state    2.0\n",
            "Name: 2011-12-14 00:00:00, dtype: float64  future value 0.36729619390606644\n",
            "loop 1 market state : step  84 market state fut market_state    3.0\n",
            "Name: 2011-12-15 00:00:00, dtype: float64  future value 0.37033209878255413\n",
            "loop 1 market state : step  85 market state fut market_state    4.0\n",
            "Name: 2011-12-16 00:00:00, dtype: float64  future value 0.3736780687862176\n",
            "loop 1 market state : step  86 market state fut market_state    4.0\n",
            "Name: 2011-12-19 00:00:00, dtype: float64  future value 0.3737076297929352\n",
            "loop 1 market state : step  87 market state fut market_state    2.0\n",
            "Name: 2011-12-20 00:00:00, dtype: float64  future value 0.36904450516556014\n",
            "loop 1 market state : step  88 market state fut market_state    4.0\n",
            "Name: 2011-12-21 00:00:00, dtype: float64  future value 0.3729958969784557\n",
            "loop 1 market state : step  89 market state fut market_state    3.0\n",
            "Name: 2011-12-22 00:00:00, dtype: float64  future value 0.3713952460454304\n",
            "loop 1 market state : step  90 market state fut market_state    4.0\n",
            "Name: 2011-12-23 00:00:00, dtype: float64  future value 0.3771422104631917\n",
            "loop 1 market state : step  91 market state fut market_state    4.0\n",
            "Name: 2011-12-27 00:00:00, dtype: float64  future value 0.37721308446668994\n",
            "loop 1 market state : step  92 market state fut market_state    4.0\n",
            "Name: 2011-12-28 00:00:00, dtype: float64  future value 0.37832349307493823\n",
            "loop 1 market state : step  93 market state fut market_state    4.0\n",
            "Name: 2011-12-29 00:00:00, dtype: float64  future value 0.3773637009528942\n",
            "loop 1 market state : step  94 market state fut market_state    4.0\n",
            "Name: 2011-12-30 00:00:00, dtype: float64  future value 0.3782171457452506\n",
            "loop 1 market state : step  95 market state fut market_state    4.0\n",
            "Name: 2012-01-03 00:00:00, dtype: float64  future value 0.3815778962522729\n",
            "loop 1 market state : step  96 market state fut market_state    4.0\n",
            "Name: 2012-01-04 00:00:00, dtype: float64  future value 0.38169603160114324\n",
            "loop 1 market state : step  97 market state fut market_state    4.0\n",
            "Name: 2012-01-05 00:00:00, dtype: float64  future value 0.3825879058794249\n",
            "loop 1 market state : step  98 market state fut market_state    4.0\n",
            "Name: 2012-01-06 00:00:00, dtype: float64  future value 0.3806948904531988\n",
            "loop 1 market state : step  99 market state fut market_state    4.0\n",
            "Name: 2012-01-09 00:00:00, dtype: float64  future value 0.3820474820786596\n",
            "loop 1 market state : step  100 market state fut market_state    4.0\n",
            "Name: 2012-01-10 00:00:00, dtype: float64  future value 0.38629123838475593\n",
            "loop 1 market state : step  101 market state fut market_state    4.0\n",
            "Name: 2012-01-11 00:00:00, dtype: float64  future value 0.3881989982852212\n",
            "loop 1 market state : step  102 market state fut market_state    4.0\n",
            "Name: 2012-01-12 00:00:00, dtype: float64  future value 0.3884588819364087\n",
            "loop 1 market state : step  103 market state fut market_state    4.0\n",
            "Name: 2012-01-13 00:00:00, dtype: float64  future value 0.3886419792646262\n",
            "loop 1 market state : step  104 market state fut market_state    4.0\n",
            "Name: 2012-01-17 00:00:00, dtype: float64  future value 0.3882433034708574\n",
            "loop 1 market state : step  105 market state fut market_state    4.0\n",
            "Name: 2012-01-18 00:00:00, dtype: float64  future value 0.39161292245708734\n",
            "loop 1 market state : step  106 market state fut market_state    4.0\n",
            "Name: 2012-01-19 00:00:00, dtype: float64  future value 0.38935962439857746\n",
            "loop 1 market state : step  107 market state fut market_state    4.0\n",
            "Name: 2012-01-20 00:00:00, dtype: float64  future value 0.38873942208598655\n",
            "loop 1 market state : step  108 market state fut market_state    2.0\n",
            "Name: 2012-01-23 00:00:00, dtype: float64  future value 0.3877589734655521\n",
            "loop 1 market state : step  109 market state fut market_state    2.0\n",
            "Name: 2012-01-24 00:00:00, dtype: float64  future value 0.3875817881614858\n",
            "loop 1 market state : step  110 market state fut market_state    2.0\n",
            "Name: 2012-01-25 00:00:00, dtype: float64  future value 0.39103111330598145\n",
            "loop 1 market state : step  111 market state fut market_state    3.0\n",
            "Name: 2012-01-26 00:00:00, dtype: float64  future value 0.3914593498111472\n",
            "loop 1 market state : step  112 market state fut market_state    4.0\n",
            "Name: 2012-01-27 00:00:00, dtype: float64  future value 0.3971767532221909\n",
            "loop 1 market state : step  113 market state fut market_state    4.0\n",
            "Name: 2012-01-30 00:00:00, dtype: float64  future value 0.39700840036821267\n",
            "loop 1 market state : step  114 market state fut market_state    4.0\n",
            "Name: 2012-01-31 00:00:00, dtype: float64  future value 0.397811700009021\n",
            "loop 1 market state : step  115 market state fut market_state    4.0\n",
            "Name: 2012-02-01 00:00:00, dtype: float64  future value 0.3986710571208492\n",
            "loop 1 market state : step  116 market state fut market_state    4.0\n",
            "Name: 2012-02-02 00:00:00, dtype: float64  future value 0.3992587422669866\n",
            "loop 1 market state : step  117 market state fut market_state    2.0\n",
            "Name: 2012-02-03 00:00:00, dtype: float64  future value 0.3965093258886682\n",
            "loop 1 market state : step  118 market state fut market_state    4.0\n",
            "Name: 2012-02-06 00:00:00, dtype: float64  future value 0.399205604926583\n",
            "loop 1 market state : step  119 market state fut market_state    4.0\n",
            "Name: 2012-02-07 00:00:00, dtype: float64  future value 0.39883054179094046\n",
            "loop 1 market state : step  120 market state fut market_state    2.0\n",
            "Name: 2012-02-08 00:00:00, dtype: float64  future value 0.396683554737678\n",
            "loop 1 market state : step  121 market state fut market_state    4.0\n",
            "Name: 2012-02-09 00:00:00, dtype: float64  future value 0.4010572710315882\n",
            "loop 1 market state : step  122 market state fut market_state    4.0\n",
            "Name: 2012-02-10 00:00:00, dtype: float64  future value 0.40199932649053766\n",
            "loop 1 market state : step  123 market state fut market_state    4.0\n",
            "Name: 2012-02-13 00:00:00, dtype: float64  future value 0.4022887351193231\n",
            "loop 1 market state : step  124 market state fut market_state    4.0\n",
            "Name: 2012-02-14 00:00:00, dtype: float64  future value 0.40094504770686906\n",
            "loop 1 market state : step  125 market state fut market_state    4.0\n",
            "Name: 2012-02-15 00:00:00, dtype: float64  future value 0.40265788593549395\n",
            "loop 1 market state : step  126 market state fut market_state    4.0\n",
            "Name: 2012-02-16 00:00:00, dtype: float64  future value 0.40333122558848844\n",
            "loop 1 market state : step  127 market state fut market_state    4.0\n",
            "Name: 2012-02-17 00:00:00, dtype: float64  future value 0.4038775617087255\n",
            "loop 1 market state : step  128 market state fut market_state    4.0\n",
            "Name: 2012-02-21 00:00:00, dtype: float64  future value 0.40523310949392216\n",
            "loop 1 market state : step  129 market state fut market_state    4.0\n",
            "Name: 2012-02-22 00:00:00, dtype: float64  future value 0.403313525249834\n",
            "loop 1 market state : step  130 market state fut market_state    4.0\n",
            "Name: 2012-02-23 00:00:00, dtype: float64  future value 0.40579714595281374\n",
            "loop 1 market state : step  131 market state fut market_state    4.0\n",
            "Name: 2012-02-24 00:00:00, dtype: float64  future value 0.40448002735822175\n",
            "loop 1 market state : step  132 market state fut market_state    2.0\n",
            "Name: 2012-02-27 00:00:00, dtype: float64  future value 0.4029148134269456\n",
            "loop 1 market state : step  133 market state fut market_state    2.0\n",
            "Name: 2012-02-28 00:00:00, dtype: float64  future value 0.39672194789916304\n",
            "loop 1 market state : step  134 market state fut market_state    2.0\n",
            "Name: 2012-02-29 00:00:00, dtype: float64  future value 0.39945957625829875\n",
            "loop 1 market state : step  135 market state fut market_state    2.0\n",
            "Name: 2012-03-01 00:00:00, dtype: float64  future value 0.40338144309359636\n",
            "loop 1 market state : step  136 market state fut market_state    3.0\n",
            "Name: 2012-03-02 00:00:00, dtype: float64  future value 0.4048462220146567\n",
            "loop 1 market state : step  137 market state fut market_state    3.0\n",
            "Name: 2012-03-05 00:00:00, dtype: float64  future value 0.40491118399400383\n",
            "loop 1 market state : step  138 market state fut market_state    4.0\n",
            "Name: 2012-03-06 00:00:00, dtype: float64  future value 0.41225285099619907\n",
            "loop 1 market state : step  139 market state fut market_state    4.0\n",
            "Name: 2012-03-07 00:00:00, dtype: float64  future value 0.41175968854080575\n",
            "loop 1 market state : step  140 market state fut market_state    4.0\n",
            "Name: 2012-03-08 00:00:00, dtype: float64  future value 0.41421674072124404\n",
            "loop 1 market state : step  141 market state fut market_state    4.0\n",
            "Name: 2012-03-09 00:00:00, dtype: float64  future value 0.414680414228159\n",
            "loop 1 market state : step  142 market state fut market_state    4.0\n",
            "Name: 2012-03-12 00:00:00, dtype: float64  future value 0.41632829047743675\n",
            "loop 1 market state : step  143 market state fut market_state    4.0\n",
            "Name: 2012-03-13 00:00:00, dtype: float64  future value 0.4150790900219278\n",
            "loop 1 market state : step  144 market state fut market_state    4.0\n",
            "Name: 2012-03-14 00:00:00, dtype: float64  future value 0.41430239522810114\n",
            "loop 1 market state : step  145 market state fut market_state    2.0\n",
            "Name: 2012-03-15 00:00:00, dtype: float64  future value 0.4113167075614008\n",
            "loop 1 market state : step  146 market state fut market_state    2.0\n",
            "Name: 2012-03-16 00:00:00, dtype: float64  future value 0.41259543299450774\n",
            "loop 1 market state : step  147 market state fut market_state    4.0\n",
            "Name: 2012-03-19 00:00:00, dtype: float64  future value 0.418324661044495\n",
            "loop 1 market state : step  148 market state fut market_state    4.0\n",
            "Name: 2012-03-20 00:00:00, dtype: float64  future value 0.4171463345924843\n",
            "loop 1 market state : step  149 market state fut market_state    3.0\n",
            "Name: 2012-03-21 00:00:00, dtype: float64  future value 0.4150850020460789\n",
            "loop 1 market state : step  150 market state fut market_state    3.0\n",
            "Name: 2012-03-22 00:00:00, dtype: float64  future value 0.4144175744172356\n",
            "loop 1 market state : step  151 market state fut market_state    3.0\n",
            "Name: 2012-03-23 00:00:00, dtype: float64  future value 0.4159502714773789\n",
            "loop 1 market state : step  152 market state fut market_state    4.0\n",
            "Name: 2012-03-26 00:00:00, dtype: float64  future value 0.41907183086072364\n",
            "loop 1 market state : step  153 market state fut market_state    3.0\n",
            "Name: 2012-03-27 00:00:00, dtype: float64  future value 0.4174003059242\n",
            "loop 1 market state : step  154 market state fut market_state    2.0\n",
            "Name: 2012-03-28 00:00:00, dtype: float64  future value 0.4131417691147449\n",
            "loop 1 market state : step  155 market state fut market_state    2.0\n",
            "Name: 2012-03-29 00:00:00, dtype: float64  future value 0.41288188546355736\n",
            "loop 1 market state : step  156 market state fut market_state    2.0\n",
            "Name: 2012-03-30 00:00:00, dtype: float64  future value 0.4081921920183202\n",
            "loop 1 market state : step  157 market state fut market_state    1.0\n",
            "Name: 2012-04-02 00:00:00, dtype: float64  future value 0.40121967583229573\n",
            "loop 1 market state : step  158 market state fut market_state    1.0\n",
            "Name: 2012-04-03 00:00:00, dtype: float64  future value 0.40420831936341134\n",
            "loop 1 market state : step  159 market state fut market_state    2.0\n",
            "Name: 2012-04-04 00:00:00, dtype: float64  future value 0.4097780624479867\n",
            "loop 1 market state : step  160 market state fut market_state    1.0\n",
            "Name: 2012-04-05 00:00:00, dtype: float64  future value 0.4046660808461751\n",
            "loop 1 market state : step  161 market state fut market_state    1.0\n",
            "Name: 2012-04-09 00:00:00, dtype: float64  future value 0.40446229069512707\n",
            "loop 1 market state : step  162 market state fut market_state    3.0\n",
            "Name: 2012-04-10 00:00:00, dtype: float64  future value 0.4107260662555275\n",
            "loop 1 market state : step  163 market state fut market_state    3.0\n",
            "Name: 2012-04-11 00:00:00, dtype: float64  future value 0.40906045363847565\n",
            "loop 1 market state : step  164 market state fut market_state    2.0\n",
            "Name: 2012-04-12 00:00:00, dtype: float64  future value 0.40663292643563537\n",
            "loop 1 market state : step  165 market state fut market_state    3.0\n",
            "Name: 2012-04-13 00:00:00, dtype: float64  future value 0.4071083882570536\n",
            "loop 1 market state : step  166 market state fut market_state    1.0\n",
            "Name: 2012-04-16 00:00:00, dtype: float64  future value 0.40368559590130043\n",
            "loop 1 market state : step  167 market state fut market_state    2.0\n",
            "Name: 2012-04-17 00:00:00, dtype: float64  future value 0.40517106764519134\n",
            "loop 1 market state : step  168 market state fut market_state    2.0\n",
            "Name: 2012-04-18 00:00:00, dtype: float64  future value 0.4106994614085458\n",
            "loop 1 market state : step  169 market state fut market_state    3.0\n",
            "Name: 2012-04-19 00:00:00, dtype: float64  future value 0.4134430017918327\n",
            "loop 1 market state : step  170 market state fut market_state    3.0\n",
            "Name: 2012-04-20 00:00:00, dtype: float64  future value 0.4144411870753618\n",
            "loop 1 market state : step  171 market state fut market_state    2.0\n",
            "Name: 2012-04-23 00:00:00, dtype: float64  future value 0.4128317039875691\n",
            "loop 1 market state : step  172 market state fut market_state    2.0\n",
            "Name: 2012-04-24 00:00:00, dtype: float64  future value 0.4151676643640805\n",
            "loop 1 market state : step  173 market state fut market_state    2.0\n",
            "Name: 2012-04-25 00:00:00, dtype: float64  future value 0.41413112224350657\n",
            "loop 1 market state : step  174 market state fut market_state    1.0\n",
            "Name: 2012-04-26 00:00:00, dtype: float64  future value 0.4109593450597333\n",
            "loop 1 market state : step  175 market state fut market_state    1.0\n",
            "Name: 2012-04-27 00:00:00, dtype: float64  future value 0.4043234988478664\n",
            "loop 1 market state : step  176 market state fut market_state    1.0\n",
            "Name: 2012-04-30 00:00:00, dtype: float64  future value 0.40446524685486296\n",
            "loop 1 market state : step  177 market state fut market_state    1.0\n",
            "Name: 2012-05-01 00:00:00, dtype: float64  future value 0.402734672258464\n",
            "loop 1 market state : step  178 market state fut market_state    0.0\n",
            "Name: 2012-05-02 00:00:00, dtype: float64  future value 0.40003543706081324\n",
            "loop 1 market state : step  179 market state fut market_state    0.0\n",
            "Name: 2012-05-03 00:00:00, dtype: float64  future value 0.40104249052822943\n",
            "loop 1 market state : step  180 market state fut market_state    0.0\n",
            "Name: 2012-05-04 00:00:00, dtype: float64  future value 0.39968402290773714\n",
            "loop 1 market state : step  181 market state fut market_state    0.0\n",
            "Name: 2012-05-07 00:00:00, dtype: float64  future value 0.3952423887700645\n",
            "loop 1 market state : step  182 market state fut market_state    0.0\n",
            "Name: 2012-05-08 00:00:00, dtype: float64  future value 0.3929713900775796\n",
            "loop 1 market state : step  183 market state fut market_state    0.0\n",
            "Name: 2012-05-09 00:00:00, dtype: float64  future value 0.3912408154811806\n",
            "loop 1 market state : step  184 market state fut market_state    0.0\n",
            "Name: 2012-05-10 00:00:00, dtype: float64  future value 0.38535210276110216\n",
            "loop 1 market state : step  185 market state fut market_state    0.0\n",
            "Name: 2012-05-11 00:00:00, dtype: float64  future value 0.3825052075323037\n",
            "loop 1 market state : step  186 market state fut market_state    0.0\n",
            "Name: 2012-05-14 00:00:00, dtype: float64  future value 0.3886390231048903\n",
            "loop 1 market state : step  187 market state fut market_state    0.0\n",
            "Name: 2012-05-15 00:00:00, dtype: float64  future value 0.38882803275257954\n",
            "loop 1 market state : step  188 market state fut market_state    0.0\n",
            "Name: 2012-05-16 00:00:00, dtype: float64  future value 0.3894865919022152\n",
            "loop 1 market state : step  189 market state fut market_state    0.0\n",
            "Name: 2012-05-17 00:00:00, dtype: float64  future value 0.39002409586768494\n",
            "loop 1 market state : step  190 market state fut market_state    2.0\n",
            "Name: 2012-05-18 00:00:00, dtype: float64  future value 0.38917944690565565\n",
            "loop 1 market state : step  191 market state fut market_state    2.0\n",
            "Name: 2012-05-21 00:00:00, dtype: float64  future value 0.3934911573799546\n",
            "loop 1 market state : step  192 market state fut market_state    1.0\n",
            "Name: 2012-05-22 00:00:00, dtype: float64  future value 0.38785050396744075\n",
            "loop 1 market state : step  193 market state fut market_state    1.0\n",
            "Name: 2012-05-23 00:00:00, dtype: float64  future value 0.3869674981683667\n",
            "loop 1 market state : step  194 market state fut market_state    0.0\n",
            "Name: 2012-05-24 00:00:00, dtype: float64  future value 0.37743161879665654\n",
            "loop 1 market state : step  195 market state fut market_state    0.0\n",
            "Name: 2012-05-25 00:00:00, dtype: float64  future value 0.37747296811787745\n",
            "loop 1 market state : step  196 market state fut market_state    0.0\n",
            "Name: 2012-05-29 00:00:00, dtype: float64  future value 0.37963469935005845\n",
            "loop 1 market state : step  197 market state fut market_state    2.0\n",
            "Name: 2012-05-30 00:00:00, dtype: float64  future value 0.38838505177317456\n",
            "loop 1 market state : step  198 market state fut market_state    2.0\n",
            "Name: 2012-05-31 00:00:00, dtype: float64  future value 0.38834370245195365\n",
            "loop 1 market state : step  199 market state fut market_state    2.0\n",
            "Name: 2012-06-01 00:00:00, dtype: float64  future value 0.3914947868128964\n",
            "loop 1 market state : step  200 market state fut market_state    2.0\n",
            "Name: 2012-06-04 00:00:00, dtype: float64  future value 0.38655407819567933\n",
            "loop 1 market state : step  201 market state fut market_state    2.0\n",
            "Name: 2012-06-05 00:00:00, dtype: float64  future value 0.3910577181529632\n",
            "loop 1 market state : step  202 market state fut market_state    1.0\n",
            "Name: 2012-06-06 00:00:00, dtype: float64  future value 0.3883112216099404\n",
            "loop 1 market state : step  203 market state fut market_state    2.0\n",
            "Name: 2012-06-07 00:00:00, dtype: float64  future value 0.3925106727304006\n",
            "loop 1 market state : step  204 market state fut market_state    2.0\n",
            "Name: 2012-06-08 00:00:00, dtype: float64  future value 0.3965683755485436\n",
            "loop 1 market state : step  205 market state fut market_state    2.0\n",
            "Name: 2012-06-11 00:00:00, dtype: float64  future value 0.3971413162204418\n",
            "loop 1 market state : step  206 market state fut market_state    2.0\n",
            "Name: 2012-06-12 00:00:00, dtype: float64  future value 0.40103953436849354\n",
            "loop 1 market state : step  207 market state fut market_state    2.0\n",
            "Name: 2012-06-13 00:00:00, dtype: float64  future value 0.40036323855576317\n",
            "loop 1 market state : step  208 market state fut market_state    1.0\n",
            "Name: 2012-06-14 00:00:00, dtype: float64  future value 0.3914504816272602\n",
            "loop 1 market state : step  209 market state fut market_state    1.0\n",
            "Name: 2012-06-15 00:00:00, dtype: float64  future value 0.39425898398989423\n",
            "loop 1 market state : step  210 market state fut market_state    1.0\n",
            "Name: 2012-06-18 00:00:00, dtype: float64  future value 0.3879686396116317\n",
            "loop 1 market state : step  211 market state fut market_state    1.0\n",
            "Name: 2012-06-19 00:00:00, dtype: float64  future value 0.3898203057166369\n",
            "loop 1 market state : step  212 market state fut market_state    2.0\n",
            "Name: 2012-06-20 00:00:00, dtype: float64  future value 0.39332280452597634\n",
            "loop 1 market state : step  213 market state fut market_state    3.0\n",
            "Name: 2012-06-21 00:00:00, dtype: float64  future value 0.3924929720964255\n",
            "loop 1 market state : step  214 market state fut market_state    4.0\n",
            "Name: 2012-06-22 00:00:00, dtype: float64  future value 0.40227399064508396\n",
            "loop 1 market state : step  215 market state fut market_state    4.0\n",
            "Name: 2012-06-25 00:00:00, dtype: float64  future value 0.403263307744726\n",
            "loop 1 market state : step  216 market state fut market_state    4.0\n",
            "Name: 2012-06-26 00:00:00, dtype: float64  future value 0.4057764894544234\n",
            "loop 1 market state : step  217 market state fut market_state    4.0\n",
            "Name: 2012-06-27 00:00:00, dtype: float64  future value 0.4038746055489897\n",
            "loop 1 market state : step  218 market state fut market_state    3.0\n",
            "Name: 2012-06-28 00:00:00, dtype: float64  future value 0.4000649980675309\n",
            "loop 1 market state : step  219 market state fut market_state    2.0\n",
            "Name: 2012-06-29 00:00:00, dtype: float64  future value 0.39940935875319084\n",
            "loop 1 market state : step  220 market state fut market_state    2.0\n",
            "Name: 2012-07-02 00:00:00, dtype: float64  future value 0.3961637877306236\n",
            "loop 1 market state : step  221 market state fut market_state    2.0\n",
            "Name: 2012-07-03 00:00:00, dtype: float64  future value 0.3961578754111518\n",
            "loop 1 market state : step  222 market state fut market_state    2.0\n",
            "Name: 2012-07-05 00:00:00, dtype: float64  future value 0.3941821976669242\n",
            "loop 1 market state : step  223 market state fut market_state    3.0\n",
            "Name: 2012-07-06 00:00:00, dtype: float64  future value 0.40068516405568155\n",
            "loop 1 market state : step  224 market state fut market_state    3.0\n",
            "Name: 2012-07-09 00:00:00, dtype: float64  future value 0.39975785307097134\n",
            "loop 1 market state : step  225 market state fut market_state    3.0\n",
            "Name: 2012-07-10 00:00:00, dtype: float64  future value 0.40271992778422483\n",
            "loop 1 market state : step  226 market state fut market_state    3.0\n",
            "Name: 2012-07-11 00:00:00, dtype: float64  future value 0.40541029450266786\n",
            "loop 1 market state : step  227 market state fut market_state    4.0\n",
            "Name: 2012-07-12 00:00:00, dtype: float64  future value 0.40651183492702914\n",
            "loop 1 market state : step  228 market state fut market_state    3.0\n",
            "Name: 2012-07-13 00:00:00, dtype: float64  future value 0.4024216509715523\n",
            "loop 1 market state : step  229 market state fut market_state    2.0\n",
            "Name: 2012-07-16 00:00:00, dtype: float64  future value 0.39883645411041224\n",
            "loop 1 market state : step  230 market state fut market_state    2.0\n",
            "Name: 2012-07-17 00:00:00, dtype: float64  future value 0.39523060045556124\n",
            "loop 1 market state : step  231 market state fut market_state    2.0\n",
            "Name: 2012-07-18 00:00:00, dtype: float64  future value 0.3951065527872191\n",
            "loop 1 market state : step  232 market state fut market_state    2.0\n",
            "Name: 2012-07-19 00:00:00, dtype: float64  future value 0.4016420003133104\n",
            "loop 1 market state : step  233 market state fut market_state    4.0\n",
            "Name: 2012-07-20 00:00:00, dtype: float64  future value 0.40930555678630437\n",
            "loop 1 market state : step  234 market state fut market_state    4.0\n",
            "Name: 2012-07-23 00:00:00, dtype: float64  future value 0.40910771498384774\n",
            "loop 1 market state : step  235 market state fut market_state    4.0\n",
            "Name: 2012-07-24 00:00:00, dtype: float64  future value 0.40734166706125935\n",
            "loop 1 market state : step  236 market state fut market_state    3.0\n",
            "Name: 2012-07-25 00:00:00, dtype: float64  future value 0.4061603844495128\n",
            "loop 1 market state : step  237 market state fut market_state    3.0\n",
            "Name: 2012-07-26 00:00:00, dtype: float64  future value 0.40311269125852184\n",
            "loop 1 market state : step  238 market state fut market_state    4.0\n",
            "Name: 2012-07-27 00:00:00, dtype: float64  future value 0.41078807207513873\n",
            "loop 1 market state : step  239 market state fut market_state    4.0\n",
            "Name: 2012-07-30 00:00:00, dtype: float64  future value 0.41174490803744695\n",
            "loop 1 market state : step  240 market state fut market_state    4.0\n",
            "Name: 2012-07-31 00:00:00, dtype: float64  future value 0.41384758990507325\n",
            "loop 1 market state : step  241 market state fut market_state    4.0\n",
            "Name: 2012-08-01 00:00:00, dtype: float64  future value 0.4141045173965249\n",
            "loop 1 market state : step  242 market state fut market_state    4.0\n",
            "Name: 2012-08-02 00:00:00, dtype: float64  future value 0.414275826410239\n",
            "loop 1 market state : step  243 market state fut market_state    4.0\n",
            "Name: 2012-08-03 00:00:00, dtype: float64  future value 0.41518244486743927\n",
            "loop 1 market state : step  244 market state fut market_state    4.0\n",
            "Name: 2012-08-06 00:00:00, dtype: float64  future value 0.41466267756506425\n",
            "loop 1 market state : step  245 market state fut market_state    4.0\n",
            "Name: 2012-08-07 00:00:00, dtype: float64  future value 0.4146095402246607\n",
            "loop 1 market state : step  246 market state fut market_state    4.0\n",
            "Name: 2012-08-08 00:00:00, dtype: float64  future value 0.415082045886343\n",
            "loop 1 market state : step  247 market state fut market_state    4.0\n",
            "Name: 2012-08-09 00:00:00, dtype: float64  future value 0.41802934039155837\n",
            "loop 1 market state : step  248 market state fut market_state    4.0\n",
            "Name: 2012-08-10 00:00:00, dtype: float64  future value 0.41881194720953613\n",
            "loop 1 market state : step  249 market state fut market_state    4.0\n",
            "Name: 2012-08-13 00:00:00, dtype: float64  future value 0.4188030790256491\n",
            "loop 1 market state : step  250 market state fut market_state    4.0\n",
            "Name: 2012-08-14 00:00:00, dtype: float64  future value 0.4173383001045888\n",
            "loop 1 market state : step  251 market state fut market_state    4.0\n",
            "Name: 2012-08-15 00:00:00, dtype: float64  future value 0.41743278676621326\n",
            "loop 1 market state : step  252 market state fut market_state    2.0\n",
            "Name: 2012-08-16 00:00:00, dtype: float64  future value 0.41406316807530397\n",
            "loop 1 market state : step  253 market state fut market_state    2.0\n",
            "Name: 2012-08-17 00:00:00, dtype: float64  future value 0.41673583445509255\n",
            "loop 1 market state : step  254 market state fut market_state    2.0\n",
            "Name: 2012-08-20 00:00:00, dtype: float64  future value 0.41653204430404456\n",
            "loop 1 market state : step  255 market state fut market_state    2.0\n",
            "Name: 2012-08-21 00:00:00, dtype: float64  future value 0.41619541065432725\n",
            "loop 1 market state : step  256 market state fut market_state    2.0\n",
            "Name: 2012-08-22 00:00:00, dtype: float64  future value 0.41654682480740335\n",
            "loop 1 market state : step  257 market state fut market_state    2.0\n",
            "Name: 2012-08-23 00:00:00, dtype: float64  future value 0.41329534146536434\n",
            "loop 1 market state : step  258 market state fut market_state    2.0\n",
            "Name: 2012-08-24 00:00:00, dtype: float64  future value 0.41539211101351886\n",
            "loop 1 market state : step  259 market state fut market_state    2.0\n",
            "Name: 2012-08-27 00:00:00, dtype: float64  future value 0.41490778071289297\n",
            "loop 1 market state : step  260 market state fut market_state    2.0\n",
            "Name: 2012-08-28 00:00:00, dtype: float64  future value 0.414464799733488\n",
            "loop 1 market state : step  261 market state fut market_state    4.0\n",
            "Name: 2012-08-29 00:00:00, dtype: float64  future value 0.42293461200702626\n",
            "loop 1 market state : step  262 market state fut market_state    4.0\n",
            "Name: 2012-08-30 00:00:00, dtype: float64  future value 0.4246474862647708\n",
            "loop 1 market state : step  263 market state fut market_state    4.0\n",
            "Name: 2012-08-31 00:00:00, dtype: float64  future value 0.4220368257045934\n",
            "loop 1 market state : step  264 market state fut market_state    4.0\n",
            "Name: 2012-09-04 00:00:00, dtype: float64  future value 0.4233598926477768\n",
            "loop 1 market state : step  265 market state fut market_state    4.0\n",
            "Name: 2012-09-05 00:00:00, dtype: float64  future value 0.4242458546065867\n",
            "loop 1 market state : step  266 market state fut market_state    4.0\n",
            "Name: 2012-09-06 00:00:00, dtype: float64  future value 0.4311651971277673\n",
            "loop 1 market state : step  267 market state fut market_state    4.0\n",
            "Name: 2012-09-07 00:00:00, dtype: float64  future value 0.4328721593613607\n",
            "loop 1 market state : step  268 market state fut market_state    4.0\n",
            "Name: 2012-09-10 00:00:00, dtype: float64  future value 0.43151956744057934\n",
            "loop 1 market state : step  269 market state fut market_state    4.0\n",
            "Name: 2012-09-11 00:00:00, dtype: float64  future value 0.43096731929619103\n",
            "loop 1 market state : step  270 market state fut market_state    4.0\n",
            "Name: 2012-09-12 00:00:00, dtype: float64  future value 0.43147825444379867\n",
            "loop 1 market state : step  271 market state fut market_state    4.0\n",
            "Name: 2012-09-13 00:00:00, dtype: float64  future value 0.4312449396104733\n",
            "loop 1 market state : step  272 market state fut market_state    2.0\n",
            "Name: 2012-09-14 00:00:00, dtype: float64  future value 0.4312124584731394\n",
            "loop 1 market state : step  273 market state fut market_state    2.0\n",
            "Name: 2012-09-17 00:00:00, dtype: float64  future value 0.43024971048668004\n",
            "loop 1 market state : step  274 market state fut market_state    2.0\n",
            "Name: 2012-09-18 00:00:00, dtype: float64  future value 0.4257312900260374\n",
            "loop 1 market state : step  275 market state fut market_state    2.0\n",
            "Name: 2012-09-19 00:00:00, dtype: float64  future value 0.42328898231983825\n",
            "loop 1 market state : step  276 market state fut market_state    2.0\n",
            "Name: 2012-09-20 00:00:00, dtype: float64  future value 0.427373289984963\n",
            "loop 1 market state : step  277 market state fut market_state    2.0\n",
            "Name: 2012-09-21 00:00:00, dtype: float64  future value 0.42545961806034654\n",
            "loop 1 market state : step  278 market state fut market_state    2.0\n",
            "Name: 2012-09-24 00:00:00, dtype: float64  future value 0.4265877270072493\n",
            "loop 1 market state : step  279 market state fut market_state    3.0\n",
            "Name: 2012-09-25 00:00:00, dtype: float64  future value 0.426959833983156\n",
            "loop 1 market state : step  280 market state fut market_state    3.0\n",
            "Name: 2012-09-26 00:00:00, dtype: float64  future value 0.4285073112513375\n",
            "loop 1 market state : step  281 market state fut market_state    3.0\n",
            "Name: 2012-09-27 00:00:00, dtype: float64  future value 0.43158160928931016\n",
            "loop 1 market state : step  282 market state fut market_state    3.0\n",
            "Name: 2012-09-28 00:00:00, dtype: float64  future value 0.43144281744204954\n",
            "loop 1 market state : step  283 market state fut market_state    3.0\n",
            "Name: 2012-10-01 00:00:00, dtype: float64  future value 0.42995143367400745\n",
            "loop 1 market state : step  284 market state fut market_state    2.0\n",
            "Name: 2012-10-02 00:00:00, dtype: float64  future value 0.4256988088887035\n",
            "loop 1 market state : step  285 market state fut market_state    2.0\n",
            "Name: 2012-10-03 00:00:00, dtype: float64  future value 0.42306457199484016\n",
            "loop 1 market state : step  286 market state fut market_state    2.0\n",
            "Name: 2012-10-04 00:00:00, dtype: float64  future value 0.4231472343128417\n",
            "loop 1 market state : step  287 market state fut market_state    2.0\n",
            "Name: 2012-10-05 00:00:00, dtype: float64  future value 0.42189212153786093\n",
            "loop 1 market state : step  288 market state fut market_state    2.0\n",
            "Name: 2012-10-08 00:00:00, dtype: float64  future value 0.4253001333902553\n",
            "loop 1 market state : step  289 market state fut market_state    3.0\n",
            "Name: 2012-10-09 00:00:00, dtype: float64  future value 0.4296679373646938\n",
            "loop 1 market state : step  290 market state fut market_state    3.0\n",
            "Name: 2012-10-10 00:00:00, dtype: float64  future value 0.43143690512257776\n",
            "loop 1 market state : step  291 market state fut market_state    3.0\n",
            "Name: 2012-10-11 00:00:00, dtype: float64  future value 0.43038259030978954\n",
            "loop 1 market state : step  292 market state fut market_state    2.0\n",
            "Name: 2012-10-12 00:00:00, dtype: float64  future value 0.42325058915835323\n",
            "loop 1 market state : step  293 market state fut market_state    2.0\n",
            "Name: 2012-10-15 00:00:00, dtype: float64  future value 0.4234366426463066\n",
            "loop 1 market state : step  294 market state fut market_state    0.0\n",
            "Name: 2012-10-16 00:00:00, dtype: float64  future value 0.4173205634414941\n",
            "loop 1 market state : step  295 market state fut market_state    0.0\n",
            "Name: 2012-10-17 00:00:00, dtype: float64  future value 0.4160329698245001\n",
            "loop 1 market state : step  296 market state fut market_state    0.0\n",
            "Name: 2012-10-18 00:00:00, dtype: float64  future value 0.4172792144155938\n",
            "loop 1 market state : step  297 market state fut market_state    0.0\n",
            "Name: 2012-10-19 00:00:00, dtype: float64  future value 0.4169750252834495\n",
            "loop 1 market state : step  298 market state fut market_state    0.0\n",
            "Name: 2012-10-22 00:00:00, dtype: float64  future value 0.41704002329191625\n",
            "loop 1 market state : step  299 market state fut market_state    0.0\n",
            "Name: 2012-10-23 00:00:00, dtype: float64  future value 0.4215968008849243\n",
            "loop 1 market state : step  300 market state fut market_state    2.0\n",
            "Name: 2012-10-24 00:00:00, dtype: float64  future value 0.41764245291229285\n",
            "loop 1 market state : step  301 market state fut market_state    2.0\n",
            "Name: 2012-10-25 00:00:00, dtype: float64  future value 0.4185461515341975\n",
            "loop 1 market state : step  302 market state fut market_state    2.0\n",
            "Name: 2012-10-26 00:00:00, dtype: float64  future value 0.4218330718779856\n",
            "loop 1 market state : step  303 market state fut market_state    0.0\n",
            "Name: 2012-10-31 00:00:00, dtype: float64  future value 0.41183351870403995\n",
            "loop 1 market state : step  304 market state fut market_state    0.0\n",
            "Name: 2012-11-01 00:00:00, dtype: float64  future value 0.4068071555799658\n",
            "loop 1 market state : step  305 market state fut market_state    0.0\n",
            "Name: 2012-11-02 00:00:00, dtype: float64  future value 0.40749819586693536\n",
            "loop 1 market state : step  306 market state fut market_state    0.0\n",
            "Name: 2012-11-05 00:00:00, dtype: float64  future value 0.40755136923645857\n",
            "loop 1 market state : step  307 market state fut market_state    0.0\n",
            "Name: 2012-11-06 00:00:00, dtype: float64  future value 0.405927105645307\n",
            "loop 1 market state : step  308 market state fut market_state    0.0\n",
            "Name: 2012-11-07 00:00:00, dtype: float64  future value 0.4003041888958878\n",
            "loop 1 market state : step  309 market state fut market_state    0.0\n",
            "Name: 2012-11-08 00:00:00, dtype: float64  future value 0.39966628624464245\n",
            "loop 1 market state : step  310 market state fut market_state    0.0\n",
            "Name: 2012-11-09 00:00:00, dtype: float64  future value 0.40160065099208947\n",
            "loop 1 market state : step  311 market state fut market_state    2.0\n",
            "Name: 2012-11-12 00:00:00, dtype: float64  future value 0.4095772647811148\n",
            "loop 1 market state : step  312 market state fut market_state    2.0\n",
            "Name: 2012-11-13 00:00:00, dtype: float64  future value 0.40984897277592525\n",
            "loop 1 market state : step  313 market state fut market_state    2.0\n",
            "Name: 2012-11-14 00:00:00, dtype: float64  future value 0.4107998964187617\n",
            "loop 1 market state : step  314 market state fut market_state    2.0\n",
            "Name: 2012-11-15 00:00:00, dtype: float64  future value 0.41615110517337045\n",
            "loop 1 market state : step  315 market state fut market_state    2.0\n",
            "Name: 2012-11-16 00:00:00, dtype: float64  future value 0.4153064925357814\n",
            "loop 1 market state : step  316 market state fut market_state    2.0\n",
            "Name: 2012-11-19 00:00:00, dtype: float64  future value 0.4131358567952731\n",
            "loop 1 market state : step  317 market state fut market_state    2.0\n",
            "Name: 2012-11-20 00:00:00, dtype: float64  future value 0.41638146414228056\n",
            "loop 1 market state : step  318 market state fut market_state    2.0\n",
            "Name: 2012-11-21 00:00:00, dtype: float64  future value 0.418159264054932\n",
            "loop 1 market state : step  319 market state fut market_state    2.0\n",
            "Name: 2012-11-23 00:00:00, dtype: float64  future value 0.41822721822313463\n",
            "loop 1 market state : step  320 market state fut market_state    2.0\n",
            "Name: 2012-11-26 00:00:00, dtype: float64  future value 0.41624263597057964\n",
            "loop 1 market state : step  321 market state fut market_state    2.0\n",
            "Name: 2012-11-27 00:00:00, dtype: float64  future value 0.41553093918521977\n",
            "loop 1 market state : step  322 market state fut market_state    1.0\n",
            "Name: 2012-11-28 00:00:00, dtype: float64  future value 0.41618949833485547\n",
            "loop 1 market state : step  323 market state fut market_state    1.0\n",
            "Name: 2012-11-29 00:00:00, dtype: float64  future value 0.4175656665893228\n",
            "loop 1 market state : step  324 market state fut market_state    3.0\n",
            "Name: 2012-11-30 00:00:00, dtype: float64  future value 0.4187853423625544\n",
            "loop 1 market state : step  325 market state fut market_state    3.0\n",
            "Name: 2012-12-03 00:00:00, dtype: float64  future value 0.4189271266939912\n",
            "loop 1 market state : step  326 market state fut market_state    3.0\n",
            "Name: 2012-12-04 00:00:00, dtype: float64  future value 0.4216706310481585\n",
            "loop 1 market state : step  327 market state fut market_state    4.0\n",
            "Name: 2012-12-05 00:00:00, dtype: float64  future value 0.4218596404005271\n",
            "loop 1 market state : step  328 market state fut market_state    3.0\n",
            "Name: 2012-12-06 00:00:00, dtype: float64  future value 0.41919288634021024\n",
            "loop 1 market state : step  329 market state fut market_state    1.0\n",
            "Name: 2012-12-07 00:00:00, dtype: float64  future value 0.4174593555840754\n",
            "loop 1 market state : step  330 market state fut market_state    4.0\n",
            "Name: 2012-12-10 00:00:00, dtype: float64  future value 0.42241484470465124\n",
            "loop 1 market state : step  331 market state fut market_state    4.0\n",
            "Name: 2012-12-11 00:00:00, dtype: float64  future value 0.42726697897971555\n",
            "loop 1 market state : step  332 market state fut market_state    4.0\n",
            "Name: 2012-12-12 00:00:00, dtype: float64  future value 0.4240243641168842\n",
            "loop 1 market state : step  333 market state fut market_state    4.0\n",
            "Name: 2012-12-13 00:00:00, dtype: float64  future value 0.426351456014188\n",
            "loop 1 market state : step  334 market state fut market_state    4.0\n",
            "Name: 2012-12-14 00:00:00, dtype: float64  future value 0.42235283888504\n",
            "loop 1 market state : step  335 market state fut market_state    2.0\n",
            "Name: 2012-12-17 00:00:00, dtype: float64  future value 0.4213221727594976\n",
            "loop 1 market state : step  336 market state fut market_state    2.0\n",
            "Name: 2012-12-18 00:00:00, dtype: float64  future value 0.4193051096649294\n",
            "loop 1 market state : step  337 market state fut market_state    2.0\n",
            "Name: 2012-12-19 00:00:00, dtype: float64  future value 0.41879421084176205\n",
            "loop 1 market state : step  338 market state fut market_state    2.0\n",
            "Name: 2012-12-20 00:00:00, dtype: float64  future value 0.41416655924525575\n",
            "loop 1 market state : step  339 market state fut market_state    2.0\n",
            "Name: 2012-12-21 00:00:00, dtype: float64  future value 0.4211833445877967\n",
            "loop 1 market state : step  340 market state fut market_state    4.0\n",
            "Name: 2012-12-24 00:00:00, dtype: float64  future value 0.4318828422617186\n",
            "loop 1 market state : step  341 market state fut market_state    4.0\n",
            "Name: 2012-12-26 00:00:00, dtype: float64  future value 0.4309820997995499\n",
            "loop 1 market state : step  342 market state fut market_state    4.0\n",
            "Name: 2012-12-27 00:00:00, dtype: float64  future value 0.43307886934770434\n",
            "loop 1 market state : step  343 market state fut market_state    4.0\n",
            "Name: 2012-12-28 00:00:00, dtype: float64  future value 0.4317263137513632\n",
            "loop 1 market state : step  344 market state fut market_state    4.0\n",
            "Name: 2012-12-31 00:00:00, dtype: float64  future value 0.4303264965143294\n",
            "loop 1 market state : step  345 market state fut market_state    2.0\n",
            "Name: 2013-01-02 00:00:00, dtype: float64  future value 0.4314693862599116\n",
            "loop 1 market state : step  346 market state fut market_state    4.0\n",
            "Name: 2013-01-03 00:00:00, dtype: float64  future value 0.43474743812449207\n",
            "loop 1 market state : step  347 market state fut market_state    4.0\n",
            "Name: 2013-01-04 00:00:00, dtype: float64  future value 0.4347267816261018\n",
            "loop 1 market state : step  348 market state fut market_state    4.0\n",
            "Name: 2013-01-07 00:00:00, dtype: float64  future value 0.4343221938081818\n",
            "loop 1 market state : step  349 market state fut market_state    4.0\n",
            "Name: 2013-01-08 00:00:00, dtype: float64  future value 0.4348124001038392\n",
            "loop 1 market state : step  350 market state fut market_state    4.0\n",
            "Name: 2013-01-09 00:00:00, dtype: float64  future value 0.4348980546106963\n",
            "loop 1 market state : step  351 market state fut market_state    4.0\n",
            "Name: 2013-01-10 00:00:00, dtype: float64  future value 0.43735215033607805\n",
            "loop 1 market state : step  352 market state fut market_state    4.0\n",
            "Name: 2013-01-11 00:00:00, dtype: float64  future value 0.43884057794438425\n",
            "loop 1 market state : step  353 market state fut market_state    4.0\n",
            "Name: 2013-01-14 00:00:00, dtype: float64  future value 0.4407838111710389\n",
            "loop 1 market state : step  354 market state fut market_state    4.0\n",
            "Name: 2013-01-15 00:00:00, dtype: float64  future value 0.44144828264014635\n",
            "loop 1 market state : step  355 market state fut market_state    4.0\n",
            "Name: 2013-01-16 00:00:00, dtype: float64  future value 0.441451202475442\n",
            "loop 1 market state : step  356 market state fut market_state    4.0\n",
            "Name: 2013-01-17 00:00:00, dtype: float64  future value 0.44385511702015606\n",
            "loop 1 market state : step  357 market state fut market_state    4.0\n",
            "Name: 2013-01-18 00:00:00, dtype: float64  future value 0.44303415306981286\n",
            "loop 1 market state : step  358 market state fut market_state    4.0\n",
            "Name: 2013-01-22 00:00:00, dtype: float64  future value 0.4452962832830902\n",
            "loop 1 market state : step  359 market state fut market_state    4.0\n",
            "Name: 2013-01-23 00:00:00, dtype: float64  future value 0.4435597963672194\n",
            "loop 1 market state : step  360 market state fut market_state    4.0\n",
            "Name: 2013-01-24 00:00:00, dtype: float64  future value 0.442422818941109\n",
            "loop 1 market state : step  361 market state fut market_state    4.0\n",
            "Name: 2013-01-25 00:00:00, dtype: float64  future value 0.44687036539825337\n",
            "loop 1 market state : step  362 market state fut market_state    2.0\n",
            "Name: 2013-01-28 00:00:00, dtype: float64  future value 0.44171404228636535\n",
            "loop 1 market state : step  363 market state fut market_state    4.0\n",
            "Name: 2013-01-29 00:00:00, dtype: float64  future value 0.4463151610941292\n",
            "loop 1 market state : step  364 market state fut market_state    4.0\n",
            "Name: 2013-01-30 00:00:00, dtype: float64  future value 0.44656026424195794\n",
            "loop 1 market state : step  365 market state fut market_state    4.0\n",
            "Name: 2013-01-31 00:00:00, dtype: float64  future value 0.44575404476585395\n",
            "loop 1 market state : step  366 market state fut market_state    4.0\n",
            "Name: 2013-02-01 00:00:00, dtype: float64  future value 0.44827609465943835\n",
            "loop 1 market state : step  367 market state fut market_state    4.0\n",
            "Name: 2013-02-04 00:00:00, dtype: float64  future value 0.44800438666462794\n",
            "loop 1 market state : step  368 market state fut market_state    4.0\n",
            "Name: 2013-02-05 00:00:00, dtype: float64  future value 0.44871907563884333\n",
            "loop 1 market state : step  369 market state fut market_state    4.0\n",
            "Name: 2013-02-06 00:00:00, dtype: float64  future value 0.44898483528506233\n",
            "loop 1 market state : step  370 market state fut market_state    4.0\n",
            "Name: 2013-02-07 00:00:00, dtype: float64  future value 0.4492949364413578\n",
            "loop 1 market state : step  371 market state fut market_state    4.0\n",
            "Name: 2013-02-08 00:00:00, dtype: float64  future value 0.4488253866440907\n",
            "loop 1 market state : step  372 market state fut market_state    4.0\n",
            "Name: 2013-02-11 00:00:00, dtype: float64  future value 0.4521181829829104\n",
            "loop 1 market state : step  373 market state fut market_state    2.0\n",
            "Name: 2013-02-12 00:00:00, dtype: float64  future value 0.44651004673684996\n",
            "loop 1 market state : step  374 market state fut market_state    2.0\n",
            "Name: 2013-02-13 00:00:00, dtype: float64  future value 0.44369566837918445\n",
            "loop 1 market state : step  375 market state fut market_state    2.0\n",
            "Name: 2013-02-14 00:00:00, dtype: float64  future value 0.447587974503085\n",
            "loop 1 market state : step  376 market state fut market_state    2.0\n",
            "Name: 2013-02-15 00:00:00, dtype: float64  future value 0.4393928263840931\n",
            "loop 1 market state : step  377 market state fut market_state    2.0\n",
            "Name: 2013-02-19 00:00:00, dtype: float64  future value 0.4420772807830644\n",
            "loop 1 market state : step  378 market state fut market_state    3.0\n",
            "Name: 2013-02-20 00:00:00, dtype: float64  future value 0.44770315369221947\n",
            "loop 1 market state : step  379 market state fut market_state    3.0\n",
            "Name: 2013-02-21 00:00:00, dtype: float64  future value 0.44731630253739424\n",
            "loop 1 market state : step  380 market state fut market_state    3.0\n",
            "Name: 2013-02-22 00:00:00, dtype: float64  future value 0.44835580081770404\n",
            "loop 1 market state : step  381 market state fut market_state    3.0\n",
            "Name: 2013-02-25 00:00:00, dtype: float64  future value 0.45042304538826056\n",
            "loop 1 market state : step  382 market state fut market_state    4.0\n",
            "Name: 2013-02-26 00:00:00, dtype: float64  future value 0.4547317997028236\n",
            "loop 1 market state : step  383 market state fut market_state    4.0\n",
            "Name: 2013-02-27 00:00:00, dtype: float64  future value 0.45522496215821695\n",
            "loop 1 market state : step  384 market state fut market_state    4.0\n",
            "Name: 2013-02-28 00:00:00, dtype: float64  future value 0.4560518744571515\n",
            "loop 1 market state : step  385 market state fut market_state    4.0\n",
            "Name: 2013-03-01 00:00:00, dtype: float64  future value 0.45809550636958185\n",
            "loop 1 market state : step  386 market state fut market_state    4.0\n",
            "Name: 2013-03-04 00:00:00, dtype: float64  future value 0.45958389794876836\n",
            "loop 1 market state : step  387 market state fut market_state    4.0\n",
            "Name: 2013-03-05 00:00:00, dtype: float64  future value 0.4584794013646712\n",
            "loop 1 market state : step  388 market state fut market_state    4.0\n",
            "Name: 2013-03-06 00:00:00, dtype: float64  future value 0.459081867309488\n",
            "loop 1 market state : step  389 market state fut market_state    4.0\n",
            "Name: 2013-03-07 00:00:00, dtype: float64  future value 0.4616540983837401\n",
            "loop 1 market state : step  390 market state fut market_state    4.0\n",
            "Name: 2013-03-08 00:00:00, dtype: float64  future value 0.4609069285675115\n",
            "loop 1 market state : step  391 market state fut market_state    2.0\n",
            "Name: 2013-03-11 00:00:00, dtype: float64  future value 0.45836717833527263\n",
            "loop 1 market state : step  392 market state fut market_state    2.0\n",
            "Name: 2013-03-12 00:00:00, dtype: float64  future value 0.45725676972702434\n",
            "loop 1 market state : step  393 market state fut market_state    4.0\n",
            "Name: 2013-03-13 00:00:00, dtype: float64  future value 0.4603192434213741\n",
            "loop 1 market state : step  394 market state fut market_state    2.0\n",
            "Name: 2013-03-14 00:00:00, dtype: float64  future value 0.4565066797801794\n",
            "loop 1 market state : step  395 market state fut market_state    2.0\n",
            "Name: 2013-03-15 00:00:00, dtype: float64  future value 0.4597817757803446\n",
            "loop 1 market state : step  396 market state fut market_state    2.0\n",
            "Name: 2013-03-18 00:00:00, dtype: float64  future value 0.4582460865313458\n",
            "loop 1 market state : step  397 market state fut market_state    4.0\n",
            "Name: 2013-03-19 00:00:00, dtype: float64  future value 0.461813583349152\n",
            "loop 1 market state : step  398 market state fut market_state    3.0\n",
            "Name: 2013-03-20 00:00:00, dtype: float64  future value 0.46154187535434155\n",
            "loop 1 market state : step  399 market state fut market_state    4.0\n",
            "Name: 2013-03-21 00:00:00, dtype: float64  future value 0.4634141979577371\n",
            "loop 1 market state : step  400 market state fut market_state    3.0\n",
            "Name: 2013-03-22 00:00:00, dtype: float64  future value 0.461341077392149\n",
            "loop 1 market state : step  401 market state fut market_state    4.0\n",
            "Name: 2013-03-25 00:00:00, dtype: float64  future value 0.4637272552737684\n",
            "loop 1 market state : step  402 market state fut market_state    2.0\n",
            "Name: 2013-03-26 00:00:00, dtype: float64  future value 0.45883672783721907\n",
            "loop 1 market state : step  403 market state fut market_state    2.0\n",
            "Name: 2013-03-27 00:00:00, dtype: float64  future value 0.46069430626169605\n",
            "loop 1 market state : step  404 market state fut market_state    2.0\n",
            "Name: 2013-03-28 00:00:00, dtype: float64  future value 0.4587156723577325\n",
            "loop 1 market state : step  405 market state fut market_state    3.0\n",
            "Name: 2013-04-01 00:00:00, dtype: float64  future value 0.4616068370383681\n",
            "loop 1 market state : step  406 market state fut market_state    2.0\n",
            "Name: 2013-04-02 00:00:00, dtype: float64  future value 0.46324292497314257\n",
            "loop 1 market state : step  407 market state fut market_state    4.0\n",
            "Name: 2013-04-03 00:00:00, dtype: float64  future value 0.46888945438068796\n",
            "loop 1 market state : step  408 market state fut market_state    4.0\n",
            "Name: 2013-04-04 00:00:00, dtype: float64  future value 0.4705550672930604\n",
            "loop 1 market state : step  409 market state fut market_state    4.0\n",
            "Name: 2013-04-05 00:00:00, dtype: float64  future value 0.4692202123306944\n",
            "loop 1 market state : step  410 market state fut market_state    2.0\n",
            "Name: 2013-04-08 00:00:00, dtype: float64  future value 0.45844396436292206\n",
            "loop 1 market state : step  411 market state fut market_state    4.0\n",
            "Name: 2013-04-09 00:00:00, dtype: float64  future value 0.4650030245471395\n",
            "loop 1 market state : step  412 market state fut market_state    2.0\n",
            "Name: 2013-04-10 00:00:00, dtype: float64  future value 0.4583406095174105\n",
            "loop 1 market state : step  413 market state fut market_state    2.0\n",
            "Name: 2013-04-11 00:00:00, dtype: float64  future value 0.4552692673438531\n",
            "loop 1 market state : step  414 market state fut market_state    2.0\n",
            "Name: 2013-04-12 00:00:00, dtype: float64  future value 0.45929744547971874\n",
            "loop 1 market state : step  415 market state fut market_state    3.0\n",
            "Name: 2013-04-15 00:00:00, dtype: float64  future value 0.4614385202135094\n",
            "loop 1 market state : step  416 market state fut market_state    3.0\n",
            "Name: 2013-04-16 00:00:00, dtype: float64  future value 0.46624634900761697\n",
            "loop 1 market state : step  417 market state fut market_state    3.0\n",
            "Name: 2013-04-17 00:00:00, dtype: float64  future value 0.46624930516735286\n",
            "loop 1 market state : step  418 market state fut market_state    3.0\n",
            "Name: 2013-04-18 00:00:00, dtype: float64  future value 0.468130496249956\n",
            "loop 1 market state : step  419 market state fut market_state    3.0\n",
            "Name: 2013-04-19 00:00:00, dtype: float64  future value 0.4672681469492723\n",
            "loop 1 market state : step  420 market state fut market_state    4.0\n",
            "Name: 2013-04-22 00:00:00, dtype: float64  future value 0.4706259412965587\n",
            "loop 1 market state : step  421 market state fut market_state    4.0\n",
            "Name: 2013-04-23 00:00:00, dtype: float64  future value 0.47179539956468236\n",
            "loop 1 market state : step  422 market state fut market_state    3.0\n",
            "Name: 2013-04-24 00:00:00, dtype: float64  future value 0.4674039829321177\n",
            "loop 1 market state : step  423 market state fut market_state    4.0\n",
            "Name: 2013-04-25 00:00:00, dtype: float64  future value 0.47180131188415414\n",
            "loop 1 market state : step  424 market state fut market_state    4.0\n",
            "Name: 2013-04-26 00:00:00, dtype: float64  future value 0.4767715815080888\n",
            "loop 1 market state : step  425 market state fut market_state    4.0\n",
            "Name: 2013-04-29 00:00:00, dtype: float64  future value 0.47768115612502493\n",
            "loop 1 market state : step  426 market state fut market_state    4.0\n",
            "Name: 2013-04-30 00:00:00, dtype: float64  future value 0.4801795573313635\n",
            "loop 1 market state : step  427 market state fut market_state    4.0\n",
            "Name: 2013-05-01 00:00:00, dtype: float64  future value 0.4821670594192141\n",
            "loop 1 market state : step  428 market state fut market_state    4.0\n",
            "Name: 2013-05-02 00:00:00, dtype: float64  future value 0.4803892595065627\n",
            "loop 1 market state : step  429 market state fut market_state    4.0\n",
            "Name: 2013-05-03 00:00:00, dtype: float64  future value 0.48246533623188664\n",
            "loop 1 market state : step  430 market state fut market_state    4.0\n",
            "Name: 2013-05-06 00:00:00, dtype: float64  future value 0.4824860290547172\n",
            "loop 1 market state : step  431 market state fut market_state    4.0\n",
            "Name: 2013-05-07 00:00:00, dtype: float64  future value 0.4873794763265622\n",
            "loop 1 market state : step  432 market state fut market_state    4.0\n",
            "Name: 2013-05-08 00:00:00, dtype: float64  future value 0.48987200124254865\n",
            "loop 1 market state : step  433 market state fut market_state    4.0\n",
            "Name: 2013-05-09 00:00:00, dtype: float64  future value 0.4874178694880472\n",
            "loop 1 market state : step  434 market state fut market_state    4.0\n",
            "Name: 2013-05-10 00:00:00, dtype: float64  future value 0.4924383205879702\n",
            "loop 1 market state : step  435 market state fut market_state    4.0\n",
            "Name: 2013-05-13 00:00:00, dtype: float64  future value 0.4920898622993094\n",
            "loop 1 market state : step  436 market state fut market_state    4.0\n",
            "Name: 2013-05-14 00:00:00, dtype: float64  future value 0.4929374310966343\n",
            "loop 1 market state : step  437 market state fut market_state    2.0\n",
            "Name: 2013-05-15 00:00:00, dtype: float64  future value 0.48885903575098133\n",
            "loop 1 market state : step  438 market state fut market_state    3.0\n",
            "Name: 2013-05-16 00:00:00, dtype: float64  future value 0.4874296938316702\n",
            "loop 1 market state : step  439 market state fut market_state    2.0\n",
            "Name: 2013-05-17 00:00:00, dtype: float64  future value 0.4871609419965956\n",
            "loop 1 market state : step  440 market state fut market_state    2.0\n",
            "Name: 2013-05-20 00:00:00, dtype: float64  future value 0.4902500205379271\n",
            "loop 1 market state : step  441 market state fut market_state    2.0\n",
            "Name: 2013-05-21 00:00:00, dtype: float64  future value 0.48679474704484005\n",
            "loop 1 market state : step  442 market state fut market_state    2.0\n",
            "Name: 2013-05-22 00:00:00, dtype: float64  future value 0.4885814514658188\n",
            "loop 1 market state : step  443 market state fut market_state    2.0\n",
            "Name: 2013-05-23 00:00:00, dtype: float64  future value 0.48159119861669963\n",
            "loop 1 market state : step  444 market state fut market_state    2.0\n",
            "Name: 2013-05-24 00:00:00, dtype: float64  future value 0.4844499184844416\n",
            "loop 1 market state : step  445 market state fut market_state    2.0\n",
            "Name: 2013-05-28 00:00:00, dtype: float64  future value 0.4817802082643889\n",
            "loop 1 market state : step  446 market state fut market_state    2.0\n",
            "Name: 2013-05-29 00:00:00, dtype: float64  future value 0.4751414055974655\n",
            "loop 1 market state : step  447 market state fut market_state    2.0\n",
            "Name: 2013-05-30 00:00:00, dtype: float64  future value 0.47917549605280285\n",
            "loop 1 market state : step  448 market state fut market_state    3.0\n",
            "Name: 2013-05-31 00:00:00, dtype: float64  future value 0.4853240560996286\n",
            "loop 1 market state : step  449 market state fut market_state    3.0\n",
            "Name: 2013-06-03 00:00:00, dtype: float64  future value 0.48515573927476996\n",
            "loop 1 market state : step  450 market state fut market_state    2.0\n",
            "Name: 2013-06-04 00:00:00, dtype: float64  future value 0.48022977483647145\n",
            "loop 1 market state : step  451 market state fut market_state    3.0\n",
            "Name: 2013-06-05 00:00:00, dtype: float64  future value 0.47621046517981347\n",
            "loop 1 market state : step  452 market state fut market_state    3.0\n",
            "Name: 2013-06-06 00:00:00, dtype: float64  future value 0.4832508992096003\n",
            "loop 1 market state : step  453 market state fut market_state    2.0\n",
            "Name: 2013-06-07 00:00:00, dtype: float64  future value 0.48040695984521714\n",
            "loop 1 market state : step  454 market state fut market_state    2.0\n",
            "Name: 2013-06-10 00:00:00, dtype: float64  future value 0.4840423745067858\n",
            "loop 1 market state : step  455 market state fut market_state    3.0\n",
            "Name: 2013-06-11 00:00:00, dtype: float64  future value 0.4878136251511998\n",
            "loop 1 market state : step  456 market state fut market_state    3.0\n",
            "Name: 2013-06-12 00:00:00, dtype: float64  future value 0.48105668713540606\n",
            "loop 1 market state : step  457 market state fut market_state    0.0\n",
            "Name: 2013-06-13 00:00:00, dtype: float64  future value 0.46902529036353335\n",
            "loop 1 market state : step  458 market state fut market_state    0.0\n",
            "Name: 2013-06-14 00:00:00, dtype: float64  future value 0.4702774833032185\n",
            "loop 1 market state : step  459 market state fut market_state    0.0\n",
            "Name: 2013-06-17 00:00:00, dtype: float64  future value 0.4645659558872063\n",
            "loop 1 market state : step  460 market state fut market_state    0.0\n",
            "Name: 2013-06-18 00:00:00, dtype: float64  future value 0.46897806504728096\n",
            "loop 1 market state : step  461 market state fut market_state    0.0\n",
            "Name: 2013-06-19 00:00:00, dtype: float64  future value 0.47347579298041365\n",
            "loop 1 market state : step  462 market state fut market_state    2.0\n",
            "Name: 2013-06-20 00:00:00, dtype: float64  future value 0.4764112628466854\n",
            "loop 1 market state : step  463 market state fut market_state    2.0\n",
            "Name: 2013-06-21 00:00:00, dtype: float64  future value 0.47436766696337473\n",
            "loop 1 market state : step  464 market state fut market_state    2.0\n",
            "Name: 2013-06-24 00:00:00, dtype: float64  future value 0.4769310301490604\n",
            "loop 1 market state : step  465 market state fut market_state    2.0\n",
            "Name: 2013-06-25 00:00:00, dtype: float64  future value 0.4766711464978729\n",
            "loop 1 market state : step  466 market state fut market_state    2.0\n",
            "Name: 2013-06-26 00:00:00, dtype: float64  future value 0.47706394600128954\n",
            "loop 1 market state : step  467 market state fut market_state    2.0\n",
            "Name: 2013-06-27 00:00:00, dtype: float64  future value 0.48193082475059307\n",
            "loop 1 market state : step  468 market state fut market_state    2.0\n",
            "Name: 2013-06-28 00:00:00, dtype: float64  future value 0.4844617067989449\n",
            "loop 1 market state : step  469 market state fut market_state    3.0\n",
            "Name: 2013-07-01 00:00:00, dtype: float64  future value 0.48796420531296375\n",
            "loop 1 market state : step  470 market state fut market_state    2.0\n",
            "Name: 2013-07-02 00:00:00, dtype: float64  future value 0.4880528159795567\n",
            "loop 1 market state : step  471 market state fut market_state    4.0\n",
            "Name: 2013-07-03 00:00:00, dtype: float64  future value 0.49466800598835386\n",
            "loop 1 market state : step  472 market state fut market_state    4.0\n",
            "Name: 2013-07-05 00:00:00, dtype: float64  future value 0.4961947904337048\n",
            "loop 1 market state : step  473 market state fut market_state    4.0\n",
            "Name: 2013-07-08 00:00:00, dtype: float64  future value 0.49687699856590695\n",
            "loop 1 market state : step  474 market state fut market_state    4.0\n",
            "Name: 2013-07-09 00:00:00, dtype: float64  future value 0.4950342006447888\n",
            "loop 1 market state : step  475 market state fut market_state    4.0\n",
            "Name: 2013-07-10 00:00:00, dtype: float64  future value 0.4964074487686399\n",
            "loop 1 market state : step  476 market state fut market_state    4.0\n",
            "Name: 2013-07-11 00:00:00, dtype: float64  future value 0.49890584997497844\n",
            "loop 1 market state : step  477 market state fut market_state    4.0\n",
            "Name: 2013-07-12 00:00:00, dtype: float64  future value 0.4997091135866672\n",
            "loop 1 market state : step  478 market state fut market_state    4.0\n",
            "Name: 2013-07-15 00:00:00, dtype: float64  future value 0.5007250352379704\n",
            "loop 1 market state : step  479 market state fut market_state    4.0\n",
            "Name: 2013-07-16 00:00:00, dtype: float64  future value 0.49979772425326013\n",
            "loop 1 market state : step  480 market state fut market_state    4.0\n",
            "Name: 2013-07-17 00:00:00, dtype: float64  future value 0.4978928841880905\n",
            "loop 1 market state : step  481 market state fut market_state    4.0\n",
            "Name: 2013-07-18 00:00:00, dtype: float64  future value 0.49916573362616595\n",
            "loop 1 market state : step  482 market state fut market_state    2.0\n",
            "Name: 2013-07-19 00:00:00, dtype: float64  future value 0.4995791896279729\n",
            "loop 1 market state : step  483 market state fut market_state    2.0\n",
            "Name: 2013-07-22 00:00:00, dtype: float64  future value 0.4977127430196089\n",
            "loop 1 market state : step  484 market state fut market_state    2.0\n",
            "Name: 2013-07-23 00:00:00, dtype: float64  future value 0.4978987965075623\n",
            "loop 1 market state : step  485 market state fut market_state    2.0\n",
            "Name: 2013-07-24 00:00:00, dtype: float64  future value 0.4978308783684793\n",
            "loop 1 market state : step  486 market state fut market_state    4.0\n",
            "Name: 2013-07-25 00:00:00, dtype: float64  future value 0.5040739614013697\n",
            "loop 1 market state : step  487 market state fut market_state    4.0\n",
            "Name: 2013-07-26 00:00:00, dtype: float64  future value 0.5049008737003045\n",
            "loop 1 market state : step  488 market state fut market_state    4.0\n",
            "Name: 2013-07-29 00:00:00, dtype: float64  future value 0.5041537038840757\n",
            "loop 1 market state : step  489 market state fut market_state    4.0\n",
            "Name: 2013-07-30 00:00:00, dtype: float64  future value 0.5012684151984717\n",
            "loop 1 market state : step  490 market state fut market_state    3.0\n",
            "Name: 2013-07-31 00:00:00, dtype: float64  future value 0.49936065529800633\n",
            "loop 1 market state : step  491 market state fut market_state    2.0\n",
            "Name: 2013-08-01 00:00:00, dtype: float64  future value 0.5013008960404849\n",
            "loop 1 market state : step  492 market state fut market_state    2.0\n",
            "Name: 2013-08-02 00:00:00, dtype: float64  future value 0.4995112717842106\n",
            "loop 1 market state : step  493 market state fut market_state    2.0\n",
            "Name: 2013-08-05 00:00:00, dtype: float64  future value 0.49893537495257645\n",
            "loop 1 market state : step  494 market state fut market_state    2.0\n",
            "Name: 2013-08-06 00:00:00, dtype: float64  future value 0.5003204474200504\n",
            "loop 1 market state : step  495 market state fut market_state    2.0\n",
            "Name: 2013-08-07 00:00:00, dtype: float64  future value 0.4977304796827036\n",
            "loop 1 market state : step  496 market state fut market_state    2.0\n",
            "Name: 2013-08-08 00:00:00, dtype: float64  future value 0.49062209118939354\n",
            "loop 1 market state : step  497 market state fut market_state    2.0\n",
            "Name: 2013-08-09 00:00:00, dtype: float64  future value 0.4890007837579779\n",
            "loop 1 market state : step  498 market state fut market_state    2.0\n",
            "Name: 2013-08-12 00:00:00, dtype: float64  future value 0.48611553139681407\n",
            "loop 1 market state : step  499 market state fut market_state    2.0\n",
            "Name: 2013-08-13 00:00:00, dtype: float64  future value 0.48797307379217136\n",
            "loop 1 market state : step  500 market state fut market_state    0.0\n",
            "Name: 2013-08-14 00:00:00, dtype: float64  future value 0.48515278311503407\n",
            "loop 1 market state : step  501 market state fut market_state    0.0\n",
            "Name: 2013-08-15 00:00:00, dtype: float64  future value 0.48933449757239955\n",
            "loop 1 market state : step  502 market state fut market_state    2.0\n",
            "Name: 2013-08-16 00:00:00, dtype: float64  future value 0.4912659061601107\n",
            "loop 1 market state : step  503 market state fut market_state    2.0\n",
            "Name: 2013-08-19 00:00:00, dtype: float64  future value 0.4892813599366754\n",
            "loop 1 market state : step  504 market state fut market_state    0.0\n",
            "Name: 2013-08-20 00:00:00, dtype: float64  future value 0.4815144122937296\n",
            "loop 1 market state : step  505 market state fut market_state    0.0\n",
            "Name: 2013-08-21 00:00:00, dtype: float64  future value 0.4828374432077933\n",
            "loop 1 market state : step  506 market state fut market_state    0.0\n",
            "Name: 2013-08-22 00:00:00, dtype: float64  future value 0.48378544701533416\n",
            "loop 1 market state : step  507 market state fut market_state    0.0\n",
            "Name: 2013-08-23 00:00:00, dtype: float64  future value 0.48224975806165593\n",
            "loop 1 market state : step  508 market state fut market_state    0.0\n",
            "Name: 2013-08-26 00:00:00, dtype: float64  future value 0.4842579529723371\n",
            "loop 1 market state : step  509 market state fut market_state    2.0\n",
            "Name: 2013-08-27 00:00:00, dtype: float64  future value 0.4881886519624021\n",
            "loop 1 market state : step  510 market state fut market_state    2.0\n",
            "Name: 2013-08-28 00:00:00, dtype: float64  future value 0.4887792932682754\n",
            "loop 1 market state : step  511 market state fut market_state    2.0\n",
            "Name: 2013-08-29 00:00:00, dtype: float64  future value 0.48880589811525715\n",
            "loop 1 market state : step  512 market state fut market_state    2.0\n",
            "Name: 2013-08-30 00:00:00, dtype: float64  future value 0.4936904772032151\n",
            "loop 1 market state : step  513 market state fut market_state    2.0\n",
            "Name: 2013-09-03 00:00:00, dtype: float64  future value 0.497317023385576\n",
            "loop 1 market state : step  514 market state fut market_state    3.0\n",
            "Name: 2013-09-04 00:00:00, dtype: float64  future value 0.4988349759714802\n",
            "loop 1 market state : step  515 market state fut market_state    2.0\n",
            "Name: 2013-09-05 00:00:00, dtype: float64  future value 0.49714870656071747\n",
            "loop 1 market state : step  516 market state fut market_state    2.0\n",
            "Name: 2013-09-06 00:00:00, dtype: float64  future value 0.49849830599732253\n",
            "loop 1 market state : step  517 market state fut market_state    2.0\n",
            "Name: 2013-09-09 00:00:00, dtype: float64  future value 0.5013363333375547\n",
            "loop 1 market state : step  518 market state fut market_state    2.0\n",
            "Name: 2013-09-10 00:00:00, dtype: float64  future value 0.5034508392534832\n",
            "loop 1 market state : step  519 market state fut market_state    4.0\n",
            "Name: 2013-09-11 00:00:00, dtype: float64  future value 0.5095816989616545\n",
            "loop 1 market state : step  520 market state fut market_state    4.0\n",
            "Name: 2013-09-12 00:00:00, dtype: float64  future value 0.5086425633380007\n",
            "loop 1 market state : step  521 market state fut market_state    4.0\n",
            "Name: 2013-09-13 00:00:00, dtype: float64  future value 0.5049717477038026\n",
            "loop 1 market state : step  522 market state fut market_state    4.0\n",
            "Name: 2013-09-16 00:00:00, dtype: float64  future value 0.5025884899527995\n",
            "loop 1 market state : step  523 market state fut market_state    2.0\n",
            "Name: 2013-09-17 00:00:00, dtype: float64  future value 0.5012831957018304\n",
            "loop 1 market state : step  524 market state fut market_state    1.0\n",
            "Name: 2013-09-18 00:00:00, dtype: float64  future value 0.49990994757797935\n",
            "loop 1 market state : step  525 market state fut market_state    2.0\n",
            "Name: 2013-09-19 00:00:00, dtype: float64  future value 0.5016523465180013\n",
            "loop 1 market state : step  526 market state fut market_state    2.0\n",
            "Name: 2013-09-20 00:00:00, dtype: float64  future value 0.49960871460557094\n",
            "loop 1 market state : step  527 market state fut market_state    2.0\n",
            "Name: 2013-09-23 00:00:00, dtype: float64  future value 0.4965964584163291\n",
            "loop 1 market state : step  528 market state fut market_state    2.0\n",
            "Name: 2013-09-24 00:00:00, dtype: float64  future value 0.500568506727615\n",
            "loop 1 market state : step  529 market state fut market_state    3.0\n",
            "Name: 2013-09-25 00:00:00, dtype: float64  future value 0.5002347929131934\n",
            "loop 1 market state : step  530 market state fut market_state    2.0\n",
            "Name: 2013-09-26 00:00:00, dtype: float64  future value 0.49574297729953243\n",
            "loop 1 market state : step  531 market state fut market_state    2.0\n",
            "Name: 2013-09-27 00:00:00, dtype: float64  future value 0.4992395637894001\n",
            "loop 1 market state : step  532 market state fut market_state    2.0\n",
            "Name: 2013-09-30 00:00:00, dtype: float64  future value 0.4949928513235679\n",
            "loop 1 market state : step  533 market state fut market_state    1.0\n",
            "Name: 2013-10-01 00:00:00, dtype: float64  future value 0.4888885604332587\n",
            "loop 1 market state : step  534 market state fut market_state    2.0\n",
            "Name: 2013-10-02 00:00:00, dtype: float64  future value 0.48916913661195616\n",
            "loop 1 market state : step  535 market state fut market_state    3.0\n",
            "Name: 2013-10-03 00:00:00, dtype: float64  future value 0.4998479417583681\n",
            "loop 1 market state : step  536 market state fut market_state    3.0\n",
            "Name: 2013-10-04 00:00:00, dtype: float64  future value 0.5029901216109836\n",
            "loop 1 market state : step  537 market state fut market_state    3.0\n",
            "Name: 2013-10-07 00:00:00, dtype: float64  future value 0.5050396658428856\n",
            "loop 1 market state : step  538 market state fut market_state    3.0\n",
            "Name: 2013-10-08 00:00:00, dtype: float64  future value 0.5014722053495196\n",
            "loop 1 market state : step  539 market state fut market_state    3.0\n",
            "Name: 2013-10-09 00:00:00, dtype: float64  future value 0.5084063283740591\n",
            "loop 1 market state : step  540 market state fut market_state    4.0\n",
            "Name: 2013-10-10 00:00:00, dtype: float64  future value 0.5118349967248437\n",
            "loop 1 market state : step  541 market state fut market_state    4.0\n",
            "Name: 2013-10-11 00:00:00, dtype: float64  future value 0.515186879047979\n",
            "loop 1 market state : step  542 market state fut market_state    4.0\n",
            "Name: 2013-10-14 00:00:00, dtype: float64  future value 0.515234140393351\n",
            "loop 1 market state : step  543 market state fut market_state    4.0\n",
            "Name: 2013-10-15 00:00:00, dtype: float64  future value 0.5181903030824535\n",
            "loop 1 market state : step  544 market state fut market_state    4.0\n",
            "Name: 2013-10-16 00:00:00, dtype: float64  future value 0.5157420833521031\n",
            "loop 1 market state : step  545 market state fut market_state    4.0\n",
            "Name: 2013-10-17 00:00:00, dtype: float64  future value 0.5174224404433941\n",
            "loop 1 market state : step  546 market state fut market_state    4.0\n",
            "Name: 2013-10-18 00:00:00, dtype: float64  future value 0.5196964313247346\n",
            "loop 1 market state : step  547 market state fut market_state    4.0\n",
            "Name: 2013-10-21 00:00:00, dtype: float64  future value 0.5203874713163835\n",
            "loop 1 market state : step  548 market state fut market_state    4.0\n",
            "Name: 2013-10-22 00:00:00, dtype: float64  future value 0.523293416500378\n",
            "loop 1 market state : step  549 market state fut market_state    4.0\n",
            "Name: 2013-10-23 00:00:00, dtype: float64  future value 0.5207418779536358\n",
            "loop 1 market state : step  550 market state fut market_state    4.0\n",
            "Name: 2013-10-24 00:00:00, dtype: float64  future value 0.5187425512268417\n",
            "loop 1 market state : step  551 market state fut market_state    4.0\n",
            "Name: 2013-10-25 00:00:00, dtype: float64  future value 0.5202486794691229\n",
            "loop 1 market state : step  552 market state fut market_state    4.0\n",
            "Name: 2013-10-28 00:00:00, dtype: float64  future value 0.5221062578935999\n",
            "loop 1 market state : step  553 market state fut market_state    2.0\n",
            "Name: 2013-10-29 00:00:00, dtype: float64  future value 0.52064144294342\n",
            "loop 1 market state : step  554 market state fut market_state    3.0\n",
            "Name: 2013-10-30 00:00:00, dtype: float64  future value 0.5228622598645959\n",
            "loop 1 market state : step  555 market state fut market_state    2.0\n",
            "Name: 2013-10-31 00:00:00, dtype: float64  future value 0.5159694858659567\n",
            "loop 1 market state : step  556 market state fut market_state    3.0\n",
            "Name: 2013-11-01 00:00:00, dtype: float64  future value 0.522897696866345\n",
            "loop 1 market state : step  557 market state fut market_state    3.0\n",
            "Name: 2013-11-04 00:00:00, dtype: float64  future value 0.5232757161617235\n",
            "loop 1 market state : step  558 market state fut market_state    3.0\n",
            "Name: 2013-11-05 00:00:00, dtype: float64  future value 0.5220353475656613\n",
            "loop 1 market state : step  559 market state fut market_state    4.0\n",
            "Name: 2013-11-06 00:00:00, dtype: float64  future value 0.5262614035331032\n",
            "loop 1 market state : step  560 market state fut market_state    4.0\n",
            "Name: 2013-11-07 00:00:00, dtype: float64  future value 0.5288070660848139\n",
            "loop 1 market state : step  561 market state fut market_state    4.0\n",
            "Name: 2013-11-08 00:00:00, dtype: float64  future value 0.5310397076449334\n",
            "loop 1 market state : step  562 market state fut market_state    4.0\n",
            "Name: 2013-11-11 00:00:00, dtype: float64  future value 0.5290758179198884\n",
            "loop 1 market state : step  563 market state fut market_state    4.0\n",
            "Name: 2013-11-12 00:00:00, dtype: float64  future value 0.527994934289238\n",
            "loop 1 market state : step  564 market state fut market_state    2.0\n",
            "Name: 2013-11-13 00:00:00, dtype: float64  future value 0.5260753500451498\n",
            "loop 1 market state : step  565 market state fut market_state    4.0\n",
            "Name: 2013-11-14 00:00:00, dtype: float64  future value 0.5303515874885801\n",
            "loop 1 market state : step  566 market state fut market_state    4.0\n",
            "Name: 2013-11-15 00:00:00, dtype: float64  future value 0.5329829045471479\n",
            "loop 1 market state : step  567 market state fut market_state    4.0\n",
            "Name: 2013-11-18 00:00:00, dtype: float64  future value 0.5323095645988327\n",
            "loop 1 market state : step  568 market state fut market_state    4.0\n",
            "Name: 2013-11-19 00:00:00, dtype: float64  future value 0.5323893070815386\n",
            "loop 1 market state : step  569 market state fut market_state    4.0\n",
            "Name: 2013-11-20 00:00:00, dtype: float64  future value 0.5337123377002818\n",
            "loop 1 market state : step  570 market state fut market_state    4.0\n",
            "Name: 2013-11-21 00:00:00, dtype: float64  future value 0.5332930057034433\n",
            "loop 1 market state : step  571 market state fut market_state    2.0\n",
            "Name: 2013-11-22 00:00:00, dtype: float64  future value 0.5318429709613014\n",
            "loop 1 market state : step  572 market state fut market_state    2.0\n",
            "Name: 2013-11-25 00:00:00, dtype: float64  future value 0.5301448772069157\n",
            "loop 1 market state : step  573 market state fut market_state    2.0\n",
            "Name: 2013-11-26 00:00:00, dtype: float64  future value 0.5294538372152668\n",
            "loop 1 market state : step  574 market state fut market_state    2.0\n",
            "Name: 2013-11-27 00:00:00, dtype: float64  future value 0.5271562336758002\n",
            "loop 1 market state : step  575 market state fut market_state    2.0\n",
            "Name: 2013-11-29 00:00:00, dtype: float64  future value 0.5330803473685082\n",
            "loop 1 market state : step  576 market state fut market_state    4.0\n",
            "Name: 2013-12-02 00:00:00, dtype: float64  future value 0.5340490076744393\n",
            "loop 1 market state : step  577 market state fut market_state    3.0\n",
            "Name: 2013-12-03 00:00:00, dtype: float64  future value 0.5323509139200536\n",
            "loop 1 market state : step  578 market state fut market_state    2.0\n",
            "Name: 2013-12-04 00:00:00, dtype: float64  future value 0.5263263655124504\n",
            "loop 1 market state : step  579 market state fut market_state    2.0\n",
            "Name: 2013-12-05 00:00:00, dtype: float64  future value 0.524341819289015\n",
            "loop 1 market state : step  580 market state fut market_state    2.0\n",
            "Name: 2013-12-06 00:00:00, dtype: float64  future value 0.5242886456241712\n",
            "loop 1 market state : step  581 market state fut market_state    2.0\n",
            "Name: 2013-12-09 00:00:00, dtype: float64  future value 0.5276021708149411\n",
            "loop 1 market state : step  582 market state fut market_state    2.0\n",
            "Name: 2013-12-10 00:00:00, dtype: float64  future value 0.5259660828801666\n",
            "loop 1 market state : step  583 market state fut market_state    4.0\n",
            "Name: 2013-12-11 00:00:00, dtype: float64  future value 0.5347223473274337\n",
            "loop 1 market state : step  584 market state fut market_state    4.0\n",
            "Name: 2013-12-12 00:00:00, dtype: float64  future value 0.534412246466459\n",
            "loop 1 market state : step  585 market state fut market_state    4.0\n",
            "Name: 2013-12-13 00:00:00, dtype: float64  future value 0.536987433700447\n",
            "loop 1 market state : step  586 market state fut market_state    4.0\n",
            "Name: 2013-12-16 00:00:00, dtype: float64  future value 0.539843197408453\n",
            "loop 1 market state : step  587 market state fut market_state    4.0\n",
            "Name: 2013-12-17 00:00:00, dtype: float64  future value 0.5414172434944967\n",
            "loop 1 market state : step  588 market state fut market_state    4.0\n",
            "Name: 2013-12-18 00:00:00, dtype: float64  future value 0.5439865550287738\n",
            "loop 1 market state : step  589 market state fut market_state    4.0\n",
            "Name: 2013-12-19 00:00:00, dtype: float64  future value 0.5438034574052356\n",
            "loop 1 market state : step  590 market state fut market_state    4.0\n",
            "Name: 2013-12-20 00:00:00, dtype: float64  future value 0.5437059785547557\n",
            "loop 1 market state : step  591 market state fut market_state    4.0\n",
            "Name: 2013-12-23 00:00:00, dtype: float64  future value 0.5458588776321692\n",
            "loop 1 market state : step  592 market state fut market_state    2.0\n",
            "Name: 2013-12-24 00:00:00, dtype: float64  future value 0.5410215238604638\n",
            "loop 1 market state : step  593 market state fut market_state    2.0\n",
            "Name: 2013-12-26 00:00:00, dtype: float64  future value 0.5408413826919822\n",
            "loop 1 market state : step  594 market state fut market_state    2.0\n",
            "Name: 2013-12-27 00:00:00, dtype: float64  future value 0.5394829150714899\n",
            "loop 1 market state : step  595 market state fut market_state    2.0\n",
            "Name: 2013-12-30 00:00:00, dtype: float64  future value 0.5427639230958062\n",
            "loop 1 market state : step  596 market state fut market_state    2.0\n",
            "Name: 2013-12-31 00:00:00, dtype: float64  future value 0.5426487436113512\n",
            "loop 1 market state : step  597 market state fut market_state    3.0\n",
            "Name: 2014-01-02 00:00:00, dtype: float64  future value 0.5428377532590404\n",
            "loop 1 market state : step  598 market state fut market_state    3.0\n",
            "Name: 2014-01-03 00:00:00, dtype: float64  future value 0.5440899098742853\n",
            "loop 1 market state : step  599 market state fut market_state    2.0\n",
            "Name: 2014-01-06 00:00:00, dtype: float64  future value 0.5372473173516344\n",
            "loop 1 market state : step  600 market state fut market_state    3.0\n",
            "Name: 2014-01-07 00:00:00, dtype: float64  future value 0.5430592437487429\n",
            "loop 1 market state : step  601 market state fut market_state    4.0\n",
            "Name: 2014-01-08 00:00:00, dtype: float64  future value 0.545864789951641\n",
            "loop 1 market state : step  602 market state fut market_state    3.0\n",
            "Name: 2014-01-09 00:00:00, dtype: float64  future value 0.5451294444790353\n",
            "loop 1 market state : step  603 market state fut market_state    2.0\n",
            "Name: 2014-01-10 00:00:00, dtype: float64  future value 0.5430060700838991\n",
            "loop 1 market state : step  604 market state fut market_state    3.0\n",
            "Name: 2014-01-13 00:00:00, dtype: float64  future value 0.5445122343552999\n",
            "loop 1 market state : step  605 market state fut market_state    3.0\n",
            "Name: 2014-01-14 00:00:00, dtype: float64  future value 0.544825255346891\n",
            "loop 1 market state : step  606 market state fut market_state    2.0\n",
            "Name: 2014-01-15 00:00:00, dtype: float64  future value 0.5399819895510344\n",
            "loop 1 market state : step  607 market state fut market_state    1.0\n",
            "Name: 2014-01-16 00:00:00, dtype: float64  future value 0.5287096232634535\n",
            "loop 1 market state : step  608 market state fut market_state    1.0\n",
            "Name: 2014-01-17 00:00:00, dtype: float64  future value 0.5261314798697296\n",
            "loop 1 market state : step  609 market state fut market_state    2.0\n",
            "Name: 2014-01-21 00:00:00, dtype: float64  future value 0.529362270388938\n",
            "loop 1 market state : step  610 market state fut market_state    0.0\n",
            "Name: 2014-01-22 00:00:00, dtype: float64  future value 0.5239578879694854\n",
            "loop 1 market state : step  611 market state fut market_state    2.0\n",
            "Name: 2014-01-23 00:00:00, dtype: float64  future value 0.5298613448684825\n",
            "loop 1 market state : step  612 market state fut market_state    2.0\n",
            "Name: 2014-01-24 00:00:00, dtype: float64  future value 0.5264356326774337\n",
            "loop 1 market state : step  613 market state fut market_state    0.0\n",
            "Name: 2014-01-27 00:00:00, dtype: float64  future value 0.5144160965736241\n",
            "loop 1 market state : step  614 market state fut market_state    0.0\n",
            "Name: 2014-01-28 00:00:00, dtype: float64  future value 0.5183467955636891\n",
            "loop 1 market state : step  615 market state fut market_state    0.0\n",
            "Name: 2014-01-29 00:00:00, dtype: float64  future value 0.5172954729397564\n",
            "loop 1 market state : step  616 market state fut market_state    0.0\n",
            "Name: 2014-01-30 00:00:00, dtype: float64  future value 0.5237305214847514\n",
            "loop 1 market state : step  617 market state fut market_state    2.0\n",
            "Name: 2014-01-31 00:00:00, dtype: float64  future value 0.5306971256466246\n",
            "loop 1 market state : step  618 market state fut market_state    2.0\n",
            "Name: 2014-02-03 00:00:00, dtype: float64  future value 0.5315299139405908\n",
            "loop 1 market state : step  619 market state fut market_state    2.0\n",
            "Name: 2014-02-04 00:00:00, dtype: float64  future value 0.5374097581814616\n",
            "loop 1 market state : step  620 market state fut market_state    2.0\n",
            "Name: 2014-02-05 00:00:00, dtype: float64  future value 0.5372650540147292\n",
            "loop 1 market state : step  621 market state fut market_state    2.0\n",
            "Name: 2014-02-06 00:00:00, dtype: float64  future value 0.5403865773689542\n",
            "loop 1 market state : step  622 market state fut market_state    3.0\n",
            "Name: 2014-02-07 00:00:00, dtype: float64  future value 0.5429854135855088\n",
            "loop 1 market state : step  623 market state fut market_state    3.0\n",
            "Name: 2014-02-10 00:00:00, dtype: float64  future value 0.5436144480528672\n",
            "loop 1 market state : step  624 market state fut market_state    3.0\n",
            "Name: 2014-02-11 00:00:00, dtype: float64  future value 0.5400676440578914\n",
            "loop 1 market state : step  625 market state fut market_state    3.0\n",
            "Name: 2014-02-12 00:00:00, dtype: float64  future value 0.5433250394240816\n",
            "loop 1 market state : step  626 market state fut market_state    2.0\n",
            "Name: 2014-02-13 00:00:00, dtype: float64  future value 0.5422825489549162\n",
            "loop 1 market state : step  627 market state fut market_state    3.0\n",
            "Name: 2014-02-14 00:00:00, dtype: float64  future value 0.5456373871424668\n",
            "loop 1 market state : step  628 market state fut market_state    3.0\n",
            "Name: 2014-02-18 00:00:00, dtype: float64  future value 0.5449020416698611\n",
            "loop 1 market state : step  629 market state fut market_state    3.0\n",
            "Name: 2014-02-19 00:00:00, dtype: float64  future value 0.544913866013484\n",
            "loop 1 market state : step  630 market state fut market_state    4.0\n",
            "Name: 2014-02-20 00:00:00, dtype: float64  future value 0.5476101450513988\n",
            "loop 1 market state : step  631 market state fut market_state    4.0\n",
            "Name: 2014-02-21 00:00:00, dtype: float64  future value 0.5491339736323345\n",
            "loop 1 market state : step  632 market state fut market_state    1.0\n",
            "Name: 2014-02-24 00:00:00, dtype: float64  future value 0.5450821828383426\n",
            "loop 1 market state : step  633 market state fut market_state    4.0\n",
            "Name: 2014-02-25 00:00:00, dtype: float64  future value 0.5534043347854125\n",
            "loop 1 market state : step  634 market state fut market_state    4.0\n",
            "Name: 2014-02-26 00:00:00, dtype: float64  future value 0.5533748101031352\n",
            "loop 1 market state : step  635 market state fut market_state    4.0\n",
            "Name: 2014-02-27 00:00:00, dtype: float64  future value 0.5543257337459716\n",
            "loop 1 market state : step  636 market state fut market_state    4.0\n",
            "Name: 2014-02-28 00:00:00, dtype: float64  future value 0.5546240105586442\n",
            "loop 1 market state : step  637 market state fut market_state    4.0\n",
            "Name: 2014-03-03 00:00:00, dtype: float64  future value 0.5543670830671926\n",
            "loop 1 market state : step  638 market state fut market_state    2.0\n",
            "Name: 2014-03-04 00:00:00, dtype: float64  future value 0.5515497125206715\n",
            "loop 1 market state : step  639 market state fut market_state    2.0\n",
            "Name: 2014-03-05 00:00:00, dtype: float64  future value 0.5517180293455302\n",
            "loop 1 market state : step  640 market state fut market_state    2.0\n",
            "Name: 2014-03-06 00:00:00, dtype: float64  future value 0.5452623243021448\n",
            "loop 1 market state : step  641 market state fut market_state    2.0\n",
            "Name: 2014-03-07 00:00:00, dtype: float64  future value 0.5437237152178503\n",
            "loop 1 market state : step  642 market state fut market_state    2.0\n",
            "Name: 2014-03-10 00:00:00, dtype: float64  future value 0.548950876304117\n",
            "loop 1 market state : step  643 market state fut market_state    3.0\n",
            "Name: 2014-03-11 00:00:00, dtype: float64  future value 0.5529140924606355\n",
            "loop 1 market state : step  644 market state fut market_state    2.0\n",
            "Name: 2014-03-12 00:00:00, dtype: float64  future value 0.5495238172713359\n",
            "loop 1 market state : step  645 market state fut market_state    3.0\n",
            "Name: 2014-03-13 00:00:00, dtype: float64  future value 0.5528432184571374\n",
            "loop 1 market state : step  646 market state fut market_state    3.0\n",
            "Name: 2014-03-14 00:00:00, dtype: float64  future value 0.5512219110257216\n",
            "loop 1 market state : step  647 market state fut market_state    2.0\n",
            "Name: 2014-03-17 00:00:00, dtype: float64  future value 0.5485403761667254\n",
            "loop 1 market state : step  648 market state fut market_state    2.0\n",
            "Name: 2014-03-18 00:00:00, dtype: float64  future value 0.5509561150550623\n",
            "loop 1 market state : step  649 market state fut market_state    2.0\n",
            "Name: 2014-03-19 00:00:00, dtype: float64  future value 0.5470992462282315\n",
            "loop 1 market state : step  650 market state fut market_state    2.0\n",
            "Name: 2014-03-20 00:00:00, dtype: float64  future value 0.5460597116234814\n",
            "loop 1 market state : step  651 market state fut market_state    2.0\n",
            "Name: 2014-03-21 00:00:00, dtype: float64  future value 0.5485935498315692\n",
            "loop 1 market state : step  652 market state fut market_state    3.0\n",
            "Name: 2014-03-24 00:00:00, dtype: float64  future value 0.5529406612784976\n",
            "loop 1 market state : step  653 market state fut market_state    4.0\n",
            "Name: 2014-03-25 00:00:00, dtype: float64  future value 0.5568330034315179\n",
            "loop 1 market state : step  654 market state fut market_state    4.0\n",
            "Name: 2014-03-26 00:00:00, dtype: float64  future value 0.5584218297255996\n",
            "loop 1 market state : step  655 market state fut market_state    4.0\n",
            "Name: 2014-03-27 00:00:00, dtype: float64  future value 0.557792795553562\n",
            "loop 1 market state : step  656 market state fut market_state    3.0\n",
            "Name: 2014-03-28 00:00:00, dtype: float64  future value 0.550799586544707\n",
            "loop 1 market state : step  657 market state fut market_state    1.0\n",
            "Name: 2014-03-31 00:00:00, dtype: float64  future value 0.5448784290117349\n",
            "loop 1 market state : step  658 market state fut market_state    2.0\n",
            "Name: 2014-04-01 00:00:00, dtype: float64  future value 0.5469220248950455\n",
            "loop 1 market state : step  659 market state fut market_state    2.0\n",
            "Name: 2014-04-02 00:00:00, dtype: float64  future value 0.5528934359622452\n",
            "loop 1 market state : step  660 market state fut market_state    0.0\n",
            "Name: 2014-04-03 00:00:00, dtype: float64  future value 0.5413463694909983\n",
            "loop 1 market state : step  661 market state fut market_state    0.0\n",
            "Name: 2014-04-04 00:00:00, dtype: float64  future value 0.5362107389066203\n",
            "loop 1 market state : step  662 market state fut market_state    0.0\n",
            "Name: 2014-04-07 00:00:00, dtype: float64  future value 0.5406169360425438\n",
            "loop 1 market state : step  663 market state fut market_state    1.0\n",
            "Name: 2014-04-08 00:00:00, dtype: float64  future value 0.5442700510427668\n",
            "loop 1 market state : step  664 market state fut market_state    1.0\n",
            "Name: 2014-04-09 00:00:00, dtype: float64  future value 0.5499786225943638\n",
            "loop 1 market state : step  665 market state fut market_state    2.0\n",
            "Name: 2014-04-10 00:00:00, dtype: float64  future value 0.5507287125412087\n",
            "loop 1 market state : step  666 market state fut market_state    2.0\n",
            "Name: 2014-04-11 00:00:00, dtype: float64  future value 0.5528077814553881\n",
            "loop 1 market state : step  667 market state fut market_state    3.0\n",
            "Name: 2014-04-14 00:00:00, dtype: float64  future value 0.5550699476977851\n",
            "loop 1 market state : step  668 market state fut market_state    3.0\n",
            "Name: 2014-04-15 00:00:00, dtype: float64  future value 0.5538414037406664\n",
            "loop 1 market state : step  669 market state fut market_state    3.0\n",
            "Name: 2014-04-16 00:00:00, dtype: float64  future value 0.5547923273835028\n",
            "loop 1 market state : step  670 market state fut market_state    2.0\n",
            "Name: 2014-04-17 00:00:00, dtype: float64  future value 0.5503005117698418\n",
            "loop 1 market state : step  671 market state fut market_state    2.0\n",
            "Name: 2014-04-21 00:00:00, dtype: float64  future value 0.5520813041666695\n",
            "loop 1 market state : step  672 market state fut market_state    2.0\n",
            "Name: 2014-04-22 00:00:00, dtype: float64  future value 0.5547096290363815\n",
            "loop 1 market state : step  673 market state fut market_state    3.0\n",
            "Name: 2014-04-23 00:00:00, dtype: float64  future value 0.5563693296292823\n",
            "loop 1 market state : step  674 market state fut market_state    3.0\n",
            "Name: 2014-04-24 00:00:00, dtype: float64  future value 0.5562896234710166\n",
            "loop 1 market state : step  675 market state fut market_state    3.0\n",
            "Name: 2014-04-25 00:00:00, dtype: float64  future value 0.555539497495052\n",
            "loop 1 market state : step  676 market state fut market_state    3.0\n",
            "Name: 2014-04-28 00:00:00, dtype: float64  future value 0.5565790318044815\n",
            "loop 1 market state : step  677 market state fut market_state    2.0\n",
            "Name: 2014-04-29 00:00:00, dtype: float64  future value 0.5515762813385335\n",
            "loop 1 market state : step  678 market state fut market_state    2.0\n",
            "Name: 2014-04-30 00:00:00, dtype: float64  future value 0.5546741920346325\n",
            "loop 1 market state : step  679 market state fut market_state    2.0\n",
            "Name: 2014-05-01 00:00:00, dtype: float64  future value 0.5539122777441646\n",
            "loop 1 market state : step  680 market state fut market_state    2.0\n",
            "Name: 2014-05-02 00:00:00, dtype: float64  future value 0.5547539342220178\n",
            "loop 1 market state : step  681 market state fut market_state    4.0\n",
            "Name: 2014-05-05 00:00:00, dtype: float64  future value 0.5601199234799853\n",
            "loop 1 market state : step  682 market state fut market_state    4.0\n",
            "Name: 2014-05-06 00:00:00, dtype: float64  future value 0.560356158443927\n",
            "loop 1 market state : step  683 market state fut market_state    3.0\n",
            "Name: 2014-05-07 00:00:00, dtype: float64  future value 0.5577219212547431\n",
            "loop 1 market state : step  684 market state fut market_state    1.0\n",
            "Name: 2014-05-08 00:00:00, dtype: float64  future value 0.5525006364588286\n",
            "loop 1 market state : step  685 market state fut market_state    1.0\n",
            "Name: 2014-05-09 00:00:00, dtype: float64  future value 0.5545708368938004\n",
            "loop 1 market state : step  686 market state fut market_state    2.0\n",
            "Name: 2014-05-12 00:00:00, dtype: float64  future value 0.5567030434437039\n",
            "loop 1 market state : step  687 market state fut market_state    2.0\n",
            "Name: 2014-05-13 00:00:00, dtype: float64  future value 0.55308536544523\n",
            "loop 1 market state : step  688 market state fut market_state    2.0\n",
            "Name: 2014-05-14 00:00:00, dtype: float64  future value 0.5575742609282748\n",
            "loop 1 market state : step  689 market state fut market_state    3.0\n",
            "Name: 2014-05-15 00:00:00, dtype: float64  future value 0.5588913795228666\n",
            "loop 1 market state : step  690 market state fut market_state    4.0\n",
            "Name: 2014-05-16 00:00:00, dtype: float64  future value 0.5612657690899828\n",
            "loop 1 market state : step  691 market state fut market_state    4.0\n",
            "Name: 2014-05-19 00:00:00, dtype: float64  future value 0.5646265195970052\n",
            "loop 1 market state : step  692 market state fut market_state    4.0\n",
            "Name: 2014-05-20 00:00:00, dtype: float64  future value 0.5639974851296468\n",
            "loop 1 market state : step  693 market state fut market_state    4.0\n",
            "Name: 2014-05-21 00:00:00, dtype: float64  future value 0.5670245218222474\n",
            "loop 1 market state : step  694 market state fut market_state    4.0\n",
            "Name: 2014-05-22 00:00:00, dtype: float64  future value 0.568069932422029\n",
            "loop 1 market state : step  695 market state fut market_state    4.0\n",
            "Name: 2014-05-23 00:00:00, dtype: float64  future value 0.5684833887191566\n",
            "loop 1 market state : step  696 market state fut market_state    4.0\n",
            "Name: 2014-05-27 00:00:00, dtype: float64  future value 0.5682678102536052\n",
            "loop 1 market state : step  697 market state fut market_state    4.0\n",
            "Name: 2014-05-28 00:00:00, dtype: float64  future value 0.5693427818601045\n",
            "loop 1 market state : step  698 market state fut market_state    4.0\n",
            "Name: 2014-05-29 00:00:00, dtype: float64  future value 0.5730579026799387\n",
            "loop 1 market state : step  699 market state fut market_state    4.0\n",
            "Name: 2014-05-30 00:00:00, dtype: float64  future value 0.5757098762368967\n",
            "loop 1 market state : step  700 market state fut market_state    4.0\n",
            "Name: 2014-06-02 00:00:00, dtype: float64  future value 0.5762503363621023\n",
            "loop 1 market state : step  701 market state fut market_state    4.0\n",
            "Name: 2014-06-03 00:00:00, dtype: float64  future value 0.5761085880597852\n",
            "loop 1 market state : step  702 market state fut market_state    4.0\n",
            "Name: 2014-06-04 00:00:00, dtype: float64  future value 0.5740708684668266\n",
            "loop 1 market state : step  703 market state fut market_state    2.0\n",
            "Name: 2014-06-05 00:00:00, dtype: float64  future value 0.5700013410097401\n",
            "loop 1 market state : step  704 market state fut market_state    2.0\n",
            "Name: 2014-06-06 00:00:00, dtype: float64  future value 0.5717880454307188\n",
            "loop 1 market state : step  705 market state fut market_state    2.0\n",
            "Name: 2014-06-09 00:00:00, dtype: float64  future value 0.5722664634118729\n",
            "loop 1 market state : step  706 market state fut market_state    2.0\n",
            "Name: 2014-06-10 00:00:00, dtype: float64  future value 0.5735097518432306\n",
            "loop 1 market state : step  707 market state fut market_state    4.0\n",
            "Name: 2014-06-11 00:00:00, dtype: float64  future value 0.5779366054775446\n",
            "loop 1 market state : step  708 market state fut market_state    4.0\n",
            "Name: 2014-06-12 00:00:00, dtype: float64  future value 0.5786749071098861\n",
            "loop 1 market state : step  709 market state fut market_state    4.0\n",
            "Name: 2014-06-13 00:00:00, dtype: float64  future value 0.5796760485531511\n",
            "loop 1 market state : step  710 market state fut market_state    4.0\n",
            "Name: 2014-06-16 00:00:00, dtype: float64  future value 0.5795992622301811\n",
            "loop 1 market state : step  711 market state fut market_state    3.0\n",
            "Name: 2014-06-17 00:00:00, dtype: float64  future value 0.575869360906988\n",
            "loop 1 market state : step  712 market state fut market_state    4.0\n",
            "Name: 2014-06-18 00:00:00, dtype: float64  future value 0.5786896876132449\n",
            "loop 1 market state : step  713 market state fut market_state    2.0\n",
            "Name: 2014-06-19 00:00:00, dtype: float64  future value 0.5780074797763634\n",
            "loop 1 market state : step  714 market state fut market_state    2.0\n",
            "Name: 2014-06-20 00:00:00, dtype: float64  future value 0.57911197606514\n",
            "loop 1 market state : step  715 market state fut market_state    2.0\n",
            "Name: 2014-06-23 00:00:00, dtype: float64  future value 0.5788963975995887\n",
            "loop 1 market state : step  716 market state fut market_state    4.0\n",
            "Name: 2014-06-24 00:00:00, dtype: float64  future value 0.5827621349056271\n",
            "loop 1 market state : step  717 market state fut market_state    4.0\n",
            "Name: 2014-06-25 00:00:00, dtype: float64  future value 0.5831460662251567\n",
            "loop 1 market state : step  718 market state fut market_state    4.0\n",
            "Name: 2014-06-26 00:00:00, dtype: float64  future value 0.586341419742616\n",
            "loop 1 market state : step  719 market state fut market_state    4.0\n",
            "Name: 2014-06-27 00:00:00, dtype: float64  future value 0.5840408963678536\n",
            "loop 1 market state : step  720 market state fut market_state    4.0\n",
            "Name: 2014-06-30 00:00:00, dtype: float64  future value 0.5799241078607157\n",
            "loop 1 market state : step  721 market state fut market_state    2.0\n",
            "Name: 2014-07-01 00:00:00, dtype: float64  future value 0.5826174307388946\n",
            "loop 1 market state : step  722 market state fut market_state    2.0\n",
            "Name: 2014-07-02 00:00:00, dtype: float64  future value 0.580210596358885\n",
            "loop 1 market state : step  723 market state fut market_state    2.0\n",
            "Name: 2014-07-03 00:00:00, dtype: float64  future value 0.5810640411512414\n",
            "loop 1 market state : step  724 market state fut market_state    2.0\n",
            "Name: 2014-07-07 00:00:00, dtype: float64  future value 0.5838784558333473\n",
            "loop 1 market state : step  725 market state fut market_state    3.0\n",
            "Name: 2014-07-08 00:00:00, dtype: float64  future value 0.5827503465911238\n",
            "loop 1 market state : step  726 market state fut market_state    3.0\n",
            "Name: 2014-07-09 00:00:00, dtype: float64  future value 0.5851985302923545\n",
            "loop 1 market state : step  727 market state fut market_state    2.0\n",
            "Name: 2014-07-10 00:00:00, dtype: float64  future value 0.578273275451702\n",
            "loop 1 market state : step  728 market state fut market_state    3.0\n",
            "Name: 2014-07-11 00:00:00, dtype: float64  future value 0.584209213488033\n",
            "loop 1 market state : step  729 market state fut market_state    2.0\n",
            "Name: 2014-07-14 00:00:00, dtype: float64  future value 0.5828537017319559\n",
            "loop 1 market state : step  730 market state fut market_state    3.0\n",
            "Name: 2014-07-15 00:00:00, dtype: float64  future value 0.5857773832837244\n",
            "loop 1 market state : step  731 market state fut market_state    4.0\n",
            "Name: 2014-07-16 00:00:00, dtype: float64  future value 0.5868050935448517\n",
            "loop 1 market state : step  732 market state fut market_state    4.0\n",
            "Name: 2014-07-17 00:00:00, dtype: float64  future value 0.5870915457185806\n",
            "loop 1 market state : step  733 market state fut market_state    3.0\n",
            "Name: 2014-07-18 00:00:00, dtype: float64  future value 0.5842446504897821\n",
            "loop 1 market state : step  734 market state fut market_state    3.0\n",
            "Name: 2014-07-21 00:00:00, dtype: float64  future value 0.5844130033437603\n",
            "loop 1 market state : step  735 market state fut market_state    2.0\n",
            "Name: 2014-07-22 00:00:00, dtype: float64  future value 0.5817669057818339\n",
            "loop 1 market state : step  736 market state fut market_state    2.0\n",
            "Name: 2014-07-23 00:00:00, dtype: float64  future value 0.581802342783583\n",
            "loop 1 market state : step  737 market state fut market_state    0.0\n",
            "Name: 2014-07-24 00:00:00, dtype: float64  future value 0.5701667379993032\n",
            "loop 1 market state : step  738 market state fut market_state    0.0\n",
            "Name: 2014-07-25 00:00:00, dtype: float64  future value 0.5685365620886798\n",
            "loop 1 market state : step  739 market state fut market_state    0.0\n",
            "Name: 2014-07-28 00:00:00, dtype: float64  future value 0.5726237898844208\n",
            "loop 1 market state : step  740 market state fut market_state    0.0\n",
            "Name: 2014-07-29 00:00:00, dtype: float64  future value 0.5670776594579716\n",
            "loop 1 market state : step  741 market state fut market_state    0.0\n",
            "Name: 2014-07-30 00:00:00, dtype: float64  future value 0.5670865276418586\n",
            "loop 1 market state : step  742 market state fut market_state    0.0\n",
            "Name: 2014-07-31 00:00:00, dtype: float64  future value 0.5639354432809159\n",
            "loop 1 market state : step  743 market state fut market_state    2.0\n",
            "Name: 2014-08-01 00:00:00, dtype: float64  future value 0.570438409964994\n",
            "loop 1 market state : step  744 market state fut market_state    1.0\n",
            "Name: 2014-08-04 00:00:00, dtype: float64  future value 0.5720124920801573\n",
            "loop 1 market state : step  745 market state fut market_state    2.0\n",
            "Name: 2014-08-05 00:00:00, dtype: float64  future value 0.5710763126162393\n",
            "loop 1 market state : step  746 market state fut market_state    2.0\n",
            "Name: 2014-08-06 00:00:00, dtype: float64  future value 0.5749066129205286\n",
            "loop 1 market state : step  747 market state fut market_state    2.0\n",
            "Name: 2014-08-07 00:00:00, dtype: float64  future value 0.5774050501559869\n",
            "loop 1 market state : step  748 market state fut market_state    2.0\n",
            "Name: 2014-08-08 00:00:00, dtype: float64  future value 0.5773696131542377\n",
            "loop 1 market state : step  749 market state fut market_state    2.0\n",
            "Name: 2014-08-11 00:00:00, dtype: float64  future value 0.5822955412680959\n",
            "loop 1 market state : step  750 market state fut market_state    3.0\n",
            "Name: 2014-08-12 00:00:00, dtype: float64  future value 0.5852073987715621\n",
            "loop 1 market state : step  751 market state fut market_state    3.0\n",
            "Name: 2014-08-13 00:00:00, dtype: float64  future value 0.5866574332183833\n",
            "loop 1 market state : step  752 market state fut market_state    4.0\n",
            "Name: 2014-08-14 00:00:00, dtype: float64  future value 0.5883880078147822\n",
            "loop 1 market state : step  753 market state fut market_state    4.0\n",
            "Name: 2014-08-15 00:00:00, dtype: float64  future value 0.5872155933869225\n",
            "loop 1 market state : step  754 market state fut market_state    4.0\n",
            "Name: 2014-08-18 00:00:00, dtype: float64  future value 0.5900270519092926\n",
            "loop 1 market state : step  755 market state fut market_state    4.0\n",
            "Name: 2014-08-19 00:00:00, dtype: float64  future value 0.5906472181927638\n",
            "loop 1 market state : step  756 market state fut market_state    4.0\n",
            "Name: 2014-08-20 00:00:00, dtype: float64  future value 0.5906767428750412\n",
            "loop 1 market state : step  757 market state fut market_state    4.0\n",
            "Name: 2014-08-21 00:00:00, dtype: float64  future value 0.589678557591512\n",
            "loop 1 market state : step  758 market state fut market_state    4.0\n",
            "Name: 2014-08-22 00:00:00, dtype: float64  future value 0.5916365349970854\n",
            "loop 1 market state : step  759 market state fut market_state    4.0\n",
            "Name: 2014-08-25 00:00:00, dtype: float64  future value 0.5913146455262865\n",
            "loop 1 market state : step  760 market state fut market_state    4.0\n",
            "Name: 2014-08-26 00:00:00, dtype: float64  future value 0.5908539281791075\n",
            "loop 1 market state : step  761 market state fut market_state    2.0\n",
            "Name: 2014-08-27 00:00:00, dtype: float64  future value 0.5899473094265866\n",
            "loop 1 market state : step  762 market state fut market_state    4.0\n",
            "Name: 2014-08-28 00:00:00, dtype: float64  future value 0.5929182165899282\n",
            "loop 1 market state : step  763 market state fut market_state    2.0\n",
            "Name: 2014-08-29 00:00:00, dtype: float64  future value 0.59109611119632\n",
            "loop 1 market state : step  764 market state fut market_state    2.0\n",
            "Name: 2014-09-02 00:00:00, dtype: float64  future value 0.587227381701426\n",
            "loop 1 market state : step  765 market state fut market_state    2.0\n",
            "Name: 2014-09-03 00:00:00, dtype: float64  future value 0.5893684564352166\n",
            "loop 1 market state : step  766 market state fut market_state    2.0\n",
            "Name: 2014-09-04 00:00:00, dtype: float64  future value 0.5898882237375916\n",
            "loop 1 market state : step  767 market state fut market_state    2.0\n",
            "Name: 2014-09-05 00:00:00, dtype: float64  future value 0.5863709807493336\n",
            "loop 1 market state : step  768 market state fut market_state    2.0\n",
            "Name: 2014-09-08 00:00:00, dtype: float64  future value 0.5859545685877907\n",
            "loop 1 market state : step  769 market state fut market_state    3.0\n",
            "Name: 2014-09-09 00:00:00, dtype: float64  future value 0.5903400729008836\n",
            "loop 1 market state : step  770 market state fut market_state    3.0\n",
            "Name: 2014-09-10 00:00:00, dtype: float64  future value 0.5911049433510873\n",
            "loop 1 market state : step  771 market state fut market_state    4.0\n",
            "Name: 2014-09-11 00:00:00, dtype: float64  future value 0.5939961440608426\n",
            "loop 1 market state : step  772 market state fut market_state    4.0\n",
            "Name: 2014-09-12 00:00:00, dtype: float64  future value 0.5937126477515288\n",
            "loop 1 market state : step  773 market state fut market_state    3.0\n",
            "Name: 2014-09-15 00:00:00, dtype: float64  future value 0.5889550364625292\n",
            "loop 1 market state : step  774 market state fut market_state    2.0\n",
            "Name: 2014-09-16 00:00:00, dtype: float64  future value 0.5855529369296066\n",
            "loop 1 market state : step  775 market state fut market_state    2.0\n",
            "Name: 2014-09-17 00:00:00, dtype: float64  future value 0.5901392752340118\n",
            "loop 1 market state : step  776 market state fut market_state    2.0\n",
            "Name: 2014-09-18 00:00:00, dtype: float64  future value 0.5805974475137101\n",
            "loop 1 market state : step  777 market state fut market_state    2.0\n",
            "Name: 2014-09-19 00:00:00, dtype: float64  future value 0.585576549587733\n",
            "loop 1 market state : step  778 market state fut market_state    2.0\n",
            "Name: 2014-09-22 00:00:00, dtype: float64  future value 0.5840852018488105\n",
            "loop 1 market state : step  779 market state fut market_state    2.0\n",
            "Name: 2014-09-23 00:00:00, dtype: float64  future value 0.5824579820979231\n",
            "loop 1 market state : step  780 market state fut market_state    0.0\n",
            "Name: 2014-09-24 00:00:00, dtype: float64  future value 0.5747412519600852\n",
            "loop 1 market state : step  781 market state fut market_state    0.0\n",
            "Name: 2014-09-25 00:00:00, dtype: float64  future value 0.5747442081198212\n",
            "loop 1 market state : step  782 market state fut market_state    1.0\n",
            "Name: 2014-09-26 00:00:00, dtype: float64  future value 0.5811615200017214\n",
            "loop 1 market state : step  783 market state fut market_state    0.0\n",
            "Name: 2014-09-29 00:00:00, dtype: float64  future value 0.5802519093556656\n",
            "loop 1 market state : step  784 market state fut market_state    0.0\n",
            "Name: 2014-09-30 00:00:00, dtype: float64  future value 0.5714749884100081\n",
            "loop 1 market state : step  785 market state fut market_state    2.0\n",
            "Name: 2014-10-01 00:00:00, dtype: float64  future value 0.5814538847902427\n",
            "loop 1 market state : step  786 market state fut market_state    0.0\n",
            "Name: 2014-10-02 00:00:00, dtype: float64  future value 0.5694402246814648\n",
            "loop 1 market state : step  787 market state fut market_state    0.0\n",
            "Name: 2014-10-03 00:00:00, dtype: float64  future value 0.5629195576587324\n",
            "loop 1 market state : step  788 market state fut market_state    0.0\n",
            "Name: 2014-10-06 00:00:00, dtype: float64  future value 0.5536494379332412\n",
            "loop 1 market state : step  789 market state fut market_state    0.0\n",
            "Name: 2014-10-07 00:00:00, dtype: float64  future value 0.5545235755484282\n",
            "loop 1 market state : step  790 market state fut market_state    0.0\n",
            "Name: 2014-10-08 00:00:00, dtype: float64  future value 0.5500317599347673\n",
            "loop 1 market state : step  791 market state fut market_state    0.0\n",
            "Name: 2014-10-09 00:00:00, dtype: float64  future value 0.5501115024174733\n",
            "loop 1 market state : step  792 market state fut market_state    0.0\n",
            "Name: 2014-10-10 00:00:00, dtype: float64  future value 0.5571991980879528\n",
            "loop 1 market state : step  793 market state fut market_state    0.0\n",
            "Name: 2014-10-13 00:00:00, dtype: float64  future value 0.56229347935111\n",
            "loop 1 market state : step  794 market state fut market_state    2.0\n",
            "Name: 2014-10-14 00:00:00, dtype: float64  future value 0.5733000856971512\n",
            "loop 1 market state : step  795 market state fut market_state    2.0\n",
            "Name: 2014-10-15 00:00:00, dtype: float64  future value 0.5691153790509301\n",
            "loop 1 market state : step  796 market state fut market_state    2.0\n",
            "Name: 2014-10-16 00:00:00, dtype: float64  future value 0.5761174202145526\n",
            "loop 1 market state : step  797 market state fut market_state    2.0\n",
            "Name: 2014-10-17 00:00:00, dtype: float64  future value 0.5801810353521673\n",
            "loop 1 market state : step  798 market state fut market_state    2.0\n",
            "Name: 2014-10-20 00:00:00, dtype: float64  future value 0.5793098538967162\n",
            "loop 1 market state : step  799 market state fut market_state    2.0\n",
            "Name: 2014-10-21 00:00:00, dtype: float64  future value 0.5862262765826012\n",
            "loop 1 market state : step  800 market state fut market_state    2.0\n",
            "Name: 2014-10-22 00:00:00, dtype: float64  future value 0.5854141447870254\n",
            "loop 1 market state : step  801 market state fut market_state    2.0\n",
            "Name: 2014-10-23 00:00:00, dtype: float64  future value 0.5890613474677766\n",
            "loop 1 market state : step  802 market state fut market_state    4.0\n",
            "Name: 2014-10-24 00:00:00, dtype: float64  future value 0.5959718581295105\n",
            "loop 1 market state : step  803 market state fut market_state    4.0\n",
            "Name: 2014-10-27 00:00:00, dtype: float64  future value 0.5959009841260122\n",
            "loop 1 market state : step  804 market state fut market_state    4.0\n",
            "Name: 2014-10-28 00:00:00, dtype: float64  future value 0.5942146786861299\n",
            "loop 1 market state : step  805 market state fut market_state    4.0\n",
            "Name: 2014-10-29 00:00:00, dtype: float64  future value 0.5976019977156936\n",
            "loop 1 market state : step  806 market state fut market_state    4.0\n",
            "Name: 2014-10-30 00:00:00, dtype: float64  future value 0.5998582519339394\n",
            "loop 1 market state : step  807 market state fut market_state    4.0\n",
            "Name: 2014-10-31 00:00:00, dtype: float64  future value 0.6000679541091386\n",
            "loop 1 market state : step  808 market state fut market_state    4.0\n",
            "Name: 2014-11-03 00:00:00, dtype: float64  future value 0.6019402770078547\n",
            "loop 1 market state : step  809 market state fut market_state    4.0\n",
            "Name: 2014-11-04 00:00:00, dtype: float64  future value 0.6023596453291334\n",
            "loop 1 market state : step  810 market state fut market_state    4.0\n",
            "Name: 2014-11-05 00:00:00, dtype: float64  future value 0.6019373208481188\n",
            "loop 1 market state : step  811 market state fut market_state    4.0\n",
            "Name: 2014-11-06 00:00:00, dtype: float64  future value 0.6022562541591816\n",
            "loop 1 market state : step  812 market state fut market_state    4.0\n",
            "Name: 2014-11-07 00:00:00, dtype: float64  future value 0.6024009583259141\n",
            "loop 1 market state : step  813 market state fut market_state    4.0\n",
            "Name: 2014-11-10 00:00:00, dtype: float64  future value 0.602843939305319\n",
            "loop 1 market state : step  814 market state fut market_state    4.0\n",
            "Name: 2014-11-11 00:00:00, dtype: float64  future value 0.6059389301661223\n",
            "loop 1 market state : step  815 market state fut market_state    4.0\n",
            "Name: 2014-11-12 00:00:00, dtype: float64  future value 0.6050293195200666\n",
            "loop 1 market state : step  816 market state fut market_state    4.0\n",
            "Name: 2014-11-13 00:00:00, dtype: float64  future value 0.6062194703157001\n",
            "loop 1 market state : step  817 market state fut market_state    4.0\n",
            "Name: 2014-11-14 00:00:00, dtype: float64  future value 0.609394167334769\n",
            "loop 1 market state : step  818 market state fut market_state    4.0\n",
            "Name: 2014-11-17 00:00:00, dtype: float64  future value 0.6111394864054073\n",
            "loop 1 market state : step  819 market state fut market_state    4.0\n",
            "Name: 2014-11-18 00:00:00, dtype: float64  future value 0.6104366578039344\n",
            "loop 1 market state : step  820 market state fut market_state    4.0\n",
            "Name: 2014-11-19 00:00:00, dtype: float64  future value 0.6121495320616789\n",
            "loop 1 market state : step  821 market state fut market_state    4.0\n",
            "Name: 2014-11-20 00:00:00, dtype: float64  future value 0.6105931866096104\n",
            "loop 1 market state : step  822 market state fut market_state    2.0\n",
            "Name: 2014-11-21 00:00:00, dtype: float64  future value 0.606423224142308\n",
            "loop 1 market state : step  823 market state fut market_state    2.0\n",
            "Name: 2014-11-24 00:00:00, dtype: float64  future value 0.6102949097969378\n",
            "loop 1 market state : step  824 market state fut market_state    4.0\n",
            "Name: 2014-11-25 00:00:00, dtype: float64  future value 0.6125925130410839\n",
            "loop 1 market state : step  825 market state fut market_state    2.0\n",
            "Name: 2014-11-26 00:00:00, dtype: float64  future value 0.6118807441974847\n",
            "loop 1 market state : step  826 market state fut market_state    4.0\n",
            "Name: 2014-11-28 00:00:00, dtype: float64  future value 0.6128996580376436\n",
            "loop 1 market state : step  827 market state fut market_state    3.0\n",
            "Name: 2014-12-01 00:00:00, dtype: float64  future value 0.6084521118758197\n",
            "loop 1 market state : step  828 market state fut market_state    2.0\n",
            "Name: 2014-12-02 00:00:00, dtype: float64  future value 0.6083074074137667\n",
            "loop 1 market state : step  829 market state fut market_state    2.0\n",
            "Name: 2014-12-03 00:00:00, dtype: float64  future value 0.5983609921708658\n",
            "loop 1 market state : step  830 market state fut market_state    2.0\n",
            "Name: 2014-12-04 00:00:00, dtype: float64  future value 0.6010749715474349\n",
            "loop 1 market state : step  831 market state fut market_state    2.0\n",
            "Name: 2014-12-05 00:00:00, dtype: float64  future value 0.5913293900005256\n",
            "loop 1 market state : step  832 market state fut market_state    0.0\n",
            "Name: 2014-12-08 00:00:00, dtype: float64  future value 0.5875788321789422\n",
            "loop 1 market state : step  833 market state fut market_state    0.0\n",
            "Name: 2014-12-09 00:00:00, dtype: float64  future value 0.5825908619210325\n",
            "loop 1 market state : step  834 market state fut market_state    2.0\n",
            "Name: 2014-12-10 00:00:00, dtype: float64  future value 0.5944479935194552\n",
            "loop 1 market state : step  835 market state fut market_state    3.0\n",
            "Name: 2014-12-11 00:00:00, dtype: float64  future value 0.6087237835461898\n",
            "loop 1 market state : step  836 market state fut market_state    3.0\n",
            "Name: 2014-12-12 00:00:00, dtype: float64  future value 0.6115056810618421\n",
            "loop 1 market state : step  837 market state fut market_state    4.0\n",
            "Name: 2014-12-15 00:00:00, dtype: float64  future value 0.6138358014724417\n",
            "loop 1 market state : step  838 market state fut market_state    4.0\n",
            "Name: 2014-12-16 00:00:00, dtype: float64  future value 0.6149077808900854\n",
            "loop 1 market state : step  839 market state fut market_state    4.0\n",
            "Name: 2014-12-17 00:00:00, dtype: float64  future value 0.6148221263832281\n",
            "loop 1 market state : step  840 market state fut market_state    4.0\n",
            "Name: 2014-12-18 00:00:00, dtype: float64  future value 0.6168569261408912\n",
            "loop 1 market state : step  841 market state fut market_state    4.0\n",
            "Name: 2014-12-19 00:00:00, dtype: float64  future value 0.6173885174915685\n",
            "loop 1 market state : step  842 market state fut market_state    4.0\n",
            "Name: 2014-12-22 00:00:00, dtype: float64  future value 0.6143703492781756\n",
            "loop 1 market state : step  843 market state fut market_state    2.0\n",
            "Name: 2014-12-23 00:00:00, dtype: float64  future value 0.6080356633898366\n",
            "loop 1 market state : step  844 market state fut market_state    2.0\n",
            "Name: 2014-12-24 00:00:00, dtype: float64  future value 0.6078289534034929\n",
            "loop 1 market state : step  845 market state fut market_state    1.0\n",
            "Name: 2014-12-26 00:00:00, dtype: float64  future value 0.5967189919166195\n",
            "loop 1 market state : step  846 market state fut market_state    1.0\n",
            "Name: 2014-12-29 00:00:00, dtype: float64  future value 0.5914120883476469\n",
            "loop 1 market state : step  847 market state fut market_state    1.0\n",
            "Name: 2014-12-30 00:00:00, dtype: float64  future value 0.5982901178720468\n",
            "loop 1 market state : step  848 market state fut market_state    2.0\n",
            "Name: 2014-12-31 00:00:00, dtype: float64  future value 0.6089924996474654\n",
            "loop 1 market state : step  849 market state fut market_state    1.0\n",
            "Name: 2015-01-02 00:00:00, dtype: float64  future value 0.6038746417553017\n",
            "loop 1 market state : step  850 market state fut market_state    2.0\n",
            "Name: 2015-01-05 00:00:00, dtype: float64  future value 0.5989870704784883\n",
            "loop 1 market state : step  851 market state fut market_state    2.0\n",
            "Name: 2015-01-06 00:00:00, dtype: float64  future value 0.5974425490747219\n",
            "loop 1 market state : step  852 market state fut market_state    1.0\n",
            "Name: 2015-01-07 00:00:00, dtype: float64  future value 0.5939695755383011\n",
            "loop 1 market state : step  853 market state fut market_state    1.0\n",
            "Name: 2015-01-08 00:00:00, dtype: float64  future value 0.5884766184813752\n",
            "loop 1 market state : step  854 market state fut market_state    1.0\n",
            "Name: 2015-01-09 00:00:00, dtype: float64  future value 0.5963764459474306\n",
            "loop 1 market state : step  855 market state fut market_state    1.0\n",
            "Name: 2015-01-12 00:00:00, dtype: float64  future value 0.5973008010677254\n",
            "loop 1 market state : step  856 market state fut market_state    2.0\n",
            "Name: 2015-01-13 00:00:00, dtype: float64  future value 0.6001270037690138\n",
            "loop 1 market state : step  857 market state fut market_state    3.0\n",
            "Name: 2015-01-14 00:00:00, dtype: float64  future value 0.6092907761648173\n",
            "loop 1 market state : step  858 market state fut market_state    2.0\n",
            "Name: 2015-01-15 00:00:00, dtype: float64  future value 0.6059448421902734\n",
            "loop 1 market state : step  859 market state fut market_state    3.0\n",
            "Name: 2015-01-16 00:00:00, dtype: float64  future value 0.6075011879376626\n",
            "loop 1 market state : step  860 market state fut market_state    2.0\n",
            "Name: 2015-01-20 00:00:00, dtype: float64  future value 0.5993680456382819\n",
            "loop 1 market state : step  861 market state fut market_state    1.0\n",
            "Name: 2015-01-21 00:00:00, dtype: float64  future value 0.5912792085245374\n",
            "loop 1 market state : step  862 market state fut market_state    2.0\n",
            "Name: 2015-01-22 00:00:00, dtype: float64  future value 0.5969168697481958\n",
            "loop 1 market state : step  863 market state fut market_state    2.0\n",
            "Name: 2015-01-23 00:00:00, dtype: float64  future value 0.5891617464488729\n",
            "loop 1 market state : step  864 market state fut market_state    2.0\n",
            "Name: 2015-01-26 00:00:00, dtype: float64  future value 0.5967987343993255\n",
            "loop 1 market state : step  865 market state fut market_state    2.0\n",
            "Name: 2015-01-27 00:00:00, dtype: float64  future value 0.6054162067040114\n",
            "loop 1 market state : step  866 market state fut market_state    2.0\n",
            "Name: 2015-01-28 00:00:00, dtype: float64  future value 0.6029000691298988\n",
            "loop 1 market state : step  867 market state fut market_state    2.0\n",
            "Name: 2015-01-29 00:00:00, dtype: float64  future value 0.6091047590013042\n",
            "loop 1 market state : step  868 market state fut market_state    2.0\n",
            "Name: 2015-01-30 00:00:00, dtype: float64  future value 0.6070227339273889\n",
            "loop 1 market state : step  869 market state fut market_state    2.0\n",
            "Name: 2015-02-02 00:00:00, dtype: float64  future value 0.6044445902383443\n",
            "loop 1 market state : step  870 market state fut market_state    2.0\n",
            "Name: 2015-02-03 00:00:00, dtype: float64  future value 0.6108973754464341\n",
            "loop 1 market state : step  871 market state fut market_state    2.0\n",
            "Name: 2015-02-04 00:00:00, dtype: float64  future value 0.6108796387833394\n",
            "loop 1 market state : step  872 market state fut market_state    4.0\n",
            "Name: 2015-02-05 00:00:00, dtype: float64  future value 0.6167712713387135\n",
            "loop 1 market state : step  873 market state fut market_state    4.0\n",
            "Name: 2015-02-06 00:00:00, dtype: float64  future value 0.6192844530484108\n",
            "loop 1 market state : step  874 market state fut market_state    4.0\n",
            "Name: 2015-02-09 00:00:00, dtype: float64  future value 0.6202738061771725\n",
            "loop 1 market state : step  875 market state fut market_state    4.0\n",
            "Name: 2015-02-10 00:00:00, dtype: float64  future value 0.6200788484762125\n",
            "loop 1 market state : step  876 market state fut market_state    4.0\n",
            "Name: 2015-02-11 00:00:00, dtype: float64  future value 0.6194202890312562\n",
            "loop 1 market state : step  877 market state fut market_state    4.0\n",
            "Name: 2015-02-12 00:00:00, dtype: float64  future value 0.6232151883629161\n",
            "loop 1 market state : step  878 market state fut market_state    4.0\n",
            "Name: 2015-02-13 00:00:00, dtype: float64  future value 0.6230261426861072\n",
            "loop 1 market state : step  879 market state fut market_state    4.0\n",
            "Name: 2015-02-17 00:00:00, dtype: float64  future value 0.6247449289680029\n",
            "loop 1 market state : step  880 market state fut market_state    4.0\n",
            "Name: 2015-02-18 00:00:00, dtype: float64  future value 0.6242665470159684\n",
            "loop 1 market state : step  881 market state fut market_state    4.0\n",
            "Name: 2015-02-19 00:00:00, dtype: float64  future value 0.6233451120262897\n",
            "loop 1 market state : step  882 market state fut market_state    2.0\n",
            "Name: 2015-02-20 00:00:00, dtype: float64  future value 0.6215023141051715\n",
            "loop 1 market state : step  883 market state fut market_state    4.0\n",
            "Name: 2015-02-23 00:00:00, dtype: float64  future value 0.6253089657222151\n",
            "loop 1 market state : step  884 market state fut market_state    2.0\n",
            "Name: 2015-02-24 00:00:00, dtype: float64  future value 0.6224709744111027\n",
            "loop 1 market state : step  885 market state fut market_state    2.0\n",
            "Name: 2015-02-25 00:00:00, dtype: float64  future value 0.6197392583714387\n",
            "loop 1 market state : step  886 market state fut market_state    2.0\n",
            "Name: 2015-02-26 00:00:00, dtype: float64  future value 0.6204805161635162\n",
            "loop 1 market state : step  887 market state fut market_state    2.0\n",
            "Name: 2015-02-27 00:00:00, dtype: float64  future value 0.611685858554764\n",
            "loop 1 market state : step  888 market state fut market_state    2.0\n",
            "Name: 2015-03-02 00:00:00, dtype: float64  future value 0.6140986052542454\n",
            "loop 1 market state : step  889 market state fut market_state    1.0\n",
            "Name: 2015-03-03 00:00:00, dtype: float64  future value 0.6036826759478765\n",
            "loop 1 market state : step  890 market state fut market_state    2.0\n",
            "Name: 2015-03-04 00:00:00, dtype: float64  future value 0.6025250059942561\n",
            "loop 1 market state : step  891 market state fut market_state    2.0\n",
            "Name: 2015-03-05 00:00:00, dtype: float64  future value 0.6101176884637519\n",
            "loop 1 market state : step  892 market state fut market_state    2.0\n",
            "Name: 2015-03-06 00:00:00, dtype: float64  future value 0.606411399798685\n",
            "loop 1 market state : step  893 market state fut market_state    3.0\n",
            "Name: 2015-03-09 00:00:00, dtype: float64  future value 0.6146183722612999\n",
            "loop 1 market state : step  894 market state fut market_state    3.0\n",
            "Name: 2015-03-10 00:00:00, dtype: float64  future value 0.6125777325377251\n",
            "loop 1 market state : step  895 market state fut market_state    3.0\n",
            "Name: 2015-03-11 00:00:00, dtype: float64  future value 0.6200257108404883\n",
            "loop 1 market state : step  896 market state fut market_state    3.0\n",
            "Name: 2015-03-12 00:00:00, dtype: float64  future value 0.6170045864673595\n",
            "loop 1 market state : step  897 market state fut market_state    3.0\n",
            "Name: 2015-03-13 00:00:00, dtype: float64  future value 0.6225654973971675\n",
            "loop 1 market state : step  898 market state fut market_state    3.0\n",
            "Name: 2015-03-16 00:00:00, dtype: float64  future value 0.6214786654179257\n",
            "loop 1 market state : step  899 market state fut market_state    3.0\n",
            "Name: 2015-03-17 00:00:00, dtype: float64  future value 0.6176631456169952\n",
            "loop 1 market state : step  900 market state fut market_state    1.0\n",
            "Name: 2015-03-18 00:00:00, dtype: float64  future value 0.6086706462057863\n",
            "loop 1 market state : step  901 market state fut market_state    1.0\n",
            "Name: 2015-03-19 00:00:00, dtype: float64  future value 0.6072235315942608\n",
            "loop 1 market state : step  902 market state fut market_state    1.0\n",
            "Name: 2015-03-20 00:00:00, dtype: float64  future value 0.6086617780218992\n",
            "loop 1 market state : step  903 market state fut market_state    1.0\n",
            "Name: 2015-03-23 00:00:00, dtype: float64  future value 0.6161097560293419\n",
            "loop 1 market state : step  904 market state fut market_state    1.0\n",
            "Name: 2015-03-24 00:00:00, dtype: float64  future value 0.6106905934018511\n",
            "loop 1 market state : step  905 market state fut market_state    1.0\n",
            "Name: 2015-03-25 00:00:00, dtype: float64  future value 0.608268978223162\n",
            "loop 1 market state : step  906 market state fut market_state    2.0\n",
            "Name: 2015-03-26 00:00:00, dtype: float64  future value 0.6104159652764244\n",
            "loop 1 market state : step  907 market state fut market_state    2.0\n",
            "Name: 2015-03-27 00:00:00, dtype: float64  future value 0.614450091465561\n",
            "loop 1 market state : step  908 market state fut market_state    1.0\n",
            "Name: 2015-03-30 00:00:00, dtype: float64  future value 0.6131831543469571\n",
            "loop 1 market state : step  909 market state fut market_state    2.0\n",
            "Name: 2015-03-31 00:00:00, dtype: float64  future value 0.6148280384073794\n",
            "loop 1 market state : step  910 market state fut market_state    2.0\n",
            "Name: 2015-04-01 00:00:00, dtype: float64  future value 0.617568622926251\n",
            "loop 1 market state : step  911 market state fut market_state    2.0\n",
            "Name: 2015-04-02 00:00:00, dtype: float64  future value 0.6207817491359247\n",
            "loop 1 market state : step  912 market state fut market_state    2.0\n",
            "Name: 2015-04-06 00:00:00, dtype: float64  future value 0.6179377737424219\n",
            "loop 1 market state : step  913 market state fut market_state    2.0\n",
            "Name: 2015-04-07 00:00:00, dtype: float64  future value 0.6189448632389577\n",
            "loop 1 market state : step  914 market state fut market_state    2.0\n",
            "Name: 2015-04-08 00:00:00, dtype: float64  future value 0.6221313125434101\n",
            "loop 1 market state : step  915 market state fut market_state    3.0\n",
            "Name: 2015-04-09 00:00:00, dtype: float64  future value 0.621647018271904\n",
            "loop 1 market state : step  916 market state fut market_state    1.0\n",
            "Name: 2015-04-10 00:00:00, dtype: float64  future value 0.6146154163968846\n",
            "loop 1 market state : step  917 market state fut market_state    3.0\n",
            "Name: 2015-04-13 00:00:00, dtype: float64  future value 0.6202914704867074\n",
            "loop 1 market state : step  918 market state fut market_state    3.0\n",
            "Name: 2015-04-14 00:00:00, dtype: float64  future value 0.6193730637150038\n",
            "loop 1 market state : step  919 market state fut market_state    3.0\n",
            "Name: 2015-04-15 00:00:00, dtype: float64  future value 0.6225241120468269\n",
            "loop 1 market state : step  920 market state fut market_state    4.0\n",
            "Name: 2015-04-16 00:00:00, dtype: float64  future value 0.6239918471276231\n",
            "loop 1 market state : step  921 market state fut market_state    4.0\n",
            "Name: 2015-04-17 00:00:00, dtype: float64  future value 0.6253975760934875\n",
            "loop 1 market state : step  922 market state fut market_state    4.0\n",
            "Name: 2015-04-20 00:00:00, dtype: float64  future value 0.6228076083561406\n",
            "loop 1 market state : step  923 market state fut market_state    4.0\n",
            "Name: 2015-04-21 00:00:00, dtype: float64  future value 0.624532306957508\n",
            "loop 1 market state : step  924 market state fut market_state    2.0\n",
            "Name: 2015-04-22 00:00:00, dtype: float64  future value 0.6221963465809967\n",
            "loop 1 market state : step  925 market state fut market_state    1.0\n",
            "Name: 2015-04-23 00:00:00, dtype: float64  future value 0.6158941778591112\n",
            "loop 1 market state : step  926 market state fut market_state    2.0\n",
            "Name: 2015-04-24 00:00:00, dtype: float64  future value 0.622621590897307\n",
            "loop 1 market state : step  927 market state fut market_state    3.0\n",
            "Name: 2015-04-27 00:00:00, dtype: float64  future value 0.6244525644748021\n",
            "loop 1 market state : step  928 market state fut market_state    1.0\n",
            "Name: 2015-04-28 00:00:00, dtype: float64  future value 0.6170606799674989\n",
            "loop 1 market state : step  929 market state fut market_state    1.0\n",
            "Name: 2015-04-29 00:00:00, dtype: float64  future value 0.6143112272647403\n",
            "loop 1 market state : step  930 market state fut market_state    3.0\n",
            "Name: 2015-04-30 00:00:00, dtype: float64  future value 0.6166295233317169\n",
            "loop 1 market state : step  931 market state fut market_state    3.0\n",
            "Name: 2015-05-01 00:00:00, dtype: float64  future value 0.6249280626206606\n",
            "loop 1 market state : step  932 market state fut market_state    2.0\n",
            "Name: 2015-05-04 00:00:00, dtype: float64  future value 0.6217474532821199\n",
            "loop 1 market state : step  933 market state fut market_state    3.0\n",
            "Name: 2015-05-05 00:00:00, dtype: float64  future value 0.6199135235448889\n",
            "loop 1 market state : step  934 market state fut market_state    3.0\n",
            "Name: 2015-05-06 00:00:00, dtype: float64  future value 0.6197244778680799\n",
            "loop 1 market state : step  935 market state fut market_state    4.0\n",
            "Name: 2015-05-07 00:00:00, dtype: float64  future value 0.6264046658853438\n",
            "loop 1 market state : step  936 market state fut market_state    4.0\n",
            "Name: 2015-05-08 00:00:00, dtype: float64  future value 0.6268860037017936\n",
            "loop 1 market state : step  937 market state fut market_state    4.0\n",
            "Name: 2015-05-11 00:00:00, dtype: float64  future value 0.6287967197619948\n",
            "loop 1 market state : step  938 market state fut market_state    4.0\n",
            "Name: 2015-05-12 00:00:00, dtype: float64  future value 0.6283921679731944\n",
            "loop 1 market state : step  939 market state fut market_state    4.0\n",
            "Name: 2015-05-13 00:00:00, dtype: float64  future value 0.6278074389867929\n",
            "loop 1 market state : step  940 market state fut market_state    4.0\n",
            "Name: 2015-05-14 00:00:00, dtype: float64  future value 0.6292751737722685\n",
            "loop 1 market state : step  941 market state fut market_state    4.0\n",
            "Name: 2015-05-15 00:00:00, dtype: float64  future value 0.6278694448064042\n",
            "loop 1 market state : step  942 market state fut market_state    2.0\n",
            "Name: 2015-05-18 00:00:00, dtype: float64  future value 0.6214137034385786\n",
            "loop 1 market state : step  943 market state fut market_state    2.0\n",
            "Name: 2015-05-19 00:00:00, dtype: float64  future value 0.627107494191496\n",
            "loop 1 market state : step  944 market state fut market_state    2.0\n",
            "Name: 2015-05-20 00:00:00, dtype: float64  future value 0.626313099059015\n",
            "loop 1 market state : step  945 market state fut market_state    2.0\n",
            "Name: 2015-05-21 00:00:00, dtype: float64  future value 0.6223557591928486\n",
            "loop 1 market state : step  946 market state fut market_state    2.0\n",
            "Name: 2015-05-22 00:00:00, dtype: float64  future value 0.6236374765194905\n",
            "loop 1 market state : step  947 market state fut market_state    3.0\n",
            "Name: 2015-05-26 00:00:00, dtype: float64  future value 0.6230084783765725\n",
            "loop 1 market state : step  948 market state fut market_state    2.0\n",
            "Name: 2015-05-27 00:00:00, dtype: float64  future value 0.6243285528355796\n",
            "loop 1 market state : step  949 market state fut market_state    1.0\n",
            "Name: 2015-05-28 00:00:00, dtype: float64  future value 0.6189448632389577\n",
            "loop 1 market state : step  950 market state fut market_state    1.0\n",
            "Name: 2015-05-29 00:00:00, dtype: float64  future value 0.6180559451204118\n",
            "loop 1 market state : step  951 market state fut market_state    0.0\n",
            "Name: 2015-06-01 00:00:00, dtype: float64  future value 0.6140543358024083\n",
            "loop 1 market state : step  952 market state fut market_state    1.0\n",
            "Name: 2015-06-02 00:00:00, dtype: float64  future value 0.6143112272647403\n",
            "loop 1 market state : step  953 market state fut market_state    1.0\n",
            "Name: 2015-06-03 00:00:00, dtype: float64  future value 0.6217090240915153\n",
            "loop 1 market state : step  954 market state fut market_state    3.0\n",
            "Name: 2015-06-04 00:00:00, dtype: float64  future value 0.6227899437512852\n",
            "loop 1 market state : step  955 market state fut market_state    3.0\n",
            "Name: 2015-06-05 00:00:00, dtype: float64  future value 0.6184339641204697\n",
            "loop 1 market state : step  956 market state fut market_state    2.0\n",
            "Name: 2015-06-08 00:00:00, dtype: float64  future value 0.6155752085189287\n",
            "loop 1 market state : step  957 market state fut market_state    2.0\n",
            "Name: 2015-06-09 00:00:00, dtype: float64  future value 0.6190777430620672\n",
            "loop 1 market state : step  958 market state fut market_state    2.0\n",
            "Name: 2015-06-10 00:00:00, dtype: float64  future value 0.6203032948303303\n",
            "loop 1 market state : step  959 market state fut market_state    3.0\n",
            "Name: 2015-06-11 00:00:00, dtype: float64  future value 0.6264459788821245\n",
            "loop 1 market state : step  960 market state fut market_state    3.0\n",
            "Name: 2015-06-12 00:00:00, dtype: float64  future value 0.6231236215365873\n",
            "loop 1 market state : step  961 market state fut market_state    3.0\n",
            "Name: 2015-06-15 00:00:00, dtype: float64  future value 0.626921477027983\n",
            "loop 1 market state : step  962 market state fut market_state    3.0\n",
            "Name: 2015-06-16 00:00:00, dtype: float64  future value 0.6273201164973116\n",
            "loop 1 market state : step  963 market state fut market_state    3.0\n",
            "Name: 2015-06-17 00:00:00, dtype: float64  future value 0.622707245404164\n",
            "loop 1 market state : step  964 market state fut market_state    2.0\n",
            "Name: 2015-06-18 00:00:00, dtype: float64  future value 0.6208555792991588\n",
            "loop 1 market state : step  965 market state fut market_state    1.0\n",
            "Name: 2015-06-19 00:00:00, dtype: float64  future value 0.6206133959866258\n",
            "loop 1 market state : step  966 market state fut market_state    0.0\n",
            "Name: 2015-06-22 00:00:00, dtype: float64  future value 0.6076635567092504\n",
            "loop 1 market state : step  967 market state fut market_state    0.0\n",
            "Name: 2015-06-23 00:00:00, dtype: float64  future value 0.6092790238794337\n",
            "loop 1 market state : step  968 market state fut market_state    0.0\n",
            "Name: 2015-06-24 00:00:00, dtype: float64  future value 0.6135050077886363\n",
            "loop 1 market state : step  969 market state fut market_state    0.0\n",
            "Name: 2015-06-25 00:00:00, dtype: float64  future value 0.6133160341700666\n",
            "loop 1 market state : step  970 market state fut market_state    0.0\n",
            "Name: 2015-06-26 00:00:00, dtype: float64  future value 0.6109475569224223\n",
            "loop 1 market state : step  971 market state fut market_state    2.0\n",
            "Name: 2015-06-29 00:00:00, dtype: float64  future value 0.6146627137713763\n",
            "loop 1 market state : step  972 market state fut market_state    0.0\n",
            "Name: 2015-06-30 00:00:00, dtype: float64  future value 0.60442688989969\n",
            "loop 1 market state : step  973 market state fut market_state    0.0\n",
            "Name: 2015-07-01 00:00:00, dtype: float64  future value 0.6057942259993898\n",
            "loop 1 market state : step  974 market state fut market_state    1.0\n",
            "Name: 2015-07-02 00:00:00, dtype: float64  future value 0.6132688088538144\n",
            "loop 1 market state : step  975 market state fut market_state    2.0\n",
            "Name: 2015-07-06 00:00:00, dtype: float64  future value 0.620055271847206\n",
            "loop 1 market state : step  976 market state fut market_state    2.0\n",
            "Name: 2015-07-07 00:00:00, dtype: float64  future value 0.6228164765400277\n",
            "loop 1 market state : step  977 market state fut market_state    2.0\n",
            "Name: 2015-07-08 00:00:00, dtype: float64  future value 0.6223587150572639\n",
            "loop 1 market state : step  978 market state fut market_state    4.0\n",
            "Name: 2015-07-09 00:00:00, dtype: float64  future value 0.6273467213442933\n",
            "loop 1 market state : step  979 market state fut market_state    4.0\n",
            "Name: 2015-07-10 00:00:00, dtype: float64  future value 0.6280406817618791\n",
            "loop 1 market state : step  980 market state fut market_state    4.0\n",
            "Name: 2015-07-13 00:00:00, dtype: float64  future value 0.6285250477963039\n",
            "loop 1 market state : step  981 market state fut market_state    3.0\n",
            "Name: 2015-07-14 00:00:00, dtype: float64  future value 0.6258464693923642\n",
            "loop 1 market state : step  982 market state fut market_state    3.0\n",
            "Name: 2015-07-15 00:00:00, dtype: float64  future value 0.6243521294645863\n",
            "loop 1 market state : step  983 market state fut market_state    1.0\n",
            "Name: 2015-07-16 00:00:00, dtype: float64  future value 0.6208082816293465\n",
            "loop 1 market state : step  984 market state fut market_state    1.0\n",
            "Name: 2015-07-17 00:00:00, dtype: float64  future value 0.614163566938272\n",
            "loop 1 market state : step  985 market state fut market_state    1.0\n",
            "Name: 2015-07-20 00:00:00, dtype: float64  future value 0.6106167632386169\n",
            "loop 1 market state : step  986 market state fut market_state    2.0\n",
            "Name: 2015-07-21 00:00:00, dtype: float64  future value 0.6181799567596343\n",
            "loop 1 market state : step  987 market state fut market_state    2.0\n",
            "Name: 2015-07-22 00:00:00, dtype: float64  future value 0.6227042892444281\n",
            "loop 1 market state : step  988 market state fut market_state    3.0\n",
            "Name: 2015-07-23 00:00:00, dtype: float64  future value 0.6227219538492834\n",
            "loop 1 market state : step  989 market state fut market_state    2.0\n",
            "Name: 2015-07-24 00:00:00, dtype: float64  future value 0.6213074284624508\n",
            "loop 1 market state : step  990 market state fut market_state    2.0\n",
            "Name: 2015-07-27 00:00:00, dtype: float64  future value 0.6195945542047063\n",
            "loop 1 market state : step  991 market state fut market_state    2.0\n",
            "Name: 2015-07-28 00:00:00, dtype: float64  future value 0.6182006492871442\n",
            "loop 1 market state : step  992 market state fut market_state    1.0\n",
            "Name: 2015-07-29 00:00:00, dtype: float64  future value 0.6201261458507042\n",
            "loop 1 market state : step  993 market state fut market_state    1.0\n",
            "Name: 2015-07-30 00:00:00, dtype: float64  future value 0.6153183170565967\n",
            "loop 1 market state : step  994 market state fut market_state    1.0\n",
            "Name: 2015-07-31 00:00:00, dtype: float64  future value 0.6135493490033921\n",
            "loop 1 market state : step  995 market state fut market_state    3.0\n",
            "Name: 2015-08-03 00:00:00, dtype: float64  future value 0.6214077914144275\n",
            "loop 1 market state : step  996 market state fut market_state    2.0\n",
            "Name: 2015-08-04 00:00:00, dtype: float64  future value 0.6154689332474803\n",
            "loop 1 market state : step  997 market state fut market_state    2.0\n",
            "Name: 2015-08-05 00:00:00, dtype: float64  future value 0.6160536625292025\n",
            "loop 1 market state : step  998 market state fut market_state    2.0\n",
            "Name: 2015-08-06 00:00:00, dtype: float64  future value 0.6152680635223691\n",
            "loop 1 market state : step  999 market state fut market_state    3.0\n",
            "Name: 2015-08-07 00:00:00, dtype: float64  future value 0.6176749699606181\n",
            "loop 1 market state : step  1000 market state fut market_state    2.0\n",
            "Name: 2015-08-10 00:00:00, dtype: float64  future value 0.6208939361362036\n",
            "loop 1 market state : step  1001 market state fut market_state    3.0\n",
            "Name: 2015-08-11 00:00:00, dtype: float64  future value 0.6192637605209009\n",
            "loop 1 market state : step  1002 market state fut market_state    2.0\n",
            "Name: 2015-08-12 00:00:00, dtype: float64  future value 0.6141518146528883\n",
            "loop 1 market state : step  1003 market state fut market_state    0.0\n",
            "Name: 2015-08-13 00:00:00, dtype: float64  future value 0.6011931068963053\n",
            "loop 1 market state : step  1004 market state fut market_state    0.0\n",
            "Name: 2015-08-14 00:00:00, dtype: float64  future value 0.5820445260961161\n",
            "loop 1 market state : step  1005 market state fut market_state    0.0\n",
            "Name: 2015-08-17 00:00:00, dtype: float64  future value 0.5591040018286821\n",
            "loop 1 market state : step  1006 market state fut market_state    0.0\n",
            "Name: 2015-08-18 00:00:00, dtype: float64  future value 0.5515438002011998\n",
            "loop 1 market state : step  1007 market state fut market_state    0.0\n",
            "Name: 2015-08-19 00:00:00, dtype: float64  future value 0.5730726831832976\n",
            "loop 1 market state : step  1008 market state fut market_state    0.0\n",
            "Name: 2015-08-20 00:00:00, dtype: float64  future value 0.5869970590569561\n",
            "loop 1 market state : step  1009 market state fut market_state    0.0\n",
            "Name: 2015-08-21 00:00:00, dtype: float64  future value 0.5873543855295039\n",
            "loop 1 market state : step  1010 market state fut market_state    2.0\n",
            "Name: 2015-08-24 00:00:00, dtype: float64  future value 0.5824255012559097\n",
            "loop 1 market state : step  1011 market state fut market_state    2.0\n",
            "Name: 2015-08-25 00:00:00, dtype: float64  future value 0.5651994245351044\n",
            "loop 1 market state : step  1012 market state fut market_state    2.0\n",
            "Name: 2015-08-26 00:00:00, dtype: float64  future value 0.5755386032523022\n",
            "loop 1 market state : step  1013 market state fut market_state    1.0\n",
            "Name: 2015-08-27 00:00:00, dtype: float64  future value 0.5762089870408814\n",
            "loop 1 market state : step  1014 market state fut market_state    1.0\n",
            "Name: 2015-08-28 00:00:00, dtype: float64  future value 0.5673759362706442\n",
            "loop 1 market state : step  1015 market state fut market_state    1.0\n",
            "Name: 2015-08-31 00:00:00, dtype: float64  future value 0.5816074571408623\n",
            "loop 1 market state : step  1016 market state fut market_state    2.0\n",
            "Name: 2015-09-01 00:00:00, dtype: float64  future value 0.5735245323465895\n",
            "loop 1 market state : step  1017 market state fut market_state    2.0\n",
            "Name: 2015-09-02 00:00:00, dtype: float64  future value 0.5765515690391901\n",
            "loop 1 market state : step  1018 market state fut market_state    2.0\n",
            "Name: 2015-09-03 00:00:00, dtype: float64  future value 0.5791385809121217\n",
            "loop 1 market state : step  1019 market state fut market_state    2.0\n",
            "Name: 2015-09-04 00:00:00, dtype: float64  future value 0.5767701033691567\n",
            "loop 1 market state : step  1020 market state fut market_state    2.0\n",
            "Name: 2015-09-08 00:00:00, dtype: float64  future value 0.584170820326548\n",
            "loop 1 market state : step  1021 market state fut market_state    2.0\n",
            "Name: 2015-09-09 00:00:00, dtype: float64  future value 0.5892562694349377\n",
            "loop 1 market state : step  1022 market state fut market_state    2.0\n",
            "Name: 2015-09-10 00:00:00, dtype: float64  future value 0.587747149003801\n",
            "loop 1 market state : step  1023 market state fut market_state    1.0\n",
            "Name: 2015-09-11 00:00:00, dtype: float64  future value 0.5782467066338399\n",
            "loop 1 market state : step  1024 market state fut market_state    2.0\n",
            "Name: 2015-09-14 00:00:00, dtype: float64  future value 0.5808868561424957\n",
            "loop 1 market state : step  1025 market state fut market_state    1.0\n",
            "Name: 2015-09-15 00:00:00, dtype: float64  future value 0.5737312423329332\n",
            "loop 1 market state : step  1026 market state fut market_state    1.0\n",
            "Name: 2015-09-16 00:00:00, dtype: float64  future value 0.5725558720406585\n",
            "loop 1 market state : step  1027 market state fut market_state    1.0\n",
            "Name: 2015-09-17 00:00:00, dtype: float64  future value 0.5706303754770984\n",
            "loop 1 market state : step  1028 market state fut market_state    1.0\n",
            "Name: 2015-09-18 00:00:00, dtype: float64  future value 0.5703645798017597\n",
            "loop 1 market state : step  1029 market state fut market_state    1.0\n",
            "Name: 2015-09-21 00:00:00, dtype: float64  future value 0.5557255509830055\n",
            "loop 1 market state : step  1030 market state fut market_state    1.0\n",
            "Name: 2015-09-22 00:00:00, dtype: float64  future value 0.5564106789505032\n",
            "loop 1 market state : step  1031 market state fut market_state    1.0\n",
            "Name: 2015-09-23 00:00:00, dtype: float64  future value 0.5670245218222474\n",
            "loop 1 market state : step  1032 market state fut market_state    1.0\n",
            "Name: 2015-09-24 00:00:00, dtype: float64  future value 0.5681437625852631\n",
            "loop 1 market state : step  1033 market state fut market_state    2.0\n",
            "Name: 2015-09-25 00:00:00, dtype: float64  future value 0.5762769048846438\n",
            "loop 1 market state : step  1034 market state fut market_state    3.0\n",
            "Name: 2015-09-28 00:00:00, dtype: float64  future value 0.5868169178884745\n",
            "loop 1 market state : step  1035 market state fut market_state    3.0\n",
            "Name: 2015-09-29 00:00:00, dtype: float64  future value 0.584711280156433\n",
            "loop 1 market state : step  1036 market state fut market_state    4.0\n",
            "Name: 2015-09-30 00:00:00, dtype: float64  future value 0.5894098057564374\n",
            "loop 1 market state : step  1037 market state fut market_state    4.0\n",
            "Name: 2015-10-01 00:00:00, dtype: float64  future value 0.5946074781895464\n",
            "loop 1 market state : step  1038 market state fut market_state    4.0\n",
            "Name: 2015-10-02 00:00:00, dtype: float64  future value 0.5950386348253285\n",
            "loop 1 market state : step  1039 market state fut market_state    4.0\n",
            "Name: 2015-10-05 00:00:00, dtype: float64  future value 0.5957975929560605\n",
            "loop 1 market state : step  1040 market state fut market_state    4.0\n",
            "Name: 2015-10-06 00:00:00, dtype: float64  future value 0.5917310216587098\n",
            "loop 1 market state : step  1041 market state fut market_state    2.0\n",
            "Name: 2015-10-07 00:00:00, dtype: float64  future value 0.5889402559591704\n",
            "loop 1 market state : step  1042 market state fut market_state    4.0\n",
            "Name: 2015-10-08 00:00:00, dtype: float64  future value 0.5976876522225506\n",
            "loop 1 market state : step  1043 market state fut market_state    4.0\n",
            "Name: 2015-10-09 00:00:00, dtype: float64  future value 0.6004193682622146\n",
            "loop 1 market state : step  1044 market state fut market_state    4.0\n",
            "Name: 2015-10-12 00:00:00, dtype: float64  future value 0.6005818090920417\n",
            "loop 1 market state : step  1045 market state fut market_state    4.0\n",
            "Name: 2015-10-13 00:00:00, dtype: float64  future value 0.5997283282705657\n",
            "loop 1 market state : step  1046 market state fut market_state    4.0\n",
            "Name: 2015-10-14 00:00:00, dtype: float64  future value 0.5962346616159936\n",
            "loop 1 market state : step  1047 market state fut market_state    4.0\n",
            "Name: 2015-10-15 00:00:00, dtype: float64  future value 0.6061485963122019\n",
            "loop 1 market state : step  1048 market state fut market_state    4.0\n",
            "Name: 2015-10-16 00:00:00, dtype: float64  future value 0.6128346240000571\n",
            "loop 1 market state : step  1049 market state fut market_state    4.0\n",
            "Name: 2015-10-19 00:00:00, dtype: float64  future value 0.6116622098675182\n",
            "loop 1 market state : step  1050 market state fut market_state    4.0\n",
            "Name: 2015-10-20 00:00:00, dtype: float64  future value 0.6100999520959778\n",
            "loop 1 market state : step  1051 market state fut market_state    4.0\n",
            "Name: 2015-10-21 00:00:00, dtype: float64  future value 0.617323555807542\n",
            "loop 1 market state : step  1052 market state fut market_state    4.0\n",
            "Name: 2015-10-22 00:00:00, dtype: float64  future value 0.6170458994641401\n",
            "loop 1 market state : step  1053 market state fut market_state    4.0\n",
            "Name: 2015-10-23 00:00:00, dtype: float64  future value 0.6140779844896541\n",
            "loop 1 market state : step  1054 market state fut market_state    4.0\n",
            "Name: 2015-10-26 00:00:00, dtype: float64  future value 0.621369434282062\n",
            "loop 1 market state : step  1055 market state fut market_state    4.0\n",
            "Name: 2015-10-27 00:00:00, dtype: float64  future value 0.6230645718767119\n",
            "loop 1 market state : step  1056 market state fut market_state    4.0\n",
            "Name: 2015-10-28 00:00:00, dtype: float64  future value 0.6208555792991588\n",
            "loop 1 market state : step  1057 market state fut market_state    4.0\n",
            "Name: 2015-10-29 00:00:00, dtype: float64  future value 0.6201526786394467\n",
            "loop 1 market state : step  1058 market state fut market_state    4.0\n",
            "Name: 2015-10-30 00:00:00, dtype: float64  future value 0.6199371001738954\n",
            "loop 1 market state : step  1059 market state fut market_state    2.0\n",
            "Name: 2015-11-02 00:00:00, dtype: float64  future value 0.6138476258160647\n",
            "loop 1 market state : step  1060 market state fut market_state    2.0\n",
            "Name: 2015-11-03 00:00:00, dtype: float64  future value 0.6147749010669759\n",
            "loop 1 market state : step  1061 market state fut market_state    2.0\n",
            "Name: 2015-11-04 00:00:00, dtype: float64  future value 0.6127903548435405\n",
            "loop 1 market state : step  1062 market state fut market_state    2.0\n",
            "Name: 2015-11-05 00:00:00, dtype: float64  future value 0.6042171877244907\n",
            "loop 1 market state : step  1063 market state fut market_state    2.0\n",
            "Name: 2015-11-06 00:00:00, dtype: float64  future value 0.5974455052344578\n",
            "loop 1 market state : step  1064 market state fut market_state    2.0\n",
            "Name: 2015-11-09 00:00:00, dtype: float64  future value 0.6063493939790737\n",
            "loop 1 market state : step  1065 market state fut market_state    2.0\n",
            "Name: 2015-11-10 00:00:00, dtype: float64  future value 0.605537262183498\n",
            "loop 1 market state : step  1066 market state fut market_state    3.0\n",
            "Name: 2015-11-11 00:00:00, dtype: float64  future value 0.6153242290807478\n",
            "loop 1 market state : step  1067 market state fut market_state    3.0\n",
            "Name: 2015-11-12 00:00:00, dtype: float64  future value 0.6146331527646587\n",
            "loop 1 market state : step  1068 market state fut market_state    3.0\n",
            "Name: 2015-11-13 00:00:00, dtype: float64  future value 0.6169750254606419\n",
            "loop 1 market state : step  1069 market state fut market_state    3.0\n",
            "Name: 2015-11-16 00:00:00, dtype: float64  future value 0.6162131471992937\n",
            "loop 1 market state : step  1070 market state fut market_state    3.0\n",
            "Name: 2015-11-17 00:00:00, dtype: float64  future value 0.6169661572767549\n",
            "loop 1 market state : step  1071 market state fut market_state    3.0\n",
            "Name: 2015-11-18 00:00:00, dtype: float64  future value 0.6168864868522883\n",
            "loop 1 market state : step  1072 market state fut market_state    3.0\n",
            "Name: 2015-11-19 00:00:00, dtype: float64  future value 0.6172526815087231\n",
            "loop 1 market state : step  1073 market state fut market_state    2.0\n",
            "Name: 2015-11-20 00:00:00, dtype: float64  future value 0.6143880135877103\n",
            "loop 1 market state : step  1074 market state fut market_state    3.0\n",
            "Name: 2015-11-23 00:00:00, dtype: float64  future value 0.6209500299316635\n",
            "loop 1 market state : step  1075 market state fut market_state    2.0\n",
            "Name: 2015-11-24 00:00:00, dtype: float64  future value 0.6141222539414913\n",
            "loop 1 market state : step  1076 market state fut market_state    2.0\n",
            "Name: 2015-11-25 00:00:00, dtype: float64  future value 0.6052951512245249\n",
            "loop 1 market state : step  1077 market state fut market_state    3.0\n",
            "Name: 2015-11-27 00:00:00, dtype: float64  future value 0.6177192391171347\n",
            "loop 1 market state : step  1078 market state fut market_state    2.0\n",
            "Name: 2015-11-30 00:00:00, dtype: float64  future value 0.6134016886769238\n",
            "loop 1 market state : step  1079 market state fut market_state    1.0\n",
            "Name: 2015-12-01 00:00:00, dtype: float64  future value 0.6094207721817508\n",
            "loop 1 market state : step  1080 market state fut market_state    1.0\n",
            "Name: 2015-12-02 00:00:00, dtype: float64  future value 0.6047044738895319\n",
            "loop 1 market state : step  1081 market state fut market_state    2.0\n",
            "Name: 2015-12-03 00:00:00, dtype: float64  future value 0.60606589766976\n",
            "loop 1 market state : step  1082 market state fut market_state    0.0\n",
            "Name: 2015-12-04 00:00:00, dtype: float64  future value 0.5942944208735151\n",
            "loop 1 market state : step  1083 market state fut market_state    0.0\n",
            "Name: 2015-12-07 00:00:00, dtype: float64  future value 0.5971206235748036\n",
            "loop 1 market state : step  1084 market state fut market_state    1.0\n",
            "Name: 2015-12-08 00:00:00, dtype: float64  future value 0.6034611854581741\n",
            "loop 1 market state : step  1085 market state fut market_state    2.0\n",
            "Name: 2015-12-09 00:00:00, dtype: float64  future value 0.6122204060651771\n",
            "loop 1 market state : step  1086 market state fut market_state    1.0\n",
            "Name: 2015-12-10 00:00:00, dtype: float64  future value 0.603012292454618\n",
            "loop 1 market state : step  1087 market state fut market_state    0.0\n",
            "Name: 2015-12-11 00:00:00, dtype: float64  future value 0.5922803499678024\n",
            "loop 1 market state : step  1088 market state fut market_state    1.0\n",
            "Name: 2015-12-14 00:00:00, dtype: float64  future value 0.5968873447705977\n",
            "loop 1 market state : step  1089 market state fut market_state    1.0\n",
            "Name: 2015-12-15 00:00:00, dtype: float64  future value 0.6021499431539342\n",
            "loop 1 market state : step  1090 market state fut market_state    1.0\n",
            "Name: 2015-12-16 00:00:00, dtype: float64  future value 0.6096274821680945\n",
            "loop 1 market state : step  1091 market state fut market_state    2.0\n",
            "Name: 2015-12-17 00:00:00, dtype: float64  future value 0.6086529095426916\n",
            "loop 1 market state : step  1092 market state fut market_state    3.0\n",
            "Name: 2015-12-18 00:00:00, dtype: float64  future value 0.6073269227642125\n",
            "loop 1 market state : step  1093 market state fut market_state    3.0\n",
            "Name: 2015-12-21 00:00:00, dtype: float64  future value 0.6137826638367175\n",
            "loop 1 market state : step  1094 market state fut market_state    3.0\n",
            "Name: 2015-12-22 00:00:00, dtype: float64  future value 0.6093528540426678\n",
            "loop 1 market state : step  1095 market state fut market_state    1.0\n",
            "Name: 2015-12-23 00:00:00, dtype: float64  future value 0.6036176779394098\n",
            "loop 1 market state : step  1096 market state fut market_state    1.0\n",
            "Name: 2015-12-24 00:00:00, dtype: float64  future value 0.5943800753803722\n",
            "loop 1 market state : step  1097 market state fut market_state    1.0\n",
            "Name: 2015-12-28 00:00:00, dtype: float64  future value 0.595576102466358\n",
            "loop 1 market state : step  1098 market state fut market_state    0.0\n",
            "Name: 2015-12-29 00:00:00, dtype: float64  future value 0.5877648856668957\n",
            "loop 1 market state : step  1099 market state fut market_state    0.0\n",
            "Name: 2015-12-30 00:00:00, dtype: float64  future value 0.5738345974737653\n",
            "loop 1 market state : step  1100 market state fut market_state    0.0\n",
            "Name: 2015-12-31 00:00:00, dtype: float64  future value 0.5676151631281207\n",
            "loop 1 market state : step  1101 market state fut market_state    0.0\n",
            "Name: 2016-01-04 00:00:00, dtype: float64  future value 0.5680994934287467\n",
            "loop 1 market state : step  1102 market state fut market_state    0.0\n",
            "Name: 2016-01-05 00:00:00, dtype: float64  future value 0.5725322593825322\n",
            "loop 1 market state : step  1103 market state fut market_state    0.0\n",
            "Name: 2016-01-06 00:00:00, dtype: float64  future value 0.5582387323973822\n",
            "loop 1 market state : step  1104 market state fut market_state    0.0\n",
            "Name: 2016-01-07 00:00:00, dtype: float64  future value 0.5675590335988616\n",
            "loop 1 market state : step  1105 market state fut market_state    0.0\n",
            "Name: 2016-01-08 00:00:00, dtype: float64  future value 0.5553002703422548\n",
            "loop 1 market state : step  1106 market state fut market_state    0.0\n",
            "Name: 2016-01-11 00:00:00, dtype: float64  future value 0.5555955909951915\n",
            "loop 1 market state : step  1107 market state fut market_state    0.0\n",
            "Name: 2016-01-12 00:00:00, dtype: float64  future value 0.5490985366305853\n",
            "loop 1 market state : step  1108 market state fut market_state    0.0\n",
            "Name: 2016-01-13 00:00:00, dtype: float64  future value 0.5519513441788555\n",
            "loop 1 market state : step  1109 market state fut market_state    1.0\n",
            "Name: 2016-01-14 00:00:00, dtype: float64  future value 0.5631469601725859\n",
            "loop 1 market state : step  1110 market state fut market_state    0.0\n",
            "Name: 2016-01-15 00:00:00, dtype: float64  future value 0.5543404782202107\n",
            "loop 1 market state : step  1111 market state fut market_state    2.0\n",
            "Name: 2016-01-19 00:00:00, dtype: float64  future value 0.5621812560263908\n",
            "loop 1 market state : step  1112 market state fut market_state    2.0\n",
            "Name: 2016-01-20 00:00:00, dtype: float64  future value 0.5560740089763456\n",
            "loop 1 market state : step  1113 market state fut market_state    2.0\n",
            "Name: 2016-01-21 00:00:00, dtype: float64  future value 0.5591483070143184\n",
            "loop 1 market state : step  1114 market state fut market_state    2.0\n",
            "Name: 2016-01-22 00:00:00, dtype: float64  future value 0.5729929407005915\n",
            "loop 1 market state : step  1115 market state fut market_state    2.0\n",
            "Name: 2016-01-25 00:00:00, dtype: float64  future value 0.5727389693688758\n",
            "loop 1 market state : step  1116 market state fut market_state    1.0\n",
            "Name: 2016-01-26 00:00:00, dtype: float64  future value 0.5620040707223244\n",
            "loop 1 market state : step  1117 market state fut market_state    2.0\n",
            "Name: 2016-01-27 00:00:00, dtype: float64  future value 0.5648096169252226\n",
            "loop 1 market state : step  1118 market state fut market_state    2.0\n",
            "Name: 2016-01-28 00:00:00, dtype: float64  future value 0.5656719301967866\n",
            "loop 1 market state : step  1119 market state fut market_state    1.0\n",
            "Name: 2016-01-29 00:00:00, dtype: float64  future value 0.5552176080242534\n",
            "loop 1 market state : step  1120 market state fut market_state    0.0\n",
            "Name: 2016-02-01 00:00:00, dtype: float64  future value 0.5473590935549787\n",
            "loop 1 market state : step  1121 market state fut market_state    0.0\n",
            "Name: 2016-02-02 00:00:00, dtype: float64  future value 0.5469958550582796\n",
            "loop 1 market state : step  1122 market state fut market_state    0.0\n",
            "Name: 2016-02-03 00:00:00, dtype: float64  future value 0.5468924999174475\n",
            "loop 1 market state : step  1123 market state fut market_state    0.0\n",
            "Name: 2016-02-04 00:00:00, dtype: float64  future value 0.5401650868792517\n",
            "loop 1 market state : step  1124 market state fut market_state    1.0\n",
            "Name: 2016-02-05 00:00:00, dtype: float64  future value 0.5507080557474977\n",
            "loop 1 market state : step  1125 market state fut market_state    2.0\n",
            "Name: 2016-02-08 00:00:00, dtype: float64  future value 0.5598039102995387\n",
            "loop 1 market state : step  1126 market state fut market_state    2.0\n",
            "Name: 2016-02-09 00:00:00, dtype: float64  future value 0.5690297245440731\n",
            "loop 1 market state : step  1127 market state fut market_state    2.0\n",
            "Name: 2016-02-10 00:00:00, dtype: float64  future value 0.5663747948273791\n",
            "loop 1 market state : step  1128 market state fut market_state    2.0\n",
            "Name: 2016-02-11 00:00:00, dtype: float64  future value 0.56636005035314\n",
            "loop 1 market state : step  1129 market state fut market_state    4.0\n",
            "Name: 2016-02-12 00:00:00, dtype: float64  future value 0.5745463302882449\n",
            "loop 1 market state : step  1130 market state fut market_state    2.0\n",
            "Name: 2016-02-16 00:00:00, dtype: float64  future value 0.567390716774003\n",
            "loop 1 market state : step  1131 market state fut market_state    3.0\n",
            "Name: 2016-02-17 00:00:00, dtype: float64  future value 0.5699098105078515\n",
            "loop 1 market state : step  1132 market state fut market_state    4.0\n",
            "Name: 2016-02-18 00:00:00, dtype: float64  future value 0.57637730386574\n",
            "loop 1 market state : step  1133 market state fut market_state    4.0\n",
            "Name: 2016-02-19 00:00:00, dtype: float64  future value 0.5752994124239452\n",
            "loop 1 market state : step  1134 market state fut market_state    2.0\n",
            "Name: 2016-02-22 00:00:00, dtype: float64  future value 0.5706274193173625\n",
            "loop 1 market state : step  1135 market state fut market_state    4.0\n",
            "Name: 2016-02-23 00:00:00, dtype: float64  future value 0.584247606649518\n",
            "loop 1 market state : step  1136 market state fut market_state    4.0\n",
            "Name: 2016-02-24 00:00:00, dtype: float64  future value 0.5866396965552886\n",
            "loop 1 market state : step  1137 market state fut market_state    4.0\n",
            "Name: 2016-02-25 00:00:00, dtype: float64  future value 0.5886921966516058\n",
            "loop 1 market state : step  1138 market state fut market_state    4.0\n",
            "Name: 2016-02-26 00:00:00, dtype: float64  future value 0.5906383497135561\n",
            "loop 1 market state : step  1139 market state fut market_state    4.0\n",
            "Name: 2016-02-29 00:00:00, dtype: float64  future value 0.5911610731756671\n",
            "loop 1 market state : step  1140 market state fut market_state    4.0\n",
            "Name: 2016-03-01 00:00:00, dtype: float64  future value 0.5845163584845926\n",
            "loop 1 market state : step  1141 market state fut market_state    4.0\n",
            "Name: 2016-03-02 00:00:00, dtype: float64  future value 0.5874695650139591\n",
            "loop 1 market state : step  1142 market state fut market_state    2.0\n",
            "Name: 2016-03-03 00:00:00, dtype: float64  future value 0.5875610955158476\n",
            "loop 1 market state : step  1143 market state fut market_state    4.0\n",
            "Name: 2016-03-04 00:00:00, dtype: float64  future value 0.5971944537380377\n",
            "loop 1 market state : step  1144 market state fut market_state    4.0\n",
            "Name: 2016-03-07 00:00:00, dtype: float64  future value 0.5964414079267776\n",
            "loop 1 market state : step  1145 market state fut market_state    4.0\n",
            "Name: 2016-03-08 00:00:00, dtype: float64  future value 0.5953457798218881\n",
            "loop 1 market state : step  1146 market state fut market_state    4.0\n",
            "Name: 2016-03-09 00:00:00, dtype: float64  future value 0.5986799254819286\n",
            "loop 1 market state : step  1147 market state fut market_state    4.0\n",
            "Name: 2016-03-10 00:00:00, dtype: float64  future value 0.6026283611350883\n",
            "loop 1 market state : step  1148 market state fut market_state    4.0\n",
            "Name: 2016-03-11 00:00:00, dtype: float64  future value 0.6052833268809019\n",
            "loop 1 market state : step  1149 market state fut market_state    4.0\n",
            "Name: 2016-03-14 00:00:00, dtype: float64  future value 0.605879880506247\n",
            "loop 1 market state : step  1150 market state fut market_state    4.0\n",
            "Name: 2016-03-15 00:00:00, dtype: float64  future value 0.605348288860249\n",
            "loop 1 market state : step  1151 market state fut market_state    4.0\n",
            "Name: 2016-03-16 00:00:00, dtype: float64  future value 0.6014825155250909\n",
            "loop 1 market state : step  1152 market state fut market_state    2.0\n",
            "Name: 2016-03-17 00:00:00, dtype: float64  future value 0.6012551127159166\n",
            "loop 1 market state : step  1153 market state fut market_state    2.0\n",
            "Name: 2016-03-18 00:00:00, dtype: float64  future value 0.6015829505353067\n",
            "loop 1 market state : step  1154 market state fut market_state    4.0\n",
            "Name: 2016-03-21 00:00:00, dtype: float64  future value 0.6068868979445434\n",
            "loop 1 market state : step  1155 market state fut market_state    4.0\n",
            "Name: 2016-03-22 00:00:00, dtype: float64  future value 0.6095270471578786\n",
            "loop 1 market state : step  1156 market state fut market_state    4.0\n",
            "Name: 2016-03-23 00:00:00, dtype: float64  future value 0.6082837587265207\n",
            "loop 1 market state : step  1157 market state fut market_state    4.0\n",
            "Name: 2016-03-24 00:00:00, dtype: float64  future value 0.6121347515583201\n",
            "loop 1 market state : step  1158 market state fut market_state    4.0\n",
            "Name: 2016-03-28 00:00:00, dtype: float64  future value 0.6101708260994759\n",
            "loop 1 market state : step  1159 market state fut market_state    2.0\n",
            "Name: 2016-03-29 00:00:00, dtype: float64  future value 0.6039809527605492\n",
            "loop 1 market state : step  1160 market state fut market_state    4.0\n",
            "Name: 2016-03-30 00:00:00, dtype: float64  future value 0.6103273546098315\n",
            "loop 1 market state : step  1161 market state fut market_state    2.0\n",
            "Name: 2016-03-31 00:00:00, dtype: float64  future value 0.6030182044787691\n",
            "loop 1 market state : step  1162 market state fut market_state    2.0\n",
            "Name: 2016-04-01 00:00:00, dtype: float64  future value 0.6046985618653808\n",
            "loop 1 market state : step  1163 market state fut market_state    2.0\n",
            "Name: 2016-04-04 00:00:00, dtype: float64  future value 0.6030418171368952\n",
            "loop 1 market state : step  1164 market state fut market_state    3.0\n",
            "Name: 2016-04-05 00:00:00, dtype: float64  future value 0.6088684880082429\n",
            "loop 1 market state : step  1165 market state fut market_state    4.0\n",
            "Name: 2016-04-06 00:00:00, dtype: float64  future value 0.6149816110533195\n",
            "loop 1 market state : step  1166 market state fut market_state    4.0\n",
            "Name: 2016-04-07 00:00:00, dtype: float64  future value 0.6150879580876866\n",
            "loop 1 market state : step  1167 market state fut market_state    4.0\n",
            "Name: 2016-04-08 00:00:00, dtype: float64  future value 0.6144825362784545\n",
            "loop 1 market state : step  1168 market state fut market_state    4.0\n",
            "Name: 2016-04-11 00:00:00, dtype: float64  future value 0.6185018822595527\n",
            "loop 1 market state : step  1169 market state fut market_state    4.0\n",
            "Name: 2016-04-12 00:00:00, dtype: float64  future value 0.6204096421600179\n",
            "loop 1 market state : step  1170 market state fut market_state    4.0\n",
            "Name: 2016-04-13 00:00:00, dtype: float64  future value 0.6208821117925807\n",
            "loop 1 market state : step  1171 market state fut market_state    4.0\n",
            "Name: 2016-04-14 00:00:00, dtype: float64  future value 0.6176572332975234\n",
            "loop 1 market state : step  1172 market state fut market_state    4.0\n",
            "Name: 2016-04-15 00:00:00, dtype: float64  future value 0.617686794304241\n",
            "loop 1 market state : step  1173 market state fut market_state    2.0\n",
            "Name: 2016-04-18 00:00:00, dtype: float64  future value 0.6165675175121057\n",
            "loop 1 market state : step  1174 market state fut market_state    2.0\n",
            "Name: 2016-04-19 00:00:00, dtype: float64  future value 0.6177221952768706\n",
            "loop 1 market state : step  1175 market state fut market_state    2.0\n",
            "Name: 2016-04-20 00:00:00, dtype: float64  future value 0.61874103705879\n",
            "loop 1 market state : step  1176 market state fut market_state    2.0\n",
            "Name: 2016-04-21 00:00:00, dtype: float64  future value 0.6130295819963377\n",
            "loop 1 market state : step  1177 market state fut market_state    2.0\n",
            "Name: 2016-04-22 00:00:00, dtype: float64  future value 0.6099257589807671\n",
            "loop 1 market state : step  1178 market state fut market_state    2.0\n",
            "Name: 2016-04-25 00:00:00, dtype: float64  future value 0.6146892465601187\n",
            "loop 1 market state : step  1179 market state fut market_state    2.0\n",
            "Name: 2016-04-26 00:00:00, dtype: float64  future value 0.6093558102024038\n",
            "loop 1 market state : step  1180 market state fut market_state    2.0\n",
            "Name: 2016-04-27 00:00:00, dtype: float64  future value 0.6057381322039299\n",
            "loop 1 market state : step  1181 market state fut market_state    2.0\n",
            "Name: 2016-04-28 00:00:00, dtype: float64  future value 0.6055933559789579\n",
            "loop 1 market state : step  1182 market state fut market_state    2.0\n",
            "Name: 2016-04-29 00:00:00, dtype: float64  future value 0.6075158963827821\n",
            "loop 1 market state : step  1183 market state fut market_state    2.0\n",
            "Name: 2016-05-02 00:00:00, dtype: float64  future value 0.6079736575702254\n",
            "loop 1 market state : step  1184 market state fut market_state    3.0\n",
            "Name: 2016-05-03 00:00:00, dtype: float64  future value 0.6155633841753058\n",
            "loop 1 market state : step  1185 market state fut market_state    3.0\n",
            "Name: 2016-05-04 00:00:00, dtype: float64  future value 0.6096776636440828\n",
            "loop 1 market state : step  1186 market state fut market_state    3.0\n",
            "Name: 2016-05-05 00:00:00, dtype: float64  future value 0.6095743445323702\n",
            "loop 1 market state : step  1187 market state fut market_state    1.0\n",
            "Name: 2016-05-06 00:00:00, dtype: float64  future value 0.6044061970768594\n",
            "loop 1 market state : step  1188 market state fut market_state    3.0\n",
            "Name: 2016-05-09 00:00:00, dtype: float64  future value 0.6103273546098315\n",
            "loop 1 market state : step  1189 market state fut market_state    2.0\n",
            "Name: 2016-05-10 00:00:00, dtype: float64  future value 0.6045833823809257\n",
            "loop 1 market state : step  1190 market state fut market_state    1.0\n",
            "Name: 2016-05-11 00:00:00, dtype: float64  future value 0.6047074300492677\n",
            "loop 1 market state : step  1191 market state fut market_state    0.0\n",
            "Name: 2016-05-12 00:00:00, dtype: float64  future value 0.6024659563343808\n",
            "loop 1 market state : step  1192 market state fut market_state    3.0\n",
            "Name: 2016-05-13 00:00:00, dtype: float64  future value 0.6060925025167417\n",
            "loop 1 market state : step  1193 market state fut market_state    2.0\n",
            "Name: 2016-05-16 00:00:00, dtype: float64  future value 0.604828521557874\n",
            "loop 1 market state : step  1194 market state fut market_state    3.0\n",
            "Name: 2016-05-17 00:00:00, dtype: float64  future value 0.6131034121595719\n",
            "loop 1 market state : step  1195 market state fut market_state    3.0\n",
            "Name: 2016-05-18 00:00:00, dtype: float64  future value 0.6173796493076814\n",
            "loop 1 market state : step  1196 market state fut market_state    3.0\n",
            "Name: 2016-05-19 00:00:00, dtype: float64  future value 0.6172497256443078\n",
            "loop 1 market state : step  1197 market state fut market_state    3.0\n",
            "Name: 2016-05-20 00:00:00, dtype: float64  future value 0.6198957871771147\n",
            "loop 1 market state : step  1198 market state fut market_state    3.0\n",
            "Name: 2016-05-23 00:00:00, dtype: float64  future value 0.6192726287047879\n",
            "loop 1 market state : step  1199 market state fut market_state    2.0\n",
            "Name: 2016-05-24 00:00:00, dtype: float64  future value 0.6199755293645001\n",
            "loop 1 market state : step  1200 market state fut market_state    4.0\n",
            "Name: 2016-05-25 00:00:00, dtype: float64  future value 0.62172676075461\n",
            "loop 1 market state : step  1201 market state fut market_state    4.0\n",
            "Name: 2016-05-26 00:00:00, dtype: float64  future value 0.6199164076463852\n",
            "loop 1 market state : step  1202 market state fut market_state    4.0\n",
            "Name: 2016-05-27 00:00:00, dtype: float64  future value 0.6229523125228731\n",
            "loop 1 market state : step  1203 market state fut market_state    4.0\n",
            "Name: 2016-05-31 00:00:00, dtype: float64  future value 0.6237555761345617\n",
            "loop 1 market state : step  1204 market state fut market_state    4.0\n",
            "Name: 2016-06-01 00:00:00, dtype: float64  future value 0.6258199366036219\n",
            "loop 1 market state : step  1205 market state fut market_state    4.0\n",
            "Name: 2016-06-02 00:00:00, dtype: float64  future value 0.6247449289680029\n",
            "loop 1 market state : step  1206 market state fut market_state    2.0\n",
            "Name: 2016-06-03 00:00:00, dtype: float64  future value 0.6190127810827201\n",
            "loop 1 market state : step  1207 market state fut market_state    2.0\n",
            "Name: 2016-06-06 00:00:00, dtype: float64  future value 0.6139893741183818\n",
            "loop 1 market state : step  1208 market state fut market_state    1.0\n",
            "Name: 2016-06-07 00:00:00, dtype: float64  future value 0.6128848775342847\n",
            "loop 1 market state : step  1209 market state fut market_state    2.0\n",
            "Name: 2016-06-08 00:00:00, dtype: float64  future value 0.6117567325582622\n",
            "loop 1 market state : step  1210 market state fut market_state    2.0\n",
            "Name: 2016-06-09 00:00:00, dtype: float64  future value 0.6136733606426146\n",
            "loop 1 market state : step  1211 market state fut market_state    2.0\n",
            "Name: 2016-06-10 00:00:00, dtype: float64  future value 0.6116740342111411\n",
            "loop 1 market state : step  1212 market state fut market_state    3.0\n",
            "Name: 2016-06-13 00:00:00, dtype: float64  future value 0.6152267502302678\n",
            "loop 1 market state : step  1213 market state fut market_state    3.0\n",
            "Name: 2016-06-14 00:00:00, dtype: float64  future value 0.6168952829779359\n",
            "loop 1 market state : step  1214 market state fut market_state    3.0\n",
            "Name: 2016-06-15 00:00:00, dtype: float64  future value 0.6158764411960165\n",
            "loop 1 market state : step  1215 market state fut market_state    3.0\n",
            "Name: 2016-06-16 00:00:00, dtype: float64  future value 0.6241070623458772\n",
            "loop 1 market state : step  1216 market state fut market_state    0.0\n",
            "Name: 2016-06-17 00:00:00, dtype: float64  future value 0.6016892615405541\n",
            "loop 1 market state : step  1217 market state fut market_state    0.0\n",
            "Name: 2016-06-20 00:00:00, dtype: float64  future value 0.5908007905433833\n",
            "loop 1 market state : step  1218 market state fut market_state    0.0\n",
            "Name: 2016-06-21 00:00:00, dtype: float64  future value 0.6012994181968734\n",
            "loop 1 market state : step  1219 market state fut market_state    2.0\n",
            "Name: 2016-06-22 00:00:00, dtype: float64  future value 0.6115411543880316\n",
            "loop 1 market state : step  1220 market state fut market_state    2.0\n",
            "Name: 2016-06-23 00:00:00, dtype: float64  future value 0.6198367372219188\n",
            "loop 1 market state : step  1221 market state fut market_state    3.0\n",
            "Name: 2016-06-24 00:00:00, dtype: float64  future value 0.6210445526224078\n",
            "loop 1 market state : step  1222 market state fut market_state    3.0\n",
            "Name: 2016-06-27 00:00:00, dtype: float64  future value 0.616791964161544\n",
            "loop 1 market state : step  1223 market state fut market_state    3.0\n",
            "Name: 2016-06-28 00:00:00, dtype: float64  future value 0.6200936286842508\n",
            "loop 1 market state : step  1224 market state fut market_state    3.0\n",
            "Name: 2016-06-29 00:00:00, dtype: float64  future value 0.6195531688543657\n",
            "loop 1 market state : step  1225 market state fut market_state    4.0\n",
            "Name: 2016-06-30 00:00:00, dtype: float64  future value 0.6290034297483384\n",
            "loop 1 market state : step  1226 market state fut market_state    4.0\n",
            "Name: 2016-07-01 00:00:00, dtype: float64  future value 0.631147460641865\n",
            "loop 1 market state : step  1227 market state fut market_state    4.0\n",
            "Name: 2016-07-05 00:00:00, dtype: float64  future value 0.6355713584117635\n",
            "loop 1 market state : step  1228 market state fut market_state    4.0\n",
            "Name: 2016-07-06 00:00:00, dtype: float64  future value 0.6356570129186206\n",
            "loop 1 market state : step  1229 market state fut market_state    4.0\n",
            "Name: 2016-07-07 00:00:00, dtype: float64  future value 0.6390000627916679\n",
            "loop 1 market state : step  1230 market state fut market_state    4.0\n",
            "Name: 2016-07-08 00:00:00, dtype: float64  future value 0.6384064653260587\n",
            "loop 1 market state : step  1231 market state fut market_state    4.0\n",
            "Name: 2016-07-11 00:00:00, dtype: float64  future value 0.6399273380425791\n",
            "loop 1 market state : step  1232 market state fut market_state    4.0\n",
            "Name: 2016-07-12 00:00:00, dtype: float64  future value 0.6390089309755549\n",
            "loop 1 market state : step  1233 market state fut market_state    4.0\n",
            "Name: 2016-07-13 00:00:00, dtype: float64  future value 0.6417376911508036\n",
            "loop 1 market state : step  1234 market state fut market_state    4.0\n",
            "Name: 2016-07-14 00:00:00, dtype: float64  future value 0.639419395083827\n",
            "loop 1 market state : step  1235 market state fut market_state    4.0\n",
            "Name: 2016-07-15 00:00:00, dtype: float64  future value 0.6423312883210922\n",
            "loop 1 market state : step  1236 market state fut market_state    4.0\n",
            "Name: 2016-07-18 00:00:00, dtype: float64  future value 0.6403969235736452\n",
            "loop 1 market state : step  1237 market state fut market_state    4.0\n",
            "Name: 2016-07-19 00:00:00, dtype: float64  future value 0.6406036338553094\n",
            "loop 1 market state : step  1238 market state fut market_state    2.0\n",
            "Name: 2016-07-20 00:00:00, dtype: float64  future value 0.6398358432744895\n",
            "loop 1 market state : step  1239 market state fut market_state    3.0\n",
            "Name: 2016-07-21 00:00:00, dtype: float64  future value 0.6408635535356165\n",
            "loop 1 market state : step  1240 market state fut market_state    2.0\n",
            "Name: 2016-07-22 00:00:00, dtype: float64  future value 0.6419090001645178\n",
            "loop 1 market state : step  1241 market state fut market_state    3.0\n",
            "Name: 2016-07-25 00:00:00, dtype: float64  future value 0.6410939122092061\n",
            "loop 1 market state : step  1242 market state fut market_state    2.0\n",
            "Name: 2016-07-26 00:00:00, dtype: float64  future value 0.6370155165682325\n",
            "loop 1 market state : step  1243 market state fut market_state    2.0\n",
            "Name: 2016-07-27 00:00:00, dtype: float64  future value 0.6390118871352908\n",
            "loop 1 market state : step  1244 market state fut market_state    2.0\n",
            "Name: 2016-07-28 00:00:00, dtype: float64  future value 0.6391477231181362\n",
            "loop 1 market state : step  1245 market state fut market_state    4.0\n",
            "Name: 2016-07-29 00:00:00, dtype: float64  future value 0.6446466282283331\n",
            "loop 1 market state : step  1246 market state fut market_state    4.0\n",
            "Name: 2016-08-01 00:00:00, dtype: float64  future value 0.6440618271836921\n",
            "loop 1 market state : step  1247 market state fut market_state    4.0\n",
            "Name: 2016-08-02 00:00:00, dtype: float64  future value 0.6443128783847916\n",
            "loop 1 market state : step  1248 market state fut market_state    4.0\n",
            "Name: 2016-08-03 00:00:00, dtype: float64  future value 0.6424671243039376\n",
            "loop 1 market state : step  1249 market state fut market_state    4.0\n",
            "Name: 2016-08-04 00:00:00, dtype: float64  future value 0.645508941499897\n",
            "loop 1 market state : step  1250 market state fut market_state    4.0\n",
            "Name: 2016-08-05 00:00:00, dtype: float64  future value 0.6449950865169938\n",
            "loop 1 market state : step  1251 market state fut market_state    4.0\n",
            "Name: 2016-08-08 00:00:00, dtype: float64  future value 0.6467964990877714\n",
            "loop 1 market state : step  1252 market state fut market_state    2.0\n",
            "Name: 2016-08-09 00:00:00, dtype: float64  future value 0.6432526512525316\n",
            "loop 1 market state : step  1253 market state fut market_state    3.0\n",
            "Name: 2016-08-10 00:00:00, dtype: float64  future value 0.6444546266871087\n",
            "loop 1 market state : step  1254 market state fut market_state    4.0\n",
            "Name: 2016-08-11 00:00:00, dtype: float64  future value 0.6458721802919166\n",
            "loop 1 market state : step  1255 market state fut market_state    2.0\n",
            "Name: 2016-08-12 00:00:00, dtype: float64  future value 0.6449419488812697\n",
            "loop 1 market state : step  1256 market state fut market_state    2.0\n",
            "Name: 2016-08-15 00:00:00, dtype: float64  future value 0.6445786383263312\n",
            "loop 1 market state : step  1257 market state fut market_state    3.0\n",
            "Name: 2016-08-16 00:00:00, dtype: float64  future value 0.6458367069657273\n",
            "loop 1 market state : step  1258 market state fut market_state    2.0\n",
            "Name: 2016-08-17 00:00:00, dtype: float64  future value 0.6424523438005787\n",
            "loop 1 market state : step  1259 market state fut market_state    2.0\n",
            "Name: 2016-08-18 00:00:00, dtype: float64  future value 0.6415752503209765\n",
            "loop 1 market state : step  1260 market state fut market_state    2.0\n",
            "Name: 2016-08-19 00:00:00, dtype: float64  future value 0.6405623205632082\n",
            "loop 1 market state : step  1261 market state fut market_state    2.0\n",
            "Name: 2016-08-22 00:00:00, dtype: float64  future value 0.6439112106974878\n",
            "loop 1 market state : step  1262 market state fut market_state    2.0\n",
            "Name: 2016-08-23 00:00:00, dtype: float64  future value 0.6426532138210107\n",
            "loop 1 market state : step  1263 market state fut market_state    1.0\n",
            "Name: 2016-08-24 00:00:00, dtype: float64  future value 0.6411263570220997\n",
            "loop 1 market state : step  1264 market state fut market_state    2.0\n",
            "Name: 2016-08-25 00:00:00, dtype: float64  future value 0.6410998242333572\n",
            "loop 1 market state : step  1265 market state fut market_state    3.0\n",
            "Name: 2016-08-26 00:00:00, dtype: float64  future value 0.6437931110824165\n",
            "loop 1 market state : step  1266 market state fut market_state    3.0\n",
            "Name: 2016-08-29 00:00:00, dtype: float64  future value 0.6457126953265048\n",
            "loop 1 market state : step  1267 market state fut market_state    3.0\n",
            "Name: 2016-08-30 00:00:00, dtype: float64  future value 0.6456181726357606\n",
            "loop 1 market state : step  1268 market state fut market_state    3.0\n",
            "Name: 2016-08-31 00:00:00, dtype: float64  future value 0.644182954721418\n",
            "loop 1 market state : step  1269 market state fut market_state    0.0\n",
            "Name: 2016-09-01 00:00:00, dtype: float64  future value 0.6283862559490433\n",
            "loop 1 market state : step  1270 market state fut market_state    1.0\n",
            "Name: 2016-09-02 00:00:00, dtype: float64  future value 0.6376091140338417\n",
            "loop 1 market state : step  1271 market state fut market_state    0.0\n",
            "Name: 2016-09-06 00:00:00, dtype: float64  future value 0.6281529411157178\n",
            "loop 1 market state : step  1272 market state fut market_state    0.0\n",
            "Name: 2016-09-07 00:00:00, dtype: float64  future value 0.6277837902995471\n",
            "loop 1 market state : step  1273 market state fut market_state    0.0\n",
            "Name: 2016-09-08 00:00:00, dtype: float64  future value 0.6341302281779491\n",
            "loop 1 market state : step  1274 market state fut market_state    0.0\n",
            "Name: 2016-09-09 00:00:00, dtype: float64  future value 0.6317381019477383\n",
            "loop 1 market state : step  1275 market state fut market_state    1.0\n",
            "Name: 2016-09-12 00:00:00, dtype: float64  future value 0.6317263496623547\n",
            "loop 1 market state : step  1276 market state fut market_state    2.0\n",
            "Name: 2016-09-13 00:00:00, dtype: float64  future value 0.6319153232809243\n",
            "loop 1 market state : step  1277 market state fut market_state    2.0\n",
            "Name: 2016-09-14 00:00:00, dtype: float64  future value 0.6388140453328343\n",
            "loop 1 market state : step  1278 market state fut market_state    3.0\n",
            "Name: 2016-09-15 00:00:00, dtype: float64  future value 0.6429661990788026\n",
            "loop 1 market state : step  1279 market state fut market_state    2.0\n",
            "Name: 2016-09-16 00:00:00, dtype: float64  future value 0.6392776467815098\n",
            "loop 1 market state : step  1280 market state fut market_state    2.0\n",
            "Name: 2016-09-19 00:00:00, dtype: float64  future value 0.63378768220876\n",
            "loop 1 market state : step  1281 market state fut market_state    2.0\n",
            "Name: 2016-09-20 00:00:00, dtype: float64  future value 0.6378719178156455\n",
            "loop 1 market state : step  1282 market state fut market_state    2.0\n",
            "Name: 2016-09-21 00:00:00, dtype: float64  future value 0.6412504407195616\n",
            "loop 1 market state : step  1283 market state fut market_state    1.0\n",
            "Name: 2016-09-22 00:00:00, dtype: float64  future value 0.6352730815990909\n",
            "loop 1 market state : step  1284 market state fut market_state    2.0\n",
            "Name: 2016-09-23 00:00:00, dtype: float64  future value 0.6403349180493545\n",
            "loop 1 market state : step  1285 market state fut market_state    2.0\n",
            "Name: 2016-09-26 00:00:00, dtype: float64  future value 0.6382469806559674\n",
            "loop 1 market state : step  1286 market state fut market_state    1.0\n",
            "Name: 2016-09-27 00:00:00, dtype: float64  future value 0.6350841079805214\n",
            "loop 1 market state : step  1287 market state fut market_state    1.0\n",
            "Name: 2016-09-28 00:00:00, dtype: float64  future value 0.6378128678604494\n",
            "loop 1 market state : step  1288 market state fut market_state    2.0\n",
            "Name: 2016-09-29 00:00:00, dtype: float64  future value 0.6381200131523297\n",
            "loop 1 market state : step  1289 market state fut market_state    1.0\n",
            "Name: 2016-09-30 00:00:00, dtype: float64  future value 0.6360439001025655\n",
            "loop 1 market state : step  1290 market state fut market_state    2.0\n",
            "Name: 2016-10-03 00:00:00, dtype: float64  future value 0.6389734579446861\n",
            "loop 1 market state : step  1291 market state fut market_state    1.0\n",
            "Name: 2016-10-04 00:00:00, dtype: float64  future value 0.6310204928429066\n",
            "loop 1 market state : step  1292 market state fut market_state    1.0\n",
            "Name: 2016-10-05 00:00:00, dtype: float64  future value 0.63174401426721\n",
            "loop 1 market state : step  1293 market state fut market_state    1.0\n",
            "Name: 2016-10-06 00:00:00, dtype: float64  future value 0.6297860728907565\n",
            "loop 1 market state : step  1294 market state fut market_state    1.0\n",
            "Name: 2016-10-07 00:00:00, dtype: float64  future value 0.6299130403943942\n",
            "loop 1 market state : step  1295 market state fut market_state    1.0\n",
            "Name: 2016-10-10 00:00:00, dtype: float64  future value 0.6279993684697778\n",
            "loop 1 market state : step  1296 market state fut market_state    2.0\n",
            "Name: 2016-10-11 00:00:00, dtype: float64  future value 0.6318680979646718\n",
            "loop 1 market state : step  1297 market state fut market_state    2.0\n",
            "Name: 2016-10-12 00:00:00, dtype: float64  future value 0.6332531344030262\n",
            "loop 1 market state : step  1298 market state fut market_state    2.0\n",
            "Name: 2016-10-13 00:00:00, dtype: float64  future value 0.632381952947575\n",
            "loop 1 market state : step  1299 market state fut market_state    3.0\n",
            "Name: 2016-10-14 00:00:00, dtype: float64  future value 0.6323287432536115\n",
            "loop 1 market state : step  1300 market state fut market_state    2.0\n",
            "Name: 2016-10-17 00:00:00, dtype: float64  future value 0.6353322033172056\n",
            "loop 1 market state : step  1301 market state fut market_state    3.0\n",
            "Name: 2016-10-18 00:00:00, dtype: float64  future value 0.6329193845594848\n",
            "loop 1 market state : step  1302 market state fut market_state    2.0\n",
            "Name: 2016-10-19 00:00:00, dtype: float64  future value 0.6318178444304442\n",
            "loop 1 market state : step  1303 market state fut market_state    1.0\n",
            "Name: 2016-10-20 00:00:00, dtype: float64  future value 0.6299307770574889\n",
            "loop 1 market state : step  1304 market state fut market_state    0.0\n",
            "Name: 2016-10-21 00:00:00, dtype: float64  future value 0.6279727636227961\n",
            "loop 1 market state : step  1305 market state fut market_state    0.0\n",
            "Name: 2016-10-24 00:00:00, dtype: float64  future value 0.627895977299826\n",
            "loop 1 market state : step  1306 market state fut market_state    0.0\n",
            "Name: 2016-10-25 00:00:00, dtype: float64  future value 0.6236345206550752\n",
            "loop 1 market state : step  1307 market state fut market_state    0.0\n",
            "Name: 2016-10-26 00:00:00, dtype: float64  future value 0.6195649931979886\n",
            "loop 1 market state : step  1308 market state fut market_state    0.0\n",
            "Name: 2016-10-27 00:00:00, dtype: float64  future value 0.6168244089744377\n",
            "loop 1 market state : step  1309 market state fut market_state    0.0\n",
            "Name: 2016-10-28 00:00:00, dtype: float64  future value 0.6157966990086312\n",
            "loop 1 market state : step  1310 market state fut market_state    2.0\n",
            "Name: 2016-10-31 00:00:00, dtype: float64  future value 0.6294818840539328\n",
            "loop 1 market state : step  1311 market state fut market_state    2.0\n",
            "Name: 2016-11-01 00:00:00, dtype: float64  future value 0.6318562736210489\n",
            "loop 1 market state : step  1312 market state fut market_state    2.0\n",
            "Name: 2016-11-02 00:00:00, dtype: float64  future value 0.6388553586249354\n",
            "loop 1 market state : step  1313 market state fut market_state    3.0\n",
            "Name: 2016-11-03 00:00:00, dtype: float64  future value 0.6401016029207085\n",
            "loop 1 market state : step  1314 market state fut market_state    2.0\n",
            "Name: 2016-11-04 00:00:00, dtype: float64  future value 0.6392067727780115\n",
            "loop 1 market state : step  1315 market state fut market_state    4.0\n",
            "Name: 2016-11-07 00:00:00, dtype: float64  future value 0.6391329426147774\n",
            "loop 1 market state : step  1316 market state fut market_state    4.0\n",
            "Name: 2016-11-08 00:00:00, dtype: float64  future value 0.6439141668572238\n",
            "loop 1 market state : step  1317 market state fut market_state    4.0\n",
            "Name: 2016-11-09 00:00:00, dtype: float64  future value 0.6428953247799837\n",
            "loop 1 market state : step  1318 market state fut market_state    4.0\n",
            "Name: 2016-11-10 00:00:00, dtype: float64  future value 0.6459017410033138\n",
            "loop 1 market state : step  1319 market state fut market_state    4.0\n",
            "Name: 2016-11-11 00:00:00, dtype: float64  future value 0.644360103701044\n",
            "loop 1 market state : step  1320 market state fut market_state    4.0\n",
            "Name: 2016-11-14 00:00:00, dtype: float64  future value 0.6491679327904721\n",
            "loop 1 market state : step  1321 market state fut market_state    4.0\n",
            "Name: 2016-11-15 00:00:00, dtype: float64  future value 0.6505736617563365\n",
            "loop 1 market state : step  1322 market state fut market_state    4.0\n",
            "Name: 2016-11-16 00:00:00, dtype: float64  future value 0.6510993413781833\n",
            "loop 1 market state : step  1323 market state fut market_state    4.0\n",
            "Name: 2016-11-17 00:00:00, dtype: float64  future value 0.6536479961187495\n",
            "loop 1 market state : step  1324 market state fut market_state    4.0\n",
            "Name: 2016-11-18 00:00:00, dtype: float64  future value 0.6502133794193734\n",
            "loop 1 market state : step  1325 market state fut market_state    4.0\n",
            "Name: 2016-11-21 00:00:00, dtype: float64  future value 0.6510816047150886\n",
            "loop 1 market state : step  1326 market state fut market_state    2.0\n",
            "Name: 2016-11-22 00:00:00, dtype: float64  future value 0.6493540223075451\n",
            "loop 1 market state : step  1327 market state fut market_state    2.0\n",
            "Name: 2016-11-23 00:00:00, dtype: float64  future value 0.6470711992714373\n",
            "loop 1 market state : step  1328 market state fut market_state    2.0\n",
            "Name: 2016-11-25 00:00:00, dtype: float64  future value 0.6473280907337693\n",
            "loop 1 market state : step  1329 market state fut market_state    3.0\n",
            "Name: 2016-11-28 00:00:00, dtype: float64  future value 0.6510963852184474\n",
            "loop 1 market state : step  1330 market state fut market_state    3.0\n",
            "Name: 2016-11-29 00:00:00, dtype: float64  future value 0.6533172021396234\n",
            "loop 1 market state : step  1331 market state fut market_state    4.0\n",
            "Name: 2016-11-30 00:00:00, dtype: float64  future value 0.6619169744009755\n",
            "loop 1 market state : step  1332 market state fut market_state    4.0\n",
            "Name: 2016-12-01 00:00:00, dtype: float64  future value 0.6633462799958465\n",
            "loop 1 market state : step  1333 market state fut market_state    4.0\n",
            "Name: 2016-12-02 00:00:00, dtype: float64  future value 0.6672858834942388\n",
            "loop 1 market state : step  1334 market state fut market_state    4.0\n",
            "Name: 2016-12-05 00:00:00, dtype: float64  future value 0.6665268893343872\n",
            "loop 1 market state : step  1335 market state fut market_state    4.0\n",
            "Name: 2016-12-06 00:00:00, dtype: float64  future value 0.6708858251249386\n",
            "loop 1 market state : step  1336 market state fut market_state    4.0\n",
            "Name: 2016-12-07 00:00:00, dtype: float64  future value 0.6654401294133847\n",
            "loop 1 market state : step  1337 market state fut market_state    4.0\n",
            "Name: 2016-12-08 00:00:00, dtype: float64  future value 0.6680241851265803\n",
            "loop 1 market state : step  1338 market state fut market_state    2.0\n",
            "Name: 2016-12-09 00:00:00, dtype: float64  future value 0.6668547268584567\n",
            "loop 1 market state : step  1339 market state fut market_state    4.0\n",
            "Name: 2016-12-12 00:00:00, dtype: float64  future value 0.6681718454530486\n",
            "loop 1 market state : step  1340 market state fut market_state    2.0\n",
            "Name: 2016-12-13 00:00:00, dtype: float64  future value 0.6706023288156249\n",
            "loop 1 market state : step  1341 market state fut market_state    3.0\n",
            "Name: 2016-12-14 00:00:00, dtype: float64  future value 0.6689544165372274\n",
            "loop 1 market state : step  1342 market state fut market_state    2.0\n",
            "Name: 2016-12-15 00:00:00, dtype: float64  future value 0.6677081719461337\n",
            "loop 1 market state : step  1343 market state fut market_state    3.0\n",
            "Name: 2016-12-16 00:00:00, dtype: float64  future value 0.6685439524289554\n",
            "loop 1 market state : step  1344 market state fut market_state    3.0\n",
            "Name: 2016-12-19 00:00:00, dtype: float64  future value 0.6700470884823809\n",
            "loop 1 market state : step  1345 market state fut market_state    2.0\n",
            "Name: 2016-12-20 00:00:00, dtype: float64  future value 0.6644478204202077\n",
            "loop 1 market state : step  1346 market state fut market_state    2.0\n",
            "Name: 2016-12-21 00:00:00, dtype: float64  future value 0.6642529347774869\n",
            "loop 1 market state : step  1347 market state fut market_state    2.0\n",
            "Name: 2016-12-22 00:00:00, dtype: float64  future value 0.6611727604491622\n",
            "loop 1 market state : step  1348 market state fut market_state    2.0\n",
            "Name: 2016-12-23 00:00:00, dtype: float64  future value 0.6667838528549584\n",
            "loop 1 market state : step  1349 market state fut market_state    3.0\n",
            "Name: 2016-12-27 00:00:00, dtype: float64  future value 0.670599372655889\n",
            "loop 1 market state : step  1350 market state fut market_state    3.0\n",
            "Name: 2016-12-28 00:00:00, dtype: float64  future value 0.6700825615132499\n",
            "loop 1 market state : step  1351 market state fut market_state    4.0\n",
            "Name: 2016-12-29 00:00:00, dtype: float64  future value 0.6724392144172713\n",
            "loop 1 market state : step  1352 market state fut market_state    3.0\n",
            "Name: 2016-12-30 00:00:00, dtype: float64  future value 0.6700530005065323\n",
            "loop 1 market state : step  1353 market state fut market_state    3.0\n",
            "Name: 2017-01-03 00:00:00, dtype: float64  future value 0.6700530005065323\n",
            "loop 1 market state : step  1354 market state fut market_state    4.0\n",
            "Name: 2017-01-04 00:00:00, dtype: float64  future value 0.6719490081216138\n",
            "loop 1 market state : step  1355 market state fut market_state    3.0\n",
            "Name: 2017-01-05 00:00:00, dtype: float64  future value 0.6705078058295602\n",
            "loop 1 market state : step  1356 market state fut market_state    2.0\n",
            "Name: 2017-01-06 00:00:00, dtype: float64  future value 0.6717481383965027\n",
            "loop 1 market state : step  1357 market state fut market_state    2.0\n",
            "Name: 2017-01-09 00:00:00, dtype: float64  future value 0.6697547239891803\n",
            "loop 1 market state : step  1358 market state fut market_state    3.0\n",
            "Name: 2017-01-10 00:00:00, dtype: float64  future value 0.6709360066009269\n",
            "loop 1 market state : step  1359 market state fut market_state    2.0\n",
            "Name: 2017-01-11 00:00:00, dtype: float64  future value 0.6685143914222378\n",
            "loop 1 market state : step  1360 market state fut market_state    3.0\n",
            "Name: 2017-01-12 00:00:00, dtype: float64  future value 0.670764769645452\n",
            "loop 1 market state : step  1361 market state fut market_state    2.0\n",
            "Name: 2017-01-13 00:00:00, dtype: float64  future value 0.6689603285613787\n",
            "loop 1 market state : step  1362 market state fut market_state    4.0\n",
            "Name: 2017-01-17 00:00:00, dtype: float64  future value 0.6733517812230629\n",
            "loop 1 market state : step  1363 market state fut market_state    4.0\n",
            "Name: 2017-01-18 00:00:00, dtype: float64  future value 0.6787561636425157\n",
            "loop 1 market state : step  1364 market state fut market_state    4.0\n",
            "Name: 2017-01-19 00:00:00, dtype: float64  future value 0.6782570171047319\n",
            "loop 1 market state : step  1365 market state fut market_state    4.0\n",
            "Name: 2017-01-20 00:00:00, dtype: float64  future value 0.6776693316632738\n",
            "loop 1 market state : step  1366 market state fut market_state    4.0\n",
            "Name: 2017-01-23 00:00:00, dtype: float64  future value 0.673596848341772\n",
            "loop 1 market state : step  1367 market state fut market_state    2.0\n",
            "Name: 2017-01-24 00:00:00, dtype: float64  future value 0.672997410910251\n",
            "loop 1 market state : step  1368 market state fut market_state    2.0\n",
            "Name: 2017-01-25 00:00:00, dtype: float64  future value 0.6731982088724434\n",
            "loop 1 market state : step  1369 market state fut market_state    2.0\n",
            "Name: 2017-01-26 00:00:00, dtype: float64  future value 0.673582140191973\n",
            "loop 1 market state : step  1370 market state fut market_state    3.0\n",
            "Name: 2017-01-27 00:00:00, dtype: float64  future value 0.6784755514346984\n",
            "loop 1 market state : step  1371 market state fut market_state    3.0\n",
            "Name: 2017-01-30 00:00:00, dtype: float64  future value 0.6770403335203558\n",
            "loop 1 market state : step  1372 market state fut market_state    3.0\n",
            "Name: 2017-01-31 00:00:00, dtype: float64  future value 0.6771939058709752\n",
            "loop 1 market state : step  1373 market state fut market_state    3.0\n",
            "Name: 2017-02-01 00:00:00, dtype: float64  future value 0.6776634196391226\n",
            "loop 1 market state : step  1374 market state fut market_state    4.0\n",
            "Name: 2017-02-02 00:00:00, dtype: float64  future value 0.6815617098454138\n",
            "loop 1 market state : step  1375 market state fut market_state    4.0\n",
            "Name: 2017-02-03 00:00:00, dtype: float64  future value 0.6839921932079899\n",
            "loop 1 market state : step  1376 market state fut market_state    4.0\n",
            "Name: 2017-02-06 00:00:00, dtype: float64  future value 0.6875803101997461\n",
            "loop 1 market state : step  1377 market state fut market_state    4.0\n",
            "Name: 2017-02-07 00:00:00, dtype: float64  future value 0.6903356749266559\n",
            "loop 1 market state : step  1378 market state fut market_state    4.0\n",
            "Name: 2017-02-08 00:00:00, dtype: float64  future value 0.6937820439114157\n",
            "loop 1 market state : step  1379 market state fut market_state    4.0\n",
            "Name: 2017-02-09 00:00:00, dtype: float64  future value 0.6931825344216553\n",
            "loop 1 market state : step  1380 market state fut market_state    4.0\n",
            "Name: 2017-02-10 00:00:00, dtype: float64  future value 0.6943460803703072\n",
            "loop 1 market state : step  1381 market state fut market_state    4.0\n",
            "Name: 2017-02-13 00:00:00, dtype: float64  future value 0.6985455314907673\n",
            "loop 1 market state : step  1382 market state fut market_state    4.0\n",
            "Name: 2017-02-14 00:00:00, dtype: float64  future value 0.6977895652535704\n",
            "loop 1 market state : step  1383 market state fut market_state    4.0\n",
            "Name: 2017-02-15 00:00:00, dtype: float64  future value 0.6980819300420917\n",
            "loop 1 market state : step  1384 market state fut market_state    4.0\n",
            "Name: 2017-02-16 00:00:00, dtype: float64  future value 0.699124420511257\n",
            "loop 1 market state : step  1385 market state fut market_state    4.0\n",
            "Name: 2017-02-17 00:00:00, dtype: float64  future value 0.6998361172966169\n",
            "loop 1 market state : step  1386 market state fut market_state    2.0\n",
            "Name: 2017-02-21 00:00:00, dtype: float64  future value 0.6980316765078641\n",
            "loop 1 market state : step  1387 market state fut market_state    4.0\n",
            "Name: 2017-02-22 00:00:00, dtype: float64  future value 0.707576460092581\n",
            "loop 1 market state : step  1388 market state fut market_state    4.0\n",
            "Name: 2017-02-23 00:00:00, dtype: float64  future value 0.703430146607845\n",
            "loop 1 market state : step  1389 market state fut market_state    4.0\n",
            "Name: 2017-02-24 00:00:00, dtype: float64  future value 0.7037845889788964\n",
            "loop 1 market state : step  1390 market state fut market_state    4.0\n",
            "Name: 2017-02-27 00:00:00, dtype: float64  future value 0.7014781175508632\n",
            "loop 1 market state : step  1391 market state fut market_state    3.0\n",
            "Name: 2017-02-28 00:00:00, dtype: float64  future value 0.6994344496093132\n",
            "loop 1 market state : step  1392 market state fut market_state    2.0\n",
            "Name: 2017-03-01 00:00:00, dtype: float64  future value 0.6978367905698228\n",
            "loop 1 market state : step  1393 market state fut market_state    2.0\n",
            "Name: 2017-03-02 00:00:00, dtype: float64  future value 0.6983949870628026\n",
            "loop 1 market state : step  1394 market state fut market_state    2.0\n",
            "Name: 2017-03-03 00:00:00, dtype: float64  future value 0.7006778100989103\n",
            "loop 1 market state : step  1395 market state fut market_state    2.0\n",
            "Name: 2017-03-06 00:00:00, dtype: float64  future value 0.7009347015612423\n",
            "loop 1 market state : step  1396 market state fut market_state    2.0\n",
            "Name: 2017-03-07 00:00:00, dtype: float64  future value 0.6985662240182774\n",
            "loop 1 market state : step  1397 market state fut market_state    3.0\n",
            "Name: 2017-03-08 00:00:00, dtype: float64  future value 0.7044165435768708\n",
            "loop 1 market state : step  1398 market state fut market_state    3.0\n",
            "Name: 2017-03-09 00:00:00, dtype: float64  future value 0.7032706619377536\n",
            "loop 1 market state : step  1399 market state fut market_state    3.0\n",
            "Name: 2017-03-10 00:00:00, dtype: float64  future value 0.7023463428465784\n",
            "loop 1 market state : step  1400 market state fut market_state    3.0\n",
            "Name: 2017-03-13 00:00:00, dtype: float64  future value 0.7009347015612423\n",
            "loop 1 market state : step  1401 market state fut market_state    2.0\n",
            "Name: 2017-03-14 00:00:00, dtype: float64  future value 0.69223752280297\n",
            "loop 1 market state : step  1402 market state fut market_state    2.0\n",
            "Name: 2017-03-15 00:00:00, dtype: float64  future value 0.6935457729183544\n",
            "loop 1 market state : step  1403 market state fut market_state    2.0\n",
            "Name: 2017-03-16 00:00:00, dtype: float64  future value 0.6928104274457486\n",
            "loop 1 market state : step  1404 market state fut market_state    2.0\n",
            "Name: 2017-03-17 00:00:00, dtype: float64  future value 0.6922256981640266\n",
            "loop 1 market state : step  1405 market state fut market_state    2.0\n",
            "Name: 2017-03-20 00:00:00, dtype: float64  future value 0.6915199136981384\n",
            "loop 1 market state : step  1406 market state fut market_state    3.0\n",
            "Name: 2017-03-21 00:00:00, dtype: float64  future value 0.6965344524785896\n",
            "loop 1 market state : step  1407 market state fut market_state    3.0\n",
            "Name: 2017-03-22 00:00:00, dtype: float64  future value 0.6972904187157866\n",
            "loop 1 market state : step  1408 market state fut market_state    3.0\n",
            "Name: 2017-03-23 00:00:00, dtype: float64  future value 0.6993370428170725\n",
            "loop 1 market state : step  1409 market state fut market_state    3.0\n",
            "Name: 2017-03-24 00:00:00, dtype: float64  future value 0.6977600045421734\n",
            "loop 1 market state : step  1410 market state fut market_state    3.0\n",
            "Name: 2017-03-27 00:00:00, dtype: float64  future value 0.6966141949612955\n",
            "loop 1 market state : step  1411 market state fut market_state    2.0\n",
            "Name: 2017-03-28 00:00:00, dtype: float64  future value 0.6970039662467371\n",
            "loop 1 market state : step  1412 market state fut market_state    1.0\n",
            "Name: 2017-03-29 00:00:00, dtype: float64  future value 0.6948747158565693\n",
            "loop 1 market state : step  1413 market state fut market_state    1.0\n",
            "Name: 2017-03-30 00:00:00, dtype: float64  future value 0.6962154831384071\n",
            "loop 1 market state : step  1414 market state fut market_state    1.0\n",
            "Name: 2017-03-31 00:00:00, dtype: float64  future value 0.6956396223358926\n",
            "loop 1 market state : step  1415 market state fut market_state    1.0\n",
            "Name: 2017-04-03 00:00:00, dtype: float64  future value 0.6961180042879271\n",
            "loop 1 market state : step  1416 market state fut market_state    1.0\n",
            "Name: 2017-04-04 00:00:00, dtype: float64  future value 0.6951198550335176\n",
            "loop 1 market state : step  1417 market state fut market_state    1.0\n",
            "Name: 2017-04-05 00:00:00, dtype: float64  future value 0.692506238608925\n",
            "loop 1 market state : step  1418 market state fut market_state    0.0\n",
            "Name: 2017-04-06 00:00:00, dtype: float64  future value 0.6877870201860898\n",
            "loop 1 market state : step  1419 market state fut market_state    1.0\n",
            "Name: 2017-04-07 00:00:00, dtype: float64  future value 0.6937111699079174\n",
            "loop 1 market state : step  1420 market state fut market_state    1.0\n",
            "Name: 2017-04-10 00:00:00, dtype: float64  future value 0.6916970626777644\n",
            "loop 1 market state : step  1421 market state fut market_state    0.0\n",
            "Name: 2017-04-11 00:00:00, dtype: float64  future value 0.6905098680418668\n",
            "loop 1 market state : step  1422 market state fut market_state    2.0\n",
            "Name: 2017-04-12 00:00:00, dtype: float64  future value 0.6957282330024857\n",
            "loop 1 market state : step  1423 market state fut market_state    2.0\n",
            "Name: 2017-04-13 00:00:00, dtype: float64  future value 0.6936166469218527\n",
            "loop 1 market state : step  1424 market state fut market_state    3.0\n",
            "Name: 2017-04-17 00:00:00, dtype: float64  future value 0.7011354992281142\n",
            "loop 1 market state : step  1425 market state fut market_state    4.0\n",
            "Name: 2017-04-18 00:00:00, dtype: float64  future value 0.7054058964103119\n",
            "loop 1 market state : step  1426 market state fut market_state    4.0\n",
            "Name: 2017-04-19 00:00:00, dtype: float64  future value 0.7050632783828836\n",
            "loop 1 market state : step  1427 market state fut market_state    4.0\n",
            "Name: 2017-04-20 00:00:00, dtype: float64  future value 0.7054531220218849\n",
            "loop 1 market state : step  1428 market state fut market_state    4.0\n",
            "Name: 2017-04-21 00:00:00, dtype: float64  future value 0.7041034862608395\n",
            "loop 1 market state : step  1429 market state fut market_state    4.0\n",
            "Name: 2017-04-24 00:00:00, dtype: float64  future value 0.7053231980631908\n",
            "loop 1 market state : step  1430 market state fut market_state    4.0\n",
            "Name: 2017-04-25 00:00:00, dtype: float64  future value 0.706161862647509\n",
            "loop 1 market state : step  1431 market state fut market_state    3.0\n",
            "Name: 2017-04-26 00:00:00, dtype: float64  future value 0.7052640763450759\n",
            "loop 1 market state : step  1432 market state fut market_state    4.0\n",
            "Name: 2017-04-27 00:00:00, dtype: float64  future value 0.7056746125115875\n",
            "loop 1 market state : step  1433 market state fut market_state    4.0\n",
            "Name: 2017-04-28 00:00:00, dtype: float64  future value 0.7085599009018709\n",
            "loop 1 market state : step  1434 market state fut market_state    4.0\n",
            "Name: 2017-05-01 00:00:00, dtype: float64  future value 0.7085864336906132\n",
            "loop 1 market state : step  1435 market state fut market_state    4.0\n",
            "Name: 2017-05-02 00:00:00, dtype: float64  future value 0.7078599564018947\n",
            "loop 1 market state : step  1436 market state fut market_state    4.0\n",
            "Name: 2017-05-03 00:00:00, dtype: float64  future value 0.7086602638538474\n",
            "loop 1 market state : step  1437 market state fut market_state    4.0\n",
            "Name: 2017-05-04 00:00:00, dtype: float64  future value 0.7071275667937043\n",
            "loop 1 market state : step  1438 market state fut market_state    2.0\n",
            "Name: 2017-05-05 00:00:00, dtype: float64  future value 0.706082120164803\n",
            "loop 1 market state : step  1439 market state fut market_state    4.0\n",
            "Name: 2017-05-08 00:00:00, dtype: float64  future value 0.7094547310445679\n",
            "loop 1 market state : step  1440 market state fut market_state    4.0\n",
            "Name: 2017-05-09 00:00:00, dtype: float64  future value 0.7089674088504071\n",
            "loop 1 market state : step  1441 market state fut market_state    1.0\n",
            "Name: 2017-05-10 00:00:00, dtype: float64  future value 0.6960796471555617\n",
            "loop 1 market state : step  1442 market state fut market_state    2.0\n",
            "Name: 2017-05-11 00:00:00, dtype: float64  future value 0.6986459665009833\n",
            "loop 1 market state : step  1443 market state fut market_state    2.0\n",
            "Name: 2017-05-12 00:00:00, dtype: float64  future value 0.703374052812385\n",
            "loop 1 market state : step  1444 market state fut market_state    2.0\n",
            "Name: 2017-05-15 00:00:00, dtype: float64  future value 0.7070035554498023\n",
            "loop 1 market state : step  1445 market state fut market_state    2.0\n",
            "Name: 2017-05-16 00:00:00, dtype: float64  future value 0.7083029373812997\n",
            "loop 1 market state : step  1446 market state fut market_state    4.0\n",
            "Name: 2017-05-17 00:00:00, dtype: float64  future value 0.7100659931150325\n",
            "loop 1 market state : step  1447 market state fut market_state    4.0\n",
            "Name: 2017-05-18 00:00:00, dtype: float64  future value 0.7132200693695101\n",
            "loop 1 market state : step  1448 market state fut market_state    4.0\n",
            "Name: 2017-05-19 00:00:00, dtype: float64  future value 0.7134415598592126\n",
            "loop 1 market state : step  1449 market state fut market_state    4.0\n",
            "Name: 2017-05-22 00:00:00, dtype: float64  future value 0.7125821306891451\n",
            "loop 1 market state : step  1450 market state fut market_state    4.0\n",
            "Name: 2017-05-23 00:00:00, dtype: float64  future value 0.7122543652233149\n",
            "loop 1 market state : step  1451 market state fut market_state    4.0\n",
            "Name: 2017-05-24 00:00:00, dtype: float64  future value 0.7176469232991446\n",
            "loop 1 market state : step  1452 market state fut market_state    4.0\n",
            "Name: 2017-05-25 00:00:00, dtype: float64  future value 0.7203077650399896\n",
            "loop 1 market state : step  1453 market state fut market_state    4.0\n",
            "Name: 2017-05-26 00:00:00, dtype: float64  future value 0.7194306715603873\n",
            "loop 1 market state : step  1454 market state fut market_state    4.0\n",
            "Name: 2017-05-30 00:00:00, dtype: float64  future value 0.7174313448335932\n",
            "loop 1 market state : step  1455 market state fut market_state    4.0\n",
            "Name: 2017-05-31 00:00:00, dtype: float64  future value 0.7185564618869611\n",
            "loop 1 market state : step  1456 market state fut market_state    4.0\n",
            "Name: 2017-06-01 00:00:00, dtype: float64  future value 0.7187484634281852\n",
            "loop 1 market state : step  1457 market state fut market_state    2.0\n",
            "Name: 2017-06-02 00:00:00, dtype: float64  future value 0.7181519100981607\n",
            "loop 1 market state : step  1458 market state fut market_state    2.0\n",
            "Name: 2017-06-05 00:00:00, dtype: float64  future value 0.7174490094384487\n",
            "loop 1 market state : step  1459 market state fut market_state    4.0\n",
            "Name: 2017-06-06 00:00:00, dtype: float64  future value 0.7206857843353681\n",
            "loop 1 market state : step  1460 market state fut market_state    3.0\n",
            "Name: 2017-06-07 00:00:00, dtype: float64  future value 0.7199681031722972\n",
            "loop 1 market state : step  1461 market state fut market_state    2.0\n",
            "Name: 2017-06-08 00:00:00, dtype: float64  future value 0.7183556639247686\n",
            "loop 1 market state : step  1462 market state fut market_state    3.0\n",
            "Name: 2017-06-09 00:00:00, dtype: float64  future value 0.7185594177513763\n",
            "loop 1 market state : step  1463 market state fut market_state    4.0\n",
            "Name: 2017-06-12 00:00:00, dtype: float64  future value 0.7245573976364381\n",
            "loop 1 market state : step  1464 market state fut market_state    2.0\n",
            "Name: 2017-06-13 00:00:00, dtype: float64  future value 0.7197052993904934\n",
            "loop 1 market state : step  1465 market state fut market_state    2.0\n",
            "Name: 2017-06-14 00:00:00, dtype: float64  future value 0.7192859670983343\n",
            "loop 1 market state : step  1466 market state fut market_state    3.0\n",
            "Name: 2017-06-15 00:00:00, dtype: float64  future value 0.7189581295742647\n",
            "loop 1 market state : step  1467 market state fut market_state    3.0\n",
            "Name: 2017-06-16 00:00:00, dtype: float64  future value 0.7200803625261359\n",
            "loop 1 market state : step  1468 market state fut market_state    2.0\n",
            "Name: 2017-06-19 00:00:00, dtype: float64  future value 0.7203077650399896\n",
            "loop 1 market state : step  1469 market state fut market_state    2.0\n",
            "Name: 2017-06-20 00:00:00, dtype: float64  future value 0.7144928467493462\n",
            "loop 1 market state : step  1470 market state fut market_state    3.0\n",
            "Name: 2017-06-21 00:00:00, dtype: float64  future value 0.7207861469920241\n",
            "loop 1 market state : step  1471 market state fut market_state    2.0\n",
            "Name: 2017-06-22 00:00:00, dtype: float64  future value 0.7145873694400904\n",
            "loop 1 market state : step  1472 market state fut market_state    2.0\n",
            "Name: 2017-06-23 00:00:00, dtype: float64  future value 0.7156829975449799\n",
            "loop 1 market state : step  1473 market state fut market_state    2.0\n",
            "Name: 2017-06-26 00:00:00, dtype: float64  future value 0.7173368221428491\n",
            "loop 1 market state : step  1474 market state fut market_state    3.0\n",
            "Name: 2017-06-27 00:00:00, dtype: float64  future value 0.7183793126120144\n",
            "loop 1 market state : step  1475 market state fut market_state    2.0\n",
            "Name: 2017-06-28 00:00:00, dtype: float64  future value 0.7116489434140828\n",
            "loop 1 market state : step  1476 market state fut market_state    3.0\n",
            "Name: 2017-06-29 00:00:00, dtype: float64  future value 0.7162057210070908\n",
            "loop 1 market state : step  1477 market state fut market_state    3.0\n",
            "Name: 2017-06-30 00:00:00, dtype: float64  future value 0.7168701924761983\n",
            "loop 1 market state : step  1478 market state fut market_state    2.0\n",
            "Name: 2017-07-03 00:00:00, dtype: float64  future value 0.716309111881722\n",
            "loop 1 market state : step  1479 market state fut market_state    3.0\n",
            "Name: 2017-07-05 00:00:00, dtype: float64  future value 0.7215421852874604\n",
            "loop 1 market state : step  1480 market state fut market_state    3.0\n",
            "Name: 2017-07-06 00:00:00, dtype: float64  future value 0.7228947769129211\n",
            "loop 1 market state : step  1481 market state fut market_state    4.0\n",
            "Name: 2017-07-07 00:00:00, dtype: float64  future value 0.7262732280539185\n",
            "loop 1 market state : step  1482 market state fut market_state    4.0\n",
            "Name: 2017-07-10 00:00:00, dtype: float64  future value 0.7262347988633139\n",
            "loop 1 market state : step  1483 market state fut market_state    4.0\n",
            "Name: 2017-07-11 00:00:00, dtype: float64  future value 0.7266689834217505\n",
            "loop 1 market state : step  1484 market state fut market_state    4.0\n",
            "Name: 2017-07-12 00:00:00, dtype: float64  future value 0.7305731138892739\n",
            "loop 1 market state : step  1485 market state fut market_state    4.0\n",
            "Name: 2017-07-13 00:00:00, dtype: float64  future value 0.7304608545354352\n",
            "loop 1 market state : step  1486 market state fut market_state    4.0\n",
            "Name: 2017-07-14 00:00:00, dtype: float64  future value 0.7301921387294802\n",
            "loop 1 market state : step  1487 market state fut market_state    4.0\n",
            "Name: 2017-07-17 00:00:00, dtype: float64  future value 0.7294154079065339\n",
            "loop 1 market state : step  1488 market state fut market_state    4.0\n",
            "Name: 2017-07-18 00:00:00, dtype: float64  future value 0.7315476144564375\n",
            "loop 1 market state : step  1489 market state fut market_state    4.0\n",
            "Name: 2017-07-19 00:00:00, dtype: float64  future value 0.7317543965010206\n",
            "loop 1 market state : step  1490 market state fut market_state    4.0\n",
            "Name: 2017-07-20 00:00:00, dtype: float64  future value 0.7310426276574215\n",
            "loop 1 market state : step  1491 market state fut market_state    2.0\n",
            "Name: 2017-07-21 00:00:00, dtype: float64  future value 0.7300622150661066\n",
            "loop 1 market state : step  1492 market state fut market_state    3.0\n",
            "Name: 2017-07-24 00:00:00, dtype: float64  future value 0.7295306234201087\n",
            "loop 1 market state : step  1493 market state fut market_state    2.0\n",
            "Name: 2017-07-25 00:00:00, dtype: float64  future value 0.7313173278410874\n",
            "loop 1 market state : step  1494 market state fut market_state    2.0\n",
            "Name: 2017-07-26 00:00:00, dtype: float64  future value 0.7316776101780506\n",
            "loop 1 market state : step  1495 market state fut market_state    2.0\n",
            "Name: 2017-07-27 00:00:00, dtype: float64  future value 0.7300798793756414\n",
            "loop 1 market state : step  1496 market state fut market_state    3.0\n",
            "Name: 2017-07-28 00:00:00, dtype: float64  future value 0.7314590758480839\n",
            "loop 1 market state : step  1497 market state fut market_state    4.0\n",
            "Name: 2017-07-31 00:00:00, dtype: float64  future value 0.7326639350888371\n",
            "loop 1 market state : step  1498 market state fut market_state    2.0\n",
            "Name: 2017-08-01 00:00:00, dtype: float64  future value 0.730894967330953\n",
            "loop 1 market state : step  1499 market state fut market_state    2.0\n",
            "Name: 2017-08-02 00:00:00, dtype: float64  future value 0.730629207684734\n",
            "loop 1 market state : step  1500 market state fut market_state    1.0\n",
            "Name: 2017-08-03 00:00:00, dtype: float64  future value 0.7200537576791542\n",
            "loop 1 market state : step  1501 market state fut market_state    2.0\n",
            "Name: 2017-08-04 00:00:00, dtype: float64  future value 0.720972236509097\n",
            "loop 1 market state : step  1502 market state fut market_state    2.0\n",
            "Name: 2017-08-07 00:00:00, dtype: float64  future value 0.7282135048255167\n",
            "loop 1 market state : step  1503 market state fut market_state    2.0\n",
            "Name: 2017-08-08 00:00:00, dtype: float64  future value 0.727850266033497\n",
            "loop 1 market state : step  1504 market state fut market_state    2.0\n",
            "Name: 2017-08-09 00:00:00, dtype: float64  future value 0.7288838883187753\n",
            "loop 1 market state : step  1505 market state fut market_state    2.0\n",
            "Name: 2017-08-10 00:00:00, dtype: float64  future value 0.7176321427957857\n",
            "loop 1 market state : step  1506 market state fut market_state    2.0\n",
            "Name: 2017-08-11 00:00:00, dtype: float64  future value 0.7163150242011938\n",
            "loop 1 market state : step  1507 market state fut market_state    2.0\n",
            "Name: 2017-08-14 00:00:00, dtype: float64  future value 0.7171478485242796\n",
            "loop 1 market state : step  1508 market state fut market_state    2.0\n",
            "Name: 2017-08-15 00:00:00, dtype: float64  future value 0.7242768574868603\n",
            "loop 1 market state : step  1509 market state fut market_state    2.0\n",
            "Name: 2017-08-16 00:00:00, dtype: float64  future value 0.7217755001207858\n",
            "loop 1 market state : step  1510 market state fut market_state    2.0\n",
            "Name: 2017-08-17 00:00:00, dtype: float64  future value 0.7202782043285926\n",
            "loop 1 market state : step  1511 market state fut market_state    2.0\n",
            "Name: 2017-08-18 00:00:00, dtype: float64  future value 0.721483135627585\n",
            "loop 1 market state : step  1512 market state fut market_state    2.0\n",
            "Name: 2017-08-21 00:00:00, dtype: float64  future value 0.7218345497806612\n",
            "loop 1 market state : step  1513 market state fut market_state    1.0\n",
            "Name: 2017-08-22 00:00:00, dtype: float64  future value 0.7224429277496291\n",
            "loop 1 market state : step  1514 market state fut market_state    2.0\n",
            "Name: 2017-08-23 00:00:00, dtype: float64  future value 0.7257771094387894\n",
            "loop 1 market state : step  1515 market state fut market_state    2.0\n",
            "Name: 2017-08-24 00:00:00, dtype: float64  future value 0.7299292628894372\n",
            "loop 1 market state : step  1516 market state fut market_state    3.0\n",
            "Name: 2017-08-25 00:00:00, dtype: float64  future value 0.7313763775009627\n",
            "loop 1 market state : step  1517 market state fut market_state    2.0\n",
            "Name: 2017-08-28 00:00:00, dtype: float64  future value 0.7258538957617594\n",
            "loop 1 market state : step  1518 market state fut market_state    2.0\n",
            "Name: 2017-08-29 00:00:00, dtype: float64  future value 0.7281248941589237\n",
            "loop 1 market state : step  1519 market state fut market_state    2.0\n",
            "Name: 2017-08-30 00:00:00, dtype: float64  future value 0.7279949704955501\n",
            "loop 1 market state : step  1520 market state fut market_state    1.0\n",
            "Name: 2017-08-31 00:00:00, dtype: float64  future value 0.7269110946760442\n",
            "loop 1 market state : step  1521 market state fut market_state    4.0\n",
            "Name: 2017-09-01 00:00:00, dtype: float64  future value 0.7347903013775082\n",
            "loop 1 market state : step  1522 market state fut market_state    4.0\n",
            "Name: 2017-09-05 00:00:00, dtype: float64  future value 0.737262097736865\n",
            "loop 1 market state : step  1523 market state fut market_state    4.0\n",
            "Name: 2017-09-06 00:00:00, dtype: float64  future value 0.7378202942298449\n",
            "loop 1 market state : step  1524 market state fut market_state    4.0\n",
            "Name: 2017-09-07 00:00:00, dtype: float64  future value 0.737008162434269\n",
            "loop 1 market state : step  1525 market state fut market_state    4.0\n",
            "Name: 2017-09-08 00:00:00, dtype: float64  future value 0.7383695501853774\n",
            "loop 1 market state : step  1526 market state fut market_state    4.0\n",
            "Name: 2017-09-11 00:00:00, dtype: float64  future value 0.7394445578209964\n",
            "loop 1 market state : step  1527 market state fut market_state    4.0\n",
            "Name: 2017-09-12 00:00:00, dtype: float64  future value 0.7402654857422198\n",
            "loop 1 market state : step  1528 market state fut market_state    4.0\n",
            "Name: 2017-09-13 00:00:00, dtype: float64  future value 0.7407350715686065\n",
            "loop 1 market state : step  1529 market state fut market_state    4.0\n",
            "Name: 2017-09-14 00:00:00, dtype: float64  future value 0.738478853674801\n",
            "loop 1 market state : step  1530 market state fut market_state    4.0\n",
            "Name: 2017-09-15 00:00:00, dtype: float64  future value 0.7389572356268355\n",
            "loop 1 market state : step  1531 market state fut market_state    2.0\n",
            "Name: 2017-09-18 00:00:00, dtype: float64  future value 0.7373152353725893\n",
            "loop 1 market state : step  1532 market state fut market_state    2.0\n",
            "Name: 2017-09-19 00:00:00, dtype: float64  future value 0.7373684450665527\n",
            "loop 1 market state : step  1533 market state fut market_state    2.0\n",
            "Name: 2017-09-20 00:00:00, dtype: float64  future value 0.7403807012557945\n",
            "loop 1 market state : step  1534 market state fut market_state    4.0\n",
            "Name: 2017-09-21 00:00:00, dtype: float64  future value 0.7412725755340762\n",
            "loop 1 market state : step  1535 market state fut market_state    4.0\n",
            "Name: 2017-09-22 00:00:00, dtype: float64  future value 0.7440190717817784\n",
            "loop 1 market state : step  1536 market state fut market_state    4.0\n",
            "Name: 2017-09-25 00:00:00, dtype: float64  future value 0.7469014043076467\n",
            "loop 1 market state : step  1537 market state fut market_state    4.0\n",
            "Name: 2017-09-26 00:00:00, dtype: float64  future value 0.7485138435551753\n",
            "loop 1 market state : step  1538 market state fut market_state    4.0\n",
            "Name: 2017-09-27 00:00:00, dtype: float64  future value 0.7494470308302376\n",
            "loop 1 market state : step  1539 market state fut market_state    4.0\n",
            "Name: 2017-09-28 00:00:00, dtype: float64  future value 0.7536789988218306\n",
            "loop 1 market state : step  1540 market state fut market_state    4.0\n",
            "Name: 2017-09-29 00:00:00, dtype: float64  future value 0.7528698231859907\n",
            "loop 1 market state : step  1541 market state fut market_state    4.0\n",
            "Name: 2017-10-02 00:00:00, dtype: float64  future value 0.7515113192410582\n",
            "loop 1 market state : step  1542 market state fut market_state    4.0\n",
            "Name: 2017-10-03 00:00:00, dtype: float64  future value 0.753256638607017\n",
            "loop 1 market state : step  1543 market state fut market_state    4.0\n",
            "Name: 2017-10-04 00:00:00, dtype: float64  future value 0.7546151422566288\n",
            "loop 1 market state : step  1544 market state fut market_state    2.0\n",
            "Name: 2017-10-05 00:00:00, dtype: float64  future value 0.753342293113874\n",
            "loop 1 market state : step  1545 market state fut market_state    4.0\n",
            "Name: 2017-10-06 00:00:00, dtype: float64  future value 0.7540038084232457\n",
            "loop 1 market state : step  1546 market state fut market_state    4.0\n",
            "Name: 2017-10-09 00:00:00, dtype: float64  future value 0.7553238831775735\n",
            "loop 1 market state : step  1547 market state fut market_state    4.0\n",
            "Name: 2017-10-10 00:00:00, dtype: float64  future value 0.7558318978992442\n",
            "loop 1 market state : step  1548 market state fut market_state    4.0\n",
            "Name: 2017-10-11 00:00:00, dtype: float64  future value 0.7563929784937206\n",
            "loop 1 market state : step  1549 market state fut market_state    4.0\n",
            "Name: 2017-10-12 00:00:00, dtype: float64  future value 0.7566410738304048\n",
            "loop 1 market state : step  1550 market state fut market_state    4.0\n",
            "Name: 2017-10-13 00:00:00, dtype: float64  future value 0.7605126871314748\n",
            "loop 1 market state : step  1551 market state fut market_state    4.0\n",
            "Name: 2017-10-16 00:00:00, dtype: float64  future value 0.7574915624630253\n",
            "loop 1 market state : step  1552 market state fut market_state    4.0\n",
            "Name: 2017-10-17 00:00:00, dtype: float64  future value 0.758717114526609\n",
            "loop 1 market state : step  1553 market state fut market_state    2.0\n",
            "Name: 2017-10-18 00:00:00, dtype: float64  future value 0.7551791787155204\n",
            "loop 1 market state : step  1554 market state fut market_state    2.0\n",
            "Name: 2017-10-19 00:00:00, dtype: float64  future value 0.7561389708375645\n",
            "loop 1 market state : step  1555 market state fut market_state    4.0\n",
            "Name: 2017-10-20 00:00:00, dtype: float64  future value 0.7622432977569934\n",
            "loop 1 market state : step  1556 market state fut market_state    3.0\n",
            "Name: 2017-10-23 00:00:00, dtype: float64  future value 0.7598098585300019\n",
            "loop 1 market state : step  1557 market state fut market_state    4.0\n",
            "Name: 2017-10-24 00:00:00, dtype: float64  future value 0.7605274676348336\n",
            "loop 1 market state : step  1558 market state fut market_state    4.0\n",
            "Name: 2017-10-25 00:00:00, dtype: float64  future value 0.7617383109579772\n",
            "loop 1 market state : step  1559 market state fut market_state    4.0\n",
            "Name: 2017-10-26 00:00:00, dtype: float64  future value 0.7618830154200302\n",
            "loop 1 market state : step  1560 market state fut market_state    4.0\n",
            "Name: 2017-10-27 00:00:00, dtype: float64  future value 0.7642426244837875\n",
            "loop 1 market state : step  1561 market state fut market_state    4.0\n",
            "Name: 2017-10-30 00:00:00, dtype: float64  future value 0.7652141688912151\n",
            "loop 1 market state : step  1562 market state fut market_state    4.0\n",
            "Name: 2017-10-31 00:00:00, dtype: float64  future value 0.7650694647244828\n",
            "loop 1 market state : step  1563 market state fut market_state    4.0\n",
            "Name: 2017-11-01 00:00:00, dtype: float64  future value 0.7661739610132592\n",
            "loop 1 market state : step  1564 market state fut market_state    4.0\n",
            "Name: 2017-11-02 00:00:00, dtype: float64  future value 0.7632917005456306\n",
            "loop 1 market state : step  1565 market state fut market_state    2.0\n",
            "Name: 2017-11-03 00:00:00, dtype: float64  future value 0.762606536549013\n",
            "loop 1 market state : step  1566 market state fut market_state    2.0\n",
            "Name: 2017-11-06 00:00:00, dtype: float64  future value 0.7633566625249776\n",
            "loop 1 market state : step  1567 market state fut market_state    2.0\n",
            "Name: 2017-11-07 00:00:00, dtype: float64  future value 0.7615936067912449\n",
            "loop 1 market state : step  1568 market state fut market_state    2.0\n",
            "Name: 2017-11-08 00:00:00, dtype: float64  future value 0.7573852874868976\n",
            "loop 1 market state : step  1569 market state fut market_state    3.0\n",
            "Name: 2017-11-09 00:00:00, dtype: float64  future value 0.7635928614597995\n",
            "loop 1 market state : step  1570 market state fut market_state    2.0\n",
            "Name: 2017-11-10 00:00:00, dtype: float64  future value 0.7615876947670935\n",
            "loop 1 market state : step  1571 market state fut market_state    2.0\n",
            "Name: 2017-11-13 00:00:00, dtype: float64  future value 0.7625592391745213\n",
            "loop 1 market state : step  1572 market state fut market_state    4.0\n",
            "Name: 2017-11-14 00:00:00, dtype: float64  future value 0.7675472451662301\n",
            "loop 1 market state : step  1573 market state fut market_state    4.0\n",
            "Name: 2017-11-15 00:00:00, dtype: float64  future value 0.7669713843637156\n",
            "loop 1 market state : step  1574 market state fut market_state    4.0\n",
            "Name: 2017-11-16 00:00:00, dtype: float64  future value 0.7685483505803754\n",
            "loop 1 market state : step  1575 market state fut market_state    4.0\n",
            "Name: 2017-11-17 00:00:00, dtype: float64  future value 0.7682530299274388\n",
            "loop 1 market state : step  1576 market state fut market_state    4.0\n",
            "Name: 2017-11-20 00:00:00, dtype: float64  future value 0.775819179608192\n",
            "loop 1 market state : step  1577 market state fut market_state    4.0\n",
            "Name: 2017-11-21 00:00:00, dtype: float64  future value 0.7755327271391425\n",
            "loop 1 market state : step  1578 market state fut market_state    4.0\n",
            "Name: 2017-11-22 00:00:00, dtype: float64  future value 0.7818850773370162\n",
            "loop 1 market state : step  1579 market state fut market_state    4.0\n",
            "Name: 2017-11-24 00:00:00, dtype: float64  future value 0.780302127037966\n",
            "loop 1 market state : step  1580 market state fut market_state    4.0\n",
            "Name: 2017-11-27 00:00:00, dtype: float64  future value 0.7794811267631825\n",
            "loop 1 market state : step  1581 market state fut market_state    4.0\n",
            "Name: 2017-11-28 00:00:00, dtype: float64  future value 0.7765663494244207\n",
            "loop 1 market state : step  1582 market state fut market_state    4.0\n",
            "Name: 2017-11-29 00:00:00, dtype: float64  future value 0.7764777390531483\n",
            "loop 1 market state : step  1583 market state fut market_state    2.0\n",
            "Name: 2017-11-30 00:00:00, dtype: float64  future value 0.7787546494744638\n",
            "loop 1 market state : step  1584 market state fut market_state    4.0\n",
            "Name: 2017-12-01 00:00:00, dtype: float64  future value 0.7830427112615169\n",
            "loop 1 market state : step  1585 market state fut market_state    4.0\n",
            "Name: 2017-12-04 00:00:00, dtype: float64  future value 0.7855499806517425\n",
            "loop 1 market state : step  1586 market state fut market_state    4.0\n",
            "Name: 2017-12-05 00:00:00, dtype: float64  future value 0.7867667362943579\n",
            "loop 1 market state : step  1587 market state fut market_state    4.0\n",
            "Name: 2017-12-06 00:00:00, dtype: float64  future value 0.7863946296137718\n",
            "loop 1 market state : step  1588 market state fut market_state    4.0\n",
            "Name: 2017-12-07 00:00:00, dtype: float64  future value 0.7831933277477212\n",
            "loop 1 market state : step  1589 market state fut market_state    4.0\n",
            "Name: 2017-12-08 00:00:00, dtype: float64  future value 0.7902219737583253\n",
            "loop 1 market state : step  1590 market state fut market_state    4.0\n",
            "Name: 2017-12-11 00:00:00, dtype: float64  future value 0.7944597817158302\n",
            "loop 1 market state : step  1591 market state fut market_state    4.0\n",
            "Name: 2017-12-12 00:00:00, dtype: float64  future value 0.7918934626657292\n",
            "loop 1 market state : step  1592 market state fut market_state    4.0\n",
            "Name: 2017-12-13 00:00:00, dtype: float64  future value 0.7912378593805088\n",
            "loop 1 market state : step  1593 market state fut market_state    4.0\n",
            "Name: 2017-12-14 00:00:00, dtype: float64  future value 0.7928089853359362\n",
            "loop 1 market state : step  1594 market state fut market_state    4.0\n",
            "Name: 2017-12-15 00:00:00, dtype: float64  future value 0.7924457468392372\n",
            "loop 1 market state : step  1595 market state fut market_state    2.0\n",
            "Name: 2017-12-18 00:00:00, dtype: float64  future value 0.7916070101966797\n",
            "loop 1 market state : step  1596 market state fut market_state    3.0\n",
            "Name: 2017-12-19 00:00:00, dtype: float64  future value 0.7922331245334219\n",
            "loop 1 market state : step  1597 market state fut market_state    3.0\n",
            "Name: 2017-12-20 00:00:00, dtype: float64  future value 0.7936860791108591\n",
            "loop 1 market state : step  1598 market state fut market_state    2.0\n",
            "Name: 2017-12-21 00:00:00, dtype: float64  future value 0.7895722824972561\n",
            "loop 1 market state : step  1599 market state fut market_state    4.0\n",
            "Name: 2017-12-22 00:00:00, dtype: float64  future value 0.7961283868170582\n",
            "loop 1 market state : step  1600 market state fut market_state    4.0\n",
            "Name: 2017-12-26 00:00:00, dtype: float64  future value 0.8012226680802154\n",
            "loop 1 market state : step  1601 market state fut market_state    4.0\n",
            "Name: 2017-12-27 00:00:00, dtype: float64  future value 0.8044505024396879\n",
            "loop 1 market state : step  1602 market state fut market_state    4.0\n",
            "Name: 2017-12-28 00:00:00, dtype: float64  future value 0.8101088201617366\n",
            "loop 1 market state : step  1603 market state fut market_state    4.0\n",
            "Name: 2017-12-29 00:00:00, dtype: float64  future value 0.8114554997630462\n",
            "loop 1 market state : step  1604 market state fut market_state    4.0\n",
            "Name: 2018-01-02 00:00:00, dtype: float64  future value 0.8125127707355703\n",
            "loop 1 market state : step  1605 market state fut market_state    4.0\n",
            "Name: 2018-01-03 00:00:00, dtype: float64  future value 0.8116090721136657\n",
            "loop 1 market state : step  1606 market state fut market_state    4.0\n",
            "Name: 2018-01-04 00:00:00, dtype: float64  future value 0.8173176436652626\n",
            "loop 1 market state : step  1607 market state fut market_state    4.0\n",
            "Name: 2018-01-05 00:00:00, dtype: float64  future value 0.8228342130849942\n",
            "loop 1 market state : step  1608 market state fut market_state    4.0\n",
            "Name: 2018-01-08 00:00:00, dtype: float64  future value 0.8199341441913518\n",
            "loop 1 market state : step  1609 market state fut market_state    4.0\n",
            "Name: 2018-01-09 00:00:00, dtype: float64  future value 0.8276538665180452\n",
            "loop 1 market state : step  1610 market state fut market_state    4.0\n",
            "Name: 2018-01-10 00:00:00, dtype: float64  future value 0.8263160551006227\n",
            "loop 1 market state : step  1611 market state fut market_state    4.0\n",
            "Name: 2018-01-11 00:00:00, dtype: float64  future value 0.8299396454185684\n",
            "loop 1 market state : step  1612 market state fut market_state    4.0\n",
            "Name: 2018-01-12 00:00:00, dtype: float64  future value 0.8366345415856312\n",
            "loop 1 market state : step  1613 market state fut market_state    4.0\n",
            "Name: 2018-01-16 00:00:00, dtype: float64  future value 0.8384536908195034\n",
            "loop 1 market state : step  1614 market state fut market_state    4.0\n",
            "Name: 2018-01-17 00:00:00, dtype: float64  future value 0.8379841770513561\n",
            "loop 1 market state : step  1615 market state fut market_state    4.0\n",
            "Name: 2018-01-18 00:00:00, dtype: float64  future value 0.8384891638503722\n",
            "loop 1 market state : step  1616 market state fut market_state    4.0\n",
            "Name: 2018-01-19 00:00:00, dtype: float64  future value 0.8484178787546188\n",
            "loop 1 market state : step  1617 market state fut market_state    4.0\n",
            "Name: 2018-01-22 00:00:00, dtype: float64  future value 0.8427063513386065\n",
            "loop 1 market state : step  1618 market state fut market_state    2.0\n",
            "Name: 2018-01-23 00:00:00, dtype: float64  future value 0.8335218503861734\n",
            "loop 1 market state : step  1619 market state fut market_state    2.0\n",
            "Name: 2018-01-24 00:00:00, dtype: float64  future value 0.833929430392949\n",
            "loop 1 market state : step  1620 market state fut market_state    2.0\n",
            "Name: 2018-01-25 00:00:00, dtype: float64  future value 0.8333889702677433\n",
            "loop 1 market state : step  1621 market state fut market_state    2.0\n",
            "Name: 2018-01-26 00:00:00, dtype: float64  future value 0.8157140005433816\n",
            "loop 1 market state : step  1622 market state fut market_state    0.0\n",
            "Name: 2018-01-29 00:00:00, dtype: float64  future value 0.7822866729660807\n",
            "loop 1 market state : step  1623 market state fut market_state    2.0\n",
            "Name: 2018-01-30 00:00:00, dtype: float64  future value 0.7959304729563623\n",
            "loop 1 market state : step  1624 market state fut market_state    1.0\n",
            "Name: 2018-01-31 00:00:00, dtype: float64  future value 0.7919495561658687\n",
            "loop 1 market state : step  1625 market state fut market_state    0.0\n",
            "Name: 2018-02-01 00:00:00, dtype: float64  future value 0.7622226052294834\n",
            "loop 1 market state : step  1626 market state fut market_state    0.0\n",
            "Name: 2018-02-02 00:00:00, dtype: float64  future value 0.7736072308709031\n",
            "loop 1 market state : step  1627 market state fut market_state    0.0\n",
            "Name: 2018-02-05 00:00:00, dtype: float64  future value 0.7843716541997319\n",
            "loop 1 market state : step  1628 market state fut market_state    1.0\n",
            "Name: 2018-02-06 00:00:00, dtype: float64  future value 0.7864211621071937\n",
            "loop 1 market state : step  1629 market state fut market_state    3.0\n",
            "Name: 2018-02-07 00:00:00, dtype: float64  future value 0.7969611390819046\n",
            "loop 1 market state : step  1630 market state fut market_state    3.0\n",
            "Name: 2018-02-08 00:00:00, dtype: float64  future value 0.8065797528298556\n",
            "loop 1 market state : step  1631 market state fut market_state    3.0\n",
            "Name: 2018-02-09 00:00:00, dtype: float64  future value 0.8068809858022641\n",
            "loop 1 market state : step  1632 market state fut market_state    2.0\n",
            "Name: 2018-02-12 00:00:00, dtype: float64  future value 0.8021676796989007\n",
            "loop 1 market state : step  1633 market state fut market_state    2.0\n",
            "Name: 2018-02-13 00:00:00, dtype: float64  future value 0.797758562432361\n",
            "loop 1 market state : step  1634 market state fut market_state    2.0\n",
            "Name: 2018-02-14 00:00:00, dtype: float64  future value 0.798535221197068\n",
            "loop 1 market state : step  1635 market state fut market_state    2.0\n",
            "Name: 2018-02-15 00:00:00, dtype: float64  future value 0.8113344442835596\n",
            "loop 1 market state : step  1636 market state fut market_state    3.0\n",
            "Name: 2018-02-16 00:00:00, dtype: float64  future value 0.8208733158441253\n",
            "loop 1 market state : step  1637 market state fut market_state    2.0\n",
            "Name: 2018-02-20 00:00:00, dtype: float64  future value 0.8104425700052779\n",
            "loop 1 market state : step  1638 market state fut market_state    2.0\n",
            "Name: 2018-02-21 00:00:00, dtype: float64  future value 0.801450070594069\n",
            "loop 1 market state : step  1639 market state fut market_state    1.0\n",
            "Name: 2018-02-22 00:00:00, dtype: float64  future value 0.790771229713858\n",
            "loop 1 market state : step  1640 market state fut market_state    1.0\n",
            "Name: 2018-02-23 00:00:00, dtype: float64  future value 0.7947817072157486\n",
            "loop 1 market state : step  1641 market state fut market_state    1.0\n",
            "Name: 2018-02-26 00:00:00, dtype: float64  future value 0.8035497599775192\n",
            "loop 1 market state : step  1642 market state fut market_state    1.0\n",
            "Name: 2018-02-27 00:00:00, dtype: float64  future value 0.8056702142420393\n",
            "loop 1 market state : step  1643 market state fut market_state    2.0\n",
            "Name: 2018-02-28 00:00:00, dtype: float64  future value 0.8052803708983584\n",
            "loop 1 market state : step  1644 market state fut market_state    2.0\n",
            "Name: 2018-03-01 00:00:00, dtype: float64  future value 0.8088744002095865\n",
            "loop 1 market state : step  1645 market state fut market_state    2.0\n",
            "Name: 2018-03-02 00:00:00, dtype: float64  future value 0.8229316919354741\n",
            "loop 1 market state : step  1646 market state fut market_state    2.0\n",
            "Name: 2018-03-05 00:00:00, dtype: float64  future value 0.8218832894421577\n",
            "loop 1 market state : step  1647 market state fut market_state    2.0\n",
            "Name: 2018-03-06 00:00:00, dtype: float64  future value 0.8166531721961551\n",
            "loop 1 market state : step  1648 market state fut market_state    2.0\n",
            "Name: 2018-03-07 00:00:00, dtype: float64  future value 0.8119782229298365\n",
            "loop 1 market state : step  1649 market state fut market_state    2.0\n",
            "Name: 2018-03-08 00:00:00, dtype: float64  future value 0.8113433124674466\n",
            "loop 1 market state : step  1650 market state fut market_state    1.0\n",
            "Name: 2018-03-09 00:00:00, dtype: float64  future value 0.8127253930413858\n",
            "loop 1 market state : step  1651 market state fut market_state    1.0\n",
            "Name: 2018-03-12 00:00:00, dtype: float64  future value 0.8011812827298749\n",
            "loop 1 market state : step  1652 market state fut market_state    2.0\n",
            "Name: 2018-03-13 00:00:00, dtype: float64  future value 0.8023684773657725\n",
            "loop 1 market state : step  1653 market state fut market_state    2.0\n",
            "Name: 2018-03-14 00:00:00, dtype: float64  future value 0.8008889182366741\n",
            "loop 1 market state : step  1654 market state fut market_state    1.0\n",
            "Name: 2018-03-15 00:00:00, dtype: float64  future value 0.7807362395381633\n",
            "loop 1 market state : step  1655 market state fut market_state    2.0\n",
            "Name: 2018-03-16 00:00:00, dtype: float64  future value 0.7643666361230099\n",
            "loop 1 market state : step  1656 market state fut market_state    2.0\n",
            "Name: 2018-03-19 00:00:00, dtype: float64  future value 0.7851247363354322\n",
            "loop 1 market state : step  1657 market state fut market_state    0.0\n",
            "Name: 2018-03-20 00:00:00, dtype: float64  future value 0.7715606788278566\n",
            "loop 1 market state : step  1658 market state fut market_state    0.0\n",
            "Name: 2018-03-21 00:00:00, dtype: float64  future value 0.7693103008999629\n",
            "loop 1 market state : step  1659 market state fut market_state    0.0\n",
            "Name: 2018-03-22 00:00:00, dtype: float64  future value 0.7799034872733169\n",
            "loop 1 market state : step  1660 market state fut market_state    0.0\n",
            "Name: 2018-03-23 00:00:00, dtype: float64  future value 0.7624824528515511\n",
            "loop 1 market state : step  1661 market state fut market_state    1.0\n",
            "Name: 2018-03-26 00:00:00, dtype: float64  future value 0.7721010665995023\n",
            "loop 1 market state : step  1662 market state fut market_state    2.0\n",
            "Name: 2018-03-27 00:00:00, dtype: float64  future value 0.7810315601910999\n",
            "loop 1 market state : step  1663 market state fut market_state    2.0\n",
            "Name: 2018-03-28 00:00:00, dtype: float64  future value 0.7863916734540359\n",
            "loop 1 market state : step  1664 market state fut market_state    1.0\n",
            "Name: 2018-03-29 00:00:00, dtype: float64  future value 0.7691537723896076\n",
            "loop 1 market state : step  1665 market state fut market_state    2.0\n",
            "Name: 2018-04-02 00:00:00, dtype: float64  future value 0.7717200914397085\n",
            "loop 1 market state : step  1666 market state fut market_state    2.0\n",
            "Name: 2018-04-03 00:00:00, dtype: float64  future value 0.7846286177203032\n",
            "loop 1 market state : step  1667 market state fut market_state    1.0\n",
            "Name: 2018-04-04 00:00:00, dtype: float64  future value 0.7802932585587583\n",
            "loop 1 market state : step  1668 market state fut market_state    2.0\n",
            "Name: 2018-04-05 00:00:00, dtype: float64  future value 0.7867312632634892\n",
            "loop 1 market state : step  1669 market state fut market_state    2.0\n",
            "Name: 2018-04-06 00:00:00, dtype: float64  future value 0.7844602648663248\n",
            "loop 1 market state : step  1670 market state fut market_state    2.0\n",
            "Name: 2018-04-09 00:00:00, dtype: float64  future value 0.7908214832480857\n",
            "loop 1 market state : step  1671 market state fut market_state    2.0\n",
            "Name: 2018-04-10 00:00:00, dtype: float64  future value 0.7992528303018996\n",
            "loop 1 market state : step  1672 market state fut market_state    2.0\n",
            "Name: 2018-04-11 00:00:00, dtype: float64  future value 0.7999173017710071\n",
            "loop 1 market state : step  1673 market state fut market_state    2.0\n",
            "Name: 2018-04-12 00:00:00, dtype: float64  future value 0.795336875490753\n",
            "loop 1 market state : step  1674 market state fut market_state    2.0\n",
            "Name: 2018-04-13 00:00:00, dtype: float64  future value 0.7885474566329461\n",
            "loop 1 market state : step  1675 market state fut market_state    1.0\n",
            "Name: 2018-04-16 00:00:00, dtype: float64  future value 0.788591797847702\n",
            "loop 1 market state : step  1676 market state fut market_state    1.0\n",
            "Name: 2018-04-17 00:00:00, dtype: float64  future value 0.7780399968246886\n",
            "loop 1 market state : step  1677 market state fut market_state    1.0\n",
            "Name: 2018-04-18 00:00:00, dtype: float64  future value 0.7794693024195596\n",
            "loop 1 market state : step  1678 market state fut market_state    1.0\n",
            "Name: 2018-04-19 00:00:00, dtype: float64  future value 0.7876024447189403\n",
            "loop 1 market state : step  1679 market state fut market_state    1.0\n",
            "Name: 2018-04-20 00:00:00, dtype: float64  future value 0.7884795384938632\n",
            "loop 1 market state : step  1680 market state fut market_state    1.0\n",
            "Name: 2018-04-23 00:00:00, dtype: float64  future value 0.7820238694795975\n",
            "loop 1 market state : step  1681 market state fut market_state    2.0\n",
            "Name: 2018-04-24 00:00:00, dtype: float64  future value 0.7840172838869198\n",
            "loop 1 market state : step  1682 market state fut market_state    1.0\n",
            "Name: 2018-04-25 00:00:00, dtype: float64  future value 0.778367762290519\n",
            "loop 1 market state : step  1683 market state fut market_state    1.0\n",
            "Name: 2018-04-26 00:00:00, dtype: float64  future value 0.7766135747406732\n",
            "loop 1 market state : step  1684 market state fut market_state    2.0\n",
            "Name: 2018-04-27 00:00:00, dtype: float64  future value 0.7865629104095109\n",
            "loop 1 market state : step  1685 market state fut market_state    3.0\n",
            "Name: 2018-04-30 00:00:00, dtype: float64  future value 0.7892828021055518\n",
            "loop 1 market state : step  1686 market state fut market_state    3.0\n",
            "Name: 2018-05-01 00:00:00, dtype: float64  future value 0.7890731359594724\n",
            "loop 1 market state : step  1687 market state fut market_state    3.0\n",
            "Name: 2018-05-02 00:00:00, dtype: float64  future value 0.7967131158034597\n",
            "loop 1 market state : step  1688 market state fut market_state    4.0\n",
            "Name: 2018-05-03 00:00:00, dtype: float64  future value 0.8041788304739971\n",
            "loop 1 market state : step  1689 market state fut market_state    4.0\n",
            "Name: 2018-05-04 00:00:00, dtype: float64  future value 0.8055520428640491\n",
            "loop 1 market state : step  1690 market state fut market_state    4.0\n",
            "Name: 2018-05-07 00:00:00, dtype: float64  future value 0.8062637396494089\n",
            "loop 1 market state : step  1691 market state fut market_state    4.0\n",
            "Name: 2018-05-08 00:00:00, dtype: float64  future value 0.8007471699343569\n",
            "loop 1 market state : step  1692 market state fut market_state    4.0\n",
            "Name: 2018-05-09 00:00:00, dtype: float64  future value 0.8039986532763959\n",
            "loop 1 market state : step  1693 market state fut market_state    2.0\n",
            "Name: 2018-05-10 00:00:00, dtype: float64  future value 0.8033105331200424\n",
            "loop 1 market state : step  1694 market state fut market_state    2.0\n",
            "Name: 2018-05-11 00:00:00, dtype: float64  future value 0.8011960632332337\n",
            "loop 1 market state : step  1695 market state fut market_state    4.0\n",
            "Name: 2018-05-14 00:00:00, dtype: float64  future value 0.8071143006355895\n",
            "loop 1 market state : step  1696 market state fut market_state    3.0\n",
            "Name: 2018-05-15 00:00:00, dtype: float64  future value 0.8045833822627975\n",
            "loop 1 market state : step  1697 market state fut market_state    4.0\n",
            "Name: 2018-05-16 00:00:00, dtype: float64  future value 0.8071969989827107\n",
            "loop 1 market state : step  1698 market state fut market_state    3.0\n",
            "Name: 2018-05-17 00:00:00, dtype: float64  future value 0.8055638672076721\n",
            "loop 1 market state : step  1699 market state fut market_state    3.0\n",
            "Name: 2018-05-18 00:00:00, dtype: float64  future value 0.8036649754910938\n",
            "loop 1 market state : step  1700 market state fut market_state    2.0\n",
            "Name: 2018-05-21 00:00:00, dtype: float64  future value 0.7943712431074765\n",
            "loop 1 market state : step  1701 market state fut market_state    2.0\n",
            "Name: 2018-05-22 00:00:00, dtype: float64  future value 0.8044564147591596\n",
            "loop 1 market state : step  1702 market state fut market_state    1.0\n",
            "Name: 2018-05-23 00:00:00, dtype: float64  future value 0.7989221086763334\n",
            "loop 1 market state : step  1703 market state fut market_state    4.0\n",
            "Name: 2018-05-24 00:00:00, dtype: float64  future value 0.8075897984861274\n",
            "loop 1 market state : step  1704 market state fut market_state    4.0\n",
            "Name: 2018-05-25 00:00:00, dtype: float64  future value 0.8112074764846013\n",
            "loop 1 market state : step  1705 market state fut market_state    4.0\n",
            "Name: 2018-05-29 00:00:00, dtype: float64  future value 0.8117774252629646\n",
            "loop 1 market state : step  1706 market state fut market_state    4.0\n",
            "Name: 2018-05-30 00:00:00, dtype: float64  future value 0.8187322411103346\n",
            "loop 1 market state : step  1707 market state fut market_state    4.0\n",
            "Name: 2018-05-31 00:00:00, dtype: float64  future value 0.8181475118286126\n",
            "loop 1 market state : step  1708 market state fut market_state    4.0\n",
            "Name: 2018-06-01 00:00:00, dtype: float64  future value 0.8207049626948264\n",
            "loop 1 market state : step  1709 market state fut market_state    4.0\n",
            "Name: 2018-06-04 00:00:00, dtype: float64  future value 0.8215820564697492\n",
            "loop 1 market state : step  1710 market state fut market_state    4.0\n",
            "Name: 2018-06-05 00:00:00, dtype: float64  future value 0.8230143905779159\n",
            "loop 1 market state : step  1711 market state fut market_state    4.0\n",
            "Name: 2018-06-06 00:00:00, dtype: float64  future value 0.8197008293580264\n",
            "loop 1 market state : step  1712 market state fut market_state    4.0\n",
            "Name: 2018-06-07 00:00:00, dtype: float64  future value 0.8217267606364816\n",
            "loop 1 market state : step  1713 market state fut market_state    4.0\n",
            "Name: 2018-06-08 00:00:00, dtype: float64  future value 0.82089098015366\n",
            "loop 1 market state : step  1714 market state fut market_state    2.0\n",
            "Name: 2018-06-11 00:00:00, dtype: float64  future value 0.8191456610830219\n",
            "loop 1 market state : step  1715 market state fut market_state    2.0\n",
            "Name: 2018-06-12 00:00:00, dtype: float64  future value 0.8158499085844664\n",
            "loop 1 market state : step  1716 market state fut market_state    2.0\n",
            "Name: 2018-06-13 00:00:00, dtype: float64  future value 0.8172467693664437\n",
            "loop 1 market state : step  1717 market state fut market_state    2.0\n",
            "Name: 2018-06-14 00:00:00, dtype: float64  future value 0.8120609215722783\n",
            "loop 1 market state : step  1718 market state fut market_state    2.0\n",
            "Name: 2018-06-15 00:00:00, dtype: float64  future value 0.813572925809591\n",
            "loop 1 market state : step  1719 market state fut market_state    1.0\n",
            "Name: 2018-06-18 00:00:00, dtype: float64  future value 0.8024069065563773\n",
            "loop 1 market state : step  1720 market state fut market_state    1.0\n",
            "Name: 2018-06-19 00:00:00, dtype: float64  future value 0.8041758746095818\n",
            "loop 1 market state : step  1721 market state fut market_state    1.0\n",
            "Name: 2018-06-20 00:00:00, dtype: float64  future value 0.7972564597348412\n",
            "loop 1 market state : step  1722 market state fut market_state    1.0\n",
            "Name: 2018-06-21 00:00:00, dtype: float64  future value 0.8021824602022595\n",
            "loop 1 market state : step  1723 market state fut market_state    1.0\n",
            "Name: 2018-06-22 00:00:00, dtype: float64  future value 0.802790837875907\n",
            "loop 1 market state : step  1724 market state fut market_state    3.0\n",
            "Name: 2018-06-25 00:00:00, dtype: float64  future value 0.8052537660513767\n",
            "loop 1 market state : step  1725 market state fut market_state    1.0\n",
            "Name: 2018-06-26 00:00:00, dtype: float64  future value 0.8012698933964678\n",
            "loop 1 market state : step  1726 market state fut market_state    3.0\n",
            "Name: 2018-06-27 00:00:00, dtype: float64  future value 0.8081774836322647\n",
            "loop 1 market state : step  1727 market state fut market_state    3.0\n",
            "Name: 2018-06-28 00:00:00, dtype: float64  future value 0.8150318644694189\n",
            "loop 1 market state : step  1728 market state fut market_state    3.0\n",
            "Name: 2018-06-29 00:00:00, dtype: float64  future value 0.8222228792516109\n",
            "loop 1 market state : step  1729 market state fut market_state    4.0\n",
            "Name: 2018-07-02 00:00:00, dtype: float64  future value 0.8250786789887365\n",
            "loop 1 market state : step  1730 market state fut market_state    3.0\n",
            "Name: 2018-07-03 00:00:00, dtype: float64  future value 0.8192254035657278\n",
            "loop 1 market state : step  1731 market state fut market_state    4.0\n",
            "Name: 2018-07-05 00:00:00, dtype: float64  future value 0.8263928414235927\n",
            "loop 1 market state : step  1732 market state fut market_state    4.0\n",
            "Name: 2018-07-06 00:00:00, dtype: float64  future value 0.8272847157018743\n",
            "loop 1 market state : step  1733 market state fut market_state    4.0\n",
            "Name: 2018-07-09 00:00:00, dtype: float64  future value 0.8264341547156939\n",
            "loop 1 market state : step  1734 market state fut market_state    4.0\n",
            "Name: 2018-07-10 00:00:00, dtype: float64  future value 0.8297181549288658\n",
            "loop 1 market state : step  1735 market state fut market_state    4.0\n",
            "Name: 2018-07-11 00:00:00, dtype: float64  future value 0.8315107713739958\n",
            "loop 1 market state : step  1736 market state fut market_state    4.0\n",
            "Name: 2018-07-12 00:00:00, dtype: float64  future value 0.8282238150010879\n",
            "loop 1 market state : step  1737 market state fut market_state    4.0\n",
            "Name: 2018-07-13 00:00:00, dtype: float64  future value 0.8274382880524939\n",
            "loop 1 market state : step  1738 market state fut market_state    4.0\n",
            "Name: 2018-07-16 00:00:00, dtype: float64  future value 0.8289591604736937\n",
            "loop 1 market state : step  1739 market state fut market_state    4.0\n",
            "Name: 2018-07-17 00:00:00, dtype: float64  future value 0.8329223406010925\n",
            "loop 1 market state : step  1740 market state fut market_state    4.0\n",
            "Name: 2018-07-18 00:00:00, dtype: float64  future value 0.8405032707852046\n",
            "loop 1 market state : step  1741 market state fut market_state    4.0\n",
            "Name: 2018-07-19 00:00:00, dtype: float64  future value 0.8379546160446384\n",
            "loop 1 market state : step  1742 market state fut market_state    4.0\n",
            "Name: 2018-07-20 00:00:00, dtype: float64  future value 0.832455782992681\n",
            "loop 1 market state : step  1743 market state fut market_state    2.0\n",
            "Name: 2018-07-23 00:00:00, dtype: float64  future value 0.8276656908616681\n",
            "loop 1 market state : step  1744 market state fut market_state    2.0\n",
            "Name: 2018-07-24 00:00:00, dtype: float64  future value 0.8317086131764523\n",
            "loop 1 market state : step  1745 market state fut market_state    2.0\n",
            "Name: 2018-07-25 00:00:00, dtype: float64  future value 0.8308433437451523\n",
            "loop 1 market state : step  1746 market state fut market_state    2.0\n",
            "Name: 2018-07-26 00:00:00, dtype: float64  future value 0.8349364478312454\n",
            "loop 1 market state : step  1747 market state fut market_state    3.0\n",
            "Name: 2018-07-27 00:00:00, dtype: float64  future value 0.8388140455100265\n",
            "loop 1 market state : step  1748 market state fut market_state    4.0\n",
            "Name: 2018-07-30 00:00:00, dtype: float64  future value 0.8417819601891919\n",
            "loop 1 market state : step  1749 market state fut market_state    4.0\n",
            "Name: 2018-07-31 00:00:00, dtype: float64  future value 0.8441593059160438\n",
            "loop 1 market state : step  1750 market state fut market_state    4.0\n",
            "Name: 2018-08-01 00:00:00, dtype: float64  future value 0.8439378154263414\n",
            "loop 1 market state : step  1751 market state fut market_state    4.0\n",
            "Name: 2018-08-02 00:00:00, dtype: float64  future value 0.8427211318419653\n",
            "loop 1 market state : step  1752 market state fut market_state    2.0\n",
            "Name: 2018-08-03 00:00:00, dtype: float64  future value 0.8367261081166394\n",
            "loop 1 market state : step  1753 market state fut market_state    2.0\n",
            "Name: 2018-08-06 00:00:00, dtype: float64  future value 0.8333741900597051\n",
            "loop 1 market state : step  1754 market state fut market_state    2.0\n",
            "Name: 2018-08-07 00:00:00, dtype: float64  future value 0.8386988299964518\n",
            "loop 1 market state : step  1755 market state fut market_state    2.0\n",
            "Name: 2018-08-08 00:00:00, dtype: float64  future value 0.8323229031695716\n",
            "loop 1 market state : step  1756 market state fut market_state    2.0\n",
            "Name: 2018-08-09 00:00:00, dtype: float64  future value 0.8389144081666825\n",
            "loop 1 market state : step  1757 market state fut market_state    3.0\n",
            "Name: 2018-08-10 00:00:00, dtype: float64  future value 0.8417022180018064\n",
            "loop 1 market state : step  1758 market state fut market_state    3.0\n",
            "Name: 2018-08-13 00:00:00, dtype: float64  future value 0.8437458859433565\n",
            "loop 1 market state : step  1759 market state fut market_state    4.0\n",
            "Name: 2018-08-14 00:00:00, dtype: float64  future value 0.8454912050139947\n",
            "loop 1 market state : step  1760 market state fut market_state    4.0\n",
            "Name: 2018-08-15 00:00:00, dtype: float64  future value 0.8451545710689568\n",
            "loop 1 market state : step  1761 market state fut market_state    3.0\n",
            "Name: 2018-08-16 00:00:00, dtype: float64  future value 0.843725193120526\n",
            "loop 1 market state : step  1762 market state fut market_state    4.0\n",
            "Name: 2018-08-17 00:00:00, dtype: float64  future value 0.8489553103665285\n",
            "loop 1 market state : step  1763 market state fut market_state    4.0\n",
            "Name: 2018-08-20 00:00:00, dtype: float64  future value 0.8554671452344935\n",
            "loop 1 market state : step  1764 market state fut market_state    4.0\n",
            "Name: 2018-08-21 00:00:00, dtype: float64  future value 0.8556975042034036\n",
            "loop 1 market state : step  1765 market state fut market_state    4.0\n",
            "Name: 2018-08-22 00:00:00, dtype: float64  future value 0.8605762070010095\n",
            "loop 1 market state : step  1766 market state fut market_state    4.0\n",
            "Name: 2018-08-23 00:00:00, dtype: float64  future value 0.8567635713015754\n",
            "loop 1 market state : step  1767 market state fut market_state    4.0\n",
            "Name: 2018-08-24 00:00:00, dtype: float64  future value 0.8568787868151502\n",
            "loop 1 market state : step  1768 market state fut market_state    2.0\n",
            "Name: 2018-08-27 00:00:00, dtype: float64  future value 0.8554612332103424\n",
            "loop 1 market state : step  1769 market state fut market_state    2.0\n",
            "Name: 2018-08-28 00:00:00, dtype: float64  future value 0.8530632670142196\n",
            "loop 1 market state : step  1770 market state fut market_state    2.0\n",
            "Name: 2018-08-29 00:00:00, dtype: float64  future value 0.8499476196550261\n",
            "loop 1 market state : step  1771 market state fut market_state    2.0\n",
            "Name: 2018-08-30 00:00:00, dtype: float64  future value 0.8480663925433033\n",
            "loop 1 market state : step  1772 market state fut market_state    2.0\n",
            "Name: 2018-08-31 00:00:00, dtype: float64  future value 0.8496758756310959\n",
            "loop 1 market state : step  1773 market state fut market_state    2.0\n",
            "Name: 2018-09-04 00:00:00, dtype: float64  future value 0.8528535288099008\n",
            "loop 1 market state : step  1774 market state fut market_state    3.0\n",
            "Name: 2018-09-05 00:00:00, dtype: float64  future value 0.8531577176467245\n",
            "loop 1 market state : step  1775 market state fut market_state    3.0\n",
            "Name: 2018-09-06 00:00:00, dtype: float64  future value 0.8576643137637443\n",
            "loop 1 market state : step  1776 market state fut market_state    3.0\n",
            "Name: 2018-09-07 00:00:00, dtype: float64  future value 0.857900584461485\n",
            "loop 1 market state : step  1777 market state fut market_state    3.0\n",
            "Name: 2018-09-10 00:00:00, dtype: float64  future value 0.853122316674095\n",
            "loop 1 market state : step  1778 market state fut market_state    3.0\n",
            "Name: 2018-09-11 00:00:00, dtype: float64  future value 0.8577027429543489\n",
            "loop 1 market state : step  1779 market state fut market_state    3.0\n",
            "Name: 2018-09-12 00:00:00, dtype: float64  future value 0.8587776782364078\n",
            "loop 1 market state : step  1780 market state fut market_state    4.0\n",
            "Name: 2018-09-13 00:00:00, dtype: float64  future value 0.8655110035940754\n",
            "loop 1 market state : step  1781 market state fut market_state    4.0\n",
            "Name: 2018-09-14 00:00:00, dtype: float64  future value 0.8651920342538929\n",
            "loop 1 market state : step  1782 market state fut market_state    4.0\n",
            "Name: 2018-09-17 00:00:00, dtype: float64  future value 0.8621502891161729\n",
            "loop 1 market state : step  1783 market state fut market_state    4.0\n",
            "Name: 2018-09-18 00:00:00, dtype: float64  future value 0.8610251002998862\n",
            "loop 1 market state : step  1784 market state fut market_state    2.0\n",
            "Name: 2018-09-19 00:00:00, dtype: float64  future value 0.8581929492500063\n",
            "loop 1 market state : step  1785 market state fut market_state    2.0\n",
            "Name: 2018-09-20 00:00:00, dtype: float64  future value 0.8605643826573866\n",
            "loop 1 market state : step  1786 market state fut market_state    2.0\n",
            "Name: 2018-09-21 00:00:00, dtype: float64  future value 0.8605584703379148\n",
            "loop 1 market state : step  1787 market state fut market_state    3.0\n",
            "Name: 2018-09-24 00:00:00, dtype: float64  future value 0.863691854360203\n",
            "loop 1 market state : step  1788 market state fut market_state    3.0\n",
            "Name: 2018-09-25 00:00:00, dtype: float64  future value 0.8633492363327747\n",
            "loop 1 market state : step  1789 market state fut market_state    3.0\n",
            "Name: 2018-09-26 00:00:00, dtype: float64  future value 0.8639635263258938\n",
            "loop 1 market state : step  1790 market state fut market_state    2.0\n",
            "Name: 2018-09-27 00:00:00, dtype: float64  future value 0.8569053913668113\n",
            "loop 1 market state : step  1791 market state fut market_state    2.0\n",
            "Name: 2018-09-28 00:00:00, dtype: float64  future value 0.8521684365762021\n",
            "loop 1 market state : step  1792 market state fut market_state    2.0\n",
            "Name: 2018-10-01 00:00:00, dtype: float64  future value 0.8518317308682455\n",
            "loop 1 market state : step  1793 market state fut market_state    1.0\n",
            "Name: 2018-10-02 00:00:00, dtype: float64  future value 0.8506239154677565\n",
            "loop 1 market state : step  1794 market state fut market_state    0.0\n",
            "Name: 2018-10-03 00:00:00, dtype: float64  future value 0.8226688163907517\n",
            "loop 1 market state : step  1795 market state fut market_state    0.0\n",
            "Name: 2018-10-04 00:00:00, dtype: float64  future value 0.8057440444052735\n",
            "loop 1 market state : step  1796 market state fut market_state    0.0\n",
            "Name: 2018-10-05 00:00:00, dtype: float64  future value 0.8171906038080649\n",
            "loop 1 market state : step  1797 market state fut market_state    0.0\n",
            "Name: 2018-10-08 00:00:00, dtype: float64  future value 0.812365110409102\n",
            "loop 1 market state : step  1798 market state fut market_state    0.0\n",
            "Name: 2018-10-09 00:00:00, dtype: float64  future value 0.8298273860647295\n",
            "loop 1 market state : step  1799 market state fut market_state    0.0\n",
            "Name: 2018-10-10 00:00:00, dtype: float64  future value 0.82961771991865\n",
            "loop 1 market state : step  1800 market state fut market_state    0.0\n",
            "Name: 2018-10-11 00:00:00, dtype: float64  future value 0.8176779260022258\n",
            "loop 1 market state : step  1801 market state fut market_state    2.0\n",
            "Name: 2018-10-12 00:00:00, dtype: float64  future value 0.8173826053492891\n",
            "loop 1 market state : step  1802 market state fut market_state    2.0\n",
            "Name: 2018-10-15 00:00:00, dtype: float64  future value 0.8138682464625275\n",
            "loop 1 market state : step  1803 market state fut market_state    1.0\n",
            "Name: 2018-10-16 00:00:00, dtype: float64  future value 0.8093823428730179\n",
            "loop 1 market state : step  1804 market state fut market_state    0.0\n",
            "Name: 2018-10-17 00:00:00, dtype: float64  future value 0.7844012152064495\n",
            "loop 1 market state : step  1805 market state fut market_state    0.0\n",
            "Name: 2018-10-18 00:00:00, dtype: float64  future value 0.7990107190476058\n",
            "loop 1 market state : step  1806 market state fut market_state    0.0\n",
            "Name: 2018-10-19 00:00:00, dtype: float64  future value 0.7851660493322129\n",
            "loop 1 market state : step  1807 market state fut market_state    0.0\n",
            "Name: 2018-10-22 00:00:00, dtype: float64  future value 0.7800156745689163\n",
            "loop 1 market state : step  1808 market state fut market_state    0.0\n",
            "Name: 2018-10-23 00:00:00, dtype: float64  future value 0.7922360086349183\n",
            "loop 1 market state : step  1809 market state fut market_state    0.0\n",
            "Name: 2018-10-24 00:00:00, dtype: float64  future value 0.800832824441214\n",
            "loop 1 market state : step  1810 market state fut market_state    2.0\n",
            "Name: 2018-10-25 00:00:00, dtype: float64  future value 0.8092878922405132\n",
            "loop 1 market state : step  1811 market state fut market_state    2.0\n",
            "Name: 2018-10-26 00:00:00, dtype: float64  future value 0.8041758746095818\n",
            "loop 1 market state : step  1812 market state fut market_state    2.0\n",
            "Name: 2018-10-29 00:00:00, dtype: float64  future value 0.8086795145668657\n",
            "loop 1 market state : step  1813 market state fut market_state    2.0\n",
            "Name: 2018-10-30 00:00:00, dtype: float64  future value 0.8137412786635694\n",
            "loop 1 market state : step  1814 market state fut market_state    2.0\n",
            "Name: 2018-10-31 00:00:00, dtype: float64  future value 0.830999800492589\n",
            "loop 1 market state : step  1815 market state fut market_state    2.0\n",
            "Name: 2018-11-01 00:00:00, dtype: float64  future value 0.8289148913171771\n",
            "loop 1 market state : step  1816 market state fut market_state    2.0\n",
            "Name: 2018-11-02 00:00:00, dtype: float64  future value 0.8212896919765484\n",
            "loop 1 market state : step  1817 market state fut market_state    1.0\n",
            "Name: 2018-11-05 00:00:00, dtype: float64  future value 0.8051090618846442\n",
            "loop 1 market state : step  1818 market state fut market_state    1.0\n",
            "Name: 2018-11-06 00:00:00, dtype: float64  future value 0.8039159549292747\n",
            "loop 1 market state : step  1819 market state fut market_state    1.0\n",
            "Name: 2018-11-07 00:00:00, dtype: float64  future value 0.7978323925955951\n",
            "loop 1 market state : step  1820 market state fut market_state    1.0\n",
            "Name: 2018-11-08 00:00:00, dtype: float64  future value 0.806284432176919\n",
            "loop 1 market state : step  1821 market state fut market_state    1.0\n",
            "Name: 2018-11-09 00:00:00, dtype: float64  future value 0.8080770489173694\n",
            "loop 1 market state : step  1822 market state fut market_state    1.0\n",
            "Name: 2018-11-12 00:00:00, dtype: float64  future value 0.7946281345698085\n",
            "loop 1 market state : step  1823 market state fut market_state    1.0\n",
            "Name: 2018-11-13 00:00:00, dtype: float64  future value 0.7802046481874859\n",
            "loop 1 market state : step  1824 market state fut market_state    1.0\n",
            "Name: 2018-11-14 00:00:00, dtype: float64  future value 0.782579037754602\n",
            "loop 1 market state : step  1825 market state fut market_state    0.0\n",
            "Name: 2018-11-15 00:00:00, dtype: float64  future value 0.7774493555188153\n",
            "loop 1 market state : step  1826 market state fut market_state    1.0\n",
            "Name: 2018-11-16 00:00:00, dtype: float64  future value 0.7895249851227644\n",
            "loop 1 market state : step  1827 market state fut market_state    1.0\n",
            "Name: 2018-11-19 00:00:00, dtype: float64  future value 0.792100172652073\n",
            "loop 1 market state : step  1828 market state fut market_state    2.0\n",
            "Name: 2018-11-20 00:00:00, dtype: float64  future value 0.8102978658385455\n",
            "loop 1 market state : step  1829 market state fut market_state    2.0\n",
            "Name: 2018-11-21 00:00:00, dtype: float64  future value 0.8085288980806615\n",
            "loop 1 market state : step  1830 market state fut market_state    2.0\n",
            "Name: 2018-11-23 00:00:00, dtype: float64  future value 0.8151351835811314\n",
            "loop 1 market state : step  1831 market state fut market_state    3.0\n",
            "Name: 2018-11-26 00:00:00, dtype: float64  future value 0.8240539248873455\n",
            "loop 1 market state : step  1832 market state fut market_state    2.0\n",
            "Name: 2018-11-27 00:00:00, dtype: float64  future value 0.797383499592039\n",
            "loop 1 market state : step  1833 market state fut market_state    1.0\n",
            "Name: 2018-11-28 00:00:00, dtype: float64  future value 0.7961696998138389\n",
            "loop 1 market state : step  1834 market state fut market_state    1.0\n",
            "Name: 2018-11-29 00:00:00, dtype: float64  future value 0.7776029278694349\n",
            "loop 1 market state : step  1835 market state fut market_state    1.0\n",
            "Name: 2018-11-30 00:00:00, dtype: float64  future value 0.778973184099751\n",
            "loop 1 market state : step  1836 market state fut market_state    1.0\n",
            "Name: 2018-12-03 00:00:00, dtype: float64  future value 0.7786955998145885\n",
            "loop 1 market state : step  1837 market state fut market_state    2.0\n",
            "Name: 2018-12-04 00:00:00, dtype: float64  future value 0.7829157434625585\n",
            "loop 1 market state : step  1838 market state fut market_state    1.0\n",
            "Name: 2018-12-06 00:00:00, dtype: float64  future value 0.7827592149522032\n",
            "loop 1 market state : step  1839 market state fut market_state    0.0\n",
            "Name: 2018-12-07 00:00:00, dtype: float64  future value 0.7678189171319209\n",
            "loop 1 market state : step  1840 market state fut market_state    0.0\n",
            "Name: 2018-12-10 00:00:00, dtype: float64  future value 0.7518686457136061\n",
            "loop 1 market state : step  1841 market state fut market_state    0.0\n",
            "Name: 2018-12-11 00:00:00, dtype: float64  future value 0.7519336076929533\n",
            "loop 1 market state : step  1842 market state fut market_state    0.0\n",
            "Name: 2018-12-12 00:00:00, dtype: float64  future value 0.7403570525685487\n",
            "loop 1 market state : step  1843 market state fut market_state    0.0\n",
            "Name: 2018-12-13 00:00:00, dtype: float64  future value 0.7286800624339282\n",
            "loop 1 market state : step  1844 market state fut market_state    0.0\n",
            "Name: 2018-12-14 00:00:00, dtype: float64  future value 0.713677830852274\n",
            "loop 1 market state : step  1845 market state fut market_state    0.0\n",
            "Name: 2018-12-17 00:00:00, dtype: float64  future value 0.6943284160607724\n",
            "loop 1 market state : step  1846 market state fut market_state    0.0\n",
            "Name: 2018-12-18 00:00:00, dtype: float64  future value 0.7287627607810494\n",
            "loop 1 market state : step  1847 market state fut market_state    0.0\n",
            "Name: 2018-12-19 00:00:00, dtype: float64  future value 0.7350029236833237\n",
            "loop 1 market state : step  1848 market state fut market_state    0.0\n",
            "Name: 2018-12-20 00:00:00, dtype: float64  future value 0.734090356877532\n",
            "loop 1 market state : step  1849 market state fut market_state    2.0\n",
            "Name: 2018-12-21 00:00:00, dtype: float64  future value 0.740324607755655\n",
            "loop 1 market state : step  1850 market state fut market_state    2.0\n",
            "Name: 2018-12-24 00:00:00, dtype: float64  future value 0.7412637070548685\n",
            "loop 1 market state : step  1851 market state fut market_state    1.0\n",
            "Name: 2018-12-26 00:00:00, dtype: float64  future value 0.7229124415177766\n",
            "loop 1 market state : step  1852 market state fut market_state    2.0\n",
            "Name: 2018-12-27 00:00:00, dtype: float64  future value 0.7477341565724931\n",
            "loop 1 market state : step  1853 market state fut market_state    2.0\n",
            "Name: 2018-12-28 00:00:00, dtype: float64  future value 0.7529760981621185\n",
            "loop 1 market state : step  1854 market state fut market_state    2.0\n",
            "Name: 2018-12-31 00:00:00, dtype: float64  future value 0.7602764161384135\n",
            "loop 1 market state : step  1855 market state fut market_state    2.0\n",
            "Name: 2019-01-02 00:00:00, dtype: float64  future value 0.763392063497607\n",
            "loop 1 market state : step  1856 market state fut market_state    2.0\n",
            "Name: 2019-01-03 00:00:00, dtype: float64  future value 0.7668413886421027\n",
            "loop 1 market state : step  1857 market state fut market_state    2.0\n",
            "Name: 2019-01-04 00:00:00, dtype: float64  future value 0.7667292013465031\n",
            "loop 1 market state : step  1858 market state fut market_state    2.0\n",
            "Name: 2019-01-07 00:00:00, dtype: float64  future value 0.7626981030800213\n",
            "loop 1 market state : step  1859 market state fut market_state    2.0\n",
            "Name: 2019-01-08 00:00:00, dtype: float64  future value 0.7708755148312391\n",
            "loop 1 market state : step  1860 market state fut market_state    2.0\n",
            "Name: 2019-01-09 00:00:00, dtype: float64  future value 0.7725883890889836\n",
            "loop 1 market state : step  1861 market state fut market_state    2.0\n",
            "Name: 2019-01-10 00:00:00, dtype: float64  future value 0.778453416797376\n",
            "loop 1 market state : step  1862 market state fut market_state    2.0\n",
            "Name: 2019-01-11 00:00:00, dtype: float64  future value 0.7887158094869244\n",
            "loop 1 market state : step  1863 market state fut market_state    2.0\n",
            "Name: 2019-01-14 00:00:00, dtype: float64  future value 0.7775497181754714\n",
            "loop 1 market state : step  1864 market state fut market_state    3.0\n",
            "Name: 2019-01-15 00:00:00, dtype: float64  future value 0.7792625924332159\n",
            "loop 1 market state : step  1865 market state fut market_state    3.0\n",
            "Name: 2019-01-16 00:00:00, dtype: float64  future value 0.7803346439090988\n",
            "loop 1 market state : step  1866 market state fut market_state    4.0\n",
            "Name: 2019-01-17 00:00:00, dtype: float64  future value 0.7869586660726634\n",
            "loop 1 market state : step  1867 market state fut market_state    1.0\n",
            "Name: 2019-01-18 00:00:00, dtype: float64  future value 0.7807835372079756\n",
            "loop 1 market state : step  1868 market state fut market_state    2.0\n",
            "Name: 2019-01-22 00:00:00, dtype: float64  future value 0.7796465237527456\n",
            "loop 1 market state : step  1869 market state fut market_state    4.0\n",
            "Name: 2019-01-23 00:00:00, dtype: float64  future value 0.7917694510265069\n",
            "loop 1 market state : step  1870 market state fut market_state    4.0\n",
            "Name: 2019-01-24 00:00:00, dtype: float64  future value 0.7985766065474085\n",
            "loop 1 market state : step  1871 market state fut market_state    4.0\n",
            "Name: 2019-01-25 00:00:00, dtype: float64  future value 0.7992942153569195\n",
            "loop 1 market state : step  1872 market state fut market_state    4.0\n",
            "Name: 2019-01-28 00:00:00, dtype: float64  future value 0.8047104221199952\n",
            "loop 1 market state : step  1873 market state fut market_state    4.0\n",
            "Name: 2019-01-29 00:00:00, dtype: float64  future value 0.8084993370739438\n",
            "loop 1 market state : step  1874 market state fut market_state    4.0\n",
            "Name: 2019-01-30 00:00:00, dtype: float64  future value 0.8067008803675815\n",
            "loop 1 market state : step  1875 market state fut market_state    4.0\n",
            "Name: 2019-01-31 00:00:00, dtype: float64  future value 0.799152467349923\n",
            "loop 1 market state : step  1876 market state fut market_state    4.0\n",
            "Name: 2019-02-01 00:00:00, dtype: float64  future value 0.7996928551215685\n",
            "loop 1 market state : step  1877 market state fut market_state    2.0\n",
            "Name: 2019-02-04 00:00:00, dtype: float64  future value 0.8002599197984354\n",
            "loop 1 market state : step  1878 market state fut market_state    4.0\n",
            "Name: 2019-02-05 00:00:00, dtype: float64  future value 0.8105754498283875\n",
            "loop 1 market state : step  1879 market state fut market_state    4.0\n",
            "Name: 2019-02-06 00:00:00, dtype: float64  future value 0.8130266257184735\n",
            "loop 1 market state : step  1880 market state fut market_state    4.0\n",
            "Name: 2019-02-07 00:00:00, dtype: float64  future value 0.810870770481324\n",
            "loop 1 market state : step  1881 market state fut market_state    4.0\n",
            "Name: 2019-02-08 00:00:00, dtype: float64  future value 0.8196920332323787\n",
            "loop 1 market state : step  1882 market state fut market_state    4.0\n",
            "Name: 2019-02-11 00:00:00, dtype: float64  future value 0.8209205411603777\n",
            "loop 1 market state : step  1883 market state fut market_state    4.0\n",
            "Name: 2019-02-12 00:00:00, dtype: float64  future value 0.8223794077619663\n",
            "loop 1 market state : step  1884 market state fut market_state    4.0\n",
            "Name: 2019-02-13 00:00:00, dtype: float64  future value 0.8194793388683238\n",
            "loop 1 market state : step  1885 market state fut market_state    4.0\n",
            "Name: 2019-02-14 00:00:00, dtype: float64  future value 0.8247331048015724\n",
            "loop 1 market state : step  1886 market state fut market_state    4.0\n",
            "Name: 2019-02-15 00:00:00, dtype: float64  future value 0.8257490624819952\n",
            "loop 1 market state : step  1887 market state fut market_state    4.0\n",
            "Name: 2019-02-19 00:00:00, dtype: float64  future value 0.8250963432982714\n",
            "loop 1 market state : step  1888 market state fut market_state    4.0\n",
            "Name: 2019-02-20 00:00:00, dtype: float64  future value 0.8246474502947151\n",
            "loop 1 market state : step  1889 market state fut market_state    3.0\n",
            "Name: 2019-02-21 00:00:00, dtype: float64  future value 0.822317401942355\n",
            "loop 1 market state : step  1890 market state fut market_state    4.0\n",
            "Name: 2019-02-22 00:00:00, dtype: float64  future value 0.8279875440080267\n",
            "loop 1 market state : step  1891 market state fut market_state    2.0\n",
            "Name: 2019-02-25 00:00:00, dtype: float64  future value 0.8247744901519128\n",
            "loop 1 market state : step  1892 market state fut market_state    2.0\n",
            "Name: 2019-02-26 00:00:00, dtype: float64  future value 0.8238412305232906\n",
            "loop 1 market state : step  1893 market state fut market_state    2.0\n",
            "Name: 2019-02-27 00:00:00, dtype: float64  future value 0.8184664091105557\n",
            "loop 1 market state : step  1894 market state fut market_state    2.0\n",
            "Name: 2019-02-28 00:00:00, dtype: float64  future value 0.8118157823953299\n",
            "loop 1 market state : step  1895 market state fut market_state    2.0\n",
            "Name: 2019-03-01 00:00:00, dtype: float64  future value 0.81008524353273\n",
            "loop 1 market state : step  1896 market state fut market_state    2.0\n",
            "Name: 2019-03-04 00:00:00, dtype: float64  future value 0.8219659877892789\n",
            "loop 1 market state : step  1897 market state fut market_state    3.0\n",
            "Name: 2019-03-05 00:00:00, dtype: float64  future value 0.8243935149921192\n",
            "loop 1 market state : step  1898 market state fut market_state    4.0\n",
            "Name: 2019-03-06 00:00:00, dtype: float64  future value 0.8301227067176662\n",
            "loop 1 market state : step  1899 market state fut market_state    4.0\n",
            "Name: 2019-03-07 00:00:00, dtype: float64  future value 0.8294021414530987\n",
            "loop 1 market state : step  1900 market state fut market_state    4.0\n",
            "Name: 2019-03-08 00:00:00, dtype: float64  future value 0.8335366305942117\n",
            "loop 1 market state : step  1901 market state fut market_state    4.0\n",
            "Name: 2019-03-11 00:00:00, dtype: float64  future value 0.8366256731064236\n",
            "loop 1 market state : step  1902 market state fut market_state    4.0\n",
            "Name: 2019-03-12 00:00:00, dtype: float64  future value 0.8365164419705599\n",
            "loop 1 market state : step  1903 market state fut market_state    4.0\n",
            "Name: 2019-03-13 00:00:00, dtype: float64  future value 0.8340534417368508\n",
            "loop 1 market state : step  1904 market state fut market_state    4.0\n",
            "Name: 2019-03-14 00:00:00, dtype: float64  future value 0.8431049911032555\n",
            "loop 1 market state : step  1905 market state fut market_state    2.0\n",
            "Name: 2019-03-15 00:00:00, dtype: float64  future value 0.8271074943686885\n",
            "loop 1 market state : step  1906 market state fut market_state    2.0\n",
            "Name: 2019-03-18 00:00:00, dtype: float64  future value 0.8264135339511026\n",
            "loop 1 market state : step  1907 market state fut market_state    2.0\n",
            "Name: 2019-03-19 00:00:00, dtype: float64  future value 0.8323494359583139\n",
            "loop 1 market state : step  1908 market state fut market_state    2.0\n",
            "Name: 2019-03-20 00:00:00, dtype: float64  future value 0.8284837346813951\n",
            "loop 1 market state : step  1909 market state fut market_state    2.0\n",
            "Name: 2019-03-21 00:00:00, dtype: float64  future value 0.8314575616800322\n",
            "loop 1 market state : step  1910 market state fut market_state    3.0\n",
            "Name: 2019-03-22 00:00:00, dtype: float64  future value 0.8370568297422055\n",
            "loop 1 market state : step  1911 market state fut market_state    4.0\n",
            "Name: 2019-03-25 00:00:00, dtype: float64  future value 0.8467404054695037\n",
            "loop 1 market state : step  1912 market state fut market_state    4.0\n",
            "Name: 2019-03-26 00:00:00, dtype: float64  future value 0.8467551859728625\n",
            "loop 1 market state : step  1913 market state fut market_state    4.0\n",
            "Name: 2019-03-27 00:00:00, dtype: float64  future value 0.8485743352067348\n",
            "loop 1 market state : step  1914 market state fut market_state    4.0\n",
            "Name: 2019-03-28 00:00:00, dtype: float64  future value 0.8503433032599393\n",
            "loop 1 market state : step  1915 market state fut market_state    4.0\n",
            "Name: 2019-03-29 00:00:00, dtype: float64  future value 0.854285862622747\n",
            "loop 1 market state : step  1916 market state fut market_state    4.0\n",
            "Name: 2019-04-01 00:00:00, dtype: float64  future value 0.8551806930607645\n",
            "loop 1 market state : step  1917 market state fut market_state    4.0\n",
            "Name: 2019-04-02 00:00:00, dtype: float64  future value 0.8499918888115426\n",
            "loop 1 market state : step  1918 market state fut market_state    4.0\n",
            "Name: 2019-04-03 00:00:00, dtype: float64  future value 0.852948051500645\n",
            "loop 1 market state : step  1919 market state fut market_state    4.0\n",
            "Name: 2019-04-04 00:00:00, dtype: float64  future value 0.8529805683717778\n",
            "loop 1 market state : step  1920 market state fut market_state    4.0\n",
            "Name: 2019-04-05 00:00:00, dtype: float64  future value 0.8586181935663166\n",
            "loop 1 market state : step  1921 market state fut market_state    4.0\n",
            "Name: 2019-04-08 00:00:00, dtype: float64  future value 0.8580778057946709\n",
            "loop 1 market state : step  1922 market state fut market_state    4.0\n",
            "Name: 2019-04-09 00:00:00, dtype: float64  future value 0.8585148747499247\n",
            "loop 1 market state : step  1923 market state fut market_state    4.0\n",
            "Name: 2019-04-10 00:00:00, dtype: float64  future value 0.856562773339383\n",
            "loop 1 market state : step  1924 market state fut market_state    4.0\n",
            "Name: 2019-04-11 00:00:00, dtype: float64  future value 0.8579153649648438\n",
            "loop 1 market state : step  1925 market state fut market_state    4.0\n",
            "Name: 2019-04-12 00:00:00, dtype: float64  future value 0.8587835905558796\n",
            "loop 1 market state : step  1926 market state fut market_state    4.0\n",
            "Name: 2019-04-15 00:00:00, dtype: float64  future value 0.8663762730253753\n",
            "loop 1 market state : step  1927 market state fut market_state    4.0\n",
            "Name: 2019-04-16 00:00:00, dtype: float64  future value 0.8644773813087971\n",
            "loop 1 market state : step  1928 market state fut market_state    4.0\n",
            "Name: 2019-04-17 00:00:00, dtype: float64  future value 0.8641584119686146\n",
            "loop 1 market state : step  1929 market state fut market_state    4.0\n",
            "Name: 2019-04-18 00:00:00, dtype: float64  future value 0.8682072466028704\n",
            "loop 1 market state : step  1930 market state fut market_state    4.0\n",
            "Name: 2019-04-22 00:00:00, dtype: float64  future value 0.8691375497764363\n",
            "loop 1 market state : step  1931 market state fut market_state    4.0\n",
            "Name: 2019-04-23 00:00:00, dtype: float64  future value 0.8699644620753709\n",
            "loop 1 market state : step  1932 market state fut market_state    2.0\n",
            "Name: 2019-04-24 00:00:00, dtype: float64  future value 0.8634378467040471\n",
            "loop 1 market state : step  1933 market state fut market_state    2.0\n",
            "Name: 2019-04-25 00:00:00, dtype: float64  future value 0.8616039172621366\n",
            "loop 1 market state : step  1934 market state fut market_state    4.0\n",
            "Name: 2019-04-26 00:00:00, dtype: float64  future value 0.8699082965169922\n",
            "loop 1 market state : step  1935 market state fut market_state    2.0\n",
            "Name: 2019-04-29 00:00:00, dtype: float64  future value 0.8660189465528274\n",
            "loop 1 market state : step  1936 market state fut market_state    2.0\n",
            "Name: 2019-04-30 00:00:00, dtype: float64  future value 0.851719543572646\n",
            "loop 1 market state : step  1937 market state fut market_state    2.0\n",
            "Name: 2019-05-01 00:00:00, dtype: float64  future value 0.8503521714438264\n",
            "loop 1 market state : step  1938 market state fut market_state    2.0\n",
            "Name: 2019-05-02 00:00:00, dtype: float64  future value 0.8477828962339895\n",
            "loop 1 market state : step  1939 market state fut market_state    2.0\n",
            "Name: 2019-05-03 00:00:00, dtype: float64  future value 0.850936900430228\n",
            "loop 1 market state : step  1940 market state fut market_state    0.0\n",
            "Name: 2019-05-06 00:00:00, dtype: float64  future value 0.8304033189254834\n",
            "loop 1 market state : step  1941 market state fut market_state    0.0\n",
            "Name: 2019-05-07 00:00:00, dtype: float64  future value 0.8370597859019414\n",
            "loop 1 market state : step  1942 market state fut market_state    0.0\n",
            "Name: 2019-05-08 00:00:00, dtype: float64  future value 0.8419473571787549\n",
            "loop 1 market state : step  1943 market state fut market_state    3.0\n",
            "Name: 2019-05-09 00:00:00, dtype: float64  future value 0.8494367205365381\n",
            "loop 1 market state : step  1944 market state fut market_state    0.0\n",
            "Name: 2019-05-10 00:00:00, dtype: float64  future value 0.8444782752562263\n",
            "loop 1 market state : step  1945 market state fut market_state    0.0\n",
            "Name: 2019-05-13 00:00:00, dtype: float64  future value 0.8387785721838371\n",
            "loop 1 market state : step  1946 market state fut market_state    2.0\n",
            "Name: 2019-05-14 00:00:00, dtype: float64  future value 0.8459046970449213\n",
            "loop 1 market state : step  1947 market state fut market_state    2.0\n",
            "Name: 2019-05-15 00:00:00, dtype: float64  future value 0.843515527269767\n",
            "loop 1 market state : step  1948 market state fut market_state    1.0\n",
            "Name: 2019-05-16 00:00:00, dtype: float64  future value 0.8334657565907134\n",
            "loop 1 market state : step  1949 market state fut market_state    1.0\n",
            "Name: 2019-05-17 00:00:00, dtype: float64  future value 0.8345939018620564\n",
            "loop 1 market state : step  1950 market state fut market_state    0.0\n",
            "Name: 2019-05-20 00:00:00, dtype: float64  future value 0.8276036129838176\n",
            "loop 1 market state : step  1951 market state fut market_state    0.0\n",
            "Name: 2019-05-21 00:00:00, dtype: float64  future value 0.8218832894421577\n",
            "loop 1 market state : step  1952 market state fut market_state    0.0\n",
            "Name: 2019-05-22 00:00:00, dtype: float64  future value 0.8236079877482045\n",
            "loop 1 market state : step  1953 market state fut market_state    0.0\n",
            "Name: 2019-05-23 00:00:00, dtype: float64  future value 0.8127401735447446\n",
            "loop 1 market state : step  1954 market state fut market_state    0.0\n",
            "Name: 2019-05-24 00:00:00, dtype: float64  future value 0.8104927514812662\n",
            "loop 1 market state : step  1955 market state fut market_state    0.0\n",
            "Name: 2019-05-28 00:00:00, dtype: float64  future value 0.8278635326641247\n",
            "loop 1 market state : step  1956 market state fut market_state    2.0\n",
            "Name: 2019-05-29 00:00:00, dtype: float64  future value 0.8346204343554782\n",
            "loop 1 market state : step  1957 market state fut market_state    2.0\n",
            "Name: 2019-05-30 00:00:00, dtype: float64  future value 0.8397413204656171\n",
            "loop 1 market state : step  1958 market state fut market_state    2.0\n",
            "Name: 2019-05-31 00:00:00, dtype: float64  future value 0.8485566708972\n",
            "loop 1 market state : step  1959 market state fut market_state    2.0\n",
            "Name: 2019-06-03 00:00:00, dtype: float64  future value 0.8525109825453911\n",
            "loop 1 market state : step  1960 market state fut market_state    2.0\n",
            "Name: 2019-06-04 00:00:00, dtype: float64  future value 0.8522127060280392\n",
            "loop 1 market state : step  1961 market state fut market_state    2.0\n",
            "Name: 2019-06-05 00:00:00, dtype: float64  future value 0.8504762551412881\n",
            "loop 1 market state : step  1962 market state fut market_state    2.0\n",
            "Name: 2019-06-06 00:00:00, dtype: float64  future value 0.8539609812584132\n",
            "loop 1 market state : step  1963 market state fut market_state    2.0\n",
            "Name: 2019-06-07 00:00:00, dtype: float64  future value 0.8525848127086253\n",
            "loop 1 market state : step  1964 market state fut market_state    2.0\n",
            "Name: 2019-06-10 00:00:00, dtype: float64  future value 0.853379208136427\n",
            "loop 1 market state : step  1965 market state fut market_state    2.0\n",
            "Name: 2019-06-11 00:00:00, dtype: float64  future value 0.861671835105899\n",
            "loop 1 market state : step  1966 market state fut market_state    4.0\n",
            "Name: 2019-06-12 00:00:00, dtype: float64  future value 0.8642440664754717\n",
            "loop 1 market state : step  1967 market state fut market_state    4.0\n",
            "Name: 2019-06-13 00:00:00, dtype: float64  future value 0.8724303464105766\n",
            "loop 1 market state : step  1968 market state fut market_state    4.0\n",
            "Name: 2019-06-14 00:00:00, dtype: float64  future value 0.8713317621459512\n",
            "loop 1 market state : step  1969 market state fut market_state    4.0\n",
            "Name: 2019-06-17 00:00:00, dtype: float64  future value 0.8698227140683743\n",
            "loop 1 market state : step  1970 market state fut market_state    2.0\n",
            "Name: 2019-06-18 00:00:00, dtype: float64  future value 0.8615625319117959\n",
            "loop 1 market state : step  1971 market state fut market_state    2.0\n",
            "Name: 2019-06-19 00:00:00, dtype: float64  future value 0.8604994206780394\n",
            "loop 1 market state : step  1972 market state fut market_state    2.0\n",
            "Name: 2019-06-20 00:00:00, dtype: float64  future value 0.8637892611524438\n",
            "loop 1 market state : step  1973 market state fut market_state    2.0\n",
            "Name: 2019-06-21 00:00:00, dtype: float64  future value 0.8687624869361144\n",
            "loop 1 market state : step  1974 market state fut market_state    4.0\n",
            "Name: 2019-06-24 00:00:00, dtype: float64  future value 0.8754278941546988\n",
            "loop 1 market state : step  1975 market state fut market_state    4.0\n",
            "Name: 2019-06-25 00:00:00, dtype: float64  future value 0.8779912573403845\n",
            "loop 1 market state : step  1976 market state fut market_state    4.0\n",
            "Name: 2019-06-26 00:00:00, dtype: float64  future value 0.8847275385624673\n",
            "loop 1 market state : step  1977 market state fut market_state    4.0\n",
            "Name: 2019-06-27 00:00:00, dtype: float64  future value 0.8831298077600582\n",
            "loop 1 market state : step  1978 market state fut market_state    4.0\n",
            "Name: 2019-06-28 00:00:00, dtype: float64  future value 0.8788594826360998\n",
            "loop 1 market state : step  1979 market state fut market_state    4.0\n",
            "Name: 2019-07-01 00:00:00, dtype: float64  future value 0.8799462425571021\n",
            "loop 1 market state : step  1980 market state fut market_state    4.0\n",
            "Name: 2019-07-02 00:00:00, dtype: float64  future value 0.8839154067668915\n",
            "loop 1 market state : step  1981 market state fut market_state    4.0\n",
            "Name: 2019-07-03 00:00:00, dtype: float64  future value 0.8859353539629563\n",
            "loop 1 market state : step  1982 market state fut market_state    4.0\n",
            "Name: 2019-07-05 00:00:00, dtype: float64  future value 0.8900285301072888\n",
            "loop 1 market state : step  1983 market state fut market_state    4.0\n",
            "Name: 2019-07-08 00:00:00, dtype: float64  future value 0.8901850586176441\n",
            "loop 1 market state : step  1984 market state fut market_state    4.0\n",
            "Name: 2019-07-09 00:00:00, dtype: float64  future value 0.8871550657653076\n",
            "loop 1 market state : step  1985 market state fut market_state    2.0\n",
            "Name: 2019-07-10 00:00:00, dtype: float64  future value 0.8813608400021742\n",
            "loop 1 market state : step  1986 market state fut market_state    2.0\n",
            "Name: 2019-07-11 00:00:00, dtype: float64  future value 0.8845178724163878\n",
            "loop 1 market state : step  1987 market state fut market_state    2.0\n",
            "Name: 2019-07-12 00:00:00, dtype: float64  future value 0.8790544403370598\n",
            "loop 1 market state : step  1988 market state fut market_state    2.0\n",
            "Name: 2019-07-15 00:00:00, dtype: float64  future value 0.8815410171997754\n",
            "loop 1 market state : step  1989 market state fut market_state    3.0\n",
            "Name: 2019-07-16 00:00:00, dtype: float64  future value 0.8875773542172026\n",
            "loop 1 market state : step  1990 market state fut market_state    4.0\n",
            "Name: 2019-07-17 00:00:00, dtype: float64  future value 0.8917384482052974\n",
            "loop 1 market state : step  1991 market state fut market_state    3.0\n",
            "Name: 2019-07-18 00:00:00, dtype: float64  future value 0.8870457625712047\n",
            "loop 1 market state : step  1992 market state fut market_state    4.0\n",
            "Name: 2019-07-19 00:00:00, dtype: float64  future value 0.8935989824941896\n",
            "loop 1 market state : step  1993 market state fut market_state    4.0\n",
            "Name: 2019-07-22 00:00:00, dtype: float64  future value 0.8921548243377206\n",
            "loop 1 market state : step  1994 market state fut market_state    3.0\n",
            "Name: 2019-07-23 00:00:00, dtype: float64  future value 0.8898542649338387\n",
            "loop 1 market state : step  1995 market state fut market_state    2.0\n",
            "Name: 2019-07-24 00:00:00, dtype: float64  future value 0.8801677330468046\n",
            "loop 1 market state : step  1996 market state fut market_state    2.0\n",
            "Name: 2019-07-25 00:00:00, dtype: float64  future value 0.8722472851114788\n",
            "loop 1 market state : step  1997 market state fut market_state    1.0\n",
            "Name: 2019-07-26 00:00:00, dtype: float64  future value 0.8658949349136049\n",
            "loop 1 market state : step  1998 market state fut market_state    0.0\n",
            "Name: 2019-07-29 00:00:00, dtype: float64  future value 0.840110471281788\n",
            "loop 1 market state : step  1999 market state fut market_state    0.0\n",
            "Name: 2019-07-30 00:00:00, dtype: float64  future value 0.8510462039196515\n",
            "loop 1 market state : step  2000 market state fut market_state    0.0\n",
            "Name: 2019-07-31 00:00:00, dtype: float64  future value 0.8516988507498153\n",
            "loop 1 market state : step  2001 market state fut market_state    2.0\n",
            "Name: 2019-08-01 00:00:00, dtype: float64  future value 0.8676786831748478\n",
            "loop 1 market state : step  2002 market state fut market_state    0.0\n",
            "Name: 2019-08-02 00:00:00, dtype: float64  future value 0.861937594752118\n",
            "loop 1 market state : step  2003 market state fut market_state    0.0\n",
            "Name: 2019-08-05 00:00:00, dtype: float64  future value 0.8513208317497575\n",
            "loop 1 market state : step  2004 market state fut market_state    2.0\n",
            "Name: 2019-08-06 00:00:00, dtype: float64  future value 0.8642027531833704\n",
            "loop 1 market state : step  2005 market state fut market_state    0.0\n",
            "Name: 2019-08-07 00:00:00, dtype: float64  future value 0.8388878756732606\n",
            "loop 1 market state : step  2006 market state fut market_state    1.0\n",
            "Name: 2019-08-08 00:00:00, dtype: float64  future value 0.8409551202438172\n",
            "loop 1 market state : step  2007 market state fut market_state    1.0\n",
            "Name: 2019-08-09 00:00:00, dtype: float64  future value 0.8530868436432263\n",
            "loop 1 market state : step  2008 market state fut market_state    2.0\n",
            "Name: 2019-08-12 00:00:00, dtype: float64  future value 0.8634141980168012\n",
            "loop 1 market state : step  2009 market state fut market_state    1.0\n",
            "Name: 2019-08-13 00:00:00, dtype: float64  future value 0.8565805100024777\n",
            "loop 1 market state : step  2010 market state fut market_state    2.0\n",
            "Name: 2019-08-14 00:00:00, dtype: float64  future value 0.8636445569857113\n",
            "loop 1 market state : step  2011 market state fut market_state    2.0\n",
            "Name: 2019-08-15 00:00:00, dtype: float64  future value 0.8632074880304575\n",
            "loop 1 market state : step  2012 market state fut market_state    1.0\n",
            "Name: 2019-08-16 00:00:00, dtype: float64  future value 0.8408104157817642\n",
            "loop 1 market state : step  2013 market state fut market_state    1.0\n",
            "Name: 2019-08-19 00:00:00, dtype: float64  future value 0.8500450264472666\n",
            "loop 1 market state : step  2014 market state fut market_state    1.0\n",
            "Name: 2019-08-20 00:00:00, dtype: float64  future value 0.8473221785914898\n",
            "loop 1 market state : step  2015 market state fut market_state    1.0\n",
            "Name: 2019-08-21 00:00:00, dtype: float64  future value 0.8528683090179391\n",
            "loop 1 market state : step  2016 market state fut market_state    2.0\n",
            "Name: 2019-08-22 00:00:00, dtype: float64  future value 0.8636888982004671\n",
            "loop 1 market state : step  2017 market state fut market_state    2.0\n",
            "Name: 2019-08-23 00:00:00, dtype: float64  future value 0.8642440664754717\n",
            "loop 1 market state : step  2018 market state fut market_state    2.0\n",
            "Name: 2019-08-26 00:00:00, dtype: float64  future value 0.8582815599165993\n",
            "loop 1 market state : step  2019 market state fut market_state    2.0\n",
            "Name: 2019-08-27 00:00:00, dtype: float64  future value 0.8675871163485189\n",
            "loop 1 market state : step  2020 market state fut market_state    2.0\n",
            "Name: 2019-08-28 00:00:00, dtype: float64  future value 0.8788742631394586\n",
            "loop 1 market state : step  2021 market state fut market_state    2.0\n",
            "Name: 2019-08-29 00:00:00, dtype: float64  future value 0.8796745705914114\n",
            "loop 1 market state : step  2022 market state fut market_state    2.0\n",
            "Name: 2019-08-30 00:00:00, dtype: float64  future value 0.8795918722442903\n",
            "loop 1 market state : step  2023 market state fut market_state    2.0\n",
            "Name: 2019-09-03 00:00:00, dtype: float64  future value 0.8798753685536039\n",
            "loop 1 market state : step  2024 market state fut market_state    2.0\n",
            "Name: 2019-09-04 00:00:00, dtype: float64  future value 0.8862365869353648\n",
            "loop 1 market state : step  2025 market state fut market_state    4.0\n",
            "Name: 2019-09-05 00:00:00, dtype: float64  future value 0.8887881975403462\n",
            "loop 1 market state : step  2026 market state fut market_state    4.0\n",
            "Name: 2019-09-06 00:00:00, dtype: float64  future value 0.8881443468358301\n",
            "loop 1 market state : step  2027 market state fut market_state    4.0\n",
            "Name: 2019-09-09 00:00:00, dtype: float64  future value 0.8853594931604418\n",
            "loop 1 market state : step  2028 market state fut market_state    4.0\n",
            "Name: 2019-09-10 00:00:00, dtype: float64  future value 0.887645272060965\n",
            "loop 1 market state : step  2029 market state fut market_state    4.0\n",
            "Name: 2019-09-11 00:00:00, dtype: float64  future value 0.8879494608977887\n",
            "loop 1 market state : step  2030 market state fut market_state    2.0\n",
            "Name: 2019-09-12 00:00:00, dtype: float64  future value 0.8879671975608834\n",
            "loop 1 market state : step  2031 market state fut market_state    2.0\n",
            "Name: 2019-09-13 00:00:00, dtype: float64  future value 0.8836200861139549\n",
            "loop 1 market state : step  2032 market state fut market_state    2.0\n",
            "Name: 2019-09-16 00:00:00, dtype: float64  future value 0.8835344316070978\n",
            "loop 1 market state : step  2033 market state fut market_state    2.0\n",
            "Name: 2019-09-17 00:00:00, dtype: float64  future value 0.8760982779432781\n",
            "loop 1 market state : step  2034 market state fut market_state    2.0\n",
            "Name: 2019-09-18 00:00:00, dtype: float64  future value 0.8814937918835232\n",
            "loop 1 market state : step  2035 market state fut market_state    2.0\n",
            "Name: 2019-09-19 00:00:00, dtype: float64  future value 0.8793527171497325\n",
            "loop 1 market state : step  2036 market state fut market_state    2.0\n",
            "Name: 2019-09-20 00:00:00, dtype: float64  future value 0.8746777681787343\n",
            "loop 1 market state : step  2037 market state fut market_state    2.0\n",
            "Name: 2019-09-23 00:00:00, dtype: float64  future value 0.8790927974694251\n",
            "loop 1 market state : step  2038 market state fut market_state    2.0\n",
            "Name: 2019-09-24 00:00:00, dtype: float64  future value 0.8683165497969735\n",
            "loop 1 market state : step  2039 market state fut market_state    1.0\n",
            "Name: 2019-09-25 00:00:00, dtype: float64  future value 0.8527709022256983\n",
            "loop 1 market state : step  2040 market state fut market_state    1.0\n",
            "Name: 2019-09-26 00:00:00, dtype: float64  future value 0.8595691175044735\n",
            "loop 1 market state : step  2041 market state fut market_state    2.0\n",
            "Name: 2019-09-27 00:00:00, dtype: float64  future value 0.871789523628715\n",
            "loop 1 market state : step  2042 market state fut market_state    2.0\n",
            "Name: 2019-09-30 00:00:00, dtype: float64  future value 0.8678853931611914\n",
            "loop 1 market state : step  2043 market state fut market_state    2.0\n",
            "Name: 2019-10-01 00:00:00, dtype: float64  future value 0.8543803856088117\n",
            "loop 1 market state : step  2044 market state fut market_state    3.0\n",
            "Name: 2019-10-02 00:00:00, dtype: float64  future value 0.8621590852418205\n",
            "loop 1 market state : step  2045 market state fut market_state    3.0\n",
            "Name: 2019-10-03 00:00:00, dtype: float64  future value 0.8676904354602313\n",
            "loop 1 market state : step  2046 market state fut market_state    3.0\n",
            "Name: 2019-10-04 00:00:00, dtype: float64  future value 0.8771820817045446\n",
            "loop 1 market state : step  2047 market state fut market_state    3.0\n",
            "Name: 2019-10-07 00:00:00, dtype: float64  future value 0.8759653257666087\n",
            "loop 1 market state : step  2048 market state fut market_state    3.0\n",
            "Name: 2019-10-08 00:00:00, dtype: float64  future value 0.8846861535074474\n",
            "loop 1 market state : step  2049 market state fut market_state    3.0\n",
            "Name: 2019-10-09 00:00:00, dtype: float64  future value 0.8829171854542428\n",
            "loop 1 market state : step  2050 market state fut market_state    3.0\n",
            "Name: 2019-10-10 00:00:00, dtype: float64  future value 0.8853565370007059\n",
            "loop 1 market state : step  2051 market state fut market_state    3.0\n",
            "Name: 2019-10-11 00:00:00, dtype: float64  future value 0.8818865193287004\n",
            "loop 1 market state : step  2052 market state fut market_state    3.0\n",
            "Name: 2019-10-14 00:00:00, dtype: float64  future value 0.8879465050333735\n",
            "loop 1 market state : step  2053 market state fut market_state    3.0\n",
            "Name: 2019-10-15 00:00:00, dtype: float64  future value 0.8847777200384557\n",
            "loop 1 market state : step  2054 market state fut market_state    3.0\n",
            "Name: 2019-10-16 00:00:00, dtype: float64  future value 0.8872968140676247\n",
            "loop 1 market state : step  2055 market state fut market_state    4.0\n",
            "Name: 2019-10-17 00:00:00, dtype: float64  future value 0.8890008198461616\n",
            "loop 1 market state : step  2056 market state fut market_state    4.0\n",
            "Name: 2019-10-18 00:00:00, dtype: float64  future value 0.8926214540043714\n",
            "loop 1 market state : step  2057 market state fut market_state    4.0\n",
            "Name: 2019-10-21 00:00:00, dtype: float64  future value 0.8976034759136897\n",
            "loop 1 market state : step  2058 market state fut market_state    4.0\n",
            "Name: 2019-10-22 00:00:00, dtype: float64  future value 0.8968563060974611\n",
            "loop 1 market state : step  2059 market state fut market_state    4.0\n",
            "Name: 2019-10-23 00:00:00, dtype: float64  future value 0.8997741116541981\n",
            "loop 1 market state : step  2060 market state fut market_state    4.0\n",
            "Name: 2019-10-24 00:00:00, dtype: float64  future value 0.897054219958157\n",
            "loop 1 market state : step  2061 market state fut market_state    4.0\n",
            "Name: 2019-10-25 00:00:00, dtype: float64  future value 0.9057218377097116\n",
            "loop 1 market state : step  2062 market state fut market_state    4.0\n",
            "Name: 2019-10-28 00:00:00, dtype: float64  future value 0.9090767122217024\n",
            "loop 1 market state : step  2063 market state fut market_state    4.0\n",
            "Name: 2019-10-29 00:00:00, dtype: float64  future value 0.9079988204845871\n",
            "loop 1 market state : step  2064 market state fut market_state    4.0\n",
            "Name: 2019-10-30 00:00:00, dtype: float64  future value 0.9086366871067127\n",
            "loop 1 market state : step  2065 market state fut market_state    4.0\n",
            "Name: 2019-10-31 00:00:00, dtype: float64  future value 0.9111173519452772\n",
            "loop 1 market state : step  2066 market state fut market_state    4.0\n",
            "Name: 2019-11-01 00:00:00, dtype: float64  future value 0.913450428220292\n",
            "loop 1 market state : step  2067 market state fut market_state    4.0\n",
            "Name: 2019-11-04 00:00:00, dtype: float64  future value 0.9116578117751621\n",
            "loop 1 market state : step  2068 market state fut market_state    4.0\n",
            "Name: 2019-11-05 00:00:00, dtype: float64  future value 0.9130842335638572\n",
            "loop 1 market state : step  2069 market state fut market_state    4.0\n",
            "Name: 2019-11-06 00:00:00, dtype: float64  future value 0.9137339245296058\n",
            "loop 1 market state : step  2070 market state fut market_state    4.0\n",
            "Name: 2019-11-07 00:00:00, dtype: float64  future value 0.9144987589506897\n",
            "loop 1 market state : step  2071 market state fut market_state    4.0\n",
            "Name: 2019-11-08 00:00:00, dtype: float64  future value 0.921536273145181\n",
            "loop 1 market state : step  2072 market state fut market_state    4.0\n",
            "Name: 2019-11-11 00:00:00, dtype: float64  future value 0.921999946652096\n",
            "loop 1 market state : step  2073 market state fut market_state    4.0\n",
            "Name: 2019-11-12 00:00:00, dtype: float64  future value 0.9214535747980598\n",
            "loop 1 market state : step  2074 market state fut market_state    4.0\n",
            "Name: 2019-11-13 00:00:00, dtype: float64  future value 0.9179924253099413\n",
            "loop 1 market state : step  2075 market state fut market_state    4.0\n",
            "Name: 2019-11-14 00:00:00, dtype: float64  future value 0.9165394707325039\n",
            "loop 1 market state : step  2076 market state fut market_state    2.0\n",
            "Name: 2019-11-15 00:00:00, dtype: float64  future value 0.9185328851398262\n",
            "loop 1 market state : step  2077 market state fut market_state    4.0\n",
            "Name: 2019-11-18 00:00:00, dtype: float64  future value 0.9254285792690816\n",
            "loop 1 market state : step  2078 market state fut market_state    4.0\n",
            "Name: 2019-11-19 00:00:00, dtype: float64  future value 0.9274604228670086\n",
            "loop 1 market state : step  2079 market state fut market_state    4.0\n",
            "Name: 2019-11-20 00:00:00, dtype: float64  future value 0.9313320361680785\n",
            "loop 1 market state : step  2080 market state fut market_state    4.0\n",
            "Name: 2019-11-21 00:00:00, dtype: float64  future value 0.9275962585545334\n",
            "loop 1 market state : step  2081 market state fut market_state    3.0\n",
            "Name: 2019-11-22 00:00:00, dtype: float64  future value 0.9195901561123505\n",
            "loop 1 market state : step  2082 market state fut market_state    2.0\n",
            "Name: 2019-11-25 00:00:00, dtype: float64  future value 0.9134858291929215\n",
            "loop 1 market state : step  2083 market state fut market_state    2.0\n",
            "Name: 2019-11-26 00:00:00, dtype: float64  future value 0.9192623185882808\n",
            "loop 1 market state : step  2084 market state fut market_state    2.0\n",
            "Name: 2019-11-27 00:00:00, dtype: float64  future value 0.9206414430024841\n",
            "loop 1 market state : step  2085 market state fut market_state    3.0\n",
            "Name: 2019-11-29 00:00:00, dtype: float64  future value 0.9290521692917066\n",
            "loop 1 market state : step  2086 market state fut market_state    3.0\n",
            "Name: 2019-12-02 00:00:00, dtype: float64  future value 0.9261137432656991\n",
            "loop 1 market state : step  2087 market state fut market_state    3.0\n",
            "Name: 2019-12-03 00:00:00, dtype: float64  future value 0.9250978576435155\n",
            "loop 1 market state : step  2088 market state fut market_state    3.0\n",
            "Name: 2019-12-04 00:00:00, dtype: float64  future value 0.9277881883328387\n",
            "loop 1 market state : step  2089 market state fut market_state    4.0\n",
            "Name: 2019-12-05 00:00:00, dtype: float64  future value 0.9357441813572729\n",
            "loop 1 market state : step  2090 market state fut market_state    4.0\n",
            "Name: 2019-12-06 00:00:00, dtype: float64  future value 0.9358120994963559\n",
            "loop 1 market state : step  2091 market state fut market_state    4.0\n",
            "Name: 2019-12-09 00:00:00, dtype: float64  future value 0.942501083343947\n",
            "loop 1 market state : step  2092 market state fut market_state    4.0\n",
            "Name: 2019-12-10 00:00:00, dtype: float64  future value 0.9428170968197143\n",
            "loop 1 market state : step  2093 market state fut market_state    4.0\n",
            "Name: 2019-12-11 00:00:00, dtype: float64  future value 0.9424095168129387\n",
            "loop 1 market state : step  2094 market state fut market_state    4.0\n",
            "Name: 2019-12-12 00:00:00, dtype: float64  future value 0.9466119958560536\n",
            "loop 1 market state : step  2095 market state fut market_state    4.0\n",
            "Name: 2019-12-13 00:00:00, dtype: float64  future value 0.951292785088284\n",
            "loop 1 market state : step  2096 market state fut market_state    4.0\n",
            "Name: 2019-12-16 00:00:00, dtype: float64  future value 0.9521167412274827\n",
            "loop 1 market state : step  2097 market state fut market_state    4.0\n",
            "Name: 2019-12-17 00:00:00, dtype: float64  future value 0.9519306517104096\n",
            "loop 1 market state : step  2098 market state fut market_state    4.0\n",
            "Name: 2019-12-18 00:00:00, dtype: float64  future value 0.9568123106677514\n",
            "loop 1 market state : step  2099 market state fut market_state    4.0\n",
            "Name: 2019-12-19 00:00:00, dtype: float64  future value 0.9568448278342049\n",
            "loop 1 market state : step  2100 market state fut market_state    4.0\n",
            "Name: 2019-12-20 00:00:00, dtype: float64  future value 0.951313477615794\n",
            "loop 1 market state : step  2101 market state fut market_state    4.0\n",
            "Name: 2019-12-23 00:00:00, dtype: float64  future value 0.9541160676589562\n",
            "loop 1 market state : step  2102 market state fut market_state    4.0\n",
            "Name: 2019-12-24 00:00:00, dtype: float64  future value 0.9621104181110762\n",
            "loop 1 market state : step  2103 market state fut market_state    2.0\n",
            "Name: 2019-12-26 00:00:00, dtype: float64  future value 0.9553180430935334\n",
            "loop 1 market state : step  2104 market state fut market_state    4.0\n",
            "Name: 2019-12-27 00:00:00, dtype: float64  future value 0.9586935377794742\n",
            "loop 1 market state : step  2105 market state fut market_state    3.0\n",
            "Name: 2019-12-30 00:00:00, dtype: float64  future value 0.9560060911916474\n",
            "loop 1 market state : step  2106 market state fut market_state    4.0\n",
            "Name: 2019-12-31 00:00:00, dtype: float64  future value 0.9606928645062683\n",
            "loop 1 market state : step  2107 market state fut market_state    4.0\n",
            "Name: 2020-01-02 00:00:00, dtype: float64  future value 0.9670865277009227\n",
            "loop 1 market state : step  2108 market state fut market_state    4.0\n",
            "Name: 2020-01-03 00:00:00, dtype: float64  future value 0.964325323008101\n",
            "loop 1 market state : step  2109 market state fut market_state    4.0\n",
            "Name: 2020-01-06 00:00:00, dtype: float64  future value 0.9710526639880575\n",
            "loop 1 market state : step  2110 market state fut market_state    4.0\n",
            "Name: 2020-01-07 00:00:00, dtype: float64  future value 0.9695819727475254\n",
            "loop 1 market state : step  2111 market state fut market_state    4.0\n",
            "Name: 2020-01-08 00:00:00, dtype: float64  future value 0.9713952820154859\n",
            "loop 1 market state : step  2112 market state fut market_state    4.0\n",
            "Name: 2020-01-09 00:00:00, dtype: float64  future value 0.9795225122907154\n",
            "loop 1 market state : step  2113 market state fut market_state    4.0\n",
            "Name: 2020-01-10 00:00:00, dtype: float64  future value 0.9833055869834318\n",
            "loop 1 market state : step  2114 market state fut market_state    4.0\n",
            "Name: 2020-01-13 00:00:00, dtype: float64  future value 0.9806978825829902\n",
            "loop 1 market state : step  2115 market state fut market_state    4.0\n",
            "Name: 2020-01-14 00:00:00, dtype: float64  future value 0.980981378892304\n",
            "loop 1 market state : step  2116 market state fut market_state    4.0\n",
            "Name: 2020-01-15 00:00:00, dtype: float64  future value 0.9821006556844393\n",
            "loop 1 market state : step  2117 market state fut market_state    2.0\n",
            "Name: 2020-01-16 00:00:00, dtype: float64  future value 0.9732203435688299\n",
            "loop 1 market state : step  2118 market state fut market_state    2.0\n",
            "Name: 2020-01-17 00:00:00, dtype: float64  future value 0.9579108949323767\n",
            "loop 1 market state : step  2119 market state fut market_state    2.0\n",
            "Name: 2020-01-21 00:00:00, dtype: float64  future value 0.9675413330239506\n",
            "loop 1 market state : step  2120 market state fut market_state    2.0\n",
            "Name: 2020-01-22 00:00:00, dtype: float64  future value 0.9667025963813931\n",
            "loop 1 market state : step  2121 market state fut market_state    2.0\n",
            "Name: 2020-01-23 00:00:00, dtype: float64  future value 0.9697325892337296\n",
            "loop 1 market state : step  2122 market state fut market_state    2.0\n",
            "Name: 2020-01-24 00:00:00, dtype: float64  future value 0.9525626783666236\n",
            "loop 1 market state : step  2123 market state fut market_state    3.0\n",
            "Name: 2020-01-27 00:00:00, dtype: float64  future value 0.9594731527039171\n",
            "loop 1 market state : step  2124 market state fut market_state    3.0\n",
            "Name: 2020-01-28 00:00:00, dtype: float64  future value 0.973846457905572\n",
            "loop 1 market state : step  2125 market state fut market_state    4.0\n",
            "Name: 2020-01-29 00:00:00, dtype: float64  future value 0.9848028107173856\n",
            "loop 1 market state : step  2126 market state fut market_state    4.0\n",
            "Name: 2020-01-30 00:00:00, dtype: float64  future value 0.9880779427466705\n",
            "loop 1 market state : step  2127 market state fut market_state    3.0\n",
            "Name: 2020-01-31 00:00:00, dtype: float64  future value 0.9827414784663009\n",
            "loop 1 market state : step  2128 market state fut market_state    4.0\n",
            "Name: 2020-02-03 00:00:00, dtype: float64  future value 0.9899414334906192\n",
            "loop 1 market state : step  2129 market state fut market_state    4.0\n",
            "Name: 2020-02-04 00:00:00, dtype: float64  future value 0.9916129223980232\n",
            "loop 1 market state : step  2130 market state fut market_state    4.0\n",
            "Name: 2020-02-05 00:00:00, dtype: float64  future value 0.9980213660960363\n",
            "loop 1 market state : step  2131 market state fut market_state    4.0\n",
            "Name: 2020-02-06 00:00:00, dtype: float64  future value 0.9963941463451489\n",
            "loop 1 market state : step  2132 market state fut market_state    4.0\n",
            "Name: 2020-02-07 00:00:00, dtype: float64  future value 0.9982310322421158\n",
            "loop 1 market state : step  2133 market state fut market_state    4.0\n",
            "Name: 2020-02-10 00:00:00, dtype: float64  future value 0.9953162549033542\n",
            "loop 1 market state : step  2134 market state fut market_state    4.0\n",
            "Name: 2020-02-11 00:00:00, dtype: float64  future value 1.0\n",
            "loop 1 market state : step  2135 market state fut market_state    2.0\n",
            "Name: 2020-02-12 00:00:00, dtype: float64  future value 0.9961844801990694\n",
            "loop 1 market state : step  2136 market state fut market_state    2.0\n",
            "Name: 2020-02-13 00:00:00, dtype: float64  future value 0.9857065093392903\n",
            "loop 1 market state : step  2137 market state fut market_state    1.0\n",
            "Name: 2020-02-14 00:00:00, dtype: float64  future value 0.9526719095024873\n",
            "loop 1 market state : step  2138 market state fut market_state    0.0\n",
            "Name: 2020-02-18 00:00:00, dtype: float64  future value 0.92382500820544\n",
            "loop 1 market state : step  2139 market state fut market_state    0.0\n",
            "Name: 2020-02-19 00:00:00, dtype: float64  future value 0.9203342980059245\n",
            "loop 1 market state : step  2140 market state fut market_state    0.0\n",
            "Name: 2020-02-20 00:00:00, dtype: float64  future value 0.8796893510947702\n",
            "loop 1 market state : step  2141 market state fut market_state    0.0\n",
            "Name: 2020-02-21 00:00:00, dtype: float64  future value 0.8724421707541995\n",
            "loop 1 market state : step  2142 market state fut market_state    0.0\n",
            "Name: 2020-02-24 00:00:00, dtype: float64  future value 0.9126087354179986\n",
            "loop 1 market state : step  2143 market state fut market_state    0.0\n",
            "Name: 2020-02-25 00:00:00, dtype: float64  future value 0.8869572239628511\n",
            "loop 1 market state : step  2144 market state fut market_state    2.0\n",
            "Name: 2020-02-26 00:00:00, dtype: float64  future value 0.924389116722571\n",
            "loop 1 market state : step  2145 market state fut market_state    0.0\n",
            "Name: 2020-02-27 00:00:00, dtype: float64  future value 0.8930319178173229\n",
            "loop 1 market state : step  2146 market state fut market_state    0.0\n",
            "Name: 2020-02-28 00:00:00, dtype: float64  future value 0.8778022837218151\n",
            "loop 1 market state : step  2147 market state fut market_state    0.0\n",
            "Name: 2020-03-02 00:00:00, dtype: float64  future value 0.8111159099535931\n",
            "loop 1 market state : step  2148 market state fut market_state    0.0\n",
            "Name: 2020-03-03 00:00:00, dtype: float64  future value 0.8511820396071762\n",
            "loop 1 market state : step  2149 market state fut market_state    0.0\n",
            "Name: 2020-03-04 00:00:00, dtype: float64  future value 0.8095860969949462\n",
            "loop 1 market state : step  2150 market state fut market_state    0.0\n",
            "Name: 2020-03-05 00:00:00, dtype: float64  future value 0.7325841929014517\n",
            "loop 1 market state : step  2151 market state fut market_state    0.0\n",
            "Name: 2020-03-06 00:00:00, dtype: float64  future value 0.8006202024307192\n",
            "loop 1 market state : step  2152 market state fut market_state    0.0\n",
            "Name: 2020-03-09 00:00:00, dtype: float64  future value 0.7046734350392027\n",
            "loop 1 market state : step  2153 market state fut market_state    0.0\n",
            "Name: 2020-03-10 00:00:00, dtype: float64  future value 0.7469220247769173\n",
            "loop 1 market state : step  2154 market state fut market_state    0.0\n",
            "Name: 2020-03-11 00:00:00, dtype: float64  future value 0.7082084867487948\n",
            "loop 1 market state : step  2155 market state fut market_state    0.0\n",
            "Name: 2020-03-12 00:00:00, dtype: float64  future value 0.7115425963797157\n",
            "loop 1 market state : step  2156 market state fut market_state    0.0\n",
            "Name: 2020-03-13 00:00:00, dtype: float64  future value 0.6806904563317232\n",
            "loop 1 market state : step  2157 market state fut market_state    0.0\n",
            "Name: 2020-03-16 00:00:00, dtype: float64  future value 0.6607503999390278\n",
            "loop 1 market state : step  2158 market state fut market_state    1.0\n",
            "Name: 2020-03-17 00:00:00, dtype: float64  future value 0.7227471165864529\n",
            "loop 1 market state : step  2159 market state fut market_state    2.0\n",
            "Name: 2020-03-18 00:00:00, dtype: float64  future value 0.7310840130077619\n",
            "loop 1 market state : step  2160 market state fut market_state    2.0\n",
            "Name: 2020-03-19 00:00:00, dtype: float64  future value 0.776714009750889\n",
            "loop 1 market state : step  2161 market state fut market_state    2.0\n",
            "Name: 2020-03-20 00:00:00, dtype: float64  future value 0.7505485712545988\n",
            "loop 1 market state : step  2162 market state fut market_state    2.0\n",
            "Name: 2020-03-23 00:00:00, dtype: float64  future value 0.7757039640946174\n",
            "loop 1 market state : step  2163 market state fut market_state    2.0\n",
            "Name: 2020-03-24 00:00:00, dtype: float64  future value 0.7632828323617434\n",
            "loop 1 market state : step  2164 market state fut market_state    1.0\n",
            "Name: 2020-03-25 00:00:00, dtype: float64  future value 0.729589673079984\n",
            "loop 1 market state : step  2165 market state fut market_state    1.0\n",
            "Name: 2020-03-26 00:00:00, dtype: float64  future value 0.746245728964187\n",
            "loop 1 market state : step  2166 market state fut market_state    1.0\n",
            "Name: 2020-03-27 00:00:00, dtype: float64  future value 0.7349497139893602\n",
            "loop 1 market state : step  2167 market state fut market_state    2.0\n",
            "Name: 2020-03-30 00:00:00, dtype: float64  future value 0.7866396967324809\n",
            "loop 1 market state : step  2168 market state fut market_state    2.0\n",
            "Name: 2020-03-31 00:00:00, dtype: float64  future value 0.7853786716380284\n",
            "loop 1 market state : step  2169 market state fut market_state    2.0\n",
            "Name: 2020-04-01 00:00:00, dtype: float64  future value 0.8121258832563049\n",
            "loop 1 market state : step  2170 market state fut market_state    2.0\n",
            "Name: 2020-04-02 00:00:00, dtype: float64  future value 0.8238914840575182\n",
            "loop 1 market state : step  2171 market state fut market_state    2.0\n",
            "Name: 2020-04-03 00:00:00, dtype: float64  future value 0.8155663402169133\n",
            "loop 1 market state : step  2172 market state fut market_state    2.0\n",
            "Name: 2020-04-06 00:00:00, dtype: float64  future value 0.8405003149207892\n",
            "loop 1 market state : step  2173 market state fut market_state    2.0\n",
            "Name: 2020-04-07 00:00:00, dtype: float64  future value 0.8219837241570529\n",
            "loop 1 market state : step  2174 market state fut market_state    2.0\n",
            "Name: 2020-04-08 00:00:00, dtype: float64  future value 0.8267649483994993\n",
            "loop 1 market state : step  2175 market state fut market_state    2.0\n",
            "Name: 2020-04-09 00:00:00, dtype: float64  future value 0.8489169535294837\n",
            "loop 1 market state : step  2176 market state fut market_state    2.0\n",
            "Name: 2020-04-13 00:00:00, dtype: float64  future value 0.8337374285564042\n",
            "loop 1 market state : step  2177 market state fut market_state    1.0\n",
            "Name: 2020-04-14 00:00:00, dtype: float64  future value 0.8081627034242266\n",
            "loop 1 market state : step  2178 market state fut market_state    2.0\n",
            "Name: 2020-04-15 00:00:00, dtype: float64  future value 0.8266940743960011\n",
            "loop 1 market state : step  2179 market state fut market_state    2.0\n",
            "Name: 2020-04-16 00:00:00, dtype: float64  future value 0.8262481372568602\n",
            "loop 1 market state : step  2180 market state fut market_state    2.0\n",
            "Name: 2020-04-17 00:00:00, dtype: float64  future value 0.8377479060582947\n",
            "loop 1 market state : step  2181 market state fut market_state    4.0\n",
            "Name: 2020-04-20 00:00:00, dtype: float64  future value 0.8500745871586638\n",
            "loop 1 market state : step  2182 market state fut market_state    3.0\n",
            "Name: 2020-04-21 00:00:00, dtype: float64  future value 0.845618172812953\n",
            "loop 1 market state : step  2183 market state fut market_state    4.0\n",
            "Name: 2020-04-22 00:00:00, dtype: float64  future value 0.8680980154670069\n",
            "loop 1 market state : step  2184 market state fut market_state    4.0\n",
            "Name: 2020-04-23 00:00:00, dtype: float64  future value 0.8601007091504717\n",
            "loop 1 market state : step  2185 market state fut market_state    2.0\n",
            "Name: 2020-04-24 00:00:00, dtype: float64  future value 0.8359671139567878\n",
            "loop 1 market state : step  2186 market state fut market_state    2.0\n",
            "Name: 2020-04-27 00:00:00, dtype: float64  future value 0.8395198299759146\n",
            "loop 1 market state : step  2187 market state fut market_state    3.0\n",
            "Name: 2020-04-28 00:00:00, dtype: float64  future value 0.8471095562856744\n",
            "loop 1 market state : step  2188 market state fut market_state    2.0\n",
            "Name: 2020-04-29 00:00:00, dtype: float64  future value 0.8411972312027903\n",
            "loop 1 market state : step  2189 market state fut market_state    2.0\n",
            "Name: 2020-04-30 00:00:00, dtype: float64  future value 0.8508748946106167\n",
            "loop 1 market state : step  2190 market state fut market_state    3.0\n",
            "Name: 2020-05-01 00:00:00, dtype: float64  future value 0.8652304634444975\n",
            "loop 1 market state : step  2191 market state fut market_state    3.0\n",
            "Name: 2020-05-04 00:00:00, dtype: float64  future value 0.8653456066045123\n",
            "loop 1 market state : step  2192 market state fut market_state    3.0\n",
            "Name: 2020-05-05 00:00:00, dtype: float64  future value 0.847605746959043\n",
            "loop 1 market state : step  2193 market state fut market_state    2.0\n",
            "Name: 2020-05-06 00:00:00, dtype: float64  future value 0.8328042412813418\n",
            "loop 1 market state : step  2194 market state fut market_state    2.0\n",
            "Name: 2020-05-07 00:00:00, dtype: float64  future value 0.8424021625017828\n",
            "loop 1 market state : step  2195 market state fut market_state    2.0\n",
            "Name: 2020-05-08 00:00:00, dtype: float64  future value 0.8457097393439612\n",
            "loop 1 market state : step  2196 market state fut market_state    4.0\n",
            "Name: 2020-05-11 00:00:00, dtype: float64  future value 0.8723506039278707\n",
            "loop 1 market state : step  2197 market state fut market_state    3.0\n",
            "Name: 2020-05-12 00:00:00, dtype: float64  future value 0.8632045318707217\n",
            "loop 1 market state : step  2198 market state fut market_state    4.0\n",
            "Name: 2020-05-13 00:00:00, dtype: float64  future value 0.8775778370723766\n",
            "loop 1 market state : step  2199 market state fut market_state    4.0\n",
            "Name: 2020-05-14 00:00:00, dtype: float64  future value 0.8707559013434367\n",
            "loop 1 market state : step  2200 market state fut market_state    4.0\n",
            "Name: 2020-05-15 00:00:00, dtype: float64  future value 0.8728054092508986\n",
            "loop 1 market state : step  2201 market state fut market_state    4.0\n",
            "Name: 2020-05-18 00:00:00, dtype: float64  future value 0.8835314757426825\n",
            "loop 1 market state : step  2202 market state fut market_state    4.0\n",
            "Name: 2020-05-19 00:00:00, dtype: float64  future value 0.8966318594480226\n",
            "loop 1 market state : step  2203 market state fut market_state    4.0\n",
            "Name: 2020-05-20 00:00:00, dtype: float64  future value 0.8947418359153315\n",
            "loop 1 market state : step  2204 market state fut market_state    4.0\n",
            "Name: 2020-05-21 00:00:00, dtype: float64  future value 0.8990476343654794\n",
            "loop 1 market state : step  2205 market state fut market_state    4.0\n",
            "Name: 2020-05-22 00:00:00, dtype: float64  future value 0.9024201728916843\n",
            "loop 1 market state : step  2206 market state fut market_state    4.0\n",
            "Name: 2020-05-26 00:00:00, dtype: float64  future value 0.9098297940620822\n",
            "loop 1 market state : step  2207 market state fut market_state    4.0\n",
            "Name: 2020-05-27 00:00:00, dtype: float64  future value 0.9222480419887803\n",
            "loop 1 market state : step  2208 market state fut market_state    4.0\n",
            "Name: 2020-05-28 00:00:00, dtype: float64  future value 0.9191412631087942\n",
            "loop 1 market state : step  2209 market state fut market_state    4.0\n",
            "Name: 2020-05-29 00:00:00, dtype: float64  future value 0.9432334729521374\n",
            "loop 1 market state : step  2210 market state fut market_state    4.0\n",
            "Name: 2020-06-01 00:00:00, dtype: float64  future value 0.9545914937465754\n",
            "loop 1 market state : step  2211 market state fut market_state    4.0\n",
            "Name: 2020-06-02 00:00:00, dtype: float64  future value 0.947146471603548\n",
            "loop 1 market state : step  2212 market state fut market_state    4.0\n",
            "Name: 2020-06-03 00:00:00, dtype: float64  future value 0.9421141961600021\n",
            "loop 1 market state : step  2213 market state fut market_state    2.0\n",
            "Name: 2020-06-04 00:00:00, dtype: float64  future value 0.886582161122529\n",
            "loop 1 market state : step  2214 market state fut market_state    2.0\n",
            "Name: 2020-06-05 00:00:00, dtype: float64  future value 0.8981616724066694\n",
            "loop 1 market state : step  2215 market state fut market_state    2.0\n",
            "Name: 2020-06-08 00:00:00, dtype: float64  future value 0.9056273870772068\n",
            "loop 1 market state : step  2216 market state fut market_state    2.0\n",
            "Name: 2020-06-09 00:00:00, dtype: float64  future value 0.9228002541040488\n",
            "loop 1 market state : step  2217 market state fut market_state    2.0\n",
            "Name: 2020-06-10 00:00:00, dtype: float64  future value 0.9194778967585115\n",
            "loop 1 market state : step  2218 market state fut market_state    3.0\n",
            "Name: 2020-06-11 00:00:00, dtype: float64  future value 0.9200242689078683\n",
            "loop 1 market state : step  2219 market state fut market_state    3.0\n",
            "Name: 2020-06-12 00:00:00, dtype: float64  future value 0.9148265964747594\n",
            "loop 1 market state : step  2220 market state fut market_state    3.0\n",
            "Name: 2020-06-15 00:00:00, dtype: float64  future value 0.9207684825643611\n",
            "loop 1 market state : step  2221 market state fut market_state    3.0\n",
            "Name: 2020-06-16 00:00:00, dtype: float64  future value 0.9247346188514958\n",
            "loop 1 market state : step  2222 market state fut market_state    2.0\n",
            "Name: 2020-06-17 00:00:00, dtype: float64  future value 0.9008254703072504\n",
            "loop 1 market state : step  2223 market state fut market_state    2.0\n",
            "Name: 2020-06-18 00:00:00, dtype: float64  future value 0.910698019653118\n",
            "loop 1 market state : step  2224 market state fut market_state    2.0\n",
            "Name: 2020-06-19 00:00:00, dtype: float64  future value 0.8886346251897267\n",
            "loop 1 market state : step  2225 market state fut market_state    2.0\n",
            "Name: 2020-06-22 00:00:00, dtype: float64  future value 0.9016848274190786\n",
            "loop 1 market state : step  2226 market state fut market_state    2.0\n",
            "Name: 2020-06-23 00:00:00, dtype: float64  future value 0.9155796786104597\n",
            "loop 1 market state : step  2227 market state fut market_state    3.0\n",
            "Name: 2020-06-24 00:00:00, dtype: float64  future value 0.9201778412584878\n",
            "loop 1 market state : step  2228 market state fut market_state    3.0\n",
            "Name: 2020-06-25 00:00:00, dtype: float64  future value 0.9243565998514379\n",
            "loop 1 market state : step  2229 market state fut market_state    3.0\n",
            "Name: 2020-06-26 00:00:00, dtype: float64  future value 0.9390369779914132\n",
            "loop 1 market state : step  2230 market state fut market_state    3.0\n",
            "Name: 2020-06-29 00:00:00, dtype: float64  future value 0.9288779761764959\n",
            "loop 1 market state : step  2231 market state fut market_state    3.0\n",
            "Name: 2020-06-30 00:00:00, dtype: float64  future value 0.9361487331460733\n",
            "loop 1 market state : step  2232 market state fut market_state    3.0\n",
            "Name: 2020-07-01 00:00:00, dtype: float64  future value 0.9308654785596671\n",
            "loop 1 market state : step  2233 market state fut market_state    3.0\n",
            "Name: 2020-07-02 00:00:00, dtype: float64  future value 0.9406081039468406\n",
            "loop 1 market state : step  2234 market state fut market_state    2.0\n",
            "Name: 2020-07-06 00:00:00, dtype: float64  future value 0.9318016219944654\n",
            "loop 1 market state : step  2235 market state fut market_state    3.0\n",
            "Name: 2020-07-07 00:00:00, dtype: float64  future value 0.9442937000843975\n",
            "loop 1 market state : step  2236 market state fut market_state    3.0\n",
            "Name: 2020-07-08 00:00:00, dtype: float64  future value 0.9528698233631832\n",
            "loop 1 market state : step  2237 market state fut market_state    3.0\n",
            "Name: 2020-07-09 00:00:00, dtype: float64  future value 0.9496242520452953\n",
            "loop 1 market state : step  2238 market state fut market_state    3.0\n",
            "Name: 2020-07-10 00:00:00, dtype: float64  future value 0.9523293632379776\n",
            "loop 1 market state : step  2239 market state fut market_state    4.0\n",
            "Name: 2020-07-13 00:00:00, dtype: float64  future value 0.9603355380337205\n",
            "loop 1 market state : step  2240 market state fut market_state    4.0\n",
            "Name: 2020-07-14 00:00:00, dtype: float64  future value 0.9619479772812491\n",
            "loop 1 market state : step  2241 market state fut market_state    4.0\n",
            "Name: 2020-07-15 00:00:00, dtype: float64  future value 0.9674763713399241\n",
            "loop 1 market state : step  2242 market state fut market_state    4.0\n",
            "Name: 2020-07-16 00:00:00, dtype: float64  future value 0.9555571978927706\n",
            "loop 1 market state : step  2243 market state fut market_state    2.0\n",
            "Name: 2020-07-17 00:00:00, dtype: float64  future value 0.9496419166501505\n",
            "loop 1 market state : step  2244 market state fut market_state    2.0\n",
            "Name: 2020-07-20 00:00:00, dtype: float64  future value 0.9566646503412831\n",
            "loop 1 market state : step  2245 market state fut market_state    2.0\n",
            "Name: 2020-07-21 00:00:00, dtype: float64  future value 0.9504717848135006\n",
            "loop 1 market state : step  2246 market state fut market_state    2.0\n",
            "Name: 2020-07-22 00:00:00, dtype: float64  future value 0.9622846109309664\n",
            "loop 1 market state : step  2247 market state fut market_state    3.0\n",
            "Name: 2020-07-23 00:00:00, dtype: float64  future value 0.9586758014117002\n",
            "loop 1 market state : step  2248 market state fut market_state   -1.0\n",
            "Name: 2020-07-24 00:00:00, dtype: float64  future value nan\n",
            "loop 1 market state : step  2249 market state fut market_state   -1.0\n",
            "Name: 2020-07-27 00:00:00, dtype: float64  future value nan\n",
            "loop 1 market state : step  2250 market state fut market_state   -1.0\n",
            "Name: 2020-07-28 00:00:00, dtype: float64  future value nan\n",
            "loop 1 market state : step  2251 market state fut market_state   -1.0\n",
            "Name: 2020-07-29 00:00:00, dtype: float64  future value nan\n",
            "loop 1 market state : step  2252 market state fut market_state   -1.0\n",
            "Name: 2020-07-30 00:00:00, dtype: float64  future value nan\n",
            "loop 2 image : step  0\n",
            "loop 2 image : step  1\n",
            "loop 2 image : step  2\n",
            "loop 2 image : step  3\n",
            "loop 2 image : step  4\n",
            "loop 2 image : step  5\n",
            "loop 2 image : step  6\n",
            "loop 2 image : step  7\n",
            "loop 2 image : step  8\n",
            "loop 2 image : step  9\n",
            "loop 2 image : step  10\n",
            "loop 2 image : step  11\n",
            "loop 2 image : step  12\n",
            "loop 2 image : step  13\n",
            "loop 2 image : step  14\n",
            "loop 2 image : step  15\n",
            "loop 2 image : step  16\n",
            "loop 2 image : step  17\n",
            "loop 2 image : step  18\n",
            "loop 2 image : step  19\n",
            "loop 2 image : step  20\n",
            "loop 2 image : step  21\n",
            "loop 2 image : step  22\n",
            "loop 2 image : step  23\n",
            "loop 2 image : step  24\n",
            "loop 2 image : step  25\n",
            "loop 2 image : step  26\n",
            "loop 2 image : step  27\n",
            "loop 2 image : step  28\n",
            "loop 2 image : step  29\n",
            "loop 2 image : step  30\n",
            "loop 2 image : step  31\n",
            "loop 2 image : step  32\n",
            "loop 2 image : step  33\n",
            "loop 2 image : step  34\n",
            "loop 2 image : step  35\n",
            "loop 2 image : step  36\n",
            "loop 2 image : step  37\n",
            "loop 2 image : step  38\n",
            "loop 2 image : step  39\n",
            "loop 2 image : step  40\n",
            "loop 2 image : step  41\n",
            "loop 2 image : step  42\n",
            "loop 2 image : step  43\n",
            "loop 2 image : step  44\n",
            "loop 2 image : step  45\n",
            "loop 2 image : step  46\n",
            "loop 2 image : step  47\n",
            "loop 2 image : step  48\n",
            "loop 2 image : step  49\n",
            "loop 2 image : step  50\n",
            "loop 2 image : step  51\n",
            "loop 2 image : step  52\n",
            "loop 2 image : step  53\n",
            "loop 2 image : step  54\n",
            "loop 2 image : step  55\n",
            "loop 2 image : step  56\n",
            "loop 2 image : step  57\n",
            "loop 2 image : step  58\n",
            "loop 2 image : step  59\n",
            "loop 2 image : step  60\n",
            "loop 2 image : step  61\n",
            "loop 2 image : step  62\n",
            "loop 2 image : step  63\n",
            "loop 2 image : step  64\n",
            "loop 2 image : step  65\n",
            "loop 2 image : step  66\n",
            "loop 2 image : step  67\n",
            "loop 2 image : step  68\n",
            "loop 2 image : step  69\n",
            "loop 2 image : step  70\n",
            "loop 2 image : step  71\n",
            "loop 2 image : step  72\n",
            "loop 2 image : step  73\n",
            "loop 2 image : step  74\n",
            "loop 2 image : step  75\n",
            "loop 2 image : step  76\n",
            "loop 2 image : step  77\n",
            "loop 2 image : step  78\n",
            "loop 2 image : step  79\n",
            "loop 2 image : step  80\n",
            "loop 2 image : step  81\n",
            "loop 2 image : step  82\n",
            "loop 2 image : step  83\n",
            "loop 2 image : step  84\n",
            "loop 2 image : step  85\n",
            "loop 2 image : step  86\n",
            "loop 2 image : step  87\n",
            "loop 2 image : step  88\n",
            "loop 2 image : step  89\n",
            "loop 2 image : step  90\n",
            "loop 2 image : step  91\n",
            "loop 2 image : step  92\n",
            "loop 2 image : step  93\n",
            "loop 2 image : step  94\n",
            "loop 2 image : step  95\n",
            "loop 2 image : step  96\n",
            "loop 2 image : step  97\n",
            "loop 2 image : step  98\n",
            "loop 2 image : step  99\n",
            "loop 2 image : step  100\n",
            "loop 2 image : step  101\n",
            "loop 2 image : step  102\n",
            "loop 2 image : step  103\n",
            "loop 2 image : step  104\n",
            "loop 2 image : step  105\n",
            "loop 2 image : step  106\n",
            "loop 2 image : step  107\n",
            "loop 2 image : step  108\n",
            "loop 2 image : step  109\n",
            "loop 2 image : step  110\n",
            "loop 2 image : step  111\n",
            "loop 2 image : step  112\n",
            "loop 2 image : step  113\n",
            "loop 2 image : step  114\n",
            "loop 2 image : step  115\n",
            "loop 2 image : step  116\n",
            "loop 2 image : step  117\n",
            "loop 2 image : step  118\n",
            "loop 2 image : step  119\n",
            "loop 2 image : step  120\n",
            "loop 2 image : step  121\n",
            "loop 2 image : step  122\n",
            "loop 2 image : step  123\n",
            "loop 2 image : step  124\n",
            "loop 2 image : step  125\n",
            "loop 2 image : step  126\n",
            "loop 2 image : step  127\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-f9ac457b3d49>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m#Need to be optimzed with more time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mtestsp500\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msp500\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Close'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m21002\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;34m(\u001b[0m\u001b[0mX_train_image\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train_StateClass_image\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train_FutPredict_image\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX_test_image\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_test_StateClass_image\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_test_FutPredict_image\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msetup_input_NN_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestsp500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m#copy the datafrae dataset in csv format to be used after\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-814e020d532f>\u001b[0m in \u001b[0;36msetup_input_NN_image\u001b[0;34m(xdf, past_step, fut_step, split)\u001b[0m\n\u001b[1;32m    270\u001b[0m   \u001b[0mwe\u001b[0m \u001b[0mrandomize\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdates\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mretun\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0mset\u001b[0m \u001b[0mof\u001b[0m \u001b[0mdataframes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m   '''\n\u001b[0;32m--> 272\u001b[0;31m   \u001b[0mxdf_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbuild_image_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxdf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpast_step\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfut_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m   \u001b[0mtmp_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mxdf_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'market_state'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mxdf_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'future_value'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mxdf_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'df_x_image'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-814e020d532f>\u001b[0m in \u001b[0;36mbuild_image_df\u001b[0;34m(xdf, past_step, fut_step)\u001b[0m\n\u001b[1;32m    208\u001b[0m             \u001b[0mquick_build_image_from_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi_split\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_end\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp_x_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m   \u001b[0mquick_build_image_from_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_stockvaluecorrected\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp_x_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m   \u001b[0mdf_x_image\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp_x_image\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolname_d_x_image_flattened\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf_stockvaluecorrected\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-814e020d532f>\u001b[0m in \u001b[0;36mquick_build_image_from_index\u001b[0;34m(indexstart, index_end, np_x_image)\u001b[0m\n\u001b[1;32m    205\u001b[0m         \u001b[0;32melse\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m             \u001b[0mi_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindexstart\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex_end\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mindexstart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 207\u001b[0;31m             \u001b[0mquick_build_image_from_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi_split\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp_x_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    208\u001b[0m             \u001b[0mquick_build_image_from_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi_split\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_end\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp_x_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-814e020d532f>\u001b[0m in \u001b[0;36mquick_build_image_from_index\u001b[0;34m(indexstart, index_end, np_x_image)\u001b[0m\n\u001b[1;32m    205\u001b[0m         \u001b[0;32melse\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m             \u001b[0mi_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindexstart\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex_end\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mindexstart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 207\u001b[0;31m             \u001b[0mquick_build_image_from_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi_split\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp_x_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    208\u001b[0m             \u001b[0mquick_build_image_from_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi_split\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_end\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp_x_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-814e020d532f>\u001b[0m in \u001b[0;36mquick_build_image_from_index\u001b[0;34m(indexstart, index_end, np_x_image)\u001b[0m\n\u001b[1;32m    205\u001b[0m         \u001b[0;32melse\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m             \u001b[0mi_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindexstart\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex_end\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mindexstart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 207\u001b[0;31m             \u001b[0mquick_build_image_from_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi_split\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp_x_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    208\u001b[0m             \u001b[0mquick_build_image_from_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi_split\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_end\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp_x_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-814e020d532f>\u001b[0m in \u001b[0;36mquick_build_image_from_index\u001b[0;34m(indexstart, index_end, np_x_image)\u001b[0m\n\u001b[1;32m    205\u001b[0m         \u001b[0;32melse\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m             \u001b[0mi_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindexstart\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex_end\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mindexstart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 207\u001b[0;31m             \u001b[0mquick_build_image_from_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi_split\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp_x_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    208\u001b[0m             \u001b[0mquick_build_image_from_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi_split\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_end\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp_x_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-814e020d532f>\u001b[0m in \u001b[0;36mquick_build_image_from_index\u001b[0;34m(indexstart, index_end, np_x_image)\u001b[0m\n\u001b[1;32m    206\u001b[0m             \u001b[0mi_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindexstart\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex_end\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mindexstart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m             \u001b[0mquick_build_image_from_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi_split\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp_x_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m             \u001b[0mquick_build_image_from_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi_split\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_end\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp_x_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m   \u001b[0mquick_build_image_from_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_stockvaluecorrected\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp_x_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-814e020d532f>\u001b[0m in \u001b[0;36mquick_build_image_from_index\u001b[0;34m(indexstart, index_end, np_x_image)\u001b[0m\n\u001b[1;32m    206\u001b[0m             \u001b[0mi_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindexstart\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex_end\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mindexstart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m             \u001b[0mquick_build_image_from_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi_split\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp_x_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m             \u001b[0mquick_build_image_from_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi_split\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_end\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp_x_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m   \u001b[0mquick_build_image_from_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_stockvaluecorrected\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp_x_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-814e020d532f>\u001b[0m in \u001b[0;36mquick_build_image_from_index\u001b[0;34m(indexstart, index_end, np_x_image)\u001b[0m\n\u001b[1;32m    206\u001b[0m             \u001b[0mi_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindexstart\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex_end\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mindexstart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m             \u001b[0mquick_build_image_from_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi_split\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp_x_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m             \u001b[0mquick_build_image_from_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi_split\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_end\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp_x_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m   \u001b[0mquick_build_image_from_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_stockvaluecorrected\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp_x_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-814e020d532f>\u001b[0m in \u001b[0;36mquick_build_image_from_index\u001b[0;34m(indexstart, index_end, np_x_image)\u001b[0m\n\u001b[1;32m    205\u001b[0m         \u001b[0;32melse\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m             \u001b[0mi_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindexstart\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex_end\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mindexstart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 207\u001b[0;31m             \u001b[0mquick_build_image_from_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi_split\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp_x_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    208\u001b[0m             \u001b[0mquick_build_image_from_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi_split\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_end\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp_x_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-814e020d532f>\u001b[0m in \u001b[0;36mquick_build_image_from_index\u001b[0;34m(indexstart, index_end, np_x_image)\u001b[0m\n\u001b[1;32m    205\u001b[0m         \u001b[0;32melse\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m             \u001b[0mi_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindexstart\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex_end\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mindexstart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 207\u001b[0;31m             \u001b[0mquick_build_image_from_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi_split\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp_x_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    208\u001b[0m             \u001b[0mquick_build_image_from_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi_split\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_end\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp_x_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-814e020d532f>\u001b[0m in \u001b[0;36mquick_build_image_from_index\u001b[0;34m(indexstart, index_end, np_x_image)\u001b[0m\n\u001b[1;32m    206\u001b[0m             \u001b[0mi_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindexstart\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex_end\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mindexstart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m             \u001b[0mquick_build_image_from_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi_split\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp_x_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m             \u001b[0mquick_build_image_from_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi_split\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_end\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp_x_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m   \u001b[0mquick_build_image_from_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_stockvaluecorrected\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp_x_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-814e020d532f>\u001b[0m in \u001b[0;36mquick_build_image_from_index\u001b[0;34m(indexstart, index_end, np_x_image)\u001b[0m\n\u001b[1;32m    206\u001b[0m             \u001b[0mi_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindexstart\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex_end\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mindexstart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m             \u001b[0mquick_build_image_from_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi_split\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp_x_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m             \u001b[0mquick_build_image_from_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi_split\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_end\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp_x_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m   \u001b[0mquick_build_image_from_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_stockvaluecorrected\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp_x_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-814e020d532f>\u001b[0m in \u001b[0;36mquick_build_image_from_index\u001b[0;34m(indexstart, index_end, np_x_image)\u001b[0m\n\u001b[1;32m    200\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mquick_build_image_from_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_end\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp_x_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mindexstart\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mindex_end\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 202\u001b[0;31m             \u001b[0mtmpimage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbuild_image_optimfig_simplified\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexstart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    203\u001b[0m             \u001b[0mnp_x_image\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindexstart\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmpimage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"loop 2 image :\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"step \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mindexstart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-814e020d532f>\u001b[0m in \u001b[0;36mbuild_image_optimfig_simplified\u001b[0;34m(i_index)\u001b[0m\n\u001b[1;32m    196\u001b[0m   '''           \n\u001b[1;32m    197\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mbuild_image_optimfig_simplified\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 198\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mbuild_image_optimfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_stockvaluecorrected\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi_index\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpastlag\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpast_step\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfutlag\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfut_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mquick_build_image_from_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_end\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp_x_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-814e020d532f>\u001b[0m in \u001b[0;36mbuild_image_optimfig\u001b[0;34m(fig, stockindex, idate, pastlag, futlag)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m   \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msp500close\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mpastlag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m   \u001b[0mplot_img_np\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_img_from_fig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m   \u001b[0mx_tmp\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mskimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmeasure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblock_reduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mplot_img_np\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m90\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m620\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m140\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m970\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m18\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m   \u001b[0;34m(\u001b[0m\u001b[0mx_datas\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx_tmp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-814e020d532f>\u001b[0m in \u001b[0;36mget_img_from_fig\u001b[0;34m(fig, dpi)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# get_img_from_fig is function which returns an image as numpy array from figure\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mbuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msavefig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"png\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdpi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdpi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m     \u001b[0mbuf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseek\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mimg_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrombuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/figure.py\u001b[0m in \u001b[0;36msavefig\u001b[0;34m(self, fname, transparent, **kwargs)\u001b[0m\n\u001b[1;32m   2201\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_visible\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframeon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2203\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_figure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2205\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mframeon\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/backend_bases.py\u001b[0m in \u001b[0;36mprint_figure\u001b[0;34m(self, filename, dpi, facecolor, edgecolor, orientation, format, bbox_inches, **kwargs)\u001b[0m\n\u001b[1;32m   2124\u001b[0m                     \u001b[0morientation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morientation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2125\u001b[0m                     \u001b[0mbbox_inches_restore\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_bbox_inches_restore\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2126\u001b[0;31m                     **kwargs)\n\u001b[0m\u001b[1;32m   2127\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2128\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mbbox_inches\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mrestore_bbox\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py\u001b[0m in \u001b[0;36mprint_png\u001b[0;34m(self, filename_or_obj, metadata, pil_kwargs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    512\u001b[0m         }\n\u001b[1;32m    513\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 514\u001b[0;31m         \u001b[0mFigureCanvasAgg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    515\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpil_kwargs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m             \u001b[0;32mfrom\u001b[0m \u001b[0mPIL\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py\u001b[0m in \u001b[0;36mdraw\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    386\u001b[0m         \u001b[0mDraw\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mfigure\u001b[0m \u001b[0musing\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m         \"\"\"\n\u001b[0;32m--> 388\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrenderer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_renderer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcleared\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    389\u001b[0m         \u001b[0;31m# Acquire a lock on the shared font cache.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mRendererAgg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlock\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py\u001b[0m in \u001b[0;36mget_renderer\u001b[0;34m(self, cleared)\u001b[0m\n\u001b[1;32m    405\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lastKey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mcleared\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 407\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    408\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py\u001b[0m in \u001b[0;36mclear\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 270\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_renderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    271\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0moption_image_nocomposite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V_QcOSjqMHSy",
        "colab_type": "text"
      },
      "source": [
        "##Step 2: Loading training datas with vgg16 transfert model and training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R98_pBo02L2-",
        "colab_type": "text"
      },
      "source": [
        "####List of model utilisation of main example resnet \n",
        "https://towardsdatascience.com/understanding-and-coding-a-resnet-in-keras-446d7ff84d33"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fI7gHLdU15Bb",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "![1_NdCntZms6S2pBmQ3j_wyuw.png](data:image/png;base64,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)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1R3sN7G1IWIK",
        "colab_type": "text"
      },
      "source": [
        "####Loading and Cleaning Training Datas "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MTfVYoKBaQzZ",
        "colab_type": "text"
      },
      "source": [
        "This part is for loading the training dataset as it is better to generate it once for all in step 1 because of it time consuming process.\n",
        "\n",
        "This part also configure back the X_train datas from dataframe based on columns to a (32,32,3) np. array for the input of the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zqU-LK4JTEOM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "f4e7a4e8-e4fb-4dfd-f387-93a58ee1b1f2"
      },
      "source": [
        "%pwd"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/drive/My Drive/A_transfertTFMresnetSP500'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jp0_855PpN3S",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "2ddcc167-0b3e-42db-b7a2-d34496a2fc31"
      },
      "source": [
        "%cd /content/DL_Tools_For_Finance\n",
        "\n",
        "import numpy as np\n",
        "import pylab as plt\n",
        "import pandas as pd\n",
        "\n",
        "from keras.datasets import cifar10\n",
        "from keras.utils import np_utils\n",
        "from keras.models import Sequential\n",
        "from keras import optimizers\n",
        "from keras.layers import Dense, Flatten, Conv2D, MaxPooling2D\n",
        "from keras.layers import Dropout, Flatten, GlobalAveragePooling2D\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "'''\n",
        "UTILITY FUNCTIONS\n",
        "'''\n",
        "\n",
        "def change_X_df__nparray_image(df_X_train_image_flattened ):\n",
        "  '''\n",
        "  setup_input_NN_image returns a dataframe of flaten image for x train and xtest\n",
        "  then this function will change each date into a nparray list of images with 32, 32, 3 size \n",
        "  '''\n",
        "  X_train_image=df_X_train_image_flattened\n",
        "  nb_train=len(X_train_image.index)\n",
        "  \n",
        "  x_train=np.zeros((nb_train,32,32,3))\n",
        "  for i in range(nb_train):\n",
        "    tmp=np.array(X_train_image.iloc[i])\n",
        "    tmp=tmp.reshape(32,32,3)\n",
        "    x_train[i]=tmp\n",
        "  return x_train\n",
        "\n",
        "'''\n",
        "MAIN EXECUTIONS\n",
        "'''\n",
        "#recuperation of datas \n",
        "X_train_image=pd.read_csv('datas/X_train_image.csv')\n",
        "Y_train_StateClass_image=pd.read_csv('datas/Y_train_StateClass_image.csv')\n",
        "Y_train_FutPredict_image=pd.read_csv('datas/Y_train_FutPredict_image.csv')\n",
        "\n",
        "\n",
        "#setting up the index to Date\n",
        "X_train_image=X_train_image.set_index(\"Date\")\n",
        "Y_train_StateClass_image=Y_train_StateClass_image.set_index(\"Date\")\n",
        "Y_train_FutPredict_image=Y_train_FutPredict_image.set_index(\"Date\")\n",
        "\n",
        "#modify dataset to np array for input to NN\n",
        "x_train=change_X_df__nparray_image(X_train_image)\n",
        "y_train_state=np.array(Y_train_StateClass_image)\n",
        "y_train_value=np.array(Y_train_FutPredict_image)\n",
        "\n",
        "##Setting up xtrain and ytrain\n",
        "#Here we focus on predicting the future state Y_train_StateClass_image\n",
        "nb_train=len(X_train_image.index)\n",
        "x_train=np.zeros((nb_train,32,32,3))\n",
        "for i in range(nb_train):\n",
        "  tmp=np.array(X_train_image.iloc[i])\n",
        "  tmp=tmp.reshape(32,32,3)\n",
        "  x_train[i]=tmp\n",
        "  \n",
        "y_train=np.array(Y_train_StateClass_image)\n",
        "#y_train=np.array(Y_train_FutPredict_image)\n",
        "\n",
        "nb_train=len(X_train_image.index)\n",
        "x_train=np.zeros((nb_train,32,32,3))\n",
        "for i in range(nb_train):\n",
        "  tmp=np.array(X_train_image.iloc[i])\n",
        "  tmp=tmp.reshape(32,32,3)\n",
        "  x_train[i]=tmp\n",
        "\n",
        "y_train=np.array(Y_train_StateClass_image)\n",
        "#y_train=np.array(Y_train_FutPredict_image)"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/DL_Tools_For_Finance\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OiGYdJFX-aEC",
        "colab_type": "text"
      },
      "source": [
        "Now we check the datas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fwiNBaTN-Ufq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "4cd5a6af-6f2b-422a-e535-0cad1bddf1ca"
      },
      "source": [
        "print(\"number of train datas\",len(y_train))\n",
        "print(\"number of dates actual\",len(y_train))\n",
        "print(\"shape of y and x train\", y_train.shape, \" and \", x_train.shape)\n",
        "print(\"min value of datas\", np.min(y_train), \", max value of datas\",np.max(y_train))"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "number of train datas 18580\n",
            "number of dates actual 18580\n",
            "shape of y and x train (18580, 1)  and  (18580, 32, 32, 3)\n",
            "min value of datas -1.0 , max value of datas 4.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ewhz7vXwxzf4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        },
        "outputId": "f8cd89cc-224e-4acc-970c-43bb39bdaa7c"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig2 = plt.figure(figsize=(10, 6))\n",
        "x_datas=x_train[np.random.randint(low=10,high=1000,size=8)]\n",
        "for i in range(0,8,1):\n",
        "    img = x_datas[i][:-1][:-1]\n",
        "    fig2.add_subplot(2, 4, i+1)\n",
        "    plt.imshow(img)\n",
        "\n",
        "\n",
        "print('Shape of each image in the training data: ', x_datas.shape[:])"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of each image in the training data:  (8, 32, 32, 3)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x432 with 8 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F6Z8846EWC3F",
        "colab_type": "text"
      },
      "source": [
        "####Build up and training of the NN model "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tbRtrFYtaDI5",
        "colab_type": "text"
      },
      "source": [
        "In this part we suppose that we have the training dataset taken from step 2. We use a Transfert model for vgg16 and some other layers. we use for this example a categorical_crossentropy loss and rmsprop optimizer. This part can be fined tuned for each financial index or stock index (layers, optimmizer, metrics, dropout) but in this case we introduced a simplier case. We train and save the model, please refer to XX to see the convergence of the model.\n",
        "\n",
        "We have 14.7M parameters and 66k trainable parametres. the size of training input is 571M only for the image not including rolling volatility, moving average etc\n",
        "\n",
        "NB: If you have an error on size of input of the ytrain you have to reload the input executing  step 2 once again"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gTHxq8RVg-vE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pylab as plt\n",
        "import pandas as pd\n",
        "\n",
        "from keras.utils import np_utils\n",
        "from keras.models import Sequential, Model\n",
        "from keras import optimizers\n",
        "from keras.layers import Dense, Flatten, Conv2D, MaxPooling2D\n",
        "from keras.layers import Dropout, Flatten, GlobalAveragePooling2D\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras import applications\n",
        "\n",
        "from datetime import datetime\n",
        "from packaging import version\n",
        "import tensorboard\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V-aVAzmKU9t8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "99551224-3b1a-4400-b57d-a59a533cc0fd"
      },
      "source": [
        "x_train.shape"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(18580, 32, 32, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D-2BAtxzmPpe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "c7a15e80-ae42-4855-ba7e-06618763406a"
      },
      "source": [
        "'''\n",
        "PARAMETERS to change so as to improve the training\n",
        "'''\n",
        "\n",
        "#We can modify batch size and epochs to adjust improve the training\n",
        "batch_size=64\n",
        "epochs=5\n",
        "sp500_learning_rate=0.001\n",
        "\n",
        "#Use of exponential decay for learning rate\n",
        "#https://keras.io/api/optimizers/learning_rate_schedules/exponential_decay/\n",
        "\n",
        "lr_schedule = sp500_learning_rate  #or lr_scheduleExp\n",
        "\n",
        "sp500_decay_rate=0.90\n",
        "\n",
        "\n",
        "lr_scheduleExp = tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "    sp500_learning_rate,\n",
        "    decay_steps=100,\n",
        "    decay_rate=vggsp500_decay_rate,\n",
        "    staircase=True)\n",
        "\n",
        "##https://keras.io/api/losses/\n",
        "sp500loss='categorical_crossentropy'                                 \n",
        "\n",
        "##https://keras.io/api/optimizers/\n",
        "sp500optimizer_name='Adam'\n",
        "sp500optimizer=keras.optimizers.Adam(learning_rate=lr_schedule)  \n",
        "\n",
        "##https://keras.io/api/metrics/\n",
        "sp500metrics=['accuracy']                                           \n",
        "\n",
        "\n",
        "\n",
        "'''\n",
        "model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=lr_schedule),\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(data, labels, epochs=5)\n",
        "'''"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\nmodel.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=lr_schedule),\\n              loss='sparse_categorical_crossentropy',\\n              metrics=['accuracy'])\\n\\nmodel.fit(data, labels, epochs=5)\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7cRVj5gY5R3E",
        "colab_type": "text"
      },
      "source": [
        "###Version with resnet\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vw_E83udAoLa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "87dda025-7131-4d02-af71-a715eb0054a3"
      },
      "source": [
        "transfer_model.summary()"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"functional_3\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_20 (InputLayer)           [(None, 32, 32, 3)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv1_pad (ZeroPadding2D)       (None, 38, 38, 3)    0           input_20[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv1_conv (Conv2D)             (None, 16, 16, 64)   9472        conv1_pad[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1_bn (BatchNormalization)   (None, 16, 16, 64)   256         conv1_conv[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv1_relu (Activation)         (None, 16, 16, 64)   0           conv1_bn[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "pool1_pad (ZeroPadding2D)       (None, 18, 18, 64)   0           conv1_relu[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "pool1_pool (MaxPooling2D)       (None, 8, 8, 64)     0           pool1_pad[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_1_conv (Conv2D)    (None, 8, 8, 64)     4160        pool1_pool[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_1_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_1_relu (Activation (None, 8, 8, 64)     0           conv2_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_2_conv (Conv2D)    (None, 8, 8, 64)     36928       conv2_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_2_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_2_relu (Activation (None, 8, 8, 64)     0           conv2_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_0_conv (Conv2D)    (None, 8, 8, 256)    16640       pool1_pool[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_3_conv (Conv2D)    (None, 8, 8, 256)    16640       conv2_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_0_bn (BatchNormali (None, 8, 8, 256)    1024        conv2_block1_0_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_3_bn (BatchNormali (None, 8, 8, 256)    1024        conv2_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_add (Add)          (None, 8, 8, 256)    0           conv2_block1_0_bn[0][0]          \n",
            "                                                                 conv2_block1_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_out (Activation)   (None, 8, 8, 256)    0           conv2_block1_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_1_conv (Conv2D)    (None, 8, 8, 64)     16448       conv2_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_1_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_1_relu (Activation (None, 8, 8, 64)     0           conv2_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_2_conv (Conv2D)    (None, 8, 8, 64)     36928       conv2_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_2_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_2_relu (Activation (None, 8, 8, 64)     0           conv2_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_3_conv (Conv2D)    (None, 8, 8, 256)    16640       conv2_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_3_bn (BatchNormali (None, 8, 8, 256)    1024        conv2_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_add (Add)          (None, 8, 8, 256)    0           conv2_block1_out[0][0]           \n",
            "                                                                 conv2_block2_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_out (Activation)   (None, 8, 8, 256)    0           conv2_block2_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_1_conv (Conv2D)    (None, 8, 8, 64)     16448       conv2_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_1_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_1_relu (Activation (None, 8, 8, 64)     0           conv2_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_2_conv (Conv2D)    (None, 8, 8, 64)     36928       conv2_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_2_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_2_relu (Activation (None, 8, 8, 64)     0           conv2_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_3_conv (Conv2D)    (None, 8, 8, 256)    16640       conv2_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_3_bn (BatchNormali (None, 8, 8, 256)    1024        conv2_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_add (Add)          (None, 8, 8, 256)    0           conv2_block2_out[0][0]           \n",
            "                                                                 conv2_block3_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_out (Activation)   (None, 8, 8, 256)    0           conv2_block3_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_1_conv (Conv2D)    (None, 4, 4, 128)    32896       conv2_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_1_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_1_relu (Activation (None, 4, 4, 128)    0           conv3_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_2_conv (Conv2D)    (None, 4, 4, 128)    147584      conv3_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_2_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_2_relu (Activation (None, 4, 4, 128)    0           conv3_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_0_conv (Conv2D)    (None, 4, 4, 512)    131584      conv2_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_3_conv (Conv2D)    (None, 4, 4, 512)    66048       conv3_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_0_bn (BatchNormali (None, 4, 4, 512)    2048        conv3_block1_0_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_3_bn (BatchNormali (None, 4, 4, 512)    2048        conv3_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_add (Add)          (None, 4, 4, 512)    0           conv3_block1_0_bn[0][0]          \n",
            "                                                                 conv3_block1_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_out (Activation)   (None, 4, 4, 512)    0           conv3_block1_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_1_conv (Conv2D)    (None, 4, 4, 128)    65664       conv3_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_1_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_1_relu (Activation (None, 4, 4, 128)    0           conv3_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_2_conv (Conv2D)    (None, 4, 4, 128)    147584      conv3_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_2_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_2_relu (Activation (None, 4, 4, 128)    0           conv3_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_3_conv (Conv2D)    (None, 4, 4, 512)    66048       conv3_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_3_bn (BatchNormali (None, 4, 4, 512)    2048        conv3_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_add (Add)          (None, 4, 4, 512)    0           conv3_block1_out[0][0]           \n",
            "                                                                 conv3_block2_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_out (Activation)   (None, 4, 4, 512)    0           conv3_block2_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_1_conv (Conv2D)    (None, 4, 4, 128)    65664       conv3_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_1_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_1_relu (Activation (None, 4, 4, 128)    0           conv3_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_2_conv (Conv2D)    (None, 4, 4, 128)    147584      conv3_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_2_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_2_relu (Activation (None, 4, 4, 128)    0           conv3_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_3_conv (Conv2D)    (None, 4, 4, 512)    66048       conv3_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_3_bn (BatchNormali (None, 4, 4, 512)    2048        conv3_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_add (Add)          (None, 4, 4, 512)    0           conv3_block2_out[0][0]           \n",
            "                                                                 conv3_block3_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_out (Activation)   (None, 4, 4, 512)    0           conv3_block3_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_1_conv (Conv2D)    (None, 4, 4, 128)    65664       conv3_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_1_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block4_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_1_relu (Activation (None, 4, 4, 128)    0           conv3_block4_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_2_conv (Conv2D)    (None, 4, 4, 128)    147584      conv3_block4_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_2_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block4_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_2_relu (Activation (None, 4, 4, 128)    0           conv3_block4_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_3_conv (Conv2D)    (None, 4, 4, 512)    66048       conv3_block4_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_3_bn (BatchNormali (None, 4, 4, 512)    2048        conv3_block4_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_add (Add)          (None, 4, 4, 512)    0           conv3_block3_out[0][0]           \n",
            "                                                                 conv3_block4_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_out (Activation)   (None, 4, 4, 512)    0           conv3_block4_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_1_conv (Conv2D)    (None, 2, 2, 256)    131328      conv3_block4_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_1_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_1_relu (Activation (None, 2, 2, 256)    0           conv4_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_2_conv (Conv2D)    (None, 2, 2, 256)    590080      conv4_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_2_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_2_relu (Activation (None, 2, 2, 256)    0           conv4_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_0_conv (Conv2D)    (None, 2, 2, 1024)   525312      conv3_block4_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_3_conv (Conv2D)    (None, 2, 2, 1024)   263168      conv4_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_0_bn (BatchNormali (None, 2, 2, 1024)   4096        conv4_block1_0_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_3_bn (BatchNormali (None, 2, 2, 1024)   4096        conv4_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_add (Add)          (None, 2, 2, 1024)   0           conv4_block1_0_bn[0][0]          \n",
            "                                                                 conv4_block1_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_out (Activation)   (None, 2, 2, 1024)   0           conv4_block1_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_1_conv (Conv2D)    (None, 2, 2, 256)    262400      conv4_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_1_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_1_relu (Activation (None, 2, 2, 256)    0           conv4_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_2_conv (Conv2D)    (None, 2, 2, 256)    590080      conv4_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_2_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_2_relu (Activation (None, 2, 2, 256)    0           conv4_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_3_conv (Conv2D)    (None, 2, 2, 1024)   263168      conv4_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_3_bn (BatchNormali (None, 2, 2, 1024)   4096        conv4_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_add (Add)          (None, 2, 2, 1024)   0           conv4_block1_out[0][0]           \n",
            "                                                                 conv4_block2_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_out (Activation)   (None, 2, 2, 1024)   0           conv4_block2_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_1_conv (Conv2D)    (None, 2, 2, 256)    262400      conv4_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_1_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_1_relu (Activation (None, 2, 2, 256)    0           conv4_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_2_conv (Conv2D)    (None, 2, 2, 256)    590080      conv4_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_2_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_2_relu (Activation (None, 2, 2, 256)    0           conv4_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_3_conv (Conv2D)    (None, 2, 2, 1024)   263168      conv4_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_3_bn (BatchNormali (None, 2, 2, 1024)   4096        conv4_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_add (Add)          (None, 2, 2, 1024)   0           conv4_block2_out[0][0]           \n",
            "                                                                 conv4_block3_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_out (Activation)   (None, 2, 2, 1024)   0           conv4_block3_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_1_conv (Conv2D)    (None, 2, 2, 256)    262400      conv4_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_1_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block4_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_1_relu (Activation (None, 2, 2, 256)    0           conv4_block4_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_2_conv (Conv2D)    (None, 2, 2, 256)    590080      conv4_block4_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_2_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block4_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_2_relu (Activation (None, 2, 2, 256)    0           conv4_block4_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_3_conv (Conv2D)    (None, 2, 2, 1024)   263168      conv4_block4_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_3_bn (BatchNormali (None, 2, 2, 1024)   4096        conv4_block4_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_add (Add)          (None, 2, 2, 1024)   0           conv4_block3_out[0][0]           \n",
            "                                                                 conv4_block4_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_out (Activation)   (None, 2, 2, 1024)   0           conv4_block4_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_1_conv (Conv2D)    (None, 2, 2, 256)    262400      conv4_block4_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_1_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block5_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_1_relu (Activation (None, 2, 2, 256)    0           conv4_block5_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_2_conv (Conv2D)    (None, 2, 2, 256)    590080      conv4_block5_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_2_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block5_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_2_relu (Activation (None, 2, 2, 256)    0           conv4_block5_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_3_conv (Conv2D)    (None, 2, 2, 1024)   263168      conv4_block5_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_3_bn (BatchNormali (None, 2, 2, 1024)   4096        conv4_block5_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_add (Add)          (None, 2, 2, 1024)   0           conv4_block4_out[0][0]           \n",
            "                                                                 conv4_block5_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_out (Activation)   (None, 2, 2, 1024)   0           conv4_block5_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_1_conv (Conv2D)    (None, 2, 2, 256)    262400      conv4_block5_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_1_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block6_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_1_relu (Activation (None, 2, 2, 256)    0           conv4_block6_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_2_conv (Conv2D)    (None, 2, 2, 256)    590080      conv4_block6_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_2_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block6_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_2_relu (Activation (None, 2, 2, 256)    0           conv4_block6_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_3_conv (Conv2D)    (None, 2, 2, 1024)   263168      conv4_block6_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_3_bn (BatchNormali (None, 2, 2, 1024)   4096        conv4_block6_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_add (Add)          (None, 2, 2, 1024)   0           conv4_block5_out[0][0]           \n",
            "                                                                 conv4_block6_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_out (Activation)   (None, 2, 2, 1024)   0           conv4_block6_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_1_conv (Conv2D)    (None, 1, 1, 512)    524800      conv4_block6_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_1_bn (BatchNormali (None, 1, 1, 512)    2048        conv5_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_1_relu (Activation (None, 1, 1, 512)    0           conv5_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_2_conv (Conv2D)    (None, 1, 1, 512)    2359808     conv5_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_2_bn (BatchNormali (None, 1, 1, 512)    2048        conv5_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_2_relu (Activation (None, 1, 1, 512)    0           conv5_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_0_conv (Conv2D)    (None, 1, 1, 2048)   2099200     conv4_block6_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_3_conv (Conv2D)    (None, 1, 1, 2048)   1050624     conv5_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_0_bn (BatchNormali (None, 1, 1, 2048)   8192        conv5_block1_0_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_3_bn (BatchNormali (None, 1, 1, 2048)   8192        conv5_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_add (Add)          (None, 1, 1, 2048)   0           conv5_block1_0_bn[0][0]          \n",
            "                                                                 conv5_block1_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_out (Activation)   (None, 1, 1, 2048)   0           conv5_block1_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_1_conv (Conv2D)    (None, 1, 1, 512)    1049088     conv5_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_1_bn (BatchNormali (None, 1, 1, 512)    2048        conv5_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_1_relu (Activation (None, 1, 1, 512)    0           conv5_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_2_conv (Conv2D)    (None, 1, 1, 512)    2359808     conv5_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_2_bn (BatchNormali (None, 1, 1, 512)    2048        conv5_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_2_relu (Activation (None, 1, 1, 512)    0           conv5_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_3_conv (Conv2D)    (None, 1, 1, 2048)   1050624     conv5_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_3_bn (BatchNormali (None, 1, 1, 2048)   8192        conv5_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_add (Add)          (None, 1, 1, 2048)   0           conv5_block1_out[0][0]           \n",
            "                                                                 conv5_block2_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_out (Activation)   (None, 1, 1, 2048)   0           conv5_block2_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_1_conv (Conv2D)    (None, 1, 1, 512)    1049088     conv5_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_1_bn (BatchNormali (None, 1, 1, 512)    2048        conv5_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_1_relu (Activation (None, 1, 1, 512)    0           conv5_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_2_conv (Conv2D)    (None, 1, 1, 512)    2359808     conv5_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_2_bn (BatchNormali (None, 1, 1, 512)    2048        conv5_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_2_relu (Activation (None, 1, 1, 512)    0           conv5_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_3_conv (Conv2D)    (None, 1, 1, 2048)   1050624     conv5_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_3_bn (BatchNormali (None, 1, 1, 2048)   8192        conv5_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_add (Add)          (None, 1, 1, 2048)   0           conv5_block2_out[0][0]           \n",
            "                                                                 conv5_block3_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_out (Activation)   (None, 1, 1, 2048)   0           conv5_block3_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling2d_3 (Glo (None, 2048)         0           conv5_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "dropout_19 (Dropout)            (None, 2048)         0           global_average_pooling2d_3[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "dense_34 (Dense)                (None, 6)            12294       dropout_19[0][0]                 \n",
            "==================================================================================================\n",
            "Total params: 23,600,006\n",
            "Trainable params: 23,546,886\n",
            "Non-trainable params: 53,120\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9dBDC-Pb5Vi1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b5d106d2-d402-451c-8778-dd79eca040a2"
      },
      "source": [
        "%cd /content/drive/My Drive/A_transfertTFMresnetSP500\n",
        "\n",
        "'''\n",
        "PART 3 Resnet TRAINING AND SAVING\n",
        "we suppose that we have loaded xtrain and ytrain\n",
        "This part is based on the Design of the NN\n",
        "Her we find the  quite usefull\n",
        "'''\n",
        "\n",
        "#Importing the resnet model\n",
        "from keras.applications.resnet import ResNet50, preprocess_input\n",
        "\n",
        "#In our example we need to y into categorical as it has 6 categories\n",
        "nb_classes=6\n",
        "y_train = np_utils.to_categorical(y_train, nb_classes)\n",
        "\n",
        "\n",
        "#Loading the resnet16 model with pre-trained ImageNet weights\n",
        "resnet_model = ResNet50(weights=None, include_top=False, input_shape=(32, 32, 3))\n",
        "resnet_model.trainable = False # remove if you want to retrain resnet weights\n",
        "\n",
        "resnet_model.summary()\n",
        "\n",
        "\n",
        "##Transfert model from resnet\n",
        "transfer_model = Sequential()\n",
        "transfer_model.add(resnet_model)\n",
        "transfer_model.add(Flatten())\n",
        "transfer_model.add(Dense(128, activation='relu'))\n",
        "transfer_model.add(Dropout(0.7))\n",
        "transfer_model.add(Dense(6, activation='softmax'))\n",
        "\n",
        "\n",
        "#Transfert model 2\n",
        "img_height,img_width=32,32\n",
        "num_classes=6\n",
        "base_model = applications.resnet50.ResNet50(weights= None, include_top=False, input_shape= (img_height,img_width,3))\n",
        "\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dropout(0.7)(x)\n",
        "predictions = Dense(num_classes, activation= 'softmax')(x)\n",
        "transfer_model = Model(inputs = base_model.input, outputs = predictions)\n",
        "\n",
        "###compilation model\n",
        "\n",
        "transfer_model.compile(loss=sp500loss, optimizer=sp500optimizer,\n",
        "              metrics=sp500metrics)\n",
        "\n",
        "#Save initial weight to reinitialize it after when we trying to find the best set of parameters\n",
        "transfer_model.save_weights('model/initial_weights.h5')\n",
        "#model.load_weights('my_model_weights.h5'\n",
        "\n",
        "##Saving the best model for each parameters\n",
        "checkpoint = ModelCheckpoint(\"model/best_model\"+sp500loss+\"_\"+sp500optimizer_name+\"_Batch\"+\\\n",
        "                             str(batch_size)+\"_LR\"+str(sp500_learning_rate)+\"_\"+str(sp500_decay_rate)+\".hdf5\", \\\n",
        "                                monitor='loss', verbose=1, \\\n",
        "                                save_best_only=True, mode='auto', period=1)\n",
        "\n",
        "# Load the TensorBoard notebook extension.\n",
        "%load_ext tensorboard\n",
        "#!rm -rf ./logs/ \n",
        "\n",
        " # Define the Keras TensorBoard callback.\n",
        "logdir=\"logs/fit/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir, histogram_freq=1)\n",
        "\n",
        "##Fitting the model on the train data and labels.\n",
        "#reinitialise xtrain, ytrain to avoid change of type from np.array to tensor by keras\n",
        "m_x_train=x_train\n",
        "m_y_train=y_train\n",
        "\n",
        "history = transfer_model.fit(m_x_train, m_y_train, \\\n",
        "                              batch_size=batch_size, epochs=epochs, \\\n",
        "                              validation_split=0.2, verbose=1, shuffle=True, \\\n",
        "                              callbacks=[checkpoint, tensorboard_callback])\n",
        "\n",
        "# Saving themodel\n",
        "transfer_model.save('model/resnetforsp500.h5')\n",
        "\n",
        "#Display the graph of the model\n",
        "tf.keras.utils.plot_model(transfer_model)\n",
        "\n",
        "##Display summary of neural network\n",
        "transfer_model.summary()"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/A_transfertTFMresnetSP500\n",
            "Model: \"resnet50\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_21 (InputLayer)           [(None, 32, 32, 3)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv1_pad (ZeroPadding2D)       (None, 38, 38, 3)    0           input_21[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv1_conv (Conv2D)             (None, 16, 16, 64)   9472        conv1_pad[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1_bn (BatchNormalization)   (None, 16, 16, 64)   256         conv1_conv[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv1_relu (Activation)         (None, 16, 16, 64)   0           conv1_bn[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "pool1_pad (ZeroPadding2D)       (None, 18, 18, 64)   0           conv1_relu[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "pool1_pool (MaxPooling2D)       (None, 8, 8, 64)     0           pool1_pad[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_1_conv (Conv2D)    (None, 8, 8, 64)     4160        pool1_pool[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_1_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_1_relu (Activation (None, 8, 8, 64)     0           conv2_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_2_conv (Conv2D)    (None, 8, 8, 64)     36928       conv2_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_2_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_2_relu (Activation (None, 8, 8, 64)     0           conv2_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_0_conv (Conv2D)    (None, 8, 8, 256)    16640       pool1_pool[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_3_conv (Conv2D)    (None, 8, 8, 256)    16640       conv2_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_0_bn (BatchNormali (None, 8, 8, 256)    1024        conv2_block1_0_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_3_bn (BatchNormali (None, 8, 8, 256)    1024        conv2_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_add (Add)          (None, 8, 8, 256)    0           conv2_block1_0_bn[0][0]          \n",
            "                                                                 conv2_block1_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_out (Activation)   (None, 8, 8, 256)    0           conv2_block1_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_1_conv (Conv2D)    (None, 8, 8, 64)     16448       conv2_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_1_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_1_relu (Activation (None, 8, 8, 64)     0           conv2_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_2_conv (Conv2D)    (None, 8, 8, 64)     36928       conv2_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_2_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_2_relu (Activation (None, 8, 8, 64)     0           conv2_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_3_conv (Conv2D)    (None, 8, 8, 256)    16640       conv2_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_3_bn (BatchNormali (None, 8, 8, 256)    1024        conv2_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_add (Add)          (None, 8, 8, 256)    0           conv2_block1_out[0][0]           \n",
            "                                                                 conv2_block2_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_out (Activation)   (None, 8, 8, 256)    0           conv2_block2_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_1_conv (Conv2D)    (None, 8, 8, 64)     16448       conv2_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_1_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_1_relu (Activation (None, 8, 8, 64)     0           conv2_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_2_conv (Conv2D)    (None, 8, 8, 64)     36928       conv2_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_2_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_2_relu (Activation (None, 8, 8, 64)     0           conv2_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_3_conv (Conv2D)    (None, 8, 8, 256)    16640       conv2_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_3_bn (BatchNormali (None, 8, 8, 256)    1024        conv2_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_add (Add)          (None, 8, 8, 256)    0           conv2_block2_out[0][0]           \n",
            "                                                                 conv2_block3_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_out (Activation)   (None, 8, 8, 256)    0           conv2_block3_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_1_conv (Conv2D)    (None, 4, 4, 128)    32896       conv2_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_1_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_1_relu (Activation (None, 4, 4, 128)    0           conv3_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_2_conv (Conv2D)    (None, 4, 4, 128)    147584      conv3_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_2_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_2_relu (Activation (None, 4, 4, 128)    0           conv3_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_0_conv (Conv2D)    (None, 4, 4, 512)    131584      conv2_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_3_conv (Conv2D)    (None, 4, 4, 512)    66048       conv3_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_0_bn (BatchNormali (None, 4, 4, 512)    2048        conv3_block1_0_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_3_bn (BatchNormali (None, 4, 4, 512)    2048        conv3_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_add (Add)          (None, 4, 4, 512)    0           conv3_block1_0_bn[0][0]          \n",
            "                                                                 conv3_block1_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_out (Activation)   (None, 4, 4, 512)    0           conv3_block1_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_1_conv (Conv2D)    (None, 4, 4, 128)    65664       conv3_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_1_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_1_relu (Activation (None, 4, 4, 128)    0           conv3_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_2_conv (Conv2D)    (None, 4, 4, 128)    147584      conv3_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_2_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_2_relu (Activation (None, 4, 4, 128)    0           conv3_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_3_conv (Conv2D)    (None, 4, 4, 512)    66048       conv3_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_3_bn (BatchNormali (None, 4, 4, 512)    2048        conv3_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_add (Add)          (None, 4, 4, 512)    0           conv3_block1_out[0][0]           \n",
            "                                                                 conv3_block2_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_out (Activation)   (None, 4, 4, 512)    0           conv3_block2_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_1_conv (Conv2D)    (None, 4, 4, 128)    65664       conv3_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_1_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_1_relu (Activation (None, 4, 4, 128)    0           conv3_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_2_conv (Conv2D)    (None, 4, 4, 128)    147584      conv3_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_2_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_2_relu (Activation (None, 4, 4, 128)    0           conv3_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_3_conv (Conv2D)    (None, 4, 4, 512)    66048       conv3_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_3_bn (BatchNormali (None, 4, 4, 512)    2048        conv3_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_add (Add)          (None, 4, 4, 512)    0           conv3_block2_out[0][0]           \n",
            "                                                                 conv3_block3_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_out (Activation)   (None, 4, 4, 512)    0           conv3_block3_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_1_conv (Conv2D)    (None, 4, 4, 128)    65664       conv3_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_1_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block4_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_1_relu (Activation (None, 4, 4, 128)    0           conv3_block4_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_2_conv (Conv2D)    (None, 4, 4, 128)    147584      conv3_block4_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_2_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block4_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_2_relu (Activation (None, 4, 4, 128)    0           conv3_block4_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_3_conv (Conv2D)    (None, 4, 4, 512)    66048       conv3_block4_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_3_bn (BatchNormali (None, 4, 4, 512)    2048        conv3_block4_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_add (Add)          (None, 4, 4, 512)    0           conv3_block3_out[0][0]           \n",
            "                                                                 conv3_block4_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_out (Activation)   (None, 4, 4, 512)    0           conv3_block4_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_1_conv (Conv2D)    (None, 2, 2, 256)    131328      conv3_block4_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_1_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_1_relu (Activation (None, 2, 2, 256)    0           conv4_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_2_conv (Conv2D)    (None, 2, 2, 256)    590080      conv4_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_2_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_2_relu (Activation (None, 2, 2, 256)    0           conv4_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_0_conv (Conv2D)    (None, 2, 2, 1024)   525312      conv3_block4_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_3_conv (Conv2D)    (None, 2, 2, 1024)   263168      conv4_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_0_bn (BatchNormali (None, 2, 2, 1024)   4096        conv4_block1_0_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_3_bn (BatchNormali (None, 2, 2, 1024)   4096        conv4_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_add (Add)          (None, 2, 2, 1024)   0           conv4_block1_0_bn[0][0]          \n",
            "                                                                 conv4_block1_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_out (Activation)   (None, 2, 2, 1024)   0           conv4_block1_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_1_conv (Conv2D)    (None, 2, 2, 256)    262400      conv4_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_1_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_1_relu (Activation (None, 2, 2, 256)    0           conv4_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_2_conv (Conv2D)    (None, 2, 2, 256)    590080      conv4_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_2_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_2_relu (Activation (None, 2, 2, 256)    0           conv4_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_3_conv (Conv2D)    (None, 2, 2, 1024)   263168      conv4_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_3_bn (BatchNormali (None, 2, 2, 1024)   4096        conv4_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_add (Add)          (None, 2, 2, 1024)   0           conv4_block1_out[0][0]           \n",
            "                                                                 conv4_block2_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_out (Activation)   (None, 2, 2, 1024)   0           conv4_block2_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_1_conv (Conv2D)    (None, 2, 2, 256)    262400      conv4_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_1_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_1_relu (Activation (None, 2, 2, 256)    0           conv4_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_2_conv (Conv2D)    (None, 2, 2, 256)    590080      conv4_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_2_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_2_relu (Activation (None, 2, 2, 256)    0           conv4_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_3_conv (Conv2D)    (None, 2, 2, 1024)   263168      conv4_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_3_bn (BatchNormali (None, 2, 2, 1024)   4096        conv4_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_add (Add)          (None, 2, 2, 1024)   0           conv4_block2_out[0][0]           \n",
            "                                                                 conv4_block3_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_out (Activation)   (None, 2, 2, 1024)   0           conv4_block3_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_1_conv (Conv2D)    (None, 2, 2, 256)    262400      conv4_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_1_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block4_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_1_relu (Activation (None, 2, 2, 256)    0           conv4_block4_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_2_conv (Conv2D)    (None, 2, 2, 256)    590080      conv4_block4_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_2_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block4_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_2_relu (Activation (None, 2, 2, 256)    0           conv4_block4_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_3_conv (Conv2D)    (None, 2, 2, 1024)   263168      conv4_block4_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_3_bn (BatchNormali (None, 2, 2, 1024)   4096        conv4_block4_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_add (Add)          (None, 2, 2, 1024)   0           conv4_block3_out[0][0]           \n",
            "                                                                 conv4_block4_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_out (Activation)   (None, 2, 2, 1024)   0           conv4_block4_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_1_conv (Conv2D)    (None, 2, 2, 256)    262400      conv4_block4_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_1_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block5_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_1_relu (Activation (None, 2, 2, 256)    0           conv4_block5_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_2_conv (Conv2D)    (None, 2, 2, 256)    590080      conv4_block5_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_2_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block5_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_2_relu (Activation (None, 2, 2, 256)    0           conv4_block5_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_3_conv (Conv2D)    (None, 2, 2, 1024)   263168      conv4_block5_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_3_bn (BatchNormali (None, 2, 2, 1024)   4096        conv4_block5_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_add (Add)          (None, 2, 2, 1024)   0           conv4_block4_out[0][0]           \n",
            "                                                                 conv4_block5_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_out (Activation)   (None, 2, 2, 1024)   0           conv4_block5_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_1_conv (Conv2D)    (None, 2, 2, 256)    262400      conv4_block5_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_1_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block6_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_1_relu (Activation (None, 2, 2, 256)    0           conv4_block6_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_2_conv (Conv2D)    (None, 2, 2, 256)    590080      conv4_block6_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_2_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block6_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_2_relu (Activation (None, 2, 2, 256)    0           conv4_block6_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_3_conv (Conv2D)    (None, 2, 2, 1024)   263168      conv4_block6_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_3_bn (BatchNormali (None, 2, 2, 1024)   4096        conv4_block6_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_add (Add)          (None, 2, 2, 1024)   0           conv4_block5_out[0][0]           \n",
            "                                                                 conv4_block6_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_out (Activation)   (None, 2, 2, 1024)   0           conv4_block6_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_1_conv (Conv2D)    (None, 1, 1, 512)    524800      conv4_block6_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_1_bn (BatchNormali (None, 1, 1, 512)    2048        conv5_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_1_relu (Activation (None, 1, 1, 512)    0           conv5_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_2_conv (Conv2D)    (None, 1, 1, 512)    2359808     conv5_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_2_bn (BatchNormali (None, 1, 1, 512)    2048        conv5_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_2_relu (Activation (None, 1, 1, 512)    0           conv5_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_0_conv (Conv2D)    (None, 1, 1, 2048)   2099200     conv4_block6_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_3_conv (Conv2D)    (None, 1, 1, 2048)   1050624     conv5_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_0_bn (BatchNormali (None, 1, 1, 2048)   8192        conv5_block1_0_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_3_bn (BatchNormali (None, 1, 1, 2048)   8192        conv5_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_add (Add)          (None, 1, 1, 2048)   0           conv5_block1_0_bn[0][0]          \n",
            "                                                                 conv5_block1_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_out (Activation)   (None, 1, 1, 2048)   0           conv5_block1_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_1_conv (Conv2D)    (None, 1, 1, 512)    1049088     conv5_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_1_bn (BatchNormali (None, 1, 1, 512)    2048        conv5_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_1_relu (Activation (None, 1, 1, 512)    0           conv5_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_2_conv (Conv2D)    (None, 1, 1, 512)    2359808     conv5_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_2_bn (BatchNormali (None, 1, 1, 512)    2048        conv5_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_2_relu (Activation (None, 1, 1, 512)    0           conv5_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_3_conv (Conv2D)    (None, 1, 1, 2048)   1050624     conv5_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_3_bn (BatchNormali (None, 1, 1, 2048)   8192        conv5_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_add (Add)          (None, 1, 1, 2048)   0           conv5_block1_out[0][0]           \n",
            "                                                                 conv5_block2_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_out (Activation)   (None, 1, 1, 2048)   0           conv5_block2_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_1_conv (Conv2D)    (None, 1, 1, 512)    1049088     conv5_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_1_bn (BatchNormali (None, 1, 1, 512)    2048        conv5_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_1_relu (Activation (None, 1, 1, 512)    0           conv5_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_2_conv (Conv2D)    (None, 1, 1, 512)    2359808     conv5_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_2_bn (BatchNormali (None, 1, 1, 512)    2048        conv5_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_2_relu (Activation (None, 1, 1, 512)    0           conv5_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_3_conv (Conv2D)    (None, 1, 1, 2048)   1050624     conv5_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_3_bn (BatchNormali (None, 1, 1, 2048)   8192        conv5_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_add (Add)          (None, 1, 1, 2048)   0           conv5_block2_out[0][0]           \n",
            "                                                                 conv5_block3_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_out (Activation)   (None, 1, 1, 2048)   0           conv5_block3_add[0][0]           \n",
            "==================================================================================================\n",
            "Total params: 23,587,712\n",
            "Trainable params: 0\n",
            "Non-trainable params: 23,587,712\n",
            "__________________________________________________________________________________________________\n",
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
            "The tensorboard extension is already loaded. To reload it, use:\n",
            "  %reload_ext tensorboard\n",
            "Epoch 1/5\n",
            "233/233 [==============================] - ETA: 0s - loss: 3.1298 - accuracy: 0.2633\n",
            "Epoch 00001: loss improved from inf to 3.12980, saving model to model/best_modelcategorical_crossentropy_Adam_Batch64_LR0.001_0.9.hdf5\n",
            "233/233 [==============================] - 1057s 5s/step - loss: 3.1298 - accuracy: 0.2633 - val_loss: 2.8547 - val_accuracy: 0.3022\n",
            "Epoch 2/5\n",
            "233/233 [==============================] - ETA: 0s - loss: 2.1033 - accuracy: 0.3033\n",
            "Epoch 00002: loss improved from 3.12980 to 2.10330, saving model to model/best_modelcategorical_crossentropy_Adam_Batch64_LR0.001_0.9.hdf5\n",
            "233/233 [==============================] - 1060s 5s/step - loss: 2.1033 - accuracy: 0.3033 - val_loss: 2.0254 - val_accuracy: 0.3399\n",
            "Epoch 3/5\n",
            "233/233 [==============================] - ETA: 0s - loss: 2.0591 - accuracy: 0.3144\n",
            "Epoch 00003: loss improved from 2.10330 to 2.05908, saving model to model/best_modelcategorical_crossentropy_Adam_Batch64_LR0.001_0.9.hdf5\n",
            "233/233 [==============================] - 1058s 5s/step - loss: 2.0591 - accuracy: 0.3144 - val_loss: 2.0103 - val_accuracy: 0.2896\n",
            "Epoch 4/5\n",
            "233/233 [==============================] - ETA: 0s - loss: 1.9791 - accuracy: 0.3219\n",
            "Epoch 00004: loss improved from 2.05908 to 1.97908, saving model to model/best_modelcategorical_crossentropy_Adam_Batch64_LR0.001_0.9.hdf5\n",
            "233/233 [==============================] - 1050s 5s/step - loss: 1.9791 - accuracy: 0.3219 - val_loss: 2.3768 - val_accuracy: 0.3385\n",
            "Epoch 5/5\n",
            "233/233 [==============================] - ETA: 0s - loss: 1.9411 - accuracy: 0.3229\n",
            "Epoch 00005: loss improved from 1.97908 to 1.94115, saving model to model/best_modelcategorical_crossentropy_Adam_Batch64_LR0.001_0.9.hdf5\n",
            "233/233 [==============================] - 1044s 4s/step - loss: 1.9411 - accuracy: 0.3229 - val_loss: 1.5205 - val_accuracy: 0.3022\n",
            "Model: \"functional_5\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_22 (InputLayer)           [(None, 32, 32, 3)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv1_pad (ZeroPadding2D)       (None, 38, 38, 3)    0           input_22[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv1_conv (Conv2D)             (None, 16, 16, 64)   9472        conv1_pad[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1_bn (BatchNormalization)   (None, 16, 16, 64)   256         conv1_conv[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv1_relu (Activation)         (None, 16, 16, 64)   0           conv1_bn[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "pool1_pad (ZeroPadding2D)       (None, 18, 18, 64)   0           conv1_relu[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "pool1_pool (MaxPooling2D)       (None, 8, 8, 64)     0           pool1_pad[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_1_conv (Conv2D)    (None, 8, 8, 64)     4160        pool1_pool[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_1_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_1_relu (Activation (None, 8, 8, 64)     0           conv2_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_2_conv (Conv2D)    (None, 8, 8, 64)     36928       conv2_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_2_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_2_relu (Activation (None, 8, 8, 64)     0           conv2_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_0_conv (Conv2D)    (None, 8, 8, 256)    16640       pool1_pool[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_3_conv (Conv2D)    (None, 8, 8, 256)    16640       conv2_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_0_bn (BatchNormali (None, 8, 8, 256)    1024        conv2_block1_0_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_3_bn (BatchNormali (None, 8, 8, 256)    1024        conv2_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_add (Add)          (None, 8, 8, 256)    0           conv2_block1_0_bn[0][0]          \n",
            "                                                                 conv2_block1_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_out (Activation)   (None, 8, 8, 256)    0           conv2_block1_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_1_conv (Conv2D)    (None, 8, 8, 64)     16448       conv2_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_1_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_1_relu (Activation (None, 8, 8, 64)     0           conv2_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_2_conv (Conv2D)    (None, 8, 8, 64)     36928       conv2_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_2_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_2_relu (Activation (None, 8, 8, 64)     0           conv2_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_3_conv (Conv2D)    (None, 8, 8, 256)    16640       conv2_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_3_bn (BatchNormali (None, 8, 8, 256)    1024        conv2_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_add (Add)          (None, 8, 8, 256)    0           conv2_block1_out[0][0]           \n",
            "                                                                 conv2_block2_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_out (Activation)   (None, 8, 8, 256)    0           conv2_block2_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_1_conv (Conv2D)    (None, 8, 8, 64)     16448       conv2_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_1_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_1_relu (Activation (None, 8, 8, 64)     0           conv2_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_2_conv (Conv2D)    (None, 8, 8, 64)     36928       conv2_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_2_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_2_relu (Activation (None, 8, 8, 64)     0           conv2_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_3_conv (Conv2D)    (None, 8, 8, 256)    16640       conv2_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_3_bn (BatchNormali (None, 8, 8, 256)    1024        conv2_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_add (Add)          (None, 8, 8, 256)    0           conv2_block2_out[0][0]           \n",
            "                                                                 conv2_block3_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_out (Activation)   (None, 8, 8, 256)    0           conv2_block3_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_1_conv (Conv2D)    (None, 4, 4, 128)    32896       conv2_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_1_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_1_relu (Activation (None, 4, 4, 128)    0           conv3_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_2_conv (Conv2D)    (None, 4, 4, 128)    147584      conv3_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_2_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_2_relu (Activation (None, 4, 4, 128)    0           conv3_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_0_conv (Conv2D)    (None, 4, 4, 512)    131584      conv2_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_3_conv (Conv2D)    (None, 4, 4, 512)    66048       conv3_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_0_bn (BatchNormali (None, 4, 4, 512)    2048        conv3_block1_0_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_3_bn (BatchNormali (None, 4, 4, 512)    2048        conv3_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_add (Add)          (None, 4, 4, 512)    0           conv3_block1_0_bn[0][0]          \n",
            "                                                                 conv3_block1_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_out (Activation)   (None, 4, 4, 512)    0           conv3_block1_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_1_conv (Conv2D)    (None, 4, 4, 128)    65664       conv3_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_1_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_1_relu (Activation (None, 4, 4, 128)    0           conv3_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_2_conv (Conv2D)    (None, 4, 4, 128)    147584      conv3_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_2_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_2_relu (Activation (None, 4, 4, 128)    0           conv3_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_3_conv (Conv2D)    (None, 4, 4, 512)    66048       conv3_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_3_bn (BatchNormali (None, 4, 4, 512)    2048        conv3_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_add (Add)          (None, 4, 4, 512)    0           conv3_block1_out[0][0]           \n",
            "                                                                 conv3_block2_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_out (Activation)   (None, 4, 4, 512)    0           conv3_block2_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_1_conv (Conv2D)    (None, 4, 4, 128)    65664       conv3_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_1_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_1_relu (Activation (None, 4, 4, 128)    0           conv3_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_2_conv (Conv2D)    (None, 4, 4, 128)    147584      conv3_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_2_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_2_relu (Activation (None, 4, 4, 128)    0           conv3_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_3_conv (Conv2D)    (None, 4, 4, 512)    66048       conv3_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_3_bn (BatchNormali (None, 4, 4, 512)    2048        conv3_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_add (Add)          (None, 4, 4, 512)    0           conv3_block2_out[0][0]           \n",
            "                                                                 conv3_block3_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_out (Activation)   (None, 4, 4, 512)    0           conv3_block3_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_1_conv (Conv2D)    (None, 4, 4, 128)    65664       conv3_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_1_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block4_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_1_relu (Activation (None, 4, 4, 128)    0           conv3_block4_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_2_conv (Conv2D)    (None, 4, 4, 128)    147584      conv3_block4_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_2_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block4_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_2_relu (Activation (None, 4, 4, 128)    0           conv3_block4_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_3_conv (Conv2D)    (None, 4, 4, 512)    66048       conv3_block4_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_3_bn (BatchNormali (None, 4, 4, 512)    2048        conv3_block4_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_add (Add)          (None, 4, 4, 512)    0           conv3_block3_out[0][0]           \n",
            "                                                                 conv3_block4_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_out (Activation)   (None, 4, 4, 512)    0           conv3_block4_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_1_conv (Conv2D)    (None, 2, 2, 256)    131328      conv3_block4_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_1_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_1_relu (Activation (None, 2, 2, 256)    0           conv4_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_2_conv (Conv2D)    (None, 2, 2, 256)    590080      conv4_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_2_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_2_relu (Activation (None, 2, 2, 256)    0           conv4_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_0_conv (Conv2D)    (None, 2, 2, 1024)   525312      conv3_block4_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_3_conv (Conv2D)    (None, 2, 2, 1024)   263168      conv4_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_0_bn (BatchNormali (None, 2, 2, 1024)   4096        conv4_block1_0_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_3_bn (BatchNormali (None, 2, 2, 1024)   4096        conv4_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_add (Add)          (None, 2, 2, 1024)   0           conv4_block1_0_bn[0][0]          \n",
            "                                                                 conv4_block1_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_out (Activation)   (None, 2, 2, 1024)   0           conv4_block1_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_1_conv (Conv2D)    (None, 2, 2, 256)    262400      conv4_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_1_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_1_relu (Activation (None, 2, 2, 256)    0           conv4_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_2_conv (Conv2D)    (None, 2, 2, 256)    590080      conv4_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_2_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_2_relu (Activation (None, 2, 2, 256)    0           conv4_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_3_conv (Conv2D)    (None, 2, 2, 1024)   263168      conv4_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_3_bn (BatchNormali (None, 2, 2, 1024)   4096        conv4_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_add (Add)          (None, 2, 2, 1024)   0           conv4_block1_out[0][0]           \n",
            "                                                                 conv4_block2_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_out (Activation)   (None, 2, 2, 1024)   0           conv4_block2_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_1_conv (Conv2D)    (None, 2, 2, 256)    262400      conv4_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_1_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_1_relu (Activation (None, 2, 2, 256)    0           conv4_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_2_conv (Conv2D)    (None, 2, 2, 256)    590080      conv4_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_2_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_2_relu (Activation (None, 2, 2, 256)    0           conv4_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_3_conv (Conv2D)    (None, 2, 2, 1024)   263168      conv4_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_3_bn (BatchNormali (None, 2, 2, 1024)   4096        conv4_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_add (Add)          (None, 2, 2, 1024)   0           conv4_block2_out[0][0]           \n",
            "                                                                 conv4_block3_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_out (Activation)   (None, 2, 2, 1024)   0           conv4_block3_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_1_conv (Conv2D)    (None, 2, 2, 256)    262400      conv4_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_1_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block4_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_1_relu (Activation (None, 2, 2, 256)    0           conv4_block4_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_2_conv (Conv2D)    (None, 2, 2, 256)    590080      conv4_block4_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_2_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block4_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_2_relu (Activation (None, 2, 2, 256)    0           conv4_block4_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_3_conv (Conv2D)    (None, 2, 2, 1024)   263168      conv4_block4_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_3_bn (BatchNormali (None, 2, 2, 1024)   4096        conv4_block4_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_add (Add)          (None, 2, 2, 1024)   0           conv4_block3_out[0][0]           \n",
            "                                                                 conv4_block4_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_out (Activation)   (None, 2, 2, 1024)   0           conv4_block4_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_1_conv (Conv2D)    (None, 2, 2, 256)    262400      conv4_block4_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_1_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block5_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_1_relu (Activation (None, 2, 2, 256)    0           conv4_block5_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_2_conv (Conv2D)    (None, 2, 2, 256)    590080      conv4_block5_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_2_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block5_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_2_relu (Activation (None, 2, 2, 256)    0           conv4_block5_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_3_conv (Conv2D)    (None, 2, 2, 1024)   263168      conv4_block5_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_3_bn (BatchNormali (None, 2, 2, 1024)   4096        conv4_block5_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_add (Add)          (None, 2, 2, 1024)   0           conv4_block4_out[0][0]           \n",
            "                                                                 conv4_block5_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_out (Activation)   (None, 2, 2, 1024)   0           conv4_block5_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_1_conv (Conv2D)    (None, 2, 2, 256)    262400      conv4_block5_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_1_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block6_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_1_relu (Activation (None, 2, 2, 256)    0           conv4_block6_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_2_conv (Conv2D)    (None, 2, 2, 256)    590080      conv4_block6_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_2_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block6_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_2_relu (Activation (None, 2, 2, 256)    0           conv4_block6_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_3_conv (Conv2D)    (None, 2, 2, 1024)   263168      conv4_block6_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_3_bn (BatchNormali (None, 2, 2, 1024)   4096        conv4_block6_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_add (Add)          (None, 2, 2, 1024)   0           conv4_block5_out[0][0]           \n",
            "                                                                 conv4_block6_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_out (Activation)   (None, 2, 2, 1024)   0           conv4_block6_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_1_conv (Conv2D)    (None, 1, 1, 512)    524800      conv4_block6_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_1_bn (BatchNormali (None, 1, 1, 512)    2048        conv5_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_1_relu (Activation (None, 1, 1, 512)    0           conv5_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_2_conv (Conv2D)    (None, 1, 1, 512)    2359808     conv5_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_2_bn (BatchNormali (None, 1, 1, 512)    2048        conv5_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_2_relu (Activation (None, 1, 1, 512)    0           conv5_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_0_conv (Conv2D)    (None, 1, 1, 2048)   2099200     conv4_block6_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_3_conv (Conv2D)    (None, 1, 1, 2048)   1050624     conv5_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_0_bn (BatchNormali (None, 1, 1, 2048)   8192        conv5_block1_0_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_3_bn (BatchNormali (None, 1, 1, 2048)   8192        conv5_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_add (Add)          (None, 1, 1, 2048)   0           conv5_block1_0_bn[0][0]          \n",
            "                                                                 conv5_block1_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_out (Activation)   (None, 1, 1, 2048)   0           conv5_block1_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_1_conv (Conv2D)    (None, 1, 1, 512)    1049088     conv5_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_1_bn (BatchNormali (None, 1, 1, 512)    2048        conv5_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_1_relu (Activation (None, 1, 1, 512)    0           conv5_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_2_conv (Conv2D)    (None, 1, 1, 512)    2359808     conv5_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_2_bn (BatchNormali (None, 1, 1, 512)    2048        conv5_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_2_relu (Activation (None, 1, 1, 512)    0           conv5_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_3_conv (Conv2D)    (None, 1, 1, 2048)   1050624     conv5_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_3_bn (BatchNormali (None, 1, 1, 2048)   8192        conv5_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_add (Add)          (None, 1, 1, 2048)   0           conv5_block1_out[0][0]           \n",
            "                                                                 conv5_block2_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_out (Activation)   (None, 1, 1, 2048)   0           conv5_block2_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_1_conv (Conv2D)    (None, 1, 1, 512)    1049088     conv5_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_1_bn (BatchNormali (None, 1, 1, 512)    2048        conv5_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_1_relu (Activation (None, 1, 1, 512)    0           conv5_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_2_conv (Conv2D)    (None, 1, 1, 512)    2359808     conv5_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_2_bn (BatchNormali (None, 1, 1, 512)    2048        conv5_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_2_relu (Activation (None, 1, 1, 512)    0           conv5_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_3_conv (Conv2D)    (None, 1, 1, 2048)   1050624     conv5_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_3_bn (BatchNormali (None, 1, 1, 2048)   8192        conv5_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_add (Add)          (None, 1, 1, 2048)   0           conv5_block2_out[0][0]           \n",
            "                                                                 conv5_block3_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_out (Activation)   (None, 1, 1, 2048)   0           conv5_block3_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling2d_4 (Glo (None, 2048)         0           conv5_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "dropout_21 (Dropout)            (None, 2048)         0           global_average_pooling2d_4[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "dense_37 (Dense)                (None, 6)            12294       dropout_21[0][0]                 \n",
            "==================================================================================================\n",
            "Total params: 23,600,006\n",
            "Trainable params: 23,546,886\n",
            "Non-trainable params: 53,120\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wa7aDNZGWE6h",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "b9acec98-2994-4cff-b751-7d7643dc8d0d"
      },
      "source": [
        "%pwd"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/drive/My Drive/A_transfertTFMresnetSP500'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sqRbJ1lTWRD7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "outputId": "cce77f57-a0b0-49da-8b6f-1ae87b0bf723"
      },
      "source": [
        "ls -l model/"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 837925\n",
            "-rw------- 1 root root  97839904 Sep 19 14:40 best_modelcategorical_crossentropy_Adam_Batch32_LR0.0001_0.98.hdf5\n",
            "-rw------- 1 root root  97839896 Sep 19 13:15 best_modelcategorical_crossentropy_Adam_Batch32_LR0.01_0.98.hdf5\n",
            "-rw------- 1 root root 283765328 Sep 19 17:08 best_modelcategorical_crossentropy_Adam_Batch64_LR0.001_0.9.hdf5\n",
            "-rw------- 1 root root  94822472 Sep 19 15:40 initial_weights.h5\n",
            "-rw------- 1 root root 283765328 Sep 19 17:08 resnetforsp500.h5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uhuotgb6WcNZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%cp model/best_modelcategorical_crossentropy_Adam_Batch64_LR0.001_0.9.hdf5 "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "weqAP7PdW1On",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "transfer_model.save('model/initial_weights.h5')"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H6eFmzP24-vc",
        "colab_type": "text"
      },
      "source": [
        "###Version with vgg16"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WQNbUuu_ofxC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9930dc87-6f44-4727-a4fa-892dbc144628"
      },
      "source": [
        "%cd /content/drive/My Drive/A_transfertTFMVggSP500\n",
        "\n",
        "'''\n",
        "PART 3 VGGSP500 TRAINING AND SAVING\n",
        "we suppose that we have loaded xtrain and ytrain\n",
        "This part is based on the Design of the NN\n",
        "Her we find the Vgg16 quite usefull\n",
        "'''\n",
        "\n",
        "#Importing the VGG16 model\n",
        "from keras.applications.vgg16 import VGG16, preprocess_input\n",
        "from keras.applications.resnet import ResNet50\n",
        "\n",
        "#In our example we need to y into categorical as it has 6 categories\n",
        "nb_classes=6\n",
        "y_train = np_utils.to_categorical(y_train, nb_classes)\n",
        "\n",
        "\n",
        "#Loading the VGG16 model with pre-trained ImageNet weights\n",
        "vgg_model = VGG16(weights='imagenet', include_top=False, input_shape=(32, 32, 3))\n",
        "vgg_model.trainable = False # remove if you want to retrain vgg weights\n",
        "\n",
        "vgg_model.summary()\n",
        "\n",
        "\n",
        "##Transfert model from vgg\n",
        "transfer_model = Sequential()\n",
        "transfer_model.add(vgg_model)\n",
        "transfer_model.add(Flatten())\n",
        "transfer_model.add(Dense(128, activation='relu'))\n",
        "transfer_model.add(Dropout(0.2))\n",
        "transfer_model.add(Dense(6, activation='softmax'))\n",
        "\n",
        "\n",
        "\n",
        "transfer_model.compile(loss=vggsp500loss, optimizer=vggsp500optimizer,\n",
        "              metrics=vggsp500metrics)\n",
        "\n",
        "#Save initial weight to reinitialize it after when we trying to find the best set of parameters\n",
        "transfer_model.save_weights('model/initial_weights.h5')\n",
        "#model.load_weights('my_model_weights.h5'\n",
        "\n",
        "##Saving the best model for each parameters\n",
        "checkpoint = ModelCheckpoint(\"model/best_model\"+vggsp500loss+\"_\"+vggsp500optimizer_name+\"_Batch\"+\\\n",
        "                             str(batch_size)+\"_LR\"+str(vggsp500_learning_rate)+\"_\"+str(vggsp500_decay_rate)+\".hdf5\", \\\n",
        "                                monitor='loss', verbose=1, \\\n",
        "                                save_best_only=True, mode='auto', period=1)\n",
        "\n",
        "# Load the TensorBoard notebook extension.\n",
        "%load_ext tensorboard\n",
        "#!rm -rf ./logs/ \n",
        "\n",
        " # Define the Keras TensorBoard callback.\n",
        "logdir=\"logs/fit/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir, histogram_freq=1)\n",
        "\n",
        "##Fitting the model on the train data and labels.\n",
        "#reinitialise xtrain, ytrain to avoid change of type from np.array to tensor by keras\n",
        "m_x_train=x_train\n",
        "m_y_train=y_train\n",
        "\n",
        "history = transfer_model.fit(m_x_train, m_y_train, \\\n",
        "                              batch_size=batch_size, epochs=epochs, \\\n",
        "                              validation_split=0.2, verbose=1, shuffle=True, \\\n",
        "                              callbacks=[checkpoint, tensorboard_callback])\n",
        "\n",
        "# Saving themodel\n",
        "transfer_model.save('model/vggforsp500.h5')\n",
        "\n",
        "#Display the graph of the model\n",
        "tf.keras.utils.plot_model(transfer_model)\n",
        "\n",
        "##Display summary of neural network\n",
        "transfer_model.summary()\n",
        "\n",
        "#Display Tensorboard\n",
        "%tensorboard --logdir logs"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/A_transfertTFMVggSP500\n",
            "Model: \"vgg16\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_2 (InputLayer)         [(None, 32, 32, 3)]       0         \n",
            "_________________________________________________________________\n",
            "block1_conv1 (Conv2D)        (None, 32, 32, 64)        1792      \n",
            "_________________________________________________________________\n",
            "block1_conv2 (Conv2D)        (None, 32, 32, 64)        36928     \n",
            "_________________________________________________________________\n",
            "block1_pool (MaxPooling2D)   (None, 16, 16, 64)        0         \n",
            "_________________________________________________________________\n",
            "block2_conv1 (Conv2D)        (None, 16, 16, 128)       73856     \n",
            "_________________________________________________________________\n",
            "block2_conv2 (Conv2D)        (None, 16, 16, 128)       147584    \n",
            "_________________________________________________________________\n",
            "block2_pool (MaxPooling2D)   (None, 8, 8, 128)         0         \n",
            "_________________________________________________________________\n",
            "block3_conv1 (Conv2D)        (None, 8, 8, 256)         295168    \n",
            "_________________________________________________________________\n",
            "block3_conv2 (Conv2D)        (None, 8, 8, 256)         590080    \n",
            "_________________________________________________________________\n",
            "block3_conv3 (Conv2D)        (None, 8, 8, 256)         590080    \n",
            "_________________________________________________________________\n",
            "block3_pool (MaxPooling2D)   (None, 4, 4, 256)         0         \n",
            "_________________________________________________________________\n",
            "block4_conv1 (Conv2D)        (None, 4, 4, 512)         1180160   \n",
            "_________________________________________________________________\n",
            "block4_conv2 (Conv2D)        (None, 4, 4, 512)         2359808   \n",
            "_________________________________________________________________\n",
            "block4_conv3 (Conv2D)        (None, 4, 4, 512)         2359808   \n",
            "_________________________________________________________________\n",
            "block4_pool (MaxPooling2D)   (None, 2, 2, 512)         0         \n",
            "_________________________________________________________________\n",
            "block5_conv1 (Conv2D)        (None, 2, 2, 512)         2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv2 (Conv2D)        (None, 2, 2, 512)         2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv3 (Conv2D)        (None, 2, 2, 512)         2359808   \n",
            "_________________________________________________________________\n",
            "block5_pool (MaxPooling2D)   (None, 1, 1, 512)         0         \n",
            "=================================================================\n",
            "Total params: 14,714,688\n",
            "Trainable params: 0\n",
            "Non-trainable params: 14,714,688\n",
            "_________________________________________________________________\n",
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
            "The tensorboard extension is already loaded. To reload it, use:\n",
            "  %reload_ext tensorboard\n",
            "Epoch 1/3\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-f479dc028c0d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0mm_y_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransfer_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm_x_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm_y_train\u001b[0m\u001b[0;34m,\u001b[0m                               \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m                               \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m                               \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensorboard_callback\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;31m# Saving themodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1098\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1099\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    821\u001b[0m       \u001b[0;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 823\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    824\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m       \u001b[0;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    695\u001b[0m     self._concrete_stateful_fn = (\n\u001b[1;32m    696\u001b[0m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0;32m--> 697\u001b[0;31m             *args, **kwds))\n\u001b[0m\u001b[1;32m    698\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    699\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minvalid_creator_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0munused_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0munused_kwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2853\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2854\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2855\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2856\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2857\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3212\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3213\u001b[0;31m       \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3214\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3215\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   3073\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3074\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3075\u001b[0;31m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[1;32m   3076\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3077\u001b[0m         \u001b[0mfunction_spec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_spec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    985\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 986\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    987\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    988\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    598\u001b[0m         \u001b[0;31m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    599\u001b[0m         \u001b[0;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 600\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    601\u001b[0m     \u001b[0mweak_wrapped_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweakref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapped_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    971\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    972\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 973\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    974\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    975\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:806 train_function  *\n        return step_function(self, iterator)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:796 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:1211 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:2585 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:2945 _call_for_each_replica\n        return fn(*args, **kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:789 run_step  **\n        outputs = model.train_step(data)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:749 train_step\n        y, y_pred, sample_weight, regularization_losses=self.losses)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/compile_utils.py:204 __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/losses.py:149 __call__\n        losses = ag_call(y_true, y_pred)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/losses.py:253 call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py:201 wrapper\n        return target(*args, **kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/losses.py:1535 categorical_crossentropy\n        return K.categorical_crossentropy(y_true, y_pred, from_logits=from_logits)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py:201 wrapper\n        return target(*args, **kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/backend.py:4687 categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/tensor_shape.py:1134 assert_is_compatible_with\n        raise ValueError(\"Shapes %s and %s are incompatible\" % (self, other))\n\n    ValueError: Shapes (None, 6, 6) and (None, 6) are incompatible\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FiDWq98sOAb-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%load_ext tensorboard"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pJfc1EgdOIn8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#here you have with the logs in drive link where you can launch tensorboard and see the training process\n",
        "%cd /content/drive/My Drive/A_transfertTFMVggSP500\n",
        "%tensorboard --logdir logs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n4hYLydW_XH2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "history.history['loss']\n",
        "history.history"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SgkpclptyV7A",
        "colab_type": "text"
      },
      "source": [
        "**IN CASE OF ERROR OF SHAPE OF INPUT**\n",
        "\n",
        "please note that if some error in shape of the input you have to execute the loading datas once again\n",
        "there is a update of type between tensor and np.array of y_train and x_train\n",
        "\n",
        "Go back to\n",
        "\n",
        "Step 2: Loading training datas with vgg16 transfert model and training\n",
        "\n",
        "Loading and Cleaning Training Datas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IuzuOOsaLAsO",
        "colab_type": "text"
      },
      "source": [
        "####Search of the best parameters for training and evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WQD_FVuIK--C",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "450c342a-0c91-41ed-c32a-a902c34f04bd"
      },
      "source": [
        "%cd /content/drive/My Drive/A_transfertTFMVggSP500\n",
        "# Load the TensorBoard notebook extension.\n",
        "%reload_ext tensorboard\n",
        "!rm -rf ./logs/ \n",
        "\n",
        "#We can modify batch size and epochs to adjust improve the training\n",
        "l_batch_size=[25,50,100]\n",
        "l_epochs=[25,50,100]\n",
        "l_learning_rate=[0.001,0.01,0.1]\n",
        "l_optimizer_name=[keras.optimizers.SGD, keras.optimizers.RMSprop, keras.optimizers.Adam, keras.optimizers.Adagrad, keras.optimizers.Adamax, keras.optimizers.Ftrl]\n",
        "\n",
        "Tabres={}\n",
        "\n",
        "for x_batch_size in l_batch_size :\n",
        "  for  x_epochs in l_epochs:\n",
        "    for x_learning_rate in l_learning_rate:\n",
        "      for x_optimizer_name in l_optimizer_name :\n",
        "        batch_size= x_batch_size\n",
        "        epochs= x_epochs\n",
        "        lr_schedule =  x_learning_rate\n",
        "        \n",
        "        ##https://keras.io/api/losses/\n",
        "        vggsp500loss='categorical_crossentropy'                             \n",
        "\n",
        "        ##https://keras.io/api/optimizers/\n",
        "        vggsp500optimizer_name=x_optimizer_name(learning_rate=x_learning_rate)\n",
        "        \n",
        "        ##https://keras.io/api/metrics/\n",
        "        vggsp500metrics=['accuracy']                                           \n",
        "        \n",
        "        \n",
        "        ##ACTUALISATION OF THE MODEL AND BY EACH SET OF PARAMETERS\n",
        "        \n",
        "        #First we initialize weight for each set of param see abpve where we had saved it\n",
        "        transfer_model.load_weights('model/initial_weights.h5')\n",
        "        transfer_model.compile(loss=vggsp500loss, optimizer=vggsp500optimizer_name,\n",
        "                      metrics=vggsp500metrics)\n",
        "\n",
        "        ##Saving the best model for each parameters\n",
        "        nameof_intermediarymodel=\"best_model\"+vggsp500loss+\"_\"+str(vggsp500optimizer_name)+\"_Batch\"+\\\n",
        "                                    str(batch_size)+\"_LR\"+str(x_learning_rate)+\"_Epochs\"+str(epochs)\n",
        "        checkpoint = ModelCheckpoint('model/'+nameof_intermediarymodel+\".hdf5\", \\\n",
        "                                        monitor='loss', verbose=1, \\\n",
        "                                        save_best_only=True, mode='auto', period=1)\n",
        "\n",
        "        \n",
        "        # Define the Keras TensorBoard callback.\n",
        "        logdir=\"logs/fit/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "        tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir, histogram_freq=1)\n",
        "\n",
        "        ##Fitting the model on the train data and labels.\n",
        "        m_x_train=x_train\n",
        "        m_y_train=y_train\n",
        "\n",
        "        history = transfer_model.fit(m_x_train, m_y_train, \\\n",
        "                                      batch_size=batch_size, epochs=epochs, \\\n",
        "                                      validation_split=0.2, verbose=1, shuffle=True, \\\n",
        "                                      callbacks=[checkpoint,tensorboard_callback])\n",
        "        # Saving themodel\n",
        "        transfer_model.save('model/'+nameof_intermediarymodel+'vggforsp500'+'.h5')\n",
        "        Tabres.update({nameof_intermediarymodel:history.history})\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/A_transfertTFMVggSP500\n",
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
            "Epoch 1/25\n",
            "  2/595 [..............................] - ETA: 34s - loss: 2.4315 - accuracy: 0.0400WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0145s vs `on_train_batch_end` time: 0.1003s). Check your callbacks.\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.6045 - accuracy: 0.3101\n",
            "Epoch 00001: loss improved from inf to 1.60393, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f5b6204e8d0>_Batch25_LR0.001_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.6039 - accuracy: 0.3110 - val_loss: 1.5135 - val_accuracy: 0.3399\n",
            "Epoch 2/25\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.5611 - accuracy: 0.3199\n",
            "Epoch 00002: loss improved from 1.60393 to 1.56054, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f5b6204e8d0>_Batch25_LR0.001_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5605 - accuracy: 0.3201 - val_loss: 1.5102 - val_accuracy: 0.3399\n",
            "Epoch 3/25\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.5548 - accuracy: 0.3293\n",
            "Epoch 00003: loss improved from 1.56054 to 1.55448, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f5b6204e8d0>_Batch25_LR0.001_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5545 - accuracy: 0.3294 - val_loss: 1.5035 - val_accuracy: 0.3399\n",
            "Epoch 4/25\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5459 - accuracy: 0.3327\n",
            "Epoch 00004: loss improved from 1.55448 to 1.54585, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f5b6204e8d0>_Batch25_LR0.001_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5459 - accuracy: 0.3326 - val_loss: 1.5065 - val_accuracy: 0.3399\n",
            "Epoch 5/25\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5430 - accuracy: 0.3321\n",
            "Epoch 00005: loss improved from 1.54585 to 1.54320, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f5b6204e8d0>_Batch25_LR0.001_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5432 - accuracy: 0.3318 - val_loss: 1.5075 - val_accuracy: 0.3399\n",
            "Epoch 6/25\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5411 - accuracy: 0.3384\n",
            "Epoch 00006: loss improved from 1.54320 to 1.54099, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f5b6204e8d0>_Batch25_LR0.001_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5410 - accuracy: 0.3387 - val_loss: 1.5053 - val_accuracy: 0.3399\n",
            "Epoch 7/25\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5378 - accuracy: 0.3389\n",
            "Epoch 00007: loss improved from 1.54099 to 1.53811, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f5b6204e8d0>_Batch25_LR0.001_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5381 - accuracy: 0.3387 - val_loss: 1.5027 - val_accuracy: 0.3399\n",
            "Epoch 8/25\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5351 - accuracy: 0.3378\n",
            "Epoch 00008: loss improved from 1.53811 to 1.53514, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f5b6204e8d0>_Batch25_LR0.001_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5351 - accuracy: 0.3379 - val_loss: 1.5048 - val_accuracy: 0.3399\n",
            "Epoch 9/25\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5381 - accuracy: 0.3386\n",
            "Epoch 00009: loss did not improve from 1.53514\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5382 - accuracy: 0.3388 - val_loss: 1.5061 - val_accuracy: 0.3399\n",
            "Epoch 10/25\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5370 - accuracy: 0.3422\n",
            "Epoch 00010: loss did not improve from 1.53514\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5366 - accuracy: 0.3422 - val_loss: 1.5034 - val_accuracy: 0.3399\n",
            "Epoch 11/25\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.5342 - accuracy: 0.3403\n",
            "Epoch 00011: loss improved from 1.53514 to 1.53396, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f5b6204e8d0>_Batch25_LR0.001_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5340 - accuracy: 0.3404 - val_loss: 1.5014 - val_accuracy: 0.3399\n",
            "Epoch 12/25\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.5354 - accuracy: 0.3381\n",
            "Epoch 00012: loss did not improve from 1.53396\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5354 - accuracy: 0.3381 - val_loss: 1.5010 - val_accuracy: 0.3399\n",
            "Epoch 13/25\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.5326 - accuracy: 0.3406\n",
            "Epoch 00013: loss improved from 1.53396 to 1.53260, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f5b6204e8d0>_Batch25_LR0.001_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5326 - accuracy: 0.3406 - val_loss: 1.5043 - val_accuracy: 0.3399\n",
            "Epoch 14/25\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5333 - accuracy: 0.3436\n",
            "Epoch 00014: loss did not improve from 1.53260\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5329 - accuracy: 0.3442 - val_loss: 1.5033 - val_accuracy: 0.3399\n",
            "Epoch 15/25\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5314 - accuracy: 0.3414\n",
            "Epoch 00015: loss improved from 1.53260 to 1.53111, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f5b6204e8d0>_Batch25_LR0.001_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5311 - accuracy: 0.3417 - val_loss: 1.5036 - val_accuracy: 0.3399\n",
            "Epoch 16/25\n",
            "589/595 [============================>.] - ETA: 0s - loss: 1.5308 - accuracy: 0.3419\n",
            "Epoch 00016: loss improved from 1.53111 to 1.53053, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f5b6204e8d0>_Batch25_LR0.001_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5305 - accuracy: 0.3425 - val_loss: 1.5035 - val_accuracy: 0.3399\n",
            "Epoch 17/25\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.5306 - accuracy: 0.3430\n",
            "Epoch 00017: loss did not improve from 1.53053\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5306 - accuracy: 0.3430 - val_loss: 1.5025 - val_accuracy: 0.3399\n",
            "Epoch 18/25\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.5305 - accuracy: 0.3426\n",
            "Epoch 00018: loss did not improve from 1.53053\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5305 - accuracy: 0.3426 - val_loss: 1.5024 - val_accuracy: 0.3399\n",
            "Epoch 19/25\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5305 - accuracy: 0.3420\n",
            "Epoch 00019: loss improved from 1.53053 to 1.53039, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f5b6204e8d0>_Batch25_LR0.001_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5304 - accuracy: 0.3421 - val_loss: 1.5016 - val_accuracy: 0.3399\n",
            "Epoch 20/25\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.5282 - accuracy: 0.3417\n",
            "Epoch 00020: loss improved from 1.53039 to 1.52811, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f5b6204e8d0>_Batch25_LR0.001_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5281 - accuracy: 0.3420 - val_loss: 1.5025 - val_accuracy: 0.3399\n",
            "Epoch 21/25\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5292 - accuracy: 0.3424\n",
            "Epoch 00021: loss did not improve from 1.52811\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5293 - accuracy: 0.3422 - val_loss: 1.5011 - val_accuracy: 0.3399\n",
            "Epoch 22/25\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5287 - accuracy: 0.3414\n",
            "Epoch 00022: loss did not improve from 1.52811\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5283 - accuracy: 0.3416 - val_loss: 1.5029 - val_accuracy: 0.3399\n",
            "Epoch 23/25\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.5282 - accuracy: 0.3419\n",
            "Epoch 00023: loss did not improve from 1.52811\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5282 - accuracy: 0.3419 - val_loss: 1.4992 - val_accuracy: 0.3399\n",
            "Epoch 24/25\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5283 - accuracy: 0.3428\n",
            "Epoch 00024: loss did not improve from 1.52811\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5281 - accuracy: 0.3428 - val_loss: 1.4994 - val_accuracy: 0.3399\n",
            "Epoch 25/25\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5271 - accuracy: 0.3417\n",
            "Epoch 00025: loss improved from 1.52811 to 1.52711, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f5b6204e8d0>_Batch25_LR0.001_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5271 - accuracy: 0.3418 - val_loss: 1.4991 - val_accuracy: 0.3399\n",
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
            "Epoch 1/25\n",
            "  2/595 [..............................] - ETA: 39s - loss: 2.0196 - accuracy: 0.0800WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0155s vs `on_train_batch_end` time: 0.1178s). Check your callbacks.\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5473 - accuracy: 0.3327\n",
            "Epoch 00001: loss improved from inf to 1.54702, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.rmsprop.RMSprop object at 0x7f59cc45efd0>_Batch25_LR0.001_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5470 - accuracy: 0.3329 - val_loss: 1.5068 - val_accuracy: 0.3399\n",
            "Epoch 2/25\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5284 - accuracy: 0.3414\n",
            "Epoch 00002: loss improved from 1.54702 to 1.52771, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.rmsprop.RMSprop object at 0x7f59cc45efd0>_Batch25_LR0.001_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5277 - accuracy: 0.3420 - val_loss: 1.4969 - val_accuracy: 0.3399\n",
            "Epoch 3/25\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.5241 - accuracy: 0.3425\n",
            "Epoch 00003: loss improved from 1.52771 to 1.52421, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.rmsprop.RMSprop object at 0x7f59cc45efd0>_Batch25_LR0.001_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5242 - accuracy: 0.3423 - val_loss: 1.5041 - val_accuracy: 0.3399\n",
            "Epoch 4/25\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5199 - accuracy: 0.3434\n",
            "Epoch 00004: loss improved from 1.52421 to 1.52004, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.rmsprop.RMSprop object at 0x7f59cc45efd0>_Batch25_LR0.001_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5200 - accuracy: 0.3435 - val_loss: 1.4919 - val_accuracy: 0.3442\n",
            "Epoch 5/25\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.5180 - accuracy: 0.3408\n",
            "Epoch 00005: loss improved from 1.52004 to 1.51803, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.rmsprop.RMSprop object at 0x7f59cc45efd0>_Batch25_LR0.001_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5180 - accuracy: 0.3408 - val_loss: 1.4974 - val_accuracy: 0.3358\n",
            "Epoch 6/25\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.5138 - accuracy: 0.3458\n",
            "Epoch 00006: loss improved from 1.51803 to 1.51346, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.rmsprop.RMSprop object at 0x7f59cc45efd0>_Batch25_LR0.001_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5135 - accuracy: 0.3451 - val_loss: 1.4893 - val_accuracy: 0.3415\n",
            "Epoch 7/25\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5116 - accuracy: 0.3461\n",
            "Epoch 00007: loss improved from 1.51346 to 1.51201, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.rmsprop.RMSprop object at 0x7f59cc45efd0>_Batch25_LR0.001_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5120 - accuracy: 0.3457 - val_loss: 1.4988 - val_accuracy: 0.3286\n",
            "Epoch 8/25\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5121 - accuracy: 0.3473\n",
            "Epoch 00008: loss improved from 1.51201 to 1.51195, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.rmsprop.RMSprop object at 0x7f59cc45efd0>_Batch25_LR0.001_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5120 - accuracy: 0.3475 - val_loss: 1.5162 - val_accuracy: 0.3372\n",
            "Epoch 9/25\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5079 - accuracy: 0.3443\n",
            "Epoch 00009: loss improved from 1.51195 to 1.50823, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.rmsprop.RMSprop object at 0x7f59cc45efd0>_Batch25_LR0.001_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5082 - accuracy: 0.3443 - val_loss: 1.5151 - val_accuracy: 0.3224\n",
            "Epoch 10/25\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.5068 - accuracy: 0.3448\n",
            "Epoch 00010: loss improved from 1.50823 to 1.50675, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.rmsprop.RMSprop object at 0x7f59cc45efd0>_Batch25_LR0.001_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5068 - accuracy: 0.3448 - val_loss: 1.5074 - val_accuracy: 0.3243\n",
            "Epoch 11/25\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5046 - accuracy: 0.3450\n",
            "Epoch 00011: loss improved from 1.50675 to 1.50467, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.rmsprop.RMSprop object at 0x7f59cc45efd0>_Batch25_LR0.001_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5047 - accuracy: 0.3451 - val_loss: 1.5108 - val_accuracy: 0.3229\n",
            "Epoch 12/25\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5039 - accuracy: 0.3482\n",
            "Epoch 00012: loss improved from 1.50467 to 1.50432, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.rmsprop.RMSprop object at 0x7f59cc45efd0>_Batch25_LR0.001_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5043 - accuracy: 0.3480 - val_loss: 1.4885 - val_accuracy: 0.3202\n",
            "Epoch 13/25\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.5011 - accuracy: 0.3495\n",
            "Epoch 00013: loss improved from 1.50432 to 1.50177, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.rmsprop.RMSprop object at 0x7f59cc45efd0>_Batch25_LR0.001_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5018 - accuracy: 0.3493 - val_loss: 1.4918 - val_accuracy: 0.3294\n",
            "Epoch 14/25\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5013 - accuracy: 0.3525\n",
            "Epoch 00014: loss improved from 1.50177 to 1.50128, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.rmsprop.RMSprop object at 0x7f59cc45efd0>_Batch25_LR0.001_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5013 - accuracy: 0.3525 - val_loss: 1.5210 - val_accuracy: 0.3221\n",
            "Epoch 15/25\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.4995 - accuracy: 0.3475\n",
            "Epoch 00015: loss improved from 1.50128 to 1.49957, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.rmsprop.RMSprop object at 0x7f59cc45efd0>_Batch25_LR0.001_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.4996 - accuracy: 0.3473 - val_loss: 1.4979 - val_accuracy: 0.3200\n",
            "Epoch 16/25\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.4982 - accuracy: 0.3465\n",
            "Epoch 00016: loss improved from 1.49957 to 1.49825, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.rmsprop.RMSprop object at 0x7f59cc45efd0>_Batch25_LR0.001_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.4982 - accuracy: 0.3465 - val_loss: 1.4987 - val_accuracy: 0.3305\n",
            "Epoch 17/25\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.4980 - accuracy: 0.3498\n",
            "Epoch 00017: loss improved from 1.49825 to 1.49805, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.rmsprop.RMSprop object at 0x7f59cc45efd0>_Batch25_LR0.001_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.4980 - accuracy: 0.3498 - val_loss: 1.5270 - val_accuracy: 0.3251\n",
            "Epoch 18/25\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.4963 - accuracy: 0.3502\n",
            "Epoch 00018: loss improved from 1.49805 to 1.49669, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.rmsprop.RMSprop object at 0x7f59cc45efd0>_Batch25_LR0.001_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.4967 - accuracy: 0.3495 - val_loss: 1.5085 - val_accuracy: 0.3272\n",
            "Epoch 19/25\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.4960 - accuracy: 0.3529\n",
            "Epoch 00019: loss improved from 1.49669 to 1.49595, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.rmsprop.RMSprop object at 0x7f59cc45efd0>_Batch25_LR0.001_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.4960 - accuracy: 0.3529 - val_loss: 1.5233 - val_accuracy: 0.3369\n",
            "Epoch 20/25\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.4945 - accuracy: 0.3533\n",
            "Epoch 00020: loss improved from 1.49595 to 1.49452, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.rmsprop.RMSprop object at 0x7f59cc45efd0>_Batch25_LR0.001_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.4945 - accuracy: 0.3533 - val_loss: 1.5195 - val_accuracy: 0.3267\n",
            "Epoch 21/25\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.4946 - accuracy: 0.3554\n",
            "Epoch 00021: loss improved from 1.49452 to 1.49425, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.rmsprop.RMSprop object at 0x7f59cc45efd0>_Batch25_LR0.001_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.4942 - accuracy: 0.3559 - val_loss: 1.5107 - val_accuracy: 0.3173\n",
            "Epoch 22/25\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.4921 - accuracy: 0.3575\n",
            "Epoch 00022: loss improved from 1.49425 to 1.49210, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.rmsprop.RMSprop object at 0x7f59cc45efd0>_Batch25_LR0.001_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.4921 - accuracy: 0.3574 - val_loss: 1.5283 - val_accuracy: 0.3375\n",
            "Epoch 23/25\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.4911 - accuracy: 0.3549\n",
            "Epoch 00023: loss improved from 1.49210 to 1.49081, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.rmsprop.RMSprop object at 0x7f59cc45efd0>_Batch25_LR0.001_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.4908 - accuracy: 0.3552 - val_loss: 1.5110 - val_accuracy: 0.3393\n",
            "Epoch 24/25\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.4900 - accuracy: 0.3575\n",
            "Epoch 00024: loss improved from 1.49081 to 1.48945, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.rmsprop.RMSprop object at 0x7f59cc45efd0>_Batch25_LR0.001_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.4894 - accuracy: 0.3578 - val_loss: 1.5388 - val_accuracy: 0.3122\n",
            "Epoch 25/25\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.4890 - accuracy: 0.3592\n",
            "Epoch 00025: loss improved from 1.48945 to 1.48899, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.rmsprop.RMSprop object at 0x7f59cc45efd0>_Batch25_LR0.001_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.4890 - accuracy: 0.3591 - val_loss: 1.5243 - val_accuracy: 0.3253\n",
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
            "Epoch 1/25\n",
            "  2/595 [..............................] - ETA: 34s - loss: 2.1772 - accuracy: 0.0800WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0163s vs `on_train_batch_end` time: 0.0981s). Check your callbacks.\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.5496 - accuracy: 0.3283\n",
            "Epoch 00001: loss improved from inf to 1.54961, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f5b61ded208>_Batch25_LR0.001_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5496 - accuracy: 0.3283 - val_loss: 1.5036 - val_accuracy: 0.3399\n",
            "Epoch 2/25\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5277 - accuracy: 0.3429\n",
            "Epoch 00002: loss improved from 1.54961 to 1.52753, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f5b61ded208>_Batch25_LR0.001_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5275 - accuracy: 0.3429 - val_loss: 1.4948 - val_accuracy: 0.3399\n",
            "Epoch 3/25\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.5233 - accuracy: 0.3418\n",
            "Epoch 00003: loss improved from 1.52753 to 1.52306, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f5b61ded208>_Batch25_LR0.001_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5231 - accuracy: 0.3420 - val_loss: 1.4904 - val_accuracy: 0.3224\n",
            "Epoch 4/25\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5196 - accuracy: 0.3423\n",
            "Epoch 00004: loss improved from 1.52306 to 1.51946, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f5b61ded208>_Batch25_LR0.001_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5195 - accuracy: 0.3424 - val_loss: 1.4875 - val_accuracy: 0.3372\n",
            "Epoch 5/25\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5168 - accuracy: 0.3432\n",
            "Epoch 00005: loss improved from 1.51946 to 1.51672, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f5b61ded208>_Batch25_LR0.001_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5167 - accuracy: 0.3432 - val_loss: 1.5036 - val_accuracy: 0.3372\n",
            "Epoch 6/25\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.5151 - accuracy: 0.3447\n",
            "Epoch 00006: loss improved from 1.51672 to 1.51510, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f5b61ded208>_Batch25_LR0.001_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5151 - accuracy: 0.3447 - val_loss: 1.4885 - val_accuracy: 0.3372\n",
            "Epoch 7/25\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.5128 - accuracy: 0.3461\n",
            "Epoch 00007: loss improved from 1.51510 to 1.51256, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f5b61ded208>_Batch25_LR0.001_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5126 - accuracy: 0.3463 - val_loss: 1.5083 - val_accuracy: 0.3361\n",
            "Epoch 8/25\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5114 - accuracy: 0.3440\n",
            "Epoch 00008: loss improved from 1.51256 to 1.51128, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f5b61ded208>_Batch25_LR0.001_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5113 - accuracy: 0.3439 - val_loss: 1.5006 - val_accuracy: 0.3348\n",
            "Epoch 9/25\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5086 - accuracy: 0.3468\n",
            "Epoch 00009: loss improved from 1.51128 to 1.50812, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f5b61ded208>_Batch25_LR0.001_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5081 - accuracy: 0.3469 - val_loss: 1.4871 - val_accuracy: 0.3302\n",
            "Epoch 10/25\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5065 - accuracy: 0.3476\n",
            "Epoch 00010: loss improved from 1.50812 to 1.50689, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f5b61ded208>_Batch25_LR0.001_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5069 - accuracy: 0.3473 - val_loss: 1.4937 - val_accuracy: 0.3369\n",
            "Epoch 11/25\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.5041 - accuracy: 0.3462\n",
            "Epoch 00011: loss improved from 1.50689 to 1.50414, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f5b61ded208>_Batch25_LR0.001_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5041 - accuracy: 0.3462 - val_loss: 1.4974 - val_accuracy: 0.3345\n",
            "Epoch 12/25\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5026 - accuracy: 0.3454\n",
            "Epoch 00012: loss improved from 1.50414 to 1.50225, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f5b61ded208>_Batch25_LR0.001_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5023 - accuracy: 0.3457 - val_loss: 1.4946 - val_accuracy: 0.3219\n",
            "Epoch 13/25\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5018 - accuracy: 0.3485\n",
            "Epoch 00013: loss improved from 1.50225 to 1.50216, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f5b61ded208>_Batch25_LR0.001_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5022 - accuracy: 0.3481 - val_loss: 1.4966 - val_accuracy: 0.3294\n",
            "Epoch 14/25\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.4981 - accuracy: 0.3499\n",
            "Epoch 00014: loss improved from 1.50216 to 1.49781, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f5b61ded208>_Batch25_LR0.001_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.4978 - accuracy: 0.3500 - val_loss: 1.5049 - val_accuracy: 0.3124\n",
            "Epoch 15/25\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.4988 - accuracy: 0.3509\n",
            "Epoch 00015: loss did not improve from 1.49781\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.4990 - accuracy: 0.3507 - val_loss: 1.5013 - val_accuracy: 0.3200\n",
            "Epoch 16/25\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.4970 - accuracy: 0.3486\n",
            "Epoch 00016: loss improved from 1.49781 to 1.49698, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f5b61ded208>_Batch25_LR0.001_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.4970 - accuracy: 0.3484 - val_loss: 1.4958 - val_accuracy: 0.3294\n",
            "Epoch 17/25\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.4939 - accuracy: 0.3497\n",
            "Epoch 00017: loss improved from 1.49698 to 1.49405, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f5b61ded208>_Batch25_LR0.001_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.4941 - accuracy: 0.3496 - val_loss: 1.5069 - val_accuracy: 0.3087\n",
            "Epoch 18/25\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.4930 - accuracy: 0.3517\n",
            "Epoch 00018: loss improved from 1.49405 to 1.49286, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f5b61ded208>_Batch25_LR0.001_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.4929 - accuracy: 0.3519 - val_loss: 1.4970 - val_accuracy: 0.3291\n",
            "Epoch 19/25\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.4932 - accuracy: 0.3467\n",
            "Epoch 00019: loss did not improve from 1.49286\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.4931 - accuracy: 0.3466 - val_loss: 1.4949 - val_accuracy: 0.3130\n",
            "Epoch 20/25\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.4915 - accuracy: 0.3534\n",
            "Epoch 00020: loss improved from 1.49286 to 1.49150, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f5b61ded208>_Batch25_LR0.001_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.4915 - accuracy: 0.3534 - val_loss: 1.5029 - val_accuracy: 0.3081\n",
            "Epoch 21/25\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.4885 - accuracy: 0.3549\n",
            "Epoch 00021: loss improved from 1.49150 to 1.48841, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f5b61ded208>_Batch25_LR0.001_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.4884 - accuracy: 0.3549 - val_loss: 1.5132 - val_accuracy: 0.3130\n",
            "Epoch 22/25\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.4871 - accuracy: 0.3521\n",
            "Epoch 00022: loss improved from 1.48841 to 1.48767, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f5b61ded208>_Batch25_LR0.001_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.4877 - accuracy: 0.3516 - val_loss: 1.4946 - val_accuracy: 0.3315\n",
            "Epoch 23/25\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.4849 - accuracy: 0.3528\n",
            "Epoch 00023: loss improved from 1.48767 to 1.48474, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f5b61ded208>_Batch25_LR0.001_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.4847 - accuracy: 0.3528 - val_loss: 1.5146 - val_accuracy: 0.3221\n",
            "Epoch 24/25\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.4834 - accuracy: 0.3567\n",
            "Epoch 00024: loss improved from 1.48474 to 1.48375, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f5b61ded208>_Batch25_LR0.001_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.4837 - accuracy: 0.3569 - val_loss: 1.5243 - val_accuracy: 0.3122\n",
            "Epoch 25/25\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.4833 - accuracy: 0.3565\n",
            "Epoch 00025: loss improved from 1.48375 to 1.48263, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f5b61ded208>_Batch25_LR0.001_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.4826 - accuracy: 0.3568 - val_loss: 1.5092 - val_accuracy: 0.3399\n",
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
            "Epoch 1/25\n",
            "  2/595 [..............................] - ETA: 37s - loss: 2.1424 - accuracy: 0.1400WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0170s vs `on_train_batch_end` time: 0.1080s). Check your callbacks.\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5985 - accuracy: 0.3146\n",
            "Epoch 00001: loss improved from inf to 1.59869, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adagrad.Adagrad object at 0x7f59d1b66c50>_Batch25_LR0.001_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5987 - accuracy: 0.3144 - val_loss: 1.5194 - val_accuracy: 0.3399\n",
            "Epoch 2/25\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.5618 - accuracy: 0.3284\n",
            "Epoch 00002: loss improved from 1.59869 to 1.56176, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adagrad.Adagrad object at 0x7f59d1b66c50>_Batch25_LR0.001_Epochs25.hdf5\n",
            "595/595 [==============================] - 9s 15ms/step - loss: 1.5618 - accuracy: 0.3284 - val_loss: 1.5115 - val_accuracy: 0.3399\n",
            "Epoch 3/25\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5547 - accuracy: 0.3271\n",
            "Epoch 00003: loss improved from 1.56176 to 1.55475, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adagrad.Adagrad object at 0x7f59d1b66c50>_Batch25_LR0.001_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5548 - accuracy: 0.3272 - val_loss: 1.5069 - val_accuracy: 0.3399\n",
            "Epoch 4/25\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5489 - accuracy: 0.3295\n",
            "Epoch 00004: loss improved from 1.55475 to 1.54948, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adagrad.Adagrad object at 0x7f59d1b66c50>_Batch25_LR0.001_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5495 - accuracy: 0.3291 - val_loss: 1.5093 - val_accuracy: 0.3399\n",
            "Epoch 5/25\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5484 - accuracy: 0.3290\n",
            "Epoch 00005: loss improved from 1.54948 to 1.54797, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adagrad.Adagrad object at 0x7f59d1b66c50>_Batch25_LR0.001_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5480 - accuracy: 0.3295 - val_loss: 1.5071 - val_accuracy: 0.3399\n",
            "Epoch 6/25\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5460 - accuracy: 0.3311\n",
            "Epoch 00006: loss improved from 1.54797 to 1.54594, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adagrad.Adagrad object at 0x7f59d1b66c50>_Batch25_LR0.001_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5459 - accuracy: 0.3311 - val_loss: 1.5058 - val_accuracy: 0.3399\n",
            "Epoch 7/25\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5434 - accuracy: 0.3353\n",
            "Epoch 00007: loss improved from 1.54594 to 1.54391, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adagrad.Adagrad object at 0x7f59d1b66c50>_Batch25_LR0.001_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5439 - accuracy: 0.3349 - val_loss: 1.5040 - val_accuracy: 0.3399\n",
            "Epoch 8/25\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5402 - accuracy: 0.3349\n",
            "Epoch 00008: loss improved from 1.54391 to 1.54047, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adagrad.Adagrad object at 0x7f59d1b66c50>_Batch25_LR0.001_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5405 - accuracy: 0.3346 - val_loss: 1.5042 - val_accuracy: 0.3399\n",
            "Epoch 9/25\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5377 - accuracy: 0.3320\n",
            "Epoch 00009: loss improved from 1.54047 to 1.53778, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adagrad.Adagrad object at 0x7f59d1b66c50>_Batch25_LR0.001_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5378 - accuracy: 0.3321 - val_loss: 1.5055 - val_accuracy: 0.3399\n",
            "Epoch 10/25\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5346 - accuracy: 0.3371\n",
            "Epoch 00010: loss improved from 1.53778 to 1.53518, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adagrad.Adagrad object at 0x7f59d1b66c50>_Batch25_LR0.001_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5352 - accuracy: 0.3367 - val_loss: 1.5045 - val_accuracy: 0.3399\n",
            "Epoch 11/25\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5357 - accuracy: 0.3319\n",
            "Epoch 00011: loss did not improve from 1.53518\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5357 - accuracy: 0.3319 - val_loss: 1.5063 - val_accuracy: 0.3399\n",
            "Epoch 12/25\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5368 - accuracy: 0.3347\n",
            "Epoch 00012: loss did not improve from 1.53518\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5367 - accuracy: 0.3347 - val_loss: 1.5044 - val_accuracy: 0.3399\n",
            "Epoch 13/25\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5375 - accuracy: 0.3338\n",
            "Epoch 00013: loss did not improve from 1.53518\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5377 - accuracy: 0.3339 - val_loss: 1.5049 - val_accuracy: 0.3399\n",
            "Epoch 14/25\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5358 - accuracy: 0.3367\n",
            "Epoch 00014: loss did not improve from 1.53518\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5357 - accuracy: 0.3369 - val_loss: 1.5035 - val_accuracy: 0.3399\n",
            "Epoch 15/25\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.5363 - accuracy: 0.3365\n",
            "Epoch 00015: loss did not improve from 1.53518\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5357 - accuracy: 0.3371 - val_loss: 1.5039 - val_accuracy: 0.3399\n",
            "Epoch 16/25\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5348 - accuracy: 0.3356\n",
            "Epoch 00016: loss improved from 1.53518 to 1.53487, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adagrad.Adagrad object at 0x7f59d1b66c50>_Batch25_LR0.001_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5349 - accuracy: 0.3355 - val_loss: 1.5029 - val_accuracy: 0.3399\n",
            "Epoch 17/25\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5344 - accuracy: 0.3354\n",
            "Epoch 00017: loss improved from 1.53487 to 1.53424, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adagrad.Adagrad object at 0x7f59d1b66c50>_Batch25_LR0.001_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5342 - accuracy: 0.3356 - val_loss: 1.5028 - val_accuracy: 0.3399\n",
            "Epoch 18/25\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5347 - accuracy: 0.3370\n",
            "Epoch 00018: loss did not improve from 1.53424\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5347 - accuracy: 0.3369 - val_loss: 1.5032 - val_accuracy: 0.3399\n",
            "Epoch 19/25\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.5332 - accuracy: 0.3408\n",
            "Epoch 00019: loss improved from 1.53424 to 1.53361, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adagrad.Adagrad object at 0x7f59d1b66c50>_Batch25_LR0.001_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5336 - accuracy: 0.3405 - val_loss: 1.5025 - val_accuracy: 0.3399\n",
            "Epoch 20/25\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5320 - accuracy: 0.3413\n",
            "Epoch 00020: loss improved from 1.53361 to 1.53176, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adagrad.Adagrad object at 0x7f59d1b66c50>_Batch25_LR0.001_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5318 - accuracy: 0.3415 - val_loss: 1.5033 - val_accuracy: 0.3399\n",
            "Epoch 21/25\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5315 - accuracy: 0.3372\n",
            "Epoch 00021: loss improved from 1.53176 to 1.53164, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adagrad.Adagrad object at 0x7f59d1b66c50>_Batch25_LR0.001_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5316 - accuracy: 0.3371 - val_loss: 1.5039 - val_accuracy: 0.3399\n",
            "Epoch 22/25\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5318 - accuracy: 0.3401\n",
            "Epoch 00022: loss did not improve from 1.53164\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5320 - accuracy: 0.3403 - val_loss: 1.5035 - val_accuracy: 0.3399\n",
            "Epoch 23/25\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5325 - accuracy: 0.3372\n",
            "Epoch 00023: loss did not improve from 1.53164\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5327 - accuracy: 0.3370 - val_loss: 1.5038 - val_accuracy: 0.3399\n",
            "Epoch 24/25\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5291 - accuracy: 0.3414\n",
            "Epoch 00024: loss improved from 1.53164 to 1.52933, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adagrad.Adagrad object at 0x7f59d1b66c50>_Batch25_LR0.001_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5293 - accuracy: 0.3410 - val_loss: 1.5028 - val_accuracy: 0.3399\n",
            "Epoch 25/25\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5315 - accuracy: 0.3377\n",
            "Epoch 00025: loss did not improve from 1.52933\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5317 - accuracy: 0.3380 - val_loss: 1.5038 - val_accuracy: 0.3399\n",
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
            "Epoch 1/25\n",
            "  2/595 [..............................] - ETA: 35s - loss: 2.2563 - accuracy: 0.1000WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0149s vs `on_train_batch_end` time: 0.1030s). Check your callbacks.\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.5564 - accuracy: 0.3263\n",
            "Epoch 00001: loss improved from inf to 1.55637, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adamax.Adamax object at 0x7f59c909e1d0>_Batch25_LR0.001_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5564 - accuracy: 0.3263 - val_loss: 1.5220 - val_accuracy: 0.3399\n",
            "Epoch 2/25\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5350 - accuracy: 0.3360\n",
            "Epoch 00002: loss improved from 1.55637 to 1.53484, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adamax.Adamax object at 0x7f59c909e1d0>_Batch25_LR0.001_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5348 - accuracy: 0.3358 - val_loss: 1.4980 - val_accuracy: 0.3399\n",
            "Epoch 3/25\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5294 - accuracy: 0.3424\n",
            "Epoch 00003: loss improved from 1.53484 to 1.52974, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adamax.Adamax object at 0x7f59c909e1d0>_Batch25_LR0.001_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5297 - accuracy: 0.3421 - val_loss: 1.4993 - val_accuracy: 0.3399\n",
            "Epoch 4/25\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5241 - accuracy: 0.3391\n",
            "Epoch 00004: loss improved from 1.52974 to 1.52419, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adamax.Adamax object at 0x7f59c909e1d0>_Batch25_LR0.001_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5242 - accuracy: 0.3391 - val_loss: 1.5081 - val_accuracy: 0.3399\n",
            "Epoch 5/25\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5219 - accuracy: 0.3428\n",
            "Epoch 00005: loss improved from 1.52419 to 1.52203, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adamax.Adamax object at 0x7f59c909e1d0>_Batch25_LR0.001_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5220 - accuracy: 0.3426 - val_loss: 1.4936 - val_accuracy: 0.3399\n",
            "Epoch 6/25\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5191 - accuracy: 0.3419\n",
            "Epoch 00006: loss improved from 1.52203 to 1.51928, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adamax.Adamax object at 0x7f59c909e1d0>_Batch25_LR0.001_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5193 - accuracy: 0.3418 - val_loss: 1.4924 - val_accuracy: 0.3399\n",
            "Epoch 7/25\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5171 - accuracy: 0.3422\n",
            "Epoch 00007: loss improved from 1.51928 to 1.51689, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adamax.Adamax object at 0x7f59c909e1d0>_Batch25_LR0.001_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5169 - accuracy: 0.3423 - val_loss: 1.4939 - val_accuracy: 0.3399\n",
            "Epoch 8/25\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5163 - accuracy: 0.3422\n",
            "Epoch 00008: loss improved from 1.51689 to 1.51610, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adamax.Adamax object at 0x7f59c909e1d0>_Batch25_LR0.001_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5161 - accuracy: 0.3422 - val_loss: 1.4942 - val_accuracy: 0.3372\n",
            "Epoch 9/25\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5140 - accuracy: 0.3438\n",
            "Epoch 00009: loss improved from 1.51610 to 1.51376, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adamax.Adamax object at 0x7f59c909e1d0>_Batch25_LR0.001_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5138 - accuracy: 0.3437 - val_loss: 1.4962 - val_accuracy: 0.3372\n",
            "Epoch 10/25\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5101 - accuracy: 0.3447\n",
            "Epoch 00010: loss improved from 1.51376 to 1.51026, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adamax.Adamax object at 0x7f59c909e1d0>_Batch25_LR0.001_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5103 - accuracy: 0.3445 - val_loss: 1.4937 - val_accuracy: 0.3380\n",
            "Epoch 11/25\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.5095 - accuracy: 0.3437\n",
            "Epoch 00011: loss improved from 1.51026 to 1.50954, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adamax.Adamax object at 0x7f59c909e1d0>_Batch25_LR0.001_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5095 - accuracy: 0.3437 - val_loss: 1.4921 - val_accuracy: 0.3385\n",
            "Epoch 12/25\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5082 - accuracy: 0.3401\n",
            "Epoch 00012: loss improved from 1.50954 to 1.50815, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adamax.Adamax object at 0x7f59c909e1d0>_Batch25_LR0.001_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5081 - accuracy: 0.3402 - val_loss: 1.5015 - val_accuracy: 0.3375\n",
            "Epoch 13/25\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5081 - accuracy: 0.3456\n",
            "Epoch 00013: loss improved from 1.50815 to 1.50805, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adamax.Adamax object at 0x7f59c909e1d0>_Batch25_LR0.001_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5081 - accuracy: 0.3457 - val_loss: 1.4902 - val_accuracy: 0.3364\n",
            "Epoch 14/25\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5045 - accuracy: 0.3453\n",
            "Epoch 00014: loss improved from 1.50805 to 1.50479, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adamax.Adamax object at 0x7f59c909e1d0>_Batch25_LR0.001_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5048 - accuracy: 0.3457 - val_loss: 1.4946 - val_accuracy: 0.3356\n",
            "Epoch 15/25\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5048 - accuracy: 0.3452\n",
            "Epoch 00015: loss did not improve from 1.50479\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5049 - accuracy: 0.3452 - val_loss: 1.4905 - val_accuracy: 0.3361\n",
            "Epoch 16/25\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.5027 - accuracy: 0.3489\n",
            "Epoch 00016: loss improved from 1.50479 to 1.50248, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adamax.Adamax object at 0x7f59c909e1d0>_Batch25_LR0.001_Epochs25.hdf5\n",
            "595/595 [==============================] - 9s 14ms/step - loss: 1.5025 - accuracy: 0.3484 - val_loss: 1.4910 - val_accuracy: 0.3227\n",
            "Epoch 17/25\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5026 - accuracy: 0.3457\n",
            "Epoch 00017: loss did not improve from 1.50248\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5027 - accuracy: 0.3458 - val_loss: 1.4920 - val_accuracy: 0.3358\n",
            "Epoch 18/25\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5007 - accuracy: 0.3466\n",
            "Epoch 00018: loss improved from 1.50248 to 1.50078, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adamax.Adamax object at 0x7f59c909e1d0>_Batch25_LR0.001_Epochs25.hdf5\n",
            "595/595 [==============================] - 9s 15ms/step - loss: 1.5008 - accuracy: 0.3465 - val_loss: 1.4932 - val_accuracy: 0.3237\n",
            "Epoch 19/25\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5007 - accuracy: 0.3476\n",
            "Epoch 00019: loss improved from 1.50078 to 1.50071, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adamax.Adamax object at 0x7f59c909e1d0>_Batch25_LR0.001_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5007 - accuracy: 0.3472 - val_loss: 1.4969 - val_accuracy: 0.3353\n",
            "Epoch 20/25\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.4983 - accuracy: 0.3464\n",
            "Epoch 00020: loss improved from 1.50071 to 1.49821, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adamax.Adamax object at 0x7f59c909e1d0>_Batch25_LR0.001_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.4982 - accuracy: 0.3469 - val_loss: 1.5061 - val_accuracy: 0.3367\n",
            "Epoch 21/25\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.4974 - accuracy: 0.3492\n",
            "Epoch 00021: loss improved from 1.49821 to 1.49756, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adamax.Adamax object at 0x7f59c909e1d0>_Batch25_LR0.001_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.4976 - accuracy: 0.3494 - val_loss: 1.4918 - val_accuracy: 0.3410\n",
            "Epoch 22/25\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.4949 - accuracy: 0.3558\n",
            "Epoch 00022: loss improved from 1.49756 to 1.49498, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adamax.Adamax object at 0x7f59c909e1d0>_Batch25_LR0.001_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.4950 - accuracy: 0.3557 - val_loss: 1.4959 - val_accuracy: 0.3375\n",
            "Epoch 23/25\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.4952 - accuracy: 0.3508\n",
            "Epoch 00023: loss did not improve from 1.49498\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.4954 - accuracy: 0.3505 - val_loss: 1.4967 - val_accuracy: 0.3356\n",
            "Epoch 24/25\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.4957 - accuracy: 0.3497\n",
            "Epoch 00024: loss did not improve from 1.49498\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.4956 - accuracy: 0.3499 - val_loss: 1.4963 - val_accuracy: 0.3377\n",
            "Epoch 25/25\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.4926 - accuracy: 0.3525\n",
            "Epoch 00025: loss improved from 1.49498 to 1.49258, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adamax.Adamax object at 0x7f59c909e1d0>_Batch25_LR0.001_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.4926 - accuracy: 0.3526 - val_loss: 1.4902 - val_accuracy: 0.3283\n",
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
            "Epoch 1/25\n",
            "  2/595 [..............................] - ETA: 33s - loss: 2.1209 - accuracy: 0.0800WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0158s vs `on_train_batch_end` time: 0.0965s). Check your callbacks.\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.6698 - accuracy: 0.3214\n",
            "Epoch 00001: loss improved from inf to 1.66937, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.ftrl.Ftrl object at 0x7f59c8cb1e80>_Batch25_LR0.001_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.6694 - accuracy: 0.3216 - val_loss: 1.5491 - val_accuracy: 0.3399\n",
            "Epoch 2/25\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5571 - accuracy: 0.3437\n",
            "Epoch 00002: loss improved from 1.66937 to 1.55727, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.ftrl.Ftrl object at 0x7f59c8cb1e80>_Batch25_LR0.001_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5573 - accuracy: 0.3436 - val_loss: 1.5178 - val_accuracy: 0.3399\n",
            "Epoch 3/25\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5417 - accuracy: 0.3437\n",
            "Epoch 00003: loss improved from 1.55727 to 1.54183, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.ftrl.Ftrl object at 0x7f59c8cb1e80>_Batch25_LR0.001_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5418 - accuracy: 0.3436 - val_loss: 1.5095 - val_accuracy: 0.3399\n",
            "Epoch 4/25\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.5361 - accuracy: 0.3436\n",
            "Epoch 00004: loss improved from 1.54183 to 1.53613, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.ftrl.Ftrl object at 0x7f59c8cb1e80>_Batch25_LR0.001_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5361 - accuracy: 0.3436 - val_loss: 1.5045 - val_accuracy: 0.3399\n",
            "Epoch 5/25\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5332 - accuracy: 0.3438\n",
            "Epoch 00005: loss improved from 1.53613 to 1.53350, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.ftrl.Ftrl object at 0x7f59c8cb1e80>_Batch25_LR0.001_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5335 - accuracy: 0.3436 - val_loss: 1.5022 - val_accuracy: 0.3399\n",
            "Epoch 6/25\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5329 - accuracy: 0.3436\n",
            "Epoch 00006: loss improved from 1.53350 to 1.53267, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.ftrl.Ftrl object at 0x7f59c8cb1e80>_Batch25_LR0.001_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5327 - accuracy: 0.3436 - val_loss: 1.5012 - val_accuracy: 0.3399\n",
            "Epoch 7/25\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5309 - accuracy: 0.3436\n",
            "Epoch 00007: loss improved from 1.53267 to 1.53082, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.ftrl.Ftrl object at 0x7f59c8cb1e80>_Batch25_LR0.001_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5308 - accuracy: 0.3436 - val_loss: 1.4997 - val_accuracy: 0.3399\n",
            "Epoch 8/25\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.5305 - accuracy: 0.3437\n",
            "Epoch 00008: loss improved from 1.53082 to 1.53056, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.ftrl.Ftrl object at 0x7f59c8cb1e80>_Batch25_LR0.001_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5306 - accuracy: 0.3436 - val_loss: 1.4995 - val_accuracy: 0.3399\n",
            "Epoch 9/25\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5303 - accuracy: 0.3432\n",
            "Epoch 00009: loss improved from 1.53056 to 1.52996, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.ftrl.Ftrl object at 0x7f59c8cb1e80>_Batch25_LR0.001_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5300 - accuracy: 0.3436 - val_loss: 1.4995 - val_accuracy: 0.3399\n",
            "Epoch 10/25\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5296 - accuracy: 0.3436\n",
            "Epoch 00010: loss improved from 1.52996 to 1.52963, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.ftrl.Ftrl object at 0x7f59c8cb1e80>_Batch25_LR0.001_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5296 - accuracy: 0.3436 - val_loss: 1.4984 - val_accuracy: 0.3399\n",
            "Epoch 11/25\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5298 - accuracy: 0.3438\n",
            "Epoch 00011: loss did not improve from 1.52963\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5297 - accuracy: 0.3436 - val_loss: 1.4986 - val_accuracy: 0.3399\n",
            "Epoch 12/25\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5284 - accuracy: 0.3436\n",
            "Epoch 00012: loss improved from 1.52963 to 1.52869, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.ftrl.Ftrl object at 0x7f59c8cb1e80>_Batch25_LR0.001_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5287 - accuracy: 0.3436 - val_loss: 1.4984 - val_accuracy: 0.3399\n",
            "Epoch 13/25\n",
            "589/595 [============================>.] - ETA: 0s - loss: 1.5289 - accuracy: 0.3438\n",
            "Epoch 00013: loss did not improve from 1.52869\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5289 - accuracy: 0.3436 - val_loss: 1.4982 - val_accuracy: 0.3399\n",
            "Epoch 14/25\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5291 - accuracy: 0.3441\n",
            "Epoch 00014: loss did not improve from 1.52869\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5294 - accuracy: 0.3436 - val_loss: 1.4981 - val_accuracy: 0.3399\n",
            "Epoch 15/25\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5292 - accuracy: 0.3434\n",
            "Epoch 00015: loss did not improve from 1.52869\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5291 - accuracy: 0.3436 - val_loss: 1.4987 - val_accuracy: 0.3399\n",
            "Epoch 16/25\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5290 - accuracy: 0.3437\n",
            "Epoch 00016: loss did not improve from 1.52869\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5291 - accuracy: 0.3436 - val_loss: 1.4984 - val_accuracy: 0.3399\n",
            "Epoch 17/25\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.5290 - accuracy: 0.3436\n",
            "Epoch 00017: loss did not improve from 1.52869\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5290 - accuracy: 0.3436 - val_loss: 1.4980 - val_accuracy: 0.3399\n",
            "Epoch 18/25\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5293 - accuracy: 0.3434\n",
            "Epoch 00018: loss did not improve from 1.52869\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5292 - accuracy: 0.3436 - val_loss: 1.4981 - val_accuracy: 0.3399\n",
            "Epoch 19/25\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.5284 - accuracy: 0.3436\n",
            "Epoch 00019: loss improved from 1.52869 to 1.52842, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.ftrl.Ftrl object at 0x7f59c8cb1e80>_Batch25_LR0.001_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5284 - accuracy: 0.3436 - val_loss: 1.4979 - val_accuracy: 0.3399\n",
            "Epoch 20/25\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5287 - accuracy: 0.3438\n",
            "Epoch 00020: loss did not improve from 1.52842\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5285 - accuracy: 0.3437 - val_loss: 1.4980 - val_accuracy: 0.3399\n",
            "Epoch 21/25\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5288 - accuracy: 0.3436\n",
            "Epoch 00021: loss did not improve from 1.52842\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5287 - accuracy: 0.3436 - val_loss: 1.4978 - val_accuracy: 0.3399\n",
            "Epoch 22/25\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5292 - accuracy: 0.3433\n",
            "Epoch 00022: loss did not improve from 1.52842\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5291 - accuracy: 0.3436 - val_loss: 1.4980 - val_accuracy: 0.3399\n",
            "Epoch 23/25\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5290 - accuracy: 0.3436\n",
            "Epoch 00023: loss did not improve from 1.52842\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5291 - accuracy: 0.3436 - val_loss: 1.4981 - val_accuracy: 0.3399\n",
            "Epoch 24/25\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.5285 - accuracy: 0.3436\n",
            "Epoch 00024: loss did not improve from 1.52842\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5285 - accuracy: 0.3436 - val_loss: 1.4979 - val_accuracy: 0.3399\n",
            "Epoch 25/25\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5289 - accuracy: 0.3436\n",
            "Epoch 00025: loss did not improve from 1.52842\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5287 - accuracy: 0.3436 - val_loss: 1.4977 - val_accuracy: 0.3399\n",
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
            "Epoch 1/25\n",
            "  2/595 [..............................] - ETA: 40s - loss: 2.1576 - accuracy: 0.0600WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0153s vs `on_train_batch_end` time: 0.1207s). Check your callbacks.\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.5562 - accuracy: 0.3308\n",
            "Epoch 00001: loss improved from inf to 1.55579, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f59c8dc0f60>_Batch25_LR0.01_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5558 - accuracy: 0.3309 - val_loss: 1.4990 - val_accuracy: 0.3399\n",
            "Epoch 2/25\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.5389 - accuracy: 0.3372\n",
            "Epoch 00002: loss improved from 1.55579 to 1.53883, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f59c8dc0f60>_Batch25_LR0.01_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5388 - accuracy: 0.3374 - val_loss: 1.5072 - val_accuracy: 0.3399\n",
            "Epoch 3/25\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.5325 - accuracy: 0.3380\n",
            "Epoch 00003: loss improved from 1.53883 to 1.53253, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f59c8dc0f60>_Batch25_LR0.01_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5325 - accuracy: 0.3380 - val_loss: 1.5163 - val_accuracy: 0.3399\n",
            "Epoch 4/25\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5314 - accuracy: 0.3427\n",
            "Epoch 00004: loss improved from 1.53253 to 1.53152, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f59c8dc0f60>_Batch25_LR0.01_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5315 - accuracy: 0.3425 - val_loss: 1.5035 - val_accuracy: 0.3399\n",
            "Epoch 5/25\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5286 - accuracy: 0.3412\n",
            "Epoch 00005: loss improved from 1.53152 to 1.52848, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f59c8dc0f60>_Batch25_LR0.01_Epochs25.hdf5\n",
            "595/595 [==============================] - 9s 15ms/step - loss: 1.5285 - accuracy: 0.3415 - val_loss: 1.5018 - val_accuracy: 0.3399\n",
            "Epoch 6/25\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5268 - accuracy: 0.3419\n",
            "Epoch 00006: loss improved from 1.52848 to 1.52661, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f59c8dc0f60>_Batch25_LR0.01_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5266 - accuracy: 0.3420 - val_loss: 1.4989 - val_accuracy: 0.3399\n",
            "Epoch 7/25\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.5255 - accuracy: 0.3433\n",
            "Epoch 00007: loss improved from 1.52661 to 1.52547, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f59c8dc0f60>_Batch25_LR0.01_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5255 - accuracy: 0.3433 - val_loss: 1.5056 - val_accuracy: 0.3399\n",
            "Epoch 8/25\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.5247 - accuracy: 0.3436\n",
            "Epoch 00008: loss improved from 1.52547 to 1.52494, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f59c8dc0f60>_Batch25_LR0.01_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5249 - accuracy: 0.3434 - val_loss: 1.5007 - val_accuracy: 0.3399\n",
            "Epoch 9/25\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.5257 - accuracy: 0.3439\n",
            "Epoch 00009: loss did not improve from 1.52494\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5254 - accuracy: 0.3444 - val_loss: 1.5045 - val_accuracy: 0.3399\n",
            "Epoch 10/25\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5248 - accuracy: 0.3420\n",
            "Epoch 00010: loss improved from 1.52494 to 1.52479, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f59c8dc0f60>_Batch25_LR0.01_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5248 - accuracy: 0.3418 - val_loss: 1.4934 - val_accuracy: 0.3396\n",
            "Epoch 11/25\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5240 - accuracy: 0.3430\n",
            "Epoch 00011: loss improved from 1.52479 to 1.52407, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f59c8dc0f60>_Batch25_LR0.01_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5241 - accuracy: 0.3428 - val_loss: 1.5073 - val_accuracy: 0.3399\n",
            "Epoch 12/25\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.5242 - accuracy: 0.3437\n",
            "Epoch 00012: loss improved from 1.52407 to 1.52375, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f59c8dc0f60>_Batch25_LR0.01_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5238 - accuracy: 0.3440 - val_loss: 1.5033 - val_accuracy: 0.3399\n",
            "Epoch 13/25\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5226 - accuracy: 0.3438\n",
            "Epoch 00013: loss improved from 1.52375 to 1.52267, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f59c8dc0f60>_Batch25_LR0.01_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5227 - accuracy: 0.3434 - val_loss: 1.4944 - val_accuracy: 0.3399\n",
            "Epoch 14/25\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.5216 - accuracy: 0.3436\n",
            "Epoch 00014: loss improved from 1.52267 to 1.52157, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f59c8dc0f60>_Batch25_LR0.01_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5216 - accuracy: 0.3436 - val_loss: 1.4969 - val_accuracy: 0.3399\n",
            "Epoch 15/25\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5215 - accuracy: 0.3424\n",
            "Epoch 00015: loss did not improve from 1.52157\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5217 - accuracy: 0.3422 - val_loss: 1.5020 - val_accuracy: 0.3399\n",
            "Epoch 16/25\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.5214 - accuracy: 0.3443\n",
            "Epoch 00016: loss did not improve from 1.52157\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5217 - accuracy: 0.3442 - val_loss: 1.4994 - val_accuracy: 0.3399\n",
            "Epoch 17/25\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5212 - accuracy: 0.3424\n",
            "Epoch 00017: loss improved from 1.52157 to 1.52112, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f59c8dc0f60>_Batch25_LR0.01_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5211 - accuracy: 0.3424 - val_loss: 1.4951 - val_accuracy: 0.3399\n",
            "Epoch 18/25\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5206 - accuracy: 0.3428\n",
            "Epoch 00018: loss improved from 1.52112 to 1.52049, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f59c8dc0f60>_Batch25_LR0.01_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5205 - accuracy: 0.3429 - val_loss: 1.4941 - val_accuracy: 0.3399\n",
            "Epoch 19/25\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.5202 - accuracy: 0.3428\n",
            "Epoch 00019: loss improved from 1.52049 to 1.52019, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f59c8dc0f60>_Batch25_LR0.01_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5202 - accuracy: 0.3428 - val_loss: 1.5056 - val_accuracy: 0.3399\n",
            "Epoch 20/25\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5191 - accuracy: 0.3427\n",
            "Epoch 00020: loss improved from 1.52019 to 1.51922, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f59c8dc0f60>_Batch25_LR0.01_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5192 - accuracy: 0.3430 - val_loss: 1.5182 - val_accuracy: 0.3399\n",
            "Epoch 21/25\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.5188 - accuracy: 0.3432\n",
            "Epoch 00021: loss improved from 1.51922 to 1.51883, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f59c8dc0f60>_Batch25_LR0.01_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5188 - accuracy: 0.3432 - val_loss: 1.4992 - val_accuracy: 0.3399\n",
            "Epoch 22/25\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.5177 - accuracy: 0.3433\n",
            "Epoch 00022: loss improved from 1.51883 to 1.51818, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f59c8dc0f60>_Batch25_LR0.01_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5182 - accuracy: 0.3432 - val_loss: 1.5033 - val_accuracy: 0.3399\n",
            "Epoch 23/25\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5161 - accuracy: 0.3443\n",
            "Epoch 00023: loss improved from 1.51818 to 1.51637, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f59c8dc0f60>_Batch25_LR0.01_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5164 - accuracy: 0.3447 - val_loss: 1.5086 - val_accuracy: 0.3399\n",
            "Epoch 24/25\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5167 - accuracy: 0.3447\n",
            "Epoch 00024: loss did not improve from 1.51637\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5168 - accuracy: 0.3446 - val_loss: 1.4969 - val_accuracy: 0.3399\n",
            "Epoch 25/25\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5174 - accuracy: 0.3442\n",
            "Epoch 00025: loss did not improve from 1.51637\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5173 - accuracy: 0.3443 - val_loss: 1.5104 - val_accuracy: 0.3399\n",
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
            "Epoch 1/25\n",
            "  2/595 [..............................] - ETA: 50s - loss: 3.5273 - accuracy: 0.2000WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0208s vs `on_train_batch_end` time: 0.1441s). Check your callbacks.\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.5615 - accuracy: 0.3328\n",
            "Epoch 00001: loss improved from inf to 1.56149, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.rmsprop.RMSprop object at 0x7f59c8aa62e8>_Batch25_LR0.01_Epochs25.hdf5\n",
            "595/595 [==============================] - 9s 14ms/step - loss: 1.5615 - accuracy: 0.3328 - val_loss: 1.5115 - val_accuracy: 0.3399\n",
            "Epoch 2/25\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5301 - accuracy: 0.3432\n",
            "Epoch 00002: loss improved from 1.56149 to 1.52966, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.rmsprop.RMSprop object at 0x7f59c8aa62e8>_Batch25_LR0.01_Epochs25.hdf5\n",
            "595/595 [==============================] - 9s 14ms/step - loss: 1.5297 - accuracy: 0.3436 - val_loss: 1.4937 - val_accuracy: 0.3399\n",
            "Epoch 3/25\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.5294 - accuracy: 0.3435\n",
            "Epoch 00003: loss improved from 1.52966 to 1.52937, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.rmsprop.RMSprop object at 0x7f59c8aa62e8>_Batch25_LR0.01_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5294 - accuracy: 0.3436 - val_loss: 1.4940 - val_accuracy: 0.3399\n",
            "Epoch 4/25\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5294 - accuracy: 0.3441\n",
            "Epoch 00004: loss did not improve from 1.52937\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5295 - accuracy: 0.3436 - val_loss: 1.4967 - val_accuracy: 0.3399\n",
            "Epoch 5/25\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5297 - accuracy: 0.3436\n",
            "Epoch 00005: loss did not improve from 1.52937\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5295 - accuracy: 0.3436 - val_loss: 1.4932 - val_accuracy: 0.3399\n",
            "Epoch 6/25\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5296 - accuracy: 0.3437\n",
            "Epoch 00006: loss did not improve from 1.52937\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5297 - accuracy: 0.3436 - val_loss: 1.4963 - val_accuracy: 0.3399\n",
            "Epoch 7/25\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.5295 - accuracy: 0.3439\n",
            "Epoch 00007: loss did not improve from 1.52937\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5295 - accuracy: 0.3436 - val_loss: 1.4966 - val_accuracy: 0.3399\n",
            "Epoch 8/25\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5300 - accuracy: 0.3431\n",
            "Epoch 00008: loss did not improve from 1.52937\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5298 - accuracy: 0.3436 - val_loss: 1.4957 - val_accuracy: 0.3399\n",
            "Epoch 9/25\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5299 - accuracy: 0.3436\n",
            "Epoch 00009: loss did not improve from 1.52937\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5297 - accuracy: 0.3436 - val_loss: 1.4959 - val_accuracy: 0.3399\n",
            "Epoch 10/25\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.5302 - accuracy: 0.3436\n",
            "Epoch 00010: loss did not improve from 1.52937\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5302 - accuracy: 0.3436 - val_loss: 1.4923 - val_accuracy: 0.3399\n",
            "Epoch 11/25\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5304 - accuracy: 0.3436\n",
            "Epoch 00011: loss did not improve from 1.52937\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5305 - accuracy: 0.3436 - val_loss: 1.4912 - val_accuracy: 0.3399\n",
            "Epoch 12/25\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5307 - accuracy: 0.3435\n",
            "Epoch 00012: loss did not improve from 1.52937\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5306 - accuracy: 0.3436 - val_loss: 1.4987 - val_accuracy: 0.3399\n",
            "Epoch 13/25\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.5302 - accuracy: 0.3436\n",
            "Epoch 00013: loss did not improve from 1.52937\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5302 - accuracy: 0.3436 - val_loss: 1.4929 - val_accuracy: 0.3399\n",
            "Epoch 14/25\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5303 - accuracy: 0.3438\n",
            "Epoch 00014: loss did not improve from 1.52937\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5302 - accuracy: 0.3436 - val_loss: 1.4968 - val_accuracy: 0.3399\n",
            "Epoch 15/25\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.5299 - accuracy: 0.3436\n",
            "Epoch 00015: loss did not improve from 1.52937\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5299 - accuracy: 0.3436 - val_loss: 1.4945 - val_accuracy: 0.3399\n",
            "Epoch 16/25\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5303 - accuracy: 0.3436\n",
            "Epoch 00016: loss did not improve from 1.52937\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5303 - accuracy: 0.3436 - val_loss: 1.4920 - val_accuracy: 0.3399\n",
            "Epoch 17/25\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.5296 - accuracy: 0.3439\n",
            "Epoch 00017: loss did not improve from 1.52937\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5299 - accuracy: 0.3436 - val_loss: 1.4910 - val_accuracy: 0.3399\n",
            "Epoch 18/25\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5309 - accuracy: 0.3438\n",
            "Epoch 00018: loss did not improve from 1.52937\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5310 - accuracy: 0.3436 - val_loss: 1.4943 - val_accuracy: 0.3399\n",
            "Epoch 19/25\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5307 - accuracy: 0.3431\n",
            "Epoch 00019: loss did not improve from 1.52937\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5303 - accuracy: 0.3436 - val_loss: 1.4973 - val_accuracy: 0.3399\n",
            "Epoch 20/25\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5301 - accuracy: 0.3438\n",
            "Epoch 00020: loss did not improve from 1.52937\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5301 - accuracy: 0.3436 - val_loss: 1.4909 - val_accuracy: 0.3399\n",
            "Epoch 21/25\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5302 - accuracy: 0.3439\n",
            "Epoch 00021: loss did not improve from 1.52937\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5301 - accuracy: 0.3436 - val_loss: 1.4938 - val_accuracy: 0.3399\n",
            "Epoch 22/25\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5303 - accuracy: 0.3432\n",
            "Epoch 00022: loss did not improve from 1.52937\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5298 - accuracy: 0.3436 - val_loss: 1.4922 - val_accuracy: 0.3399\n",
            "Epoch 23/25\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.5295 - accuracy: 0.3436\n",
            "Epoch 00023: loss did not improve from 1.52937\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5295 - accuracy: 0.3436 - val_loss: 1.4925 - val_accuracy: 0.3399\n",
            "Epoch 24/25\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.5305 - accuracy: 0.3432\n",
            "Epoch 00024: loss did not improve from 1.52937\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5303 - accuracy: 0.3436 - val_loss: 1.4978 - val_accuracy: 0.3399\n",
            "Epoch 25/25\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5299 - accuracy: 0.3436\n",
            "Epoch 00025: loss did not improve from 1.52937\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5297 - accuracy: 0.3436 - val_loss: 1.4975 - val_accuracy: 0.3399\n",
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
            "Epoch 1/25\n",
            "  2/595 [..............................] - ETA: 33s - loss: 1.9792 - accuracy: 0.2200WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0152s vs `on_train_batch_end` time: 0.1007s). Check your callbacks.\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5536 - accuracy: 0.3335\n",
            "Epoch 00001: loss improved from inf to 1.55335, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f59c8bc5828>_Batch25_LR0.01_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5533 - accuracy: 0.3338 - val_loss: 1.4947 - val_accuracy: 0.3323\n",
            "Epoch 2/25\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5327 - accuracy: 0.3410\n",
            "Epoch 00002: loss improved from 1.55335 to 1.53266, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f59c8bc5828>_Batch25_LR0.01_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5327 - accuracy: 0.3411 - val_loss: 1.5036 - val_accuracy: 0.3399\n",
            "Epoch 3/25\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5295 - accuracy: 0.3437\n",
            "Epoch 00003: loss improved from 1.53266 to 1.52941, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f59c8bc5828>_Batch25_LR0.01_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5294 - accuracy: 0.3436 - val_loss: 1.4982 - val_accuracy: 0.3399\n",
            "Epoch 4/25\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5288 - accuracy: 0.3439\n",
            "Epoch 00004: loss improved from 1.52941 to 1.52909, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f59c8bc5828>_Batch25_LR0.01_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5291 - accuracy: 0.3436 - val_loss: 1.4991 - val_accuracy: 0.3399\n",
            "Epoch 5/25\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5289 - accuracy: 0.3434\n",
            "Epoch 00005: loss improved from 1.52909 to 1.52907, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f59c8bc5828>_Batch25_LR0.01_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5291 - accuracy: 0.3436 - val_loss: 1.4947 - val_accuracy: 0.3399\n",
            "Epoch 6/25\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.5291 - accuracy: 0.3436\n",
            "Epoch 00006: loss did not improve from 1.52907\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5291 - accuracy: 0.3436 - val_loss: 1.4954 - val_accuracy: 0.3399\n",
            "Epoch 7/25\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.5288 - accuracy: 0.3436\n",
            "Epoch 00007: loss improved from 1.52907 to 1.52881, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f59c8bc5828>_Batch25_LR0.01_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5288 - accuracy: 0.3436 - val_loss: 1.4999 - val_accuracy: 0.3399\n",
            "Epoch 8/25\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5288 - accuracy: 0.3437\n",
            "Epoch 00008: loss did not improve from 1.52881\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5289 - accuracy: 0.3436 - val_loss: 1.4978 - val_accuracy: 0.3399\n",
            "Epoch 9/25\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5293 - accuracy: 0.3436\n",
            "Epoch 00009: loss did not improve from 1.52881\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5292 - accuracy: 0.3436 - val_loss: 1.5000 - val_accuracy: 0.3399\n",
            "Epoch 10/25\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.5291 - accuracy: 0.3436\n",
            "Epoch 00010: loss did not improve from 1.52881\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5291 - accuracy: 0.3436 - val_loss: 1.4912 - val_accuracy: 0.3399\n",
            "Epoch 11/25\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.5289 - accuracy: 0.3436\n",
            "Epoch 00011: loss did not improve from 1.52881\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5289 - accuracy: 0.3436 - val_loss: 1.4949 - val_accuracy: 0.3399\n",
            "Epoch 12/25\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5291 - accuracy: 0.3432\n",
            "Epoch 00012: loss did not improve from 1.52881\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5290 - accuracy: 0.3436 - val_loss: 1.4972 - val_accuracy: 0.3399\n",
            "Epoch 13/25\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.5291 - accuracy: 0.3434\n",
            "Epoch 00013: loss did not improve from 1.52881\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5290 - accuracy: 0.3436 - val_loss: 1.4924 - val_accuracy: 0.3399\n",
            "Epoch 14/25\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.5294 - accuracy: 0.3435\n",
            "Epoch 00014: loss did not improve from 1.52881\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5293 - accuracy: 0.3436 - val_loss: 1.4974 - val_accuracy: 0.3399\n",
            "Epoch 15/25\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5289 - accuracy: 0.3438\n",
            "Epoch 00015: loss did not improve from 1.52881\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5288 - accuracy: 0.3436 - val_loss: 1.5003 - val_accuracy: 0.3399\n",
            "Epoch 16/25\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5285 - accuracy: 0.3440\n",
            "Epoch 00016: loss improved from 1.52881 to 1.52861, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f59c8bc5828>_Batch25_LR0.01_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5286 - accuracy: 0.3436 - val_loss: 1.4977 - val_accuracy: 0.3399\n",
            "Epoch 17/25\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5289 - accuracy: 0.3435\n",
            "Epoch 00017: loss did not improve from 1.52861\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5292 - accuracy: 0.3436 - val_loss: 1.4991 - val_accuracy: 0.3399\n",
            "Epoch 18/25\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5294 - accuracy: 0.3436\n",
            "Epoch 00018: loss did not improve from 1.52861\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5293 - accuracy: 0.3436 - val_loss: 1.4997 - val_accuracy: 0.3399\n",
            "Epoch 19/25\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5286 - accuracy: 0.3438\n",
            "Epoch 00019: loss did not improve from 1.52861\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5289 - accuracy: 0.3436 - val_loss: 1.4967 - val_accuracy: 0.3399\n",
            "Epoch 20/25\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5289 - accuracy: 0.3437\n",
            "Epoch 00020: loss did not improve from 1.52861\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5291 - accuracy: 0.3436 - val_loss: 1.5008 - val_accuracy: 0.3399\n",
            "Epoch 21/25\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5289 - accuracy: 0.3438\n",
            "Epoch 00021: loss did not improve from 1.52861\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5290 - accuracy: 0.3436 - val_loss: 1.4944 - val_accuracy: 0.3399\n",
            "Epoch 22/25\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5288 - accuracy: 0.3437\n",
            "Epoch 00022: loss did not improve from 1.52861\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5289 - accuracy: 0.3436 - val_loss: 1.4968 - val_accuracy: 0.3399\n",
            "Epoch 23/25\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5291 - accuracy: 0.3436\n",
            "Epoch 00023: loss did not improve from 1.52861\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5289 - accuracy: 0.3436 - val_loss: 1.4975 - val_accuracy: 0.3399\n",
            "Epoch 24/25\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.5289 - accuracy: 0.3436\n",
            "Epoch 00024: loss did not improve from 1.52861\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5289 - accuracy: 0.3436 - val_loss: 1.4962 - val_accuracy: 0.3399\n",
            "Epoch 25/25\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5288 - accuracy: 0.3440\n",
            "Epoch 00025: loss did not improve from 1.52861\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5287 - accuracy: 0.3436 - val_loss: 1.4947 - val_accuracy: 0.3399\n",
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
            "Epoch 1/25\n",
            "  2/595 [..............................] - ETA: 34s - loss: 2.0010 - accuracy: 0.1400WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0165s vs `on_train_batch_end` time: 0.0975s). Check your callbacks.\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.5469 - accuracy: 0.3292\n",
            "Epoch 00001: loss improved from inf to 1.54695, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adagrad.Adagrad object at 0x7f59c8ff0e80>_Batch25_LR0.01_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5469 - accuracy: 0.3292 - val_loss: 1.5165 - val_accuracy: 0.3399\n",
            "Epoch 2/25\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5330 - accuracy: 0.3392\n",
            "Epoch 00002: loss improved from 1.54695 to 1.53307, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adagrad.Adagrad object at 0x7f59c8ff0e80>_Batch25_LR0.01_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5331 - accuracy: 0.3391 - val_loss: 1.5040 - val_accuracy: 0.3399\n",
            "Epoch 3/25\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5280 - accuracy: 0.3436\n",
            "Epoch 00003: loss improved from 1.53307 to 1.52826, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adagrad.Adagrad object at 0x7f59c8ff0e80>_Batch25_LR0.01_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5283 - accuracy: 0.3430 - val_loss: 1.5046 - val_accuracy: 0.3399\n",
            "Epoch 4/25\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.5257 - accuracy: 0.3422\n",
            "Epoch 00004: loss improved from 1.52826 to 1.52569, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adagrad.Adagrad object at 0x7f59c8ff0e80>_Batch25_LR0.01_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5257 - accuracy: 0.3422 - val_loss: 1.5107 - val_accuracy: 0.3399\n",
            "Epoch 5/25\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.5236 - accuracy: 0.3441\n",
            "Epoch 00005: loss improved from 1.52569 to 1.52361, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adagrad.Adagrad object at 0x7f59c8ff0e80>_Batch25_LR0.01_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5236 - accuracy: 0.3441 - val_loss: 1.4989 - val_accuracy: 0.3399\n",
            "Epoch 6/25\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5225 - accuracy: 0.3432\n",
            "Epoch 00006: loss improved from 1.52361 to 1.52262, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adagrad.Adagrad object at 0x7f59c8ff0e80>_Batch25_LR0.01_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5226 - accuracy: 0.3433 - val_loss: 1.5001 - val_accuracy: 0.3399\n",
            "Epoch 7/25\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5218 - accuracy: 0.3446\n",
            "Epoch 00007: loss improved from 1.52262 to 1.52158, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adagrad.Adagrad object at 0x7f59c8ff0e80>_Batch25_LR0.01_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5216 - accuracy: 0.3443 - val_loss: 1.4977 - val_accuracy: 0.3399\n",
            "Epoch 8/25\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.5198 - accuracy: 0.3424\n",
            "Epoch 00008: loss improved from 1.52158 to 1.51971, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adagrad.Adagrad object at 0x7f59c8ff0e80>_Batch25_LR0.01_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5197 - accuracy: 0.3428 - val_loss: 1.5040 - val_accuracy: 0.3399\n",
            "Epoch 9/25\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.5189 - accuracy: 0.3431\n",
            "Epoch 00009: loss improved from 1.51971 to 1.51921, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adagrad.Adagrad object at 0x7f59c8ff0e80>_Batch25_LR0.01_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5192 - accuracy: 0.3434 - val_loss: 1.4987 - val_accuracy: 0.3399\n",
            "Epoch 10/25\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.5179 - accuracy: 0.3433\n",
            "Epoch 00010: loss improved from 1.51921 to 1.51790, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adagrad.Adagrad object at 0x7f59c8ff0e80>_Batch25_LR0.01_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5179 - accuracy: 0.3433 - val_loss: 1.4942 - val_accuracy: 0.3399\n",
            "Epoch 11/25\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5169 - accuracy: 0.3432\n",
            "Epoch 00011: loss improved from 1.51790 to 1.51679, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adagrad.Adagrad object at 0x7f59c8ff0e80>_Batch25_LR0.01_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5168 - accuracy: 0.3434 - val_loss: 1.4967 - val_accuracy: 0.3399\n",
            "Epoch 12/25\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.5161 - accuracy: 0.3459\n",
            "Epoch 00012: loss improved from 1.51679 to 1.51613, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adagrad.Adagrad object at 0x7f59c8ff0e80>_Batch25_LR0.01_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5161 - accuracy: 0.3459 - val_loss: 1.4969 - val_accuracy: 0.3399\n",
            "Epoch 13/25\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5153 - accuracy: 0.3431\n",
            "Epoch 00013: loss improved from 1.51613 to 1.51543, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adagrad.Adagrad object at 0x7f59c8ff0e80>_Batch25_LR0.01_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5154 - accuracy: 0.3430 - val_loss: 1.4968 - val_accuracy: 0.3399\n",
            "Epoch 14/25\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5139 - accuracy: 0.3450\n",
            "Epoch 00014: loss improved from 1.51543 to 1.51426, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adagrad.Adagrad object at 0x7f59c8ff0e80>_Batch25_LR0.01_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5143 - accuracy: 0.3448 - val_loss: 1.4954 - val_accuracy: 0.3399\n",
            "Epoch 15/25\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5142 - accuracy: 0.3452\n",
            "Epoch 00015: loss did not improve from 1.51426\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5143 - accuracy: 0.3451 - val_loss: 1.4909 - val_accuracy: 0.3399\n",
            "Epoch 16/25\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5143 - accuracy: 0.3444\n",
            "Epoch 00016: loss improved from 1.51426 to 1.51413, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adagrad.Adagrad object at 0x7f59c8ff0e80>_Batch25_LR0.01_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5141 - accuracy: 0.3443 - val_loss: 1.4950 - val_accuracy: 0.3399\n",
            "Epoch 17/25\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5128 - accuracy: 0.3468\n",
            "Epoch 00017: loss improved from 1.51413 to 1.51348, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adagrad.Adagrad object at 0x7f59c8ff0e80>_Batch25_LR0.01_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5135 - accuracy: 0.3463 - val_loss: 1.4960 - val_accuracy: 0.3396\n",
            "Epoch 18/25\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.5123 - accuracy: 0.3435\n",
            "Epoch 00018: loss improved from 1.51348 to 1.51233, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adagrad.Adagrad object at 0x7f59c8ff0e80>_Batch25_LR0.01_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5123 - accuracy: 0.3435 - val_loss: 1.4957 - val_accuracy: 0.3399\n",
            "Epoch 19/25\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5118 - accuracy: 0.3470\n",
            "Epoch 00019: loss improved from 1.51233 to 1.51209, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adagrad.Adagrad object at 0x7f59c8ff0e80>_Batch25_LR0.01_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5121 - accuracy: 0.3469 - val_loss: 1.4970 - val_accuracy: 0.3361\n",
            "Epoch 20/25\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5123 - accuracy: 0.3465\n",
            "Epoch 00020: loss did not improve from 1.51209\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5122 - accuracy: 0.3466 - val_loss: 1.4968 - val_accuracy: 0.3399\n",
            "Epoch 21/25\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5118 - accuracy: 0.3465\n",
            "Epoch 00021: loss improved from 1.51209 to 1.51166, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adagrad.Adagrad object at 0x7f59c8ff0e80>_Batch25_LR0.01_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5117 - accuracy: 0.3469 - val_loss: 1.4954 - val_accuracy: 0.3399\n",
            "Epoch 22/25\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5104 - accuracy: 0.3455\n",
            "Epoch 00022: loss improved from 1.51166 to 1.51054, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adagrad.Adagrad object at 0x7f59c8ff0e80>_Batch25_LR0.01_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5105 - accuracy: 0.3456 - val_loss: 1.4963 - val_accuracy: 0.3372\n",
            "Epoch 23/25\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5105 - accuracy: 0.3460\n",
            "Epoch 00023: loss did not improve from 1.51054\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5107 - accuracy: 0.3459 - val_loss: 1.4945 - val_accuracy: 0.3348\n",
            "Epoch 24/25\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5088 - accuracy: 0.3446\n",
            "Epoch 00024: loss improved from 1.51054 to 1.50878, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adagrad.Adagrad object at 0x7f59c8ff0e80>_Batch25_LR0.01_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5088 - accuracy: 0.3447 - val_loss: 1.4971 - val_accuracy: 0.3369\n",
            "Epoch 25/25\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.5100 - accuracy: 0.3434\n",
            "Epoch 00025: loss did not improve from 1.50878\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5100 - accuracy: 0.3434 - val_loss: 1.4946 - val_accuracy: 0.3358\n",
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
            "Epoch 1/25\n",
            "  2/595 [..............................] - ETA: 43s - loss: 2.1160 - accuracy: 0.2600WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0177s vs `on_train_batch_end` time: 0.1277s). Check your callbacks.\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5451 - accuracy: 0.3324\n",
            "Epoch 00001: loss improved from inf to 1.54523, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adamax.Adamax object at 0x7f59c8bcde48>_Batch25_LR0.01_Epochs25.hdf5\n",
            "595/595 [==============================] - 9s 14ms/step - loss: 1.5452 - accuracy: 0.3324 - val_loss: 1.5013 - val_accuracy: 0.3399\n",
            "Epoch 2/25\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5313 - accuracy: 0.3397\n",
            "Epoch 00002: loss improved from 1.54523 to 1.53107, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adamax.Adamax object at 0x7f59c8bcde48>_Batch25_LR0.01_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5311 - accuracy: 0.3400 - val_loss: 1.4999 - val_accuracy: 0.3399\n",
            "Epoch 3/25\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.5271 - accuracy: 0.3432\n",
            "Epoch 00003: loss improved from 1.53107 to 1.52712, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adamax.Adamax object at 0x7f59c8bcde48>_Batch25_LR0.01_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5271 - accuracy: 0.3432 - val_loss: 1.4883 - val_accuracy: 0.3399\n",
            "Epoch 4/25\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.5238 - accuracy: 0.3425\n",
            "Epoch 00004: loss improved from 1.52712 to 1.52378, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adamax.Adamax object at 0x7f59c8bcde48>_Batch25_LR0.01_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5238 - accuracy: 0.3425 - val_loss: 1.4862 - val_accuracy: 0.3372\n",
            "Epoch 5/25\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5213 - accuracy: 0.3443\n",
            "Epoch 00005: loss improved from 1.52378 to 1.52140, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adamax.Adamax object at 0x7f59c8bcde48>_Batch25_LR0.01_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5214 - accuracy: 0.3441 - val_loss: 1.4954 - val_accuracy: 0.3372\n",
            "Epoch 6/25\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5191 - accuracy: 0.3420\n",
            "Epoch 00006: loss improved from 1.52140 to 1.51938, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adamax.Adamax object at 0x7f59c8bcde48>_Batch25_LR0.01_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5194 - accuracy: 0.3418 - val_loss: 1.4909 - val_accuracy: 0.3372\n",
            "Epoch 7/25\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.5154 - accuracy: 0.3408\n",
            "Epoch 00007: loss improved from 1.51938 to 1.51537, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adamax.Adamax object at 0x7f59c8bcde48>_Batch25_LR0.01_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5154 - accuracy: 0.3408 - val_loss: 1.4971 - val_accuracy: 0.3380\n",
            "Epoch 8/25\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5163 - accuracy: 0.3457\n",
            "Epoch 00008: loss did not improve from 1.51537\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5169 - accuracy: 0.3457 - val_loss: 1.5035 - val_accuracy: 0.3380\n",
            "Epoch 9/25\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.5146 - accuracy: 0.3439\n",
            "Epoch 00009: loss improved from 1.51537 to 1.51445, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adamax.Adamax object at 0x7f59c8bcde48>_Batch25_LR0.01_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5144 - accuracy: 0.3443 - val_loss: 1.5064 - val_accuracy: 0.3396\n",
            "Epoch 10/25\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5111 - accuracy: 0.3425\n",
            "Epoch 00010: loss improved from 1.51445 to 1.51114, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adamax.Adamax object at 0x7f59c8bcde48>_Batch25_LR0.01_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5111 - accuracy: 0.3427 - val_loss: 1.5258 - val_accuracy: 0.3334\n",
            "Epoch 11/25\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5105 - accuracy: 0.3425\n",
            "Epoch 00011: loss improved from 1.51114 to 1.51018, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adamax.Adamax object at 0x7f59c8bcde48>_Batch25_LR0.01_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5102 - accuracy: 0.3428 - val_loss: 1.4969 - val_accuracy: 0.3219\n",
            "Epoch 12/25\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5077 - accuracy: 0.3461\n",
            "Epoch 00012: loss improved from 1.51018 to 1.50776, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adamax.Adamax object at 0x7f59c8bcde48>_Batch25_LR0.01_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5078 - accuracy: 0.3461 - val_loss: 1.4837 - val_accuracy: 0.3399\n",
            "Epoch 13/25\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5055 - accuracy: 0.3414\n",
            "Epoch 00013: loss improved from 1.50776 to 1.50534, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adamax.Adamax object at 0x7f59c8bcde48>_Batch25_LR0.01_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5053 - accuracy: 0.3412 - val_loss: 1.4943 - val_accuracy: 0.3186\n",
            "Epoch 14/25\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5063 - accuracy: 0.3429\n",
            "Epoch 00014: loss did not improve from 1.50534\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5060 - accuracy: 0.3428 - val_loss: 1.4863 - val_accuracy: 0.3259\n",
            "Epoch 15/25\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5054 - accuracy: 0.3469\n",
            "Epoch 00015: loss did not improve from 1.50534\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5059 - accuracy: 0.3471 - val_loss: 1.4820 - val_accuracy: 0.3332\n",
            "Epoch 16/25\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5019 - accuracy: 0.3453\n",
            "Epoch 00016: loss improved from 1.50534 to 1.50112, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adamax.Adamax object at 0x7f59c8bcde48>_Batch25_LR0.01_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5011 - accuracy: 0.3459 - val_loss: 1.4956 - val_accuracy: 0.3332\n",
            "Epoch 17/25\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.5033 - accuracy: 0.3467\n",
            "Epoch 00017: loss did not improve from 1.50112\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5033 - accuracy: 0.3467 - val_loss: 1.4913 - val_accuracy: 0.3393\n",
            "Epoch 18/25\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.4995 - accuracy: 0.3488\n",
            "Epoch 00018: loss improved from 1.50112 to 1.49953, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adamax.Adamax object at 0x7f59c8bcde48>_Batch25_LR0.01_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.4995 - accuracy: 0.3488 - val_loss: 1.4847 - val_accuracy: 0.3307\n",
            "Epoch 19/25\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.4992 - accuracy: 0.3455\n",
            "Epoch 00019: loss did not improve from 1.49953\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.4996 - accuracy: 0.3453 - val_loss: 1.4983 - val_accuracy: 0.3057\n",
            "Epoch 20/25\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.4983 - accuracy: 0.3489\n",
            "Epoch 00020: loss improved from 1.49953 to 1.49826, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adamax.Adamax object at 0x7f59c8bcde48>_Batch25_LR0.01_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.4983 - accuracy: 0.3488 - val_loss: 1.4890 - val_accuracy: 0.3280\n",
            "Epoch 21/25\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.4995 - accuracy: 0.3503\n",
            "Epoch 00021: loss did not improve from 1.49826\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.4990 - accuracy: 0.3504 - val_loss: 1.4883 - val_accuracy: 0.3280\n",
            "Epoch 22/25\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.4998 - accuracy: 0.3504\n",
            "Epoch 00022: loss did not improve from 1.49826\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.4998 - accuracy: 0.3504 - val_loss: 1.4912 - val_accuracy: 0.3272\n",
            "Epoch 23/25\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.4991 - accuracy: 0.3470\n",
            "Epoch 00023: loss did not improve from 1.49826\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.4993 - accuracy: 0.3469 - val_loss: 1.4950 - val_accuracy: 0.3114\n",
            "Epoch 24/25\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.4965 - accuracy: 0.3500\n",
            "Epoch 00024: loss improved from 1.49826 to 1.49655, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adamax.Adamax object at 0x7f59c8bcde48>_Batch25_LR0.01_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.4965 - accuracy: 0.3500 - val_loss: 1.4787 - val_accuracy: 0.3272\n",
            "Epoch 25/25\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.4977 - accuracy: 0.3492\n",
            "Epoch 00025: loss did not improve from 1.49655\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.4978 - accuracy: 0.3492 - val_loss: 1.5022 - val_accuracy: 0.3253\n",
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
            "Epoch 1/25\n",
            "  2/595 [..............................] - ETA: 44s - loss: 2.1071 - accuracy: 0.1400WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0151s vs `on_train_batch_end` time: 0.1349s). Check your callbacks.\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.5449 - accuracy: 0.3418\n",
            "Epoch 00001: loss improved from inf to 1.54494, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.ftrl.Ftrl object at 0x7f59c45e9fd0>_Batch25_LR0.01_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5449 - accuracy: 0.3418 - val_loss: 1.4955 - val_accuracy: 0.3399\n",
            "Epoch 2/25\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5317 - accuracy: 0.3436\n",
            "Epoch 00002: loss improved from 1.54494 to 1.53172, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.ftrl.Ftrl object at 0x7f59c45e9fd0>_Batch25_LR0.01_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5317 - accuracy: 0.3436 - val_loss: 1.4993 - val_accuracy: 0.3399\n",
            "Epoch 3/25\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5292 - accuracy: 0.3438\n",
            "Epoch 00003: loss improved from 1.53172 to 1.52936, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.ftrl.Ftrl object at 0x7f59c45e9fd0>_Batch25_LR0.01_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5294 - accuracy: 0.3436 - val_loss: 1.5000 - val_accuracy: 0.3399\n",
            "Epoch 4/25\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5290 - accuracy: 0.3432\n",
            "Epoch 00004: loss improved from 1.52936 to 1.52877, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.ftrl.Ftrl object at 0x7f59c45e9fd0>_Batch25_LR0.01_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5288 - accuracy: 0.3436 - val_loss: 1.4933 - val_accuracy: 0.3399\n",
            "Epoch 5/25\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.5294 - accuracy: 0.3435\n",
            "Epoch 00005: loss did not improve from 1.52877\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5296 - accuracy: 0.3436 - val_loss: 1.4981 - val_accuracy: 0.3399\n",
            "Epoch 6/25\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5287 - accuracy: 0.3438\n",
            "Epoch 00006: loss improved from 1.52877 to 1.52867, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.ftrl.Ftrl object at 0x7f59c45e9fd0>_Batch25_LR0.01_Epochs25.hdf5\n",
            "595/595 [==============================] - 9s 14ms/step - loss: 1.5287 - accuracy: 0.3436 - val_loss: 1.4990 - val_accuracy: 0.3399\n",
            "Epoch 7/25\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.5292 - accuracy: 0.3435\n",
            "Epoch 00007: loss did not improve from 1.52867\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5287 - accuracy: 0.3436 - val_loss: 1.4936 - val_accuracy: 0.3399\n",
            "Epoch 8/25\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.5287 - accuracy: 0.3436\n",
            "Epoch 00008: loss did not improve from 1.52867\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5287 - accuracy: 0.3436 - val_loss: 1.5011 - val_accuracy: 0.3399\n",
            "Epoch 9/25\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5286 - accuracy: 0.3434\n",
            "Epoch 00009: loss improved from 1.52867 to 1.52862, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.ftrl.Ftrl object at 0x7f59c45e9fd0>_Batch25_LR0.01_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5286 - accuracy: 0.3436 - val_loss: 1.5033 - val_accuracy: 0.3399\n",
            "Epoch 10/25\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5274 - accuracy: 0.3436\n",
            "Epoch 00010: loss improved from 1.52862 to 1.52737, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.ftrl.Ftrl object at 0x7f59c45e9fd0>_Batch25_LR0.01_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5274 - accuracy: 0.3436 - val_loss: 1.5012 - val_accuracy: 0.3399\n",
            "Epoch 11/25\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5278 - accuracy: 0.3436\n",
            "Epoch 00011: loss did not improve from 1.52737\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5276 - accuracy: 0.3436 - val_loss: 1.4964 - val_accuracy: 0.3399\n",
            "Epoch 12/25\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5262 - accuracy: 0.3437\n",
            "Epoch 00012: loss improved from 1.52737 to 1.52624, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.ftrl.Ftrl object at 0x7f59c45e9fd0>_Batch25_LR0.01_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5262 - accuracy: 0.3436 - val_loss: 1.4975 - val_accuracy: 0.3399\n",
            "Epoch 13/25\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5259 - accuracy: 0.3437\n",
            "Epoch 00013: loss improved from 1.52624 to 1.52591, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.ftrl.Ftrl object at 0x7f59c45e9fd0>_Batch25_LR0.01_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5259 - accuracy: 0.3436 - val_loss: 1.5051 - val_accuracy: 0.3399\n",
            "Epoch 14/25\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5253 - accuracy: 0.3434\n",
            "Epoch 00014: loss improved from 1.52591 to 1.52517, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.ftrl.Ftrl object at 0x7f59c45e9fd0>_Batch25_LR0.01_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5252 - accuracy: 0.3436 - val_loss: 1.4986 - val_accuracy: 0.3399\n",
            "Epoch 15/25\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.5258 - accuracy: 0.3429\n",
            "Epoch 00015: loss did not improve from 1.52517\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5252 - accuracy: 0.3436 - val_loss: 1.4991 - val_accuracy: 0.3399\n",
            "Epoch 16/25\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5249 - accuracy: 0.3436\n",
            "Epoch 00016: loss improved from 1.52517 to 1.52484, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.ftrl.Ftrl object at 0x7f59c45e9fd0>_Batch25_LR0.01_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5248 - accuracy: 0.3436 - val_loss: 1.4971 - val_accuracy: 0.3399\n",
            "Epoch 17/25\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5242 - accuracy: 0.3440\n",
            "Epoch 00017: loss improved from 1.52484 to 1.52430, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.ftrl.Ftrl object at 0x7f59c45e9fd0>_Batch25_LR0.01_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5243 - accuracy: 0.3439 - val_loss: 1.5003 - val_accuracy: 0.3399\n",
            "Epoch 18/25\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5237 - accuracy: 0.3437\n",
            "Epoch 00018: loss improved from 1.52430 to 1.52390, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.ftrl.Ftrl object at 0x7f59c45e9fd0>_Batch25_LR0.01_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5239 - accuracy: 0.3434 - val_loss: 1.5019 - val_accuracy: 0.3399\n",
            "Epoch 19/25\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5235 - accuracy: 0.3436\n",
            "Epoch 00019: loss improved from 1.52390 to 1.52346, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.ftrl.Ftrl object at 0x7f59c45e9fd0>_Batch25_LR0.01_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5235 - accuracy: 0.3435 - val_loss: 1.5084 - val_accuracy: 0.3399\n",
            "Epoch 20/25\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.5232 - accuracy: 0.3433\n",
            "Epoch 00020: loss improved from 1.52346 to 1.52335, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.ftrl.Ftrl object at 0x7f59c45e9fd0>_Batch25_LR0.01_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5233 - accuracy: 0.3436 - val_loss: 1.5022 - val_accuracy: 0.3399\n",
            "Epoch 21/25\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5231 - accuracy: 0.3435\n",
            "Epoch 00021: loss improved from 1.52335 to 1.52314, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.ftrl.Ftrl object at 0x7f59c45e9fd0>_Batch25_LR0.01_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5231 - accuracy: 0.3437 - val_loss: 1.5025 - val_accuracy: 0.3399\n",
            "Epoch 22/25\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.5234 - accuracy: 0.3435\n",
            "Epoch 00022: loss improved from 1.52314 to 1.52314, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.ftrl.Ftrl object at 0x7f59c45e9fd0>_Batch25_LR0.01_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5231 - accuracy: 0.3436 - val_loss: 1.5015 - val_accuracy: 0.3399\n",
            "Epoch 23/25\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5216 - accuracy: 0.3439\n",
            "Epoch 00023: loss improved from 1.52314 to 1.52189, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.ftrl.Ftrl object at 0x7f59c45e9fd0>_Batch25_LR0.01_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5219 - accuracy: 0.3436 - val_loss: 1.5018 - val_accuracy: 0.3399\n",
            "Epoch 24/25\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.5214 - accuracy: 0.3433\n",
            "Epoch 00024: loss improved from 1.52189 to 1.52137, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.ftrl.Ftrl object at 0x7f59c45e9fd0>_Batch25_LR0.01_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5214 - accuracy: 0.3433 - val_loss: 1.4987 - val_accuracy: 0.3399\n",
            "Epoch 25/25\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.5217 - accuracy: 0.3437\n",
            "Epoch 00025: loss did not improve from 1.52137\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5219 - accuracy: 0.3434 - val_loss: 1.4994 - val_accuracy: 0.3399\n",
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
            "Epoch 1/25\n",
            "  2/595 [..............................] - ETA: 32s - loss: 2.4005 - accuracy: 0.1800WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0155s vs `on_train_batch_end` time: 0.0944s). Check your callbacks.\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5523 - accuracy: 0.3327\n",
            "Epoch 00001: loss improved from inf to 1.55214, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f59c422fa58>_Batch25_LR0.1_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5521 - accuracy: 0.3328 - val_loss: 1.4925 - val_accuracy: 0.3399\n",
            "Epoch 2/25\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5367 - accuracy: 0.3376\n",
            "Epoch 00002: loss improved from 1.55214 to 1.53694, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f59c422fa58>_Batch25_LR0.1_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5369 - accuracy: 0.3373 - val_loss: 1.5014 - val_accuracy: 0.3399\n",
            "Epoch 3/25\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.5341 - accuracy: 0.3391\n",
            "Epoch 00003: loss improved from 1.53694 to 1.53411, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f59c422fa58>_Batch25_LR0.1_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5341 - accuracy: 0.3391 - val_loss: 1.5084 - val_accuracy: 0.3399\n",
            "Epoch 4/25\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.5334 - accuracy: 0.3396\n",
            "Epoch 00004: loss improved from 1.53411 to 1.53265, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f59c422fa58>_Batch25_LR0.1_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5327 - accuracy: 0.3397 - val_loss: 1.4998 - val_accuracy: 0.3022\n",
            "Epoch 5/25\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.5321 - accuracy: 0.3426\n",
            "Epoch 00005: loss improved from 1.53265 to 1.53207, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f59c422fa58>_Batch25_LR0.1_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5321 - accuracy: 0.3423 - val_loss: 1.4992 - val_accuracy: 0.3399\n",
            "Epoch 6/25\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.5302 - accuracy: 0.3426\n",
            "Epoch 00006: loss improved from 1.53207 to 1.53019, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f59c422fa58>_Batch25_LR0.1_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5302 - accuracy: 0.3428 - val_loss: 1.5037 - val_accuracy: 0.3399\n",
            "Epoch 7/25\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5271 - accuracy: 0.3424\n",
            "Epoch 00007: loss improved from 1.53019 to 1.52725, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f59c422fa58>_Batch25_LR0.1_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5273 - accuracy: 0.3421 - val_loss: 1.5022 - val_accuracy: 0.3399\n",
            "Epoch 8/25\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5264 - accuracy: 0.3433\n",
            "Epoch 00008: loss improved from 1.52725 to 1.52629, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f59c422fa58>_Batch25_LR0.1_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5263 - accuracy: 0.3431 - val_loss: 1.4939 - val_accuracy: 0.3399\n",
            "Epoch 9/25\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5257 - accuracy: 0.3422\n",
            "Epoch 00009: loss improved from 1.52629 to 1.52526, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f59c422fa58>_Batch25_LR0.1_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5253 - accuracy: 0.3426 - val_loss: 1.4927 - val_accuracy: 0.3399\n",
            "Epoch 10/25\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5230 - accuracy: 0.3430\n",
            "Epoch 00010: loss improved from 1.52526 to 1.52312, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f59c422fa58>_Batch25_LR0.1_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5231 - accuracy: 0.3428 - val_loss: 1.4917 - val_accuracy: 0.3399\n",
            "Epoch 11/25\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.5228 - accuracy: 0.3435\n",
            "Epoch 00011: loss improved from 1.52312 to 1.52273, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f59c422fa58>_Batch25_LR0.1_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5227 - accuracy: 0.3434 - val_loss: 1.5113 - val_accuracy: 0.3375\n",
            "Epoch 12/25\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5224 - accuracy: 0.3436\n",
            "Epoch 00012: loss improved from 1.52273 to 1.52245, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f59c422fa58>_Batch25_LR0.1_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5225 - accuracy: 0.3438 - val_loss: 1.4849 - val_accuracy: 0.3372\n",
            "Epoch 13/25\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5213 - accuracy: 0.3436\n",
            "Epoch 00013: loss improved from 1.52245 to 1.52138, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f59c422fa58>_Batch25_LR0.1_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5214 - accuracy: 0.3436 - val_loss: 1.4899 - val_accuracy: 0.3399\n",
            "Epoch 14/25\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5225 - accuracy: 0.3437\n",
            "Epoch 00014: loss did not improve from 1.52138\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5225 - accuracy: 0.3438 - val_loss: 1.5007 - val_accuracy: 0.3372\n",
            "Epoch 15/25\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.5214 - accuracy: 0.3436\n",
            "Epoch 00015: loss improved from 1.52138 to 1.52136, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f59c422fa58>_Batch25_LR0.1_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5214 - accuracy: 0.3436 - val_loss: 1.4860 - val_accuracy: 0.3372\n",
            "Epoch 16/25\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5194 - accuracy: 0.3440\n",
            "Epoch 00016: loss improved from 1.52136 to 1.51948, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f59c422fa58>_Batch25_LR0.1_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5195 - accuracy: 0.3439 - val_loss: 1.4967 - val_accuracy: 0.3391\n",
            "Epoch 17/25\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5200 - accuracy: 0.3429\n",
            "Epoch 00017: loss did not improve from 1.51948\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5202 - accuracy: 0.3430 - val_loss: 1.5008 - val_accuracy: 0.3372\n",
            "Epoch 18/25\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5203 - accuracy: 0.3435\n",
            "Epoch 00018: loss did not improve from 1.51948\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5198 - accuracy: 0.3441 - val_loss: 1.4962 - val_accuracy: 0.3399\n",
            "Epoch 19/25\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5209 - accuracy: 0.3438\n",
            "Epoch 00019: loss did not improve from 1.51948\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5206 - accuracy: 0.3438 - val_loss: 1.4920 - val_accuracy: 0.3399\n",
            "Epoch 20/25\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5191 - accuracy: 0.3434\n",
            "Epoch 00020: loss improved from 1.51948 to 1.51934, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f59c422fa58>_Batch25_LR0.1_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5193 - accuracy: 0.3434 - val_loss: 1.5000 - val_accuracy: 0.3372\n",
            "Epoch 21/25\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5197 - accuracy: 0.3425\n",
            "Epoch 00021: loss did not improve from 1.51934\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5196 - accuracy: 0.3430 - val_loss: 1.5052 - val_accuracy: 0.3240\n",
            "Epoch 22/25\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5190 - accuracy: 0.3434\n",
            "Epoch 00022: loss improved from 1.51934 to 1.51900, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f59c422fa58>_Batch25_LR0.1_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5190 - accuracy: 0.3436 - val_loss: 1.5070 - val_accuracy: 0.3375\n",
            "Epoch 23/25\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5182 - accuracy: 0.3441\n",
            "Epoch 00023: loss improved from 1.51900 to 1.51820, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f59c422fa58>_Batch25_LR0.1_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5182 - accuracy: 0.3439 - val_loss: 1.5042 - val_accuracy: 0.3291\n",
            "Epoch 24/25\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5170 - accuracy: 0.3444\n",
            "Epoch 00024: loss improved from 1.51820 to 1.51781, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f59c422fa58>_Batch25_LR0.1_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5178 - accuracy: 0.3442 - val_loss: 1.4884 - val_accuracy: 0.3372\n",
            "Epoch 25/25\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.5179 - accuracy: 0.3443\n",
            "Epoch 00025: loss did not improve from 1.51781\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5179 - accuracy: 0.3443 - val_loss: 1.5057 - val_accuracy: 0.3375\n",
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
            "Epoch 1/25\n",
            "  2/595 [..............................] - ETA: 2:01 - loss: 46.0505 - accuracy: 0.2800WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0155s vs `on_train_batch_end` time: 0.3925s). Check your callbacks.\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.8809 - accuracy: 0.3316\n",
            "Epoch 00001: loss improved from inf to 1.87838, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.rmsprop.RMSprop object at 0x7f59beee5978>_Batch25_LR0.1_Epochs25.hdf5\n",
            "595/595 [==============================] - 9s 15ms/step - loss: 1.8784 - accuracy: 0.3318 - val_loss: 1.4994 - val_accuracy: 0.3399\n",
            "Epoch 2/25\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5389 - accuracy: 0.3349\n",
            "Epoch 00002: loss improved from 1.87838 to 1.53868, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.rmsprop.RMSprop object at 0x7f59beee5978>_Batch25_LR0.1_Epochs25.hdf5\n",
            "595/595 [==============================] - 9s 14ms/step - loss: 1.5387 - accuracy: 0.3350 - val_loss: 1.5049 - val_accuracy: 0.3399\n",
            "Epoch 3/25\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5392 - accuracy: 0.3371\n",
            "Epoch 00003: loss did not improve from 1.53868\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5393 - accuracy: 0.3371 - val_loss: 1.5011 - val_accuracy: 0.3399\n",
            "Epoch 4/25\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.5402 - accuracy: 0.3350\n",
            "Epoch 00004: loss did not improve from 1.53868\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5402 - accuracy: 0.3350 - val_loss: 1.4915 - val_accuracy: 0.3399\n",
            "Epoch 5/25\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.5401 - accuracy: 0.3361\n",
            "Epoch 00005: loss did not improve from 1.53868\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5401 - accuracy: 0.3361 - val_loss: 1.5057 - val_accuracy: 0.3399\n",
            "Epoch 6/25\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5379 - accuracy: 0.3353\n",
            "Epoch 00006: loss improved from 1.53868 to 1.53809, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.rmsprop.RMSprop object at 0x7f59beee5978>_Batch25_LR0.1_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5381 - accuracy: 0.3352 - val_loss: 1.5420 - val_accuracy: 0.3399\n",
            "Epoch 7/25\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5385 - accuracy: 0.3364\n",
            "Epoch 00007: loss did not improve from 1.53809\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5385 - accuracy: 0.3364 - val_loss: 1.4961 - val_accuracy: 0.3399\n",
            "Epoch 8/25\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5392 - accuracy: 0.3366\n",
            "Epoch 00008: loss did not improve from 1.53809\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5393 - accuracy: 0.3365 - val_loss: 1.5192 - val_accuracy: 0.3022\n",
            "Epoch 9/25\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5373 - accuracy: 0.3354\n",
            "Epoch 00009: loss improved from 1.53809 to 1.53736, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.rmsprop.RMSprop object at 0x7f59beee5978>_Batch25_LR0.1_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5374 - accuracy: 0.3352 - val_loss: 1.5028 - val_accuracy: 0.3022\n",
            "Epoch 10/25\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.5391 - accuracy: 0.3346\n",
            "Epoch 00010: loss did not improve from 1.53736\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5391 - accuracy: 0.3346 - val_loss: 1.4968 - val_accuracy: 0.3399\n",
            "Epoch 11/25\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.5378 - accuracy: 0.3357\n",
            "Epoch 00011: loss did not improve from 1.53736\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5377 - accuracy: 0.3360 - val_loss: 1.4977 - val_accuracy: 0.3399\n",
            "Epoch 12/25\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5391 - accuracy: 0.3364\n",
            "Epoch 00012: loss did not improve from 1.53736\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5391 - accuracy: 0.3365 - val_loss: 1.5129 - val_accuracy: 0.3399\n",
            "Epoch 13/25\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.5384 - accuracy: 0.3349\n",
            "Epoch 00013: loss did not improve from 1.53736\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5385 - accuracy: 0.3350 - val_loss: 1.4928 - val_accuracy: 0.3399\n",
            "Epoch 14/25\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.5377 - accuracy: 0.3335\n",
            "Epoch 00014: loss did not improve from 1.53736\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5377 - accuracy: 0.3330 - val_loss: 1.4991 - val_accuracy: 0.3022\n",
            "Epoch 15/25\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5387 - accuracy: 0.3345\n",
            "Epoch 00015: loss did not improve from 1.53736\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5386 - accuracy: 0.3346 - val_loss: 1.5230 - val_accuracy: 0.3399\n",
            "Epoch 16/25\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5385 - accuracy: 0.3353\n",
            "Epoch 00016: loss did not improve from 1.53736\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5388 - accuracy: 0.3351 - val_loss: 1.5194 - val_accuracy: 0.3399\n",
            "Epoch 17/25\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5385 - accuracy: 0.3355\n",
            "Epoch 00017: loss did not improve from 1.53736\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5390 - accuracy: 0.3350 - val_loss: 1.5304 - val_accuracy: 0.3399\n",
            "Epoch 18/25\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5398 - accuracy: 0.3348\n",
            "Epoch 00018: loss did not improve from 1.53736\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5399 - accuracy: 0.3348 - val_loss: 1.4900 - val_accuracy: 0.3399\n",
            "Epoch 19/25\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.5393 - accuracy: 0.3346\n",
            "Epoch 00019: loss did not improve from 1.53736\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5399 - accuracy: 0.3346 - val_loss: 1.4903 - val_accuracy: 0.3399\n",
            "Epoch 20/25\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.5390 - accuracy: 0.3349\n",
            "Epoch 00020: loss did not improve from 1.53736\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5390 - accuracy: 0.3349 - val_loss: 1.5239 - val_accuracy: 0.3399\n",
            "Epoch 21/25\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.5383 - accuracy: 0.3354\n",
            "Epoch 00021: loss did not improve from 1.53736\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5385 - accuracy: 0.3358 - val_loss: 1.5022 - val_accuracy: 0.3399\n",
            "Epoch 22/25\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.5387 - accuracy: 0.3363\n",
            "Epoch 00022: loss did not improve from 1.53736\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5385 - accuracy: 0.3367 - val_loss: 1.4938 - val_accuracy: 0.3399\n",
            "Epoch 23/25\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5402 - accuracy: 0.3379\n",
            "Epoch 00023: loss did not improve from 1.53736\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5400 - accuracy: 0.3381 - val_loss: 1.5240 - val_accuracy: 0.3399\n",
            "Epoch 24/25\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5399 - accuracy: 0.3337\n",
            "Epoch 00024: loss did not improve from 1.53736\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5397 - accuracy: 0.3336 - val_loss: 1.4899 - val_accuracy: 0.3399\n",
            "Epoch 25/25\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5388 - accuracy: 0.3331\n",
            "Epoch 00025: loss did not improve from 1.53736\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5388 - accuracy: 0.3331 - val_loss: 1.5058 - val_accuracy: 0.3399\n",
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
            "Epoch 1/25\n",
            "  2/595 [..............................] - ETA: 34s - loss: 9.3191 - accuracy: 0.2000WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0168s vs `on_train_batch_end` time: 0.0999s). Check your callbacks.\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.6430 - accuracy: 0.3368\n",
            "Epoch 00001: loss improved from inf to 1.64303, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f59bed616d8>_Batch25_LR0.1_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.6430 - accuracy: 0.3368 - val_loss: 1.5229 - val_accuracy: 0.3399\n",
            "Epoch 2/25\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5355 - accuracy: 0.3358\n",
            "Epoch 00002: loss improved from 1.64303 to 1.53606, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f59bed616d8>_Batch25_LR0.1_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5361 - accuracy: 0.3355 - val_loss: 1.5037 - val_accuracy: 0.3399\n",
            "Epoch 3/25\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5376 - accuracy: 0.3331\n",
            "Epoch 00003: loss did not improve from 1.53606\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5375 - accuracy: 0.3332 - val_loss: 1.4957 - val_accuracy: 0.3399\n",
            "Epoch 4/25\n",
            "589/595 [============================>.] - ETA: 0s - loss: 1.5373 - accuracy: 0.3360\n",
            "Epoch 00004: loss did not improve from 1.53606\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5374 - accuracy: 0.3365 - val_loss: 1.5086 - val_accuracy: 0.3399\n",
            "Epoch 5/25\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5375 - accuracy: 0.3362\n",
            "Epoch 00005: loss did not improve from 1.53606\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5375 - accuracy: 0.3363 - val_loss: 1.4962 - val_accuracy: 0.3399\n",
            "Epoch 6/25\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.5397 - accuracy: 0.3355\n",
            "Epoch 00006: loss did not improve from 1.53606\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5397 - accuracy: 0.3355 - val_loss: 1.5431 - val_accuracy: 0.3399\n",
            "Epoch 7/25\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.5372 - accuracy: 0.3420\n",
            "Epoch 00007: loss did not improve from 1.53606\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5372 - accuracy: 0.3420 - val_loss: 1.4936 - val_accuracy: 0.3399\n",
            "Epoch 8/25\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5368 - accuracy: 0.3328\n",
            "Epoch 00008: loss did not improve from 1.53606\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5368 - accuracy: 0.3331 - val_loss: 1.4984 - val_accuracy: 0.3399\n",
            "Epoch 9/25\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5370 - accuracy: 0.3326\n",
            "Epoch 00009: loss did not improve from 1.53606\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5372 - accuracy: 0.3324 - val_loss: 1.4971 - val_accuracy: 0.3399\n",
            "Epoch 10/25\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5377 - accuracy: 0.3383\n",
            "Epoch 00010: loss did not improve from 1.53606\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5379 - accuracy: 0.3381 - val_loss: 1.5014 - val_accuracy: 0.3399\n",
            "Epoch 11/25\n",
            "589/595 [============================>.] - ETA: 0s - loss: 1.5373 - accuracy: 0.3362\n",
            "Epoch 00011: loss did not improve from 1.53606\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5371 - accuracy: 0.3364 - val_loss: 1.5107 - val_accuracy: 0.3399\n",
            "Epoch 12/25\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5362 - accuracy: 0.3387\n",
            "Epoch 00012: loss did not improve from 1.53606\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5361 - accuracy: 0.3389 - val_loss: 1.5404 - val_accuracy: 0.3399\n",
            "Epoch 13/25\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5357 - accuracy: 0.3388\n",
            "Epoch 00013: loss improved from 1.53606 to 1.53600, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f59bed616d8>_Batch25_LR0.1_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5360 - accuracy: 0.3385 - val_loss: 1.5036 - val_accuracy: 0.3399\n",
            "Epoch 14/25\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.5370 - accuracy: 0.3368\n",
            "Epoch 00014: loss did not improve from 1.53600\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5371 - accuracy: 0.3369 - val_loss: 1.5017 - val_accuracy: 0.3399\n",
            "Epoch 15/25\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5364 - accuracy: 0.3370\n",
            "Epoch 00015: loss did not improve from 1.53600\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5365 - accuracy: 0.3368 - val_loss: 1.4927 - val_accuracy: 0.3399\n",
            "Epoch 16/25\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5391 - accuracy: 0.3318\n",
            "Epoch 00016: loss did not improve from 1.53600\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5396 - accuracy: 0.3315 - val_loss: 1.5038 - val_accuracy: 0.3399\n",
            "Epoch 17/25\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5377 - accuracy: 0.3361\n",
            "Epoch 00017: loss did not improve from 1.53600\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5375 - accuracy: 0.3365 - val_loss: 1.5100 - val_accuracy: 0.3399\n",
            "Epoch 18/25\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5371 - accuracy: 0.3326\n",
            "Epoch 00018: loss did not improve from 1.53600\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5374 - accuracy: 0.3326 - val_loss: 1.5012 - val_accuracy: 0.3399\n",
            "Epoch 19/25\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5364 - accuracy: 0.3367\n",
            "Epoch 00019: loss did not improve from 1.53600\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5364 - accuracy: 0.3367 - val_loss: 1.4994 - val_accuracy: 0.3022\n",
            "Epoch 20/25\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5381 - accuracy: 0.3343\n",
            "Epoch 00020: loss did not improve from 1.53600\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5381 - accuracy: 0.3344 - val_loss: 1.4956 - val_accuracy: 0.3399\n",
            "Epoch 21/25\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5366 - accuracy: 0.3376\n",
            "Epoch 00021: loss did not improve from 1.53600\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5368 - accuracy: 0.3378 - val_loss: 1.5114 - val_accuracy: 0.3399\n",
            "Epoch 22/25\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5379 - accuracy: 0.3362\n",
            "Epoch 00022: loss did not improve from 1.53600\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5378 - accuracy: 0.3363 - val_loss: 1.4975 - val_accuracy: 0.3399\n",
            "Epoch 23/25\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.5374 - accuracy: 0.3362\n",
            "Epoch 00023: loss did not improve from 1.53600\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5374 - accuracy: 0.3362 - val_loss: 1.4947 - val_accuracy: 0.3399\n",
            "Epoch 24/25\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5363 - accuracy: 0.3410\n",
            "Epoch 00024: loss did not improve from 1.53600\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5367 - accuracy: 0.3406 - val_loss: 1.5083 - val_accuracy: 0.3399\n",
            "Epoch 25/25\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.5376 - accuracy: 0.3354\n",
            "Epoch 00025: loss did not improve from 1.53600\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5375 - accuracy: 0.3353 - val_loss: 1.4928 - val_accuracy: 0.3399\n",
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
            "Epoch 1/25\n",
            "  2/595 [..............................] - ETA: 33s - loss: 3.6261 - accuracy: 0.0600   WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0144s vs `on_train_batch_end` time: 0.0973s). Check your callbacks.\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.5532 - accuracy: 0.3406\n",
            "Epoch 00001: loss improved from inf to 1.55321, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adagrad.Adagrad object at 0x7f59bf042630>_Batch25_LR0.1_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5532 - accuracy: 0.3406 - val_loss: 1.4941 - val_accuracy: 0.3399\n",
            "Epoch 2/25\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.5320 - accuracy: 0.3419\n",
            "Epoch 00002: loss improved from 1.55321 to 1.53175, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adagrad.Adagrad object at 0x7f59bf042630>_Batch25_LR0.1_Epochs25.hdf5\n",
            "595/595 [==============================] - 9s 14ms/step - loss: 1.5317 - accuracy: 0.3417 - val_loss: 1.4948 - val_accuracy: 0.3399\n",
            "Epoch 3/25\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5314 - accuracy: 0.3430\n",
            "Epoch 00003: loss improved from 1.53175 to 1.53146, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adagrad.Adagrad object at 0x7f59bf042630>_Batch25_LR0.1_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5315 - accuracy: 0.3429 - val_loss: 1.4928 - val_accuracy: 0.3399\n",
            "Epoch 4/25\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.5296 - accuracy: 0.3435\n",
            "Epoch 00004: loss improved from 1.53146 to 1.52946, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adagrad.Adagrad object at 0x7f59bf042630>_Batch25_LR0.1_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5295 - accuracy: 0.3434 - val_loss: 1.5025 - val_accuracy: 0.3399\n",
            "Epoch 5/25\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.5285 - accuracy: 0.3436\n",
            "Epoch 00005: loss improved from 1.52946 to 1.52846, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adagrad.Adagrad object at 0x7f59bf042630>_Batch25_LR0.1_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5285 - accuracy: 0.3436 - val_loss: 1.5008 - val_accuracy: 0.3399\n",
            "Epoch 6/25\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5273 - accuracy: 0.3438\n",
            "Epoch 00006: loss improved from 1.52846 to 1.52747, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adagrad.Adagrad object at 0x7f59bf042630>_Batch25_LR0.1_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5275 - accuracy: 0.3436 - val_loss: 1.4985 - val_accuracy: 0.3399\n",
            "Epoch 7/25\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.5280 - accuracy: 0.3428\n",
            "Epoch 00007: loss improved from 1.52747 to 1.52710, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adagrad.Adagrad object at 0x7f59bf042630>_Batch25_LR0.1_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5271 - accuracy: 0.3436 - val_loss: 1.4995 - val_accuracy: 0.3399\n",
            "Epoch 8/25\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5249 - accuracy: 0.3439\n",
            "Epoch 00008: loss improved from 1.52710 to 1.52520, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adagrad.Adagrad object at 0x7f59bf042630>_Batch25_LR0.1_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5252 - accuracy: 0.3437 - val_loss: 1.5021 - val_accuracy: 0.3399\n",
            "Epoch 9/25\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.5242 - accuracy: 0.3437\n",
            "Epoch 00009: loss improved from 1.52520 to 1.52397, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adagrad.Adagrad object at 0x7f59bf042630>_Batch25_LR0.1_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5240 - accuracy: 0.3435 - val_loss: 1.4984 - val_accuracy: 0.3399\n",
            "Epoch 10/25\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5233 - accuracy: 0.3432\n",
            "Epoch 00010: loss improved from 1.52397 to 1.52304, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adagrad.Adagrad object at 0x7f59bf042630>_Batch25_LR0.1_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5230 - accuracy: 0.3436 - val_loss: 1.5028 - val_accuracy: 0.3399\n",
            "Epoch 11/25\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.5223 - accuracy: 0.3436\n",
            "Epoch 00011: loss improved from 1.52304 to 1.52224, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adagrad.Adagrad object at 0x7f59bf042630>_Batch25_LR0.1_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5222 - accuracy: 0.3435 - val_loss: 1.4970 - val_accuracy: 0.3399\n",
            "Epoch 12/25\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5214 - accuracy: 0.3432\n",
            "Epoch 00012: loss improved from 1.52224 to 1.52139, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adagrad.Adagrad object at 0x7f59bf042630>_Batch25_LR0.1_Epochs25.hdf5\n",
            "595/595 [==============================] - 9s 15ms/step - loss: 1.5214 - accuracy: 0.3432 - val_loss: 1.5031 - val_accuracy: 0.3399\n",
            "Epoch 13/25\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.5191 - accuracy: 0.3440\n",
            "Epoch 00013: loss improved from 1.52139 to 1.51914, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adagrad.Adagrad object at 0x7f59bf042630>_Batch25_LR0.1_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5191 - accuracy: 0.3436 - val_loss: 1.4992 - val_accuracy: 0.3399\n",
            "Epoch 14/25\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5184 - accuracy: 0.3431\n",
            "Epoch 00014: loss improved from 1.51914 to 1.51822, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adagrad.Adagrad object at 0x7f59bf042630>_Batch25_LR0.1_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5182 - accuracy: 0.3432 - val_loss: 1.4970 - val_accuracy: 0.3399\n",
            "Epoch 15/25\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5184 - accuracy: 0.3427\n",
            "Epoch 00015: loss did not improve from 1.51822\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5185 - accuracy: 0.3429 - val_loss: 1.4963 - val_accuracy: 0.3399\n",
            "Epoch 16/25\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5157 - accuracy: 0.3436\n",
            "Epoch 00016: loss improved from 1.51822 to 1.51597, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adagrad.Adagrad object at 0x7f59bf042630>_Batch25_LR0.1_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5160 - accuracy: 0.3434 - val_loss: 1.4996 - val_accuracy: 0.3399\n",
            "Epoch 17/25\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5161 - accuracy: 0.3435\n",
            "Epoch 00017: loss improved from 1.51597 to 1.51586, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adagrad.Adagrad object at 0x7f59bf042630>_Batch25_LR0.1_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5159 - accuracy: 0.3436 - val_loss: 1.5035 - val_accuracy: 0.3399\n",
            "Epoch 18/25\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5164 - accuracy: 0.3436\n",
            "Epoch 00018: loss did not improve from 1.51586\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5167 - accuracy: 0.3432 - val_loss: 1.4981 - val_accuracy: 0.3399\n",
            "Epoch 19/25\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5156 - accuracy: 0.3439\n",
            "Epoch 00019: loss did not improve from 1.51586\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5159 - accuracy: 0.3436 - val_loss: 1.4948 - val_accuracy: 0.3262\n",
            "Epoch 20/25\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5148 - accuracy: 0.3444\n",
            "Epoch 00020: loss improved from 1.51586 to 1.51493, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adagrad.Adagrad object at 0x7f59bf042630>_Batch25_LR0.1_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5149 - accuracy: 0.3444 - val_loss: 1.5132 - val_accuracy: 0.3410\n",
            "Epoch 21/25\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5143 - accuracy: 0.3437\n",
            "Epoch 00021: loss improved from 1.51493 to 1.51429, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adagrad.Adagrad object at 0x7f59bf042630>_Batch25_LR0.1_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5143 - accuracy: 0.3437 - val_loss: 1.5013 - val_accuracy: 0.3399\n",
            "Epoch 22/25\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5138 - accuracy: 0.3447\n",
            "Epoch 00022: loss improved from 1.51429 to 1.51322, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adagrad.Adagrad object at 0x7f59bf042630>_Batch25_LR0.1_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5132 - accuracy: 0.3451 - val_loss: 1.5113 - val_accuracy: 0.3402\n",
            "Epoch 23/25\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.5127 - accuracy: 0.3459\n",
            "Epoch 00023: loss improved from 1.51322 to 1.51262, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adagrad.Adagrad object at 0x7f59bf042630>_Batch25_LR0.1_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5126 - accuracy: 0.3461 - val_loss: 1.5067 - val_accuracy: 0.3399\n",
            "Epoch 24/25\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5126 - accuracy: 0.3453\n",
            "Epoch 00024: loss improved from 1.51262 to 1.51238, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adagrad.Adagrad object at 0x7f59bf042630>_Batch25_LR0.1_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5124 - accuracy: 0.3455 - val_loss: 1.5047 - val_accuracy: 0.3385\n",
            "Epoch 25/25\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5120 - accuracy: 0.3444\n",
            "Epoch 00025: loss improved from 1.51238 to 1.51176, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adagrad.Adagrad object at 0x7f59bf042630>_Batch25_LR0.1_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5118 - accuracy: 0.3444 - val_loss: 1.5084 - val_accuracy: 0.3410\n",
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
            "Epoch 1/25\n",
            "  2/595 [..............................] - ETA: 51s - loss: 14.8021 - accuracy: 0.1600WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0148s vs `on_train_batch_end` time: 0.1619s). Check your callbacks.\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.6659 - accuracy: 0.3361\n",
            "Epoch 00001: loss improved from inf to 1.66466, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adamax.Adamax object at 0x7f59c45e9fd0>_Batch25_LR0.1_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.6647 - accuracy: 0.3366 - val_loss: 1.4979 - val_accuracy: 0.3399\n",
            "Epoch 2/25\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5303 - accuracy: 0.3437\n",
            "Epoch 00002: loss improved from 1.66466 to 1.53024, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adamax.Adamax object at 0x7f59c45e9fd0>_Batch25_LR0.1_Epochs25.hdf5\n",
            "595/595 [==============================] - 9s 14ms/step - loss: 1.5302 - accuracy: 0.3436 - val_loss: 1.4979 - val_accuracy: 0.3399\n",
            "Epoch 3/25\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5308 - accuracy: 0.3396\n",
            "Epoch 00003: loss did not improve from 1.53024\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5309 - accuracy: 0.3395 - val_loss: 1.5025 - val_accuracy: 0.3399\n",
            "Epoch 4/25\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5315 - accuracy: 0.3407\n",
            "Epoch 00004: loss did not improve from 1.53024\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5316 - accuracy: 0.3408 - val_loss: 1.4961 - val_accuracy: 0.3399\n",
            "Epoch 5/25\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5309 - accuracy: 0.3401\n",
            "Epoch 00005: loss did not improve from 1.53024\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5307 - accuracy: 0.3404 - val_loss: 1.4961 - val_accuracy: 0.3399\n",
            "Epoch 6/25\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5313 - accuracy: 0.3434\n",
            "Epoch 00006: loss did not improve from 1.53024\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5312 - accuracy: 0.3436 - val_loss: 1.4991 - val_accuracy: 0.3399\n",
            "Epoch 7/25\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.5309 - accuracy: 0.3436\n",
            "Epoch 00007: loss did not improve from 1.53024\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5309 - accuracy: 0.3436 - val_loss: 1.4947 - val_accuracy: 0.3399\n",
            "Epoch 8/25\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5312 - accuracy: 0.3439\n",
            "Epoch 00008: loss did not improve from 1.53024\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5311 - accuracy: 0.3436 - val_loss: 1.5021 - val_accuracy: 0.3399\n",
            "Epoch 9/25\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5319 - accuracy: 0.3423\n",
            "Epoch 00009: loss did not improve from 1.53024\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5317 - accuracy: 0.3422 - val_loss: 1.4939 - val_accuracy: 0.3399\n",
            "Epoch 10/25\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.5324 - accuracy: 0.3436\n",
            "Epoch 00010: loss did not improve from 1.53024\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5324 - accuracy: 0.3436 - val_loss: 1.4992 - val_accuracy: 0.3399\n",
            "Epoch 11/25\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5306 - accuracy: 0.3435\n",
            "Epoch 00011: loss did not improve from 1.53024\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5307 - accuracy: 0.3436 - val_loss: 1.5000 - val_accuracy: 0.3399\n",
            "Epoch 12/25\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.5319 - accuracy: 0.3423\n",
            "Epoch 00012: loss did not improve from 1.53024\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5319 - accuracy: 0.3427 - val_loss: 1.4903 - val_accuracy: 0.3399\n",
            "Epoch 13/25\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.5309 - accuracy: 0.3426\n",
            "Epoch 00013: loss did not improve from 1.53024\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5314 - accuracy: 0.3424 - val_loss: 1.5094 - val_accuracy: 0.3399\n",
            "Epoch 14/25\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5320 - accuracy: 0.3433\n",
            "Epoch 00014: loss did not improve from 1.53024\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5319 - accuracy: 0.3436 - val_loss: 1.4959 - val_accuracy: 0.3399\n",
            "Epoch 15/25\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5322 - accuracy: 0.3422\n",
            "Epoch 00015: loss did not improve from 1.53024\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5320 - accuracy: 0.3422 - val_loss: 1.5034 - val_accuracy: 0.3399\n",
            "Epoch 16/25\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.5321 - accuracy: 0.3426\n",
            "Epoch 00016: loss did not improve from 1.53024\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5317 - accuracy: 0.3428 - val_loss: 1.4961 - val_accuracy: 0.3399\n",
            "Epoch 17/25\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5315 - accuracy: 0.3438\n",
            "Epoch 00017: loss did not improve from 1.53024\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5316 - accuracy: 0.3436 - val_loss: 1.5029 - val_accuracy: 0.3399\n",
            "Epoch 18/25\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5317 - accuracy: 0.3421\n",
            "Epoch 00018: loss did not improve from 1.53024\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5315 - accuracy: 0.3424 - val_loss: 1.4906 - val_accuracy: 0.3399\n",
            "Epoch 19/25\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.5315 - accuracy: 0.3428\n",
            "Epoch 00019: loss did not improve from 1.53024\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5316 - accuracy: 0.3424 - val_loss: 1.4917 - val_accuracy: 0.3399\n",
            "Epoch 20/25\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.5312 - accuracy: 0.3430\n",
            "Epoch 00020: loss did not improve from 1.53024\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5313 - accuracy: 0.3430 - val_loss: 1.4960 - val_accuracy: 0.3399\n",
            "Epoch 21/25\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5320 - accuracy: 0.3373\n",
            "Epoch 00021: loss did not improve from 1.53024\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5319 - accuracy: 0.3375 - val_loss: 1.5046 - val_accuracy: 0.3399\n",
            "Epoch 22/25\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5317 - accuracy: 0.3393\n",
            "Epoch 00022: loss did not improve from 1.53024\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5317 - accuracy: 0.3397 - val_loss: 1.5141 - val_accuracy: 0.3399\n",
            "Epoch 23/25\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5317 - accuracy: 0.3436\n",
            "Epoch 00023: loss did not improve from 1.53024\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5316 - accuracy: 0.3436 - val_loss: 1.4998 - val_accuracy: 0.3399\n",
            "Epoch 24/25\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5311 - accuracy: 0.3417\n",
            "Epoch 00024: loss did not improve from 1.53024\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5309 - accuracy: 0.3418 - val_loss: 1.5043 - val_accuracy: 0.3399\n",
            "Epoch 25/25\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5315 - accuracy: 0.3433\n",
            "Epoch 00025: loss did not improve from 1.53024\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5313 - accuracy: 0.3436 - val_loss: 1.5071 - val_accuracy: 0.3399\n",
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
            "Epoch 1/25\n",
            "  2/595 [..............................] - ETA: 34s - loss: 2.0317 - accuracy: 0.2000WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0144s vs `on_train_batch_end` time: 0.1007s). Check your callbacks.\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5444 - accuracy: 0.3372\n",
            "Epoch 00001: loss improved from inf to 1.54445, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.ftrl.Ftrl object at 0x7f59bebb3f60>_Batch25_LR0.1_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5445 - accuracy: 0.3372 - val_loss: 1.4944 - val_accuracy: 0.3399\n",
            "Epoch 2/25\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5341 - accuracy: 0.3412\n",
            "Epoch 00002: loss improved from 1.54445 to 1.53384, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.ftrl.Ftrl object at 0x7f59bebb3f60>_Batch25_LR0.1_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5338 - accuracy: 0.3414 - val_loss: 1.4945 - val_accuracy: 0.3399\n",
            "Epoch 3/25\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5304 - accuracy: 0.3422\n",
            "Epoch 00003: loss improved from 1.53384 to 1.53029, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.ftrl.Ftrl object at 0x7f59bebb3f60>_Batch25_LR0.1_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5303 - accuracy: 0.3423 - val_loss: 1.4987 - val_accuracy: 0.3399\n",
            "Epoch 4/25\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.5280 - accuracy: 0.3436\n",
            "Epoch 00004: loss improved from 1.53029 to 1.52804, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.ftrl.Ftrl object at 0x7f59bebb3f60>_Batch25_LR0.1_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5280 - accuracy: 0.3436 - val_loss: 1.5025 - val_accuracy: 0.3399\n",
            "Epoch 5/25\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5285 - accuracy: 0.3435\n",
            "Epoch 00005: loss did not improve from 1.52804\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5285 - accuracy: 0.3435 - val_loss: 1.4985 - val_accuracy: 0.3399\n",
            "Epoch 6/25\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.5259 - accuracy: 0.3437\n",
            "Epoch 00006: loss improved from 1.52804 to 1.52648, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.ftrl.Ftrl object at 0x7f59bebb3f60>_Batch25_LR0.1_Epochs25.hdf5\n",
            "595/595 [==============================] - 9s 15ms/step - loss: 1.5265 - accuracy: 0.3436 - val_loss: 1.5180 - val_accuracy: 0.3399\n",
            "Epoch 7/25\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5252 - accuracy: 0.3436\n",
            "Epoch 00007: loss improved from 1.52648 to 1.52508, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.ftrl.Ftrl object at 0x7f59bebb3f60>_Batch25_LR0.1_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5251 - accuracy: 0.3436 - val_loss: 1.5060 - val_accuracy: 0.3399\n",
            "Epoch 8/25\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5244 - accuracy: 0.3435\n",
            "Epoch 00008: loss improved from 1.52508 to 1.52425, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.ftrl.Ftrl object at 0x7f59bebb3f60>_Batch25_LR0.1_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5243 - accuracy: 0.3436 - val_loss: 1.4930 - val_accuracy: 0.3399\n",
            "Epoch 9/25\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5230 - accuracy: 0.3437\n",
            "Epoch 00009: loss improved from 1.52425 to 1.52300, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.ftrl.Ftrl object at 0x7f59bebb3f60>_Batch25_LR0.1_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5230 - accuracy: 0.3436 - val_loss: 1.4994 - val_accuracy: 0.3399\n",
            "Epoch 10/25\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.5208 - accuracy: 0.3431\n",
            "Epoch 00010: loss improved from 1.52300 to 1.52039, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.ftrl.Ftrl object at 0x7f59bebb3f60>_Batch25_LR0.1_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5204 - accuracy: 0.3434 - val_loss: 1.4962 - val_accuracy: 0.3399\n",
            "Epoch 11/25\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5208 - accuracy: 0.3438\n",
            "Epoch 00011: loss did not improve from 1.52039\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5206 - accuracy: 0.3442 - val_loss: 1.4993 - val_accuracy: 0.3399\n",
            "Epoch 12/25\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5202 - accuracy: 0.3434\n",
            "Epoch 00012: loss improved from 1.52039 to 1.52011, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.ftrl.Ftrl object at 0x7f59bebb3f60>_Batch25_LR0.1_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5201 - accuracy: 0.3435 - val_loss: 1.5054 - val_accuracy: 0.3399\n",
            "Epoch 13/25\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5193 - accuracy: 0.3434\n",
            "Epoch 00013: loss improved from 1.52011 to 1.51957, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.ftrl.Ftrl object at 0x7f59bebb3f60>_Batch25_LR0.1_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5196 - accuracy: 0.3432 - val_loss: 1.5074 - val_accuracy: 0.3399\n",
            "Epoch 14/25\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5184 - accuracy: 0.3439\n",
            "Epoch 00014: loss improved from 1.51957 to 1.51878, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.ftrl.Ftrl object at 0x7f59bebb3f60>_Batch25_LR0.1_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5188 - accuracy: 0.3440 - val_loss: 1.5065 - val_accuracy: 0.3399\n",
            "Epoch 15/25\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5180 - accuracy: 0.3433\n",
            "Epoch 00015: loss improved from 1.51878 to 1.51763, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.ftrl.Ftrl object at 0x7f59bebb3f60>_Batch25_LR0.1_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5176 - accuracy: 0.3439 - val_loss: 1.5077 - val_accuracy: 0.3399\n",
            "Epoch 16/25\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.5166 - accuracy: 0.3436\n",
            "Epoch 00016: loss improved from 1.51763 to 1.51656, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.ftrl.Ftrl object at 0x7f59bebb3f60>_Batch25_LR0.1_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5166 - accuracy: 0.3439 - val_loss: 1.5037 - val_accuracy: 0.3399\n",
            "Epoch 17/25\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5146 - accuracy: 0.3444\n",
            "Epoch 00017: loss improved from 1.51656 to 1.51479, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.ftrl.Ftrl object at 0x7f59bebb3f60>_Batch25_LR0.1_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5148 - accuracy: 0.3443 - val_loss: 1.5019 - val_accuracy: 0.3399\n",
            "Epoch 18/25\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5148 - accuracy: 0.3447\n",
            "Epoch 00018: loss improved from 1.51479 to 1.51468, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.ftrl.Ftrl object at 0x7f59bebb3f60>_Batch25_LR0.1_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5147 - accuracy: 0.3448 - val_loss: 1.5070 - val_accuracy: 0.3399\n",
            "Epoch 19/25\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5148 - accuracy: 0.3456\n",
            "Epoch 00019: loss did not improve from 1.51468\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5147 - accuracy: 0.3455 - val_loss: 1.4981 - val_accuracy: 0.3399\n",
            "Epoch 20/25\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5139 - accuracy: 0.3437\n",
            "Epoch 00020: loss improved from 1.51468 to 1.51410, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.ftrl.Ftrl object at 0x7f59bebb3f60>_Batch25_LR0.1_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5141 - accuracy: 0.3434 - val_loss: 1.5058 - val_accuracy: 0.3399\n",
            "Epoch 21/25\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5142 - accuracy: 0.3446\n",
            "Epoch 00021: loss improved from 1.51410 to 1.51384, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.ftrl.Ftrl object at 0x7f59bebb3f60>_Batch25_LR0.1_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5138 - accuracy: 0.3447 - val_loss: 1.5191 - val_accuracy: 0.3348\n",
            "Epoch 22/25\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5117 - accuracy: 0.3433\n",
            "Epoch 00022: loss improved from 1.51384 to 1.51178, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.ftrl.Ftrl object at 0x7f59bebb3f60>_Batch25_LR0.1_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5118 - accuracy: 0.3432 - val_loss: 1.5181 - val_accuracy: 0.3407\n",
            "Epoch 23/25\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.5117 - accuracy: 0.3442\n",
            "Epoch 00023: loss did not improve from 1.51178\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5123 - accuracy: 0.3439 - val_loss: 1.5160 - val_accuracy: 0.3407\n",
            "Epoch 24/25\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.5112 - accuracy: 0.3427\n",
            "Epoch 00024: loss improved from 1.51178 to 1.51093, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.ftrl.Ftrl object at 0x7f59bebb3f60>_Batch25_LR0.1_Epochs25.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5109 - accuracy: 0.3432 - val_loss: 1.5018 - val_accuracy: 0.3399\n",
            "Epoch 25/25\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.5110 - accuracy: 0.3419\n",
            "Epoch 00025: loss did not improve from 1.51093\n",
            "595/595 [==============================] - 7s 12ms/step - loss: 1.5110 - accuracy: 0.3419 - val_loss: 1.4985 - val_accuracy: 0.3399\n",
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
            "Epoch 1/50\n",
            "  2/595 [..............................] - ETA: 42s - loss: 2.3351 - accuracy: 0.0600WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0154s vs `on_train_batch_end` time: 0.1259s). Check your callbacks.\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.6065 - accuracy: 0.3098\n",
            "Epoch 00001: loss improved from inf to 1.60648, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f59b97fbf28>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.6065 - accuracy: 0.3095 - val_loss: 1.5196 - val_accuracy: 0.3399\n",
            "Epoch 2/50\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.5625 - accuracy: 0.3265\n",
            "Epoch 00002: loss improved from 1.60648 to 1.56251, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f59b97fbf28>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5625 - accuracy: 0.3265 - val_loss: 1.5098 - val_accuracy: 0.3399\n",
            "Epoch 3/50\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5525 - accuracy: 0.3343\n",
            "Epoch 00003: loss improved from 1.56251 to 1.55234, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f59b97fbf28>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5523 - accuracy: 0.3342 - val_loss: 1.5095 - val_accuracy: 0.3399\n",
            "Epoch 4/50\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5487 - accuracy: 0.3321\n",
            "Epoch 00004: loss improved from 1.55234 to 1.54854, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f59b97fbf28>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5485 - accuracy: 0.3321 - val_loss: 1.5078 - val_accuracy: 0.3399\n",
            "Epoch 5/50\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5428 - accuracy: 0.3359\n",
            "Epoch 00005: loss improved from 1.54854 to 1.54251, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f59b97fbf28>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5425 - accuracy: 0.3362 - val_loss: 1.5071 - val_accuracy: 0.3399\n",
            "Epoch 6/50\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5403 - accuracy: 0.3355\n",
            "Epoch 00006: loss improved from 1.54251 to 1.54046, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f59b97fbf28>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5405 - accuracy: 0.3355 - val_loss: 1.5058 - val_accuracy: 0.3399\n",
            "Epoch 7/50\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5391 - accuracy: 0.3374\n",
            "Epoch 00007: loss improved from 1.54046 to 1.53896, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f59b97fbf28>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5390 - accuracy: 0.3376 - val_loss: 1.5050 - val_accuracy: 0.3399\n",
            "Epoch 8/50\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5379 - accuracy: 0.3426\n",
            "Epoch 00008: loss improved from 1.53896 to 1.53789, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f59b97fbf28>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5379 - accuracy: 0.3425 - val_loss: 1.5012 - val_accuracy: 0.3399\n",
            "Epoch 9/50\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5368 - accuracy: 0.3389\n",
            "Epoch 00009: loss improved from 1.53789 to 1.53665, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f59b97fbf28>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5366 - accuracy: 0.3389 - val_loss: 1.5054 - val_accuracy: 0.3399\n",
            "Epoch 10/50\n",
            "589/595 [============================>.] - ETA: 0s - loss: 1.5375 - accuracy: 0.3357\n",
            "Epoch 00010: loss did not improve from 1.53665\n",
            "595/595 [==============================] - 7s 12ms/step - loss: 1.5369 - accuracy: 0.3362 - val_loss: 1.5060 - val_accuracy: 0.3399\n",
            "Epoch 11/50\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5350 - accuracy: 0.3386\n",
            "Epoch 00011: loss improved from 1.53665 to 1.53507, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f59b97fbf28>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5351 - accuracy: 0.3385 - val_loss: 1.5057 - val_accuracy: 0.3399\n",
            "Epoch 12/50\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5320 - accuracy: 0.3431\n",
            "Epoch 00012: loss improved from 1.53507 to 1.53211, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f59b97fbf28>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5321 - accuracy: 0.3430 - val_loss: 1.5056 - val_accuracy: 0.3399\n",
            "Epoch 13/50\n",
            "589/595 [============================>.] - ETA: 0s - loss: 1.5311 - accuracy: 0.3410\n",
            "Epoch 00013: loss improved from 1.53211 to 1.53190, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f59b97fbf28>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5319 - accuracy: 0.3402 - val_loss: 1.5020 - val_accuracy: 0.3399\n",
            "Epoch 14/50\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5342 - accuracy: 0.3392\n",
            "Epoch 00014: loss did not improve from 1.53190\n",
            "595/595 [==============================] - 7s 12ms/step - loss: 1.5340 - accuracy: 0.3391 - val_loss: 1.5023 - val_accuracy: 0.3399\n",
            "Epoch 15/50\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.5316 - accuracy: 0.3408\n",
            "Epoch 00015: loss improved from 1.53190 to 1.53089, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f59b97fbf28>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5309 - accuracy: 0.3409 - val_loss: 1.4996 - val_accuracy: 0.3399\n",
            "Epoch 16/50\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5297 - accuracy: 0.3418\n",
            "Epoch 00016: loss improved from 1.53089 to 1.53003, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f59b97fbf28>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5300 - accuracy: 0.3412 - val_loss: 1.5029 - val_accuracy: 0.3399\n",
            "Epoch 17/50\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5313 - accuracy: 0.3418\n",
            "Epoch 00017: loss did not improve from 1.53003\n",
            "595/595 [==============================] - 7s 13ms/step - loss: 1.5312 - accuracy: 0.3418 - val_loss: 1.5032 - val_accuracy: 0.3399\n",
            "Epoch 18/50\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.5303 - accuracy: 0.3443\n",
            "Epoch 00018: loss did not improve from 1.53003\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5303 - accuracy: 0.3443 - val_loss: 1.5041 - val_accuracy: 0.3399\n",
            "Epoch 19/50\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5298 - accuracy: 0.3418\n",
            "Epoch 00019: loss improved from 1.53003 to 1.52972, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f59b97fbf28>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5297 - accuracy: 0.3420 - val_loss: 1.4993 - val_accuracy: 0.3399\n",
            "Epoch 20/50\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.5287 - accuracy: 0.3397\n",
            "Epoch 00020: loss improved from 1.52972 to 1.52872, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f59b97fbf28>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5287 - accuracy: 0.3397 - val_loss: 1.5041 - val_accuracy: 0.3399\n",
            "Epoch 21/50\n",
            "589/595 [============================>.] - ETA: 0s - loss: 1.5303 - accuracy: 0.3444\n",
            "Epoch 00021: loss did not improve from 1.52872\n",
            "595/595 [==============================] - 7s 12ms/step - loss: 1.5300 - accuracy: 0.3442 - val_loss: 1.4988 - val_accuracy: 0.3399\n",
            "Epoch 22/50\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5288 - accuracy: 0.3429\n",
            "Epoch 00022: loss did not improve from 1.52872\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5288 - accuracy: 0.3430 - val_loss: 1.4982 - val_accuracy: 0.3399\n",
            "Epoch 23/50\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5276 - accuracy: 0.3426\n",
            "Epoch 00023: loss improved from 1.52872 to 1.52765, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f59b97fbf28>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5277 - accuracy: 0.3424 - val_loss: 1.5030 - val_accuracy: 0.3399\n",
            "Epoch 24/50\n",
            "589/595 [============================>.] - ETA: 0s - loss: 1.5279 - accuracy: 0.3428\n",
            "Epoch 00024: loss did not improve from 1.52765\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5277 - accuracy: 0.3433 - val_loss: 1.5051 - val_accuracy: 0.3399\n",
            "Epoch 25/50\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5270 - accuracy: 0.3421\n",
            "Epoch 00025: loss improved from 1.52765 to 1.52694, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f59b97fbf28>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5269 - accuracy: 0.3420 - val_loss: 1.5011 - val_accuracy: 0.3399\n",
            "Epoch 26/50\n",
            "589/595 [============================>.] - ETA: 0s - loss: 1.5272 - accuracy: 0.3429\n",
            "Epoch 00026: loss did not improve from 1.52694\n",
            "595/595 [==============================] - 7s 13ms/step - loss: 1.5273 - accuracy: 0.3431 - val_loss: 1.5014 - val_accuracy: 0.3399\n",
            "Epoch 27/50\n",
            "589/595 [============================>.] - ETA: 0s - loss: 1.5279 - accuracy: 0.3432\n",
            "Epoch 00027: loss did not improve from 1.52694\n",
            "595/595 [==============================] - 7s 12ms/step - loss: 1.5280 - accuracy: 0.3434 - val_loss: 1.5021 - val_accuracy: 0.3399\n",
            "Epoch 28/50\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.5270 - accuracy: 0.3426\n",
            "Epoch 00028: loss did not improve from 1.52694\n",
            "595/595 [==============================] - 7s 12ms/step - loss: 1.5271 - accuracy: 0.3429 - val_loss: 1.5011 - val_accuracy: 0.3399\n",
            "Epoch 29/50\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5270 - accuracy: 0.3436\n",
            "Epoch 00029: loss improved from 1.52694 to 1.52684, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f59b97fbf28>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5268 - accuracy: 0.3432 - val_loss: 1.4991 - val_accuracy: 0.3399\n",
            "Epoch 30/50\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5266 - accuracy: 0.3425\n",
            "Epoch 00030: loss did not improve from 1.52684\n",
            "595/595 [==============================] - 7s 13ms/step - loss: 1.5268 - accuracy: 0.3420 - val_loss: 1.4988 - val_accuracy: 0.3399\n",
            "Epoch 31/50\n",
            "589/595 [============================>.] - ETA: 0s - loss: 1.5278 - accuracy: 0.3429\n",
            "Epoch 00031: loss did not improve from 1.52684\n",
            "595/595 [==============================] - 7s 13ms/step - loss: 1.5274 - accuracy: 0.3432 - val_loss: 1.5006 - val_accuracy: 0.3399\n",
            "Epoch 32/50\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.5273 - accuracy: 0.3425\n",
            "Epoch 00032: loss did not improve from 1.52684\n",
            "595/595 [==============================] - 7s 12ms/step - loss: 1.5276 - accuracy: 0.3428 - val_loss: 1.5044 - val_accuracy: 0.3399\n",
            "Epoch 33/50\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5244 - accuracy: 0.3437\n",
            "Epoch 00033: loss improved from 1.52684 to 1.52508, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f59b97fbf28>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5251 - accuracy: 0.3437 - val_loss: 1.4996 - val_accuracy: 0.3399\n",
            "Epoch 34/50\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.5272 - accuracy: 0.3422\n",
            "Epoch 00034: loss did not improve from 1.52508\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5274 - accuracy: 0.3420 - val_loss: 1.5001 - val_accuracy: 0.3399\n",
            "Epoch 35/50\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5261 - accuracy: 0.3426\n",
            "Epoch 00035: loss did not improve from 1.52508\n",
            "595/595 [==============================] - 7s 12ms/step - loss: 1.5266 - accuracy: 0.3425 - val_loss: 1.5043 - val_accuracy: 0.3399\n",
            "Epoch 36/50\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.5258 - accuracy: 0.3434\n",
            "Epoch 00036: loss did not improve from 1.52508\n",
            "595/595 [==============================] - 7s 13ms/step - loss: 1.5258 - accuracy: 0.3434 - val_loss: 1.5010 - val_accuracy: 0.3399\n",
            "Epoch 37/50\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5246 - accuracy: 0.3442\n",
            "Epoch 00037: loss improved from 1.52508 to 1.52491, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f59b97fbf28>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5249 - accuracy: 0.3437 - val_loss: 1.4982 - val_accuracy: 0.3399\n",
            "Epoch 38/50\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.5248 - accuracy: 0.3418\n",
            "Epoch 00038: loss improved from 1.52491 to 1.52474, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f59b97fbf28>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5247 - accuracy: 0.3417 - val_loss: 1.4975 - val_accuracy: 0.3399\n",
            "Epoch 39/50\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5234 - accuracy: 0.3438\n",
            "Epoch 00039: loss improved from 1.52474 to 1.52315, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f59b97fbf28>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5231 - accuracy: 0.3440 - val_loss: 1.4962 - val_accuracy: 0.3399\n",
            "Epoch 40/50\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5248 - accuracy: 0.3428\n",
            "Epoch 00040: loss did not improve from 1.52315\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5247 - accuracy: 0.3427 - val_loss: 1.4970 - val_accuracy: 0.3399\n",
            "Epoch 41/50\n",
            "589/595 [============================>.] - ETA: 0s - loss: 1.5243 - accuracy: 0.3432\n",
            "Epoch 00041: loss did not improve from 1.52315\n",
            "595/595 [==============================] - 7s 12ms/step - loss: 1.5243 - accuracy: 0.3435 - val_loss: 1.4986 - val_accuracy: 0.3399\n",
            "Epoch 42/50\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5257 - accuracy: 0.3423\n",
            "Epoch 00042: loss did not improve from 1.52315\n",
            "595/595 [==============================] - 7s 12ms/step - loss: 1.5256 - accuracy: 0.3423 - val_loss: 1.5012 - val_accuracy: 0.3399\n",
            "Epoch 43/50\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5237 - accuracy: 0.3427\n",
            "Epoch 00043: loss did not improve from 1.52315\n",
            "595/595 [==============================] - 7s 12ms/step - loss: 1.5237 - accuracy: 0.3426 - val_loss: 1.5006 - val_accuracy: 0.3399\n",
            "Epoch 44/50\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5234 - accuracy: 0.3437\n",
            "Epoch 00044: loss did not improve from 1.52315\n",
            "595/595 [==============================] - 7s 12ms/step - loss: 1.5233 - accuracy: 0.3436 - val_loss: 1.5021 - val_accuracy: 0.3399\n",
            "Epoch 45/50\n",
            "589/595 [============================>.] - ETA: 0s - loss: 1.5238 - accuracy: 0.3434\n",
            "Epoch 00045: loss did not improve from 1.52315\n",
            "595/595 [==============================] - 7s 12ms/step - loss: 1.5235 - accuracy: 0.3439 - val_loss: 1.5009 - val_accuracy: 0.3399\n",
            "Epoch 46/50\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.5232 - accuracy: 0.3449\n",
            "Epoch 00046: loss did not improve from 1.52315\n",
            "595/595 [==============================] - 7s 12ms/step - loss: 1.5233 - accuracy: 0.3449 - val_loss: 1.5000 - val_accuracy: 0.3399\n",
            "Epoch 47/50\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5229 - accuracy: 0.3438\n",
            "Epoch 00047: loss improved from 1.52315 to 1.52275, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f59b97fbf28>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5228 - accuracy: 0.3439 - val_loss: 1.4996 - val_accuracy: 0.3399\n",
            "Epoch 48/50\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5220 - accuracy: 0.3438\n",
            "Epoch 00048: loss improved from 1.52275 to 1.52268, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f59b97fbf28>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5227 - accuracy: 0.3436 - val_loss: 1.4985 - val_accuracy: 0.3399\n",
            "Epoch 49/50\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5223 - accuracy: 0.3432\n",
            "Epoch 00049: loss did not improve from 1.52268\n",
            "595/595 [==============================] - 7s 13ms/step - loss: 1.5228 - accuracy: 0.3430 - val_loss: 1.5006 - val_accuracy: 0.3399\n",
            "Epoch 50/50\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5221 - accuracy: 0.3433\n",
            "Epoch 00050: loss improved from 1.52268 to 1.52242, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f59b97fbf28>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 10s 17ms/step - loss: 1.5224 - accuracy: 0.3432 - val_loss: 1.5030 - val_accuracy: 0.3399\n",
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
            "Epoch 1/50\n",
            "  2/595 [..............................] - ETA: 52s - loss: 2.1366 - accuracy: 0.1600WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0158s vs `on_train_batch_end` time: 0.1613s). Check your callbacks.\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5460 - accuracy: 0.3316\n",
            "Epoch 00001: loss improved from inf to 1.54612, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.rmsprop.RMSprop object at 0x7f59b9879470>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5461 - accuracy: 0.3313 - val_loss: 1.5033 - val_accuracy: 0.3399\n",
            "Epoch 2/50\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5286 - accuracy: 0.3405\n",
            "Epoch 00002: loss improved from 1.54612 to 1.52856, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.rmsprop.RMSprop object at 0x7f59b9879470>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5286 - accuracy: 0.3404 - val_loss: 1.4959 - val_accuracy: 0.3399\n",
            "Epoch 3/50\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5234 - accuracy: 0.3431\n",
            "Epoch 00003: loss improved from 1.52856 to 1.52350, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.rmsprop.RMSprop object at 0x7f59b9879470>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5235 - accuracy: 0.3432 - val_loss: 1.4909 - val_accuracy: 0.3399\n",
            "Epoch 4/50\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5203 - accuracy: 0.3426\n",
            "Epoch 00004: loss improved from 1.52350 to 1.52067, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.rmsprop.RMSprop object at 0x7f59b9879470>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5207 - accuracy: 0.3426 - val_loss: 1.4865 - val_accuracy: 0.3399\n",
            "Epoch 5/50\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5175 - accuracy: 0.3423\n",
            "Epoch 00005: loss improved from 1.52067 to 1.51755, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.rmsprop.RMSprop object at 0x7f59b9879470>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5176 - accuracy: 0.3423 - val_loss: 1.5161 - val_accuracy: 0.3372\n",
            "Epoch 6/50\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.5139 - accuracy: 0.3458\n",
            "Epoch 00006: loss improved from 1.51755 to 1.51391, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.rmsprop.RMSprop object at 0x7f59b9879470>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5139 - accuracy: 0.3458 - val_loss: 1.4960 - val_accuracy: 0.3399\n",
            "Epoch 7/50\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5138 - accuracy: 0.3434\n",
            "Epoch 00007: loss improved from 1.51391 to 1.51385, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.rmsprop.RMSprop object at 0x7f59b9879470>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5139 - accuracy: 0.3434 - val_loss: 1.4899 - val_accuracy: 0.3372\n",
            "Epoch 8/50\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5102 - accuracy: 0.3466\n",
            "Epoch 00008: loss improved from 1.51385 to 1.51018, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.rmsprop.RMSprop object at 0x7f59b9879470>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5102 - accuracy: 0.3462 - val_loss: 1.4943 - val_accuracy: 0.3321\n",
            "Epoch 9/50\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.5088 - accuracy: 0.3460\n",
            "Epoch 00009: loss improved from 1.51018 to 1.50911, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.rmsprop.RMSprop object at 0x7f59b9879470>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5091 - accuracy: 0.3460 - val_loss: 1.5073 - val_accuracy: 0.3348\n",
            "Epoch 10/50\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.5083 - accuracy: 0.3461\n",
            "Epoch 00010: loss improved from 1.50911 to 1.50829, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.rmsprop.RMSprop object at 0x7f59b9879470>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5083 - accuracy: 0.3461 - val_loss: 1.4933 - val_accuracy: 0.3337\n",
            "Epoch 11/50\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5053 - accuracy: 0.3492\n",
            "Epoch 00011: loss improved from 1.50829 to 1.50524, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.rmsprop.RMSprop object at 0x7f59b9879470>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5052 - accuracy: 0.3491 - val_loss: 1.4981 - val_accuracy: 0.3138\n",
            "Epoch 12/50\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5022 - accuracy: 0.3454\n",
            "Epoch 00012: loss improved from 1.50524 to 1.50208, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.rmsprop.RMSprop object at 0x7f59b9879470>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5021 - accuracy: 0.3453 - val_loss: 1.5023 - val_accuracy: 0.3270\n",
            "Epoch 13/50\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5034 - accuracy: 0.3448\n",
            "Epoch 00013: loss did not improve from 1.50208\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5036 - accuracy: 0.3448 - val_loss: 1.5023 - val_accuracy: 0.3407\n",
            "Epoch 14/50\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5023 - accuracy: 0.3484\n",
            "Epoch 00014: loss did not improve from 1.50208\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5023 - accuracy: 0.3484 - val_loss: 1.5032 - val_accuracy: 0.3321\n",
            "Epoch 15/50\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.5005 - accuracy: 0.3484\n",
            "Epoch 00015: loss improved from 1.50208 to 1.50048, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.rmsprop.RMSprop object at 0x7f59b9879470>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5005 - accuracy: 0.3484 - val_loss: 1.4884 - val_accuracy: 0.3305\n",
            "Epoch 16/50\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.4993 - accuracy: 0.3491\n",
            "Epoch 00016: loss improved from 1.50048 to 1.49936, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.rmsprop.RMSprop object at 0x7f59b9879470>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.4994 - accuracy: 0.3487 - val_loss: 1.5012 - val_accuracy: 0.3122\n",
            "Epoch 17/50\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.4980 - accuracy: 0.3478\n",
            "Epoch 00017: loss improved from 1.49936 to 1.49815, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.rmsprop.RMSprop object at 0x7f59b9879470>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.4982 - accuracy: 0.3477 - val_loss: 1.4997 - val_accuracy: 0.3256\n",
            "Epoch 18/50\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.4998 - accuracy: 0.3485\n",
            "Epoch 00018: loss did not improve from 1.49815\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.4998 - accuracy: 0.3485 - val_loss: 1.5157 - val_accuracy: 0.3248\n",
            "Epoch 19/50\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.4945 - accuracy: 0.3569\n",
            "Epoch 00019: loss improved from 1.49815 to 1.49437, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.rmsprop.RMSprop object at 0x7f59b9879470>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.4944 - accuracy: 0.3569 - val_loss: 1.5051 - val_accuracy: 0.3307\n",
            "Epoch 20/50\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.4955 - accuracy: 0.3519\n",
            "Epoch 00020: loss did not improve from 1.49437\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.4956 - accuracy: 0.3519 - val_loss: 1.5244 - val_accuracy: 0.3237\n",
            "Epoch 21/50\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.4950 - accuracy: 0.3540\n",
            "Epoch 00021: loss did not improve from 1.49437\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.4952 - accuracy: 0.3543 - val_loss: 1.5356 - val_accuracy: 0.3353\n",
            "Epoch 22/50\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.4942 - accuracy: 0.3526\n",
            "Epoch 00022: loss improved from 1.49437 to 1.49422, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.rmsprop.RMSprop object at 0x7f59b9879470>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.4942 - accuracy: 0.3525 - val_loss: 1.5225 - val_accuracy: 0.3178\n",
            "Epoch 23/50\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.4904 - accuracy: 0.3538\n",
            "Epoch 00023: loss improved from 1.49422 to 1.49062, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.rmsprop.RMSprop object at 0x7f59b9879470>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.4906 - accuracy: 0.3536 - val_loss: 1.5259 - val_accuracy: 0.3369\n",
            "Epoch 24/50\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.4913 - accuracy: 0.3539\n",
            "Epoch 00024: loss did not improve from 1.49062\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.4910 - accuracy: 0.3543 - val_loss: 1.5472 - val_accuracy: 0.3127\n",
            "Epoch 25/50\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.4916 - accuracy: 0.3600\n",
            "Epoch 00025: loss did not improve from 1.49062\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.4917 - accuracy: 0.3600 - val_loss: 1.5451 - val_accuracy: 0.3202\n",
            "Epoch 26/50\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.4902 - accuracy: 0.3572\n",
            "Epoch 00026: loss improved from 1.49062 to 1.49008, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.rmsprop.RMSprop object at 0x7f59b9879470>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.4901 - accuracy: 0.3574 - val_loss: 1.5259 - val_accuracy: 0.3227\n",
            "Epoch 27/50\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.4898 - accuracy: 0.3540\n",
            "Epoch 00027: loss improved from 1.49008 to 1.48953, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.rmsprop.RMSprop object at 0x7f59b9879470>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.4895 - accuracy: 0.3545 - val_loss: 1.5468 - val_accuracy: 0.3229\n",
            "Epoch 28/50\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.4896 - accuracy: 0.3556\n",
            "Epoch 00028: loss did not improve from 1.48953\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.4897 - accuracy: 0.3555 - val_loss: 1.5496 - val_accuracy: 0.3205\n",
            "Epoch 29/50\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.4889 - accuracy: 0.3593\n",
            "Epoch 00029: loss improved from 1.48953 to 1.48903, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.rmsprop.RMSprop object at 0x7f59b9879470>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.4890 - accuracy: 0.3591 - val_loss: 1.5435 - val_accuracy: 0.3450\n",
            "Epoch 30/50\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.4868 - accuracy: 0.3560\n",
            "Epoch 00030: loss improved from 1.48903 to 1.48654, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.rmsprop.RMSprop object at 0x7f59b9879470>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.4865 - accuracy: 0.3559 - val_loss: 1.5485 - val_accuracy: 0.3248\n",
            "Epoch 31/50\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.4846 - accuracy: 0.3618\n",
            "Epoch 00031: loss improved from 1.48654 to 1.48472, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.rmsprop.RMSprop object at 0x7f59b9879470>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.4847 - accuracy: 0.3615 - val_loss: 1.5656 - val_accuracy: 0.3399\n",
            "Epoch 32/50\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.4828 - accuracy: 0.3589\n",
            "Epoch 00032: loss improved from 1.48472 to 1.48311, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.rmsprop.RMSprop object at 0x7f59b9879470>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.4831 - accuracy: 0.3588 - val_loss: 1.5578 - val_accuracy: 0.3315\n",
            "Epoch 33/50\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.4839 - accuracy: 0.3646\n",
            "Epoch 00033: loss did not improve from 1.48311\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.4841 - accuracy: 0.3644 - val_loss: 1.5585 - val_accuracy: 0.3402\n",
            "Epoch 34/50\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.4831 - accuracy: 0.3621\n",
            "Epoch 00034: loss improved from 1.48311 to 1.48309, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.rmsprop.RMSprop object at 0x7f59b9879470>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.4831 - accuracy: 0.3621 - val_loss: 1.5644 - val_accuracy: 0.3380\n",
            "Epoch 35/50\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.4863 - accuracy: 0.3641\n",
            "Epoch 00035: loss did not improve from 1.48309\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.4861 - accuracy: 0.3642 - val_loss: 1.5996 - val_accuracy: 0.3186\n",
            "Epoch 36/50\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.4834 - accuracy: 0.3628\n",
            "Epoch 00036: loss did not improve from 1.48309\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.4841 - accuracy: 0.3626 - val_loss: 1.6068 - val_accuracy: 0.3219\n",
            "Epoch 37/50\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.4822 - accuracy: 0.3614\n",
            "Epoch 00037: loss improved from 1.48309 to 1.48163, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.rmsprop.RMSprop object at 0x7f59b9879470>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.4816 - accuracy: 0.3617 - val_loss: 1.5950 - val_accuracy: 0.3219\n",
            "Epoch 38/50\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.4813 - accuracy: 0.3644\n",
            "Epoch 00038: loss improved from 1.48163 to 1.48112, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.rmsprop.RMSprop object at 0x7f59b9879470>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.4811 - accuracy: 0.3644 - val_loss: 1.6051 - val_accuracy: 0.3318\n",
            "Epoch 39/50\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.4818 - accuracy: 0.3610\n",
            "Epoch 00039: loss did not improve from 1.48112\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.4817 - accuracy: 0.3611 - val_loss: 1.6593 - val_accuracy: 0.3130\n",
            "Epoch 40/50\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.4805 - accuracy: 0.3683\n",
            "Epoch 00040: loss improved from 1.48112 to 1.48046, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.rmsprop.RMSprop object at 0x7f59b9879470>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.4805 - accuracy: 0.3683 - val_loss: 1.6470 - val_accuracy: 0.3149\n",
            "Epoch 41/50\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.4799 - accuracy: 0.3651\n",
            "Epoch 00041: loss improved from 1.48046 to 1.48022, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.rmsprop.RMSprop object at 0x7f59b9879470>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.4802 - accuracy: 0.3651 - val_loss: 1.6367 - val_accuracy: 0.3170\n",
            "Epoch 42/50\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.4770 - accuracy: 0.3632\n",
            "Epoch 00042: loss improved from 1.48022 to 1.47775, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.rmsprop.RMSprop object at 0x7f59b9879470>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.4778 - accuracy: 0.3634 - val_loss: 1.6119 - val_accuracy: 0.3248\n",
            "Epoch 43/50\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.4770 - accuracy: 0.3607\n",
            "Epoch 00043: loss improved from 1.47775 to 1.47708, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.rmsprop.RMSprop object at 0x7f59b9879470>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.4771 - accuracy: 0.3605 - val_loss: 1.6690 - val_accuracy: 0.3315\n",
            "Epoch 44/50\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.4766 - accuracy: 0.3664\n",
            "Epoch 00044: loss improved from 1.47708 to 1.47662, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.rmsprop.RMSprop object at 0x7f59b9879470>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.4766 - accuracy: 0.3662 - val_loss: 1.6762 - val_accuracy: 0.3235\n",
            "Epoch 45/50\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.4753 - accuracy: 0.3686\n",
            "Epoch 00045: loss improved from 1.47662 to 1.47584, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.rmsprop.RMSprop object at 0x7f59b9879470>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.4758 - accuracy: 0.3681 - val_loss: 1.6771 - val_accuracy: 0.3087\n",
            "Epoch 46/50\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.4750 - accuracy: 0.3685\n",
            "Epoch 00046: loss improved from 1.47584 to 1.47517, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.rmsprop.RMSprop object at 0x7f59b9879470>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.4752 - accuracy: 0.3683 - val_loss: 1.7080 - val_accuracy: 0.3251\n",
            "Epoch 47/50\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.4763 - accuracy: 0.3665\n",
            "Epoch 00047: loss did not improve from 1.47517\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.4763 - accuracy: 0.3665 - val_loss: 1.6730 - val_accuracy: 0.3235\n",
            "Epoch 48/50\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.4746 - accuracy: 0.3641\n",
            "Epoch 00048: loss improved from 1.47517 to 1.47514, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.rmsprop.RMSprop object at 0x7f59b9879470>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.4751 - accuracy: 0.3642 - val_loss: 1.7208 - val_accuracy: 0.3232\n",
            "Epoch 49/50\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.4778 - accuracy: 0.3703\n",
            "Epoch 00049: loss did not improve from 1.47514\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.4778 - accuracy: 0.3703 - val_loss: 1.7109 - val_accuracy: 0.3127\n",
            "Epoch 50/50\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.4761 - accuracy: 0.3666\n",
            "Epoch 00050: loss did not improve from 1.47514\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.4761 - accuracy: 0.3666 - val_loss: 1.7159 - val_accuracy: 0.3184\n",
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
            "Epoch 1/50\n",
            "  2/595 [..............................] - ETA: 34s - loss: 2.1142 - accuracy: 0.2200WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0160s vs `on_train_batch_end` time: 0.1006s). Check your callbacks.\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5489 - accuracy: 0.3312\n",
            "Epoch 00001: loss improved from inf to 1.54839, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f59c41d7fd0>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5484 - accuracy: 0.3314 - val_loss: 1.4996 - val_accuracy: 0.3399\n",
            "Epoch 2/50\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.5272 - accuracy: 0.3434\n",
            "Epoch 00002: loss improved from 1.54839 to 1.52717, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f59c41d7fd0>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5272 - accuracy: 0.3434 - val_loss: 1.4991 - val_accuracy: 0.3399\n",
            "Epoch 3/50\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.5214 - accuracy: 0.3435\n",
            "Epoch 00003: loss improved from 1.52717 to 1.52144, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f59c41d7fd0>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5214 - accuracy: 0.3435 - val_loss: 1.4975 - val_accuracy: 0.3372\n",
            "Epoch 4/50\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.5188 - accuracy: 0.3401\n",
            "Epoch 00004: loss improved from 1.52144 to 1.51883, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f59c41d7fd0>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5188 - accuracy: 0.3401 - val_loss: 1.4923 - val_accuracy: 0.3372\n",
            "Epoch 5/50\n",
            "589/595 [============================>.] - ETA: 0s - loss: 1.5171 - accuracy: 0.3447\n",
            "Epoch 00005: loss improved from 1.51883 to 1.51661, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f59c41d7fd0>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5166 - accuracy: 0.3451 - val_loss: 1.4929 - val_accuracy: 0.3372\n",
            "Epoch 6/50\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5143 - accuracy: 0.3436\n",
            "Epoch 00006: loss improved from 1.51661 to 1.51436, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f59c41d7fd0>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5144 - accuracy: 0.3436 - val_loss: 1.5010 - val_accuracy: 0.3372\n",
            "Epoch 7/50\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.5124 - accuracy: 0.3463\n",
            "Epoch 00007: loss improved from 1.51436 to 1.51254, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f59c41d7fd0>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5125 - accuracy: 0.3461 - val_loss: 1.4939 - val_accuracy: 0.3367\n",
            "Epoch 8/50\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.5105 - accuracy: 0.3396\n",
            "Epoch 00008: loss improved from 1.51254 to 1.51046, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f59c41d7fd0>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5105 - accuracy: 0.3396 - val_loss: 1.4925 - val_accuracy: 0.3372\n",
            "Epoch 9/50\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5080 - accuracy: 0.3465\n",
            "Epoch 00009: loss improved from 1.51046 to 1.50846, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f59c41d7fd0>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5085 - accuracy: 0.3463 - val_loss: 1.4923 - val_accuracy: 0.3372\n",
            "Epoch 10/50\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5069 - accuracy: 0.3414\n",
            "Epoch 00010: loss improved from 1.50846 to 1.50677, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f59c41d7fd0>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5068 - accuracy: 0.3416 - val_loss: 1.4932 - val_accuracy: 0.3245\n",
            "Epoch 11/50\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5065 - accuracy: 0.3462\n",
            "Epoch 00011: loss improved from 1.50677 to 1.50629, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f59c41d7fd0>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5063 - accuracy: 0.3464 - val_loss: 1.4920 - val_accuracy: 0.3259\n",
            "Epoch 12/50\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5030 - accuracy: 0.3505\n",
            "Epoch 00012: loss improved from 1.50629 to 1.50314, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f59c41d7fd0>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5031 - accuracy: 0.3504 - val_loss: 1.4959 - val_accuracy: 0.3332\n",
            "Epoch 13/50\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5007 - accuracy: 0.3443\n",
            "Epoch 00013: loss improved from 1.50314 to 1.50113, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f59c41d7fd0>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5011 - accuracy: 0.3440 - val_loss: 1.5004 - val_accuracy: 0.3364\n",
            "Epoch 14/50\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.4992 - accuracy: 0.3470\n",
            "Epoch 00014: loss improved from 1.50113 to 1.49960, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f59c41d7fd0>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.4996 - accuracy: 0.3468 - val_loss: 1.5030 - val_accuracy: 0.3358\n",
            "Epoch 15/50\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5011 - accuracy: 0.3430\n",
            "Epoch 00015: loss did not improve from 1.49960\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5015 - accuracy: 0.3429 - val_loss: 1.4969 - val_accuracy: 0.3297\n",
            "Epoch 16/50\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.4987 - accuracy: 0.3448\n",
            "Epoch 00016: loss improved from 1.49960 to 1.49812, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f59c41d7fd0>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.4981 - accuracy: 0.3448 - val_loss: 1.5037 - val_accuracy: 0.3385\n",
            "Epoch 17/50\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.4949 - accuracy: 0.3480\n",
            "Epoch 00017: loss improved from 1.49812 to 1.49487, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f59c41d7fd0>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.4949 - accuracy: 0.3480 - val_loss: 1.5027 - val_accuracy: 0.3092\n",
            "Epoch 18/50\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.4927 - accuracy: 0.3526\n",
            "Epoch 00018: loss improved from 1.49487 to 1.49322, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f59c41d7fd0>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.4932 - accuracy: 0.3522 - val_loss: 1.4928 - val_accuracy: 0.3189\n",
            "Epoch 19/50\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.4925 - accuracy: 0.3500\n",
            "Epoch 00019: loss improved from 1.49322 to 1.49267, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f59c41d7fd0>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.4927 - accuracy: 0.3497 - val_loss: 1.5023 - val_accuracy: 0.3270\n",
            "Epoch 20/50\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.4903 - accuracy: 0.3536\n",
            "Epoch 00020: loss improved from 1.49267 to 1.49038, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f59c41d7fd0>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.4904 - accuracy: 0.3535 - val_loss: 1.4957 - val_accuracy: 0.3143\n",
            "Epoch 21/50\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.4893 - accuracy: 0.3513\n",
            "Epoch 00021: loss improved from 1.49038 to 1.48934, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f59c41d7fd0>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.4893 - accuracy: 0.3513 - val_loss: 1.4833 - val_accuracy: 0.3302\n",
            "Epoch 22/50\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.4881 - accuracy: 0.3514\n",
            "Epoch 00022: loss improved from 1.48934 to 1.48806, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f59c41d7fd0>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.4881 - accuracy: 0.3514 - val_loss: 1.4888 - val_accuracy: 0.3348\n",
            "Epoch 23/50\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.4865 - accuracy: 0.3543\n",
            "Epoch 00023: loss improved from 1.48806 to 1.48650, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f59c41d7fd0>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.4865 - accuracy: 0.3543 - val_loss: 1.5094 - val_accuracy: 0.3108\n",
            "Epoch 24/50\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.4876 - accuracy: 0.3522\n",
            "Epoch 00024: loss did not improve from 1.48650\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.4876 - accuracy: 0.3521 - val_loss: 1.5056 - val_accuracy: 0.3345\n",
            "Epoch 25/50\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.4864 - accuracy: 0.3555\n",
            "Epoch 00025: loss did not improve from 1.48650\n",
            "595/595 [==============================] - 7s 13ms/step - loss: 1.4868 - accuracy: 0.3554 - val_loss: 1.4891 - val_accuracy: 0.3393\n",
            "Epoch 26/50\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.4838 - accuracy: 0.3561\n",
            "Epoch 00026: loss improved from 1.48650 to 1.48367, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f59c41d7fd0>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.4837 - accuracy: 0.3566 - val_loss: 1.4946 - val_accuracy: 0.3089\n",
            "Epoch 27/50\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.4830 - accuracy: 0.3530\n",
            "Epoch 00027: loss improved from 1.48367 to 1.48297, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f59c41d7fd0>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.4830 - accuracy: 0.3532 - val_loss: 1.5004 - val_accuracy: 0.3272\n",
            "Epoch 28/50\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.4812 - accuracy: 0.3550\n",
            "Epoch 00028: loss improved from 1.48297 to 1.48077, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f59c41d7fd0>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.4808 - accuracy: 0.3549 - val_loss: 1.5266 - val_accuracy: 0.3221\n",
            "Epoch 29/50\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.4809 - accuracy: 0.3589\n",
            "Epoch 00029: loss improved from 1.48077 to 1.48072, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f59c41d7fd0>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.4807 - accuracy: 0.3591 - val_loss: 1.5171 - val_accuracy: 0.3216\n",
            "Epoch 30/50\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.4800 - accuracy: 0.3616\n",
            "Epoch 00030: loss improved from 1.48072 to 1.47967, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f59c41d7fd0>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.4797 - accuracy: 0.3616 - val_loss: 1.5120 - val_accuracy: 0.3391\n",
            "Epoch 31/50\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.4767 - accuracy: 0.3636\n",
            "Epoch 00031: loss improved from 1.47967 to 1.47647, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f59c41d7fd0>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.4765 - accuracy: 0.3636 - val_loss: 1.5117 - val_accuracy: 0.3353\n",
            "Epoch 32/50\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.4758 - accuracy: 0.3671\n",
            "Epoch 00032: loss improved from 1.47647 to 1.47612, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f59c41d7fd0>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.4761 - accuracy: 0.3668 - val_loss: 1.5309 - val_accuracy: 0.3157\n",
            "Epoch 33/50\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.4751 - accuracy: 0.3640\n",
            "Epoch 00033: loss improved from 1.47612 to 1.47511, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f59c41d7fd0>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.4751 - accuracy: 0.3638 - val_loss: 1.5156 - val_accuracy: 0.3399\n",
            "Epoch 34/50\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.4754 - accuracy: 0.3634\n",
            "Epoch 00034: loss did not improve from 1.47511\n",
            "595/595 [==============================] - 7s 13ms/step - loss: 1.4759 - accuracy: 0.3634 - val_loss: 1.5284 - val_accuracy: 0.3235\n",
            "Epoch 35/50\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.4707 - accuracy: 0.3696\n",
            "Epoch 00035: loss improved from 1.47511 to 1.47077, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f59c41d7fd0>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.4708 - accuracy: 0.3696 - val_loss: 1.5433 - val_accuracy: 0.3229\n",
            "Epoch 36/50\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.4745 - accuracy: 0.3657\n",
            "Epoch 00036: loss did not improve from 1.47077\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.4745 - accuracy: 0.3657 - val_loss: 1.5182 - val_accuracy: 0.3345\n",
            "Epoch 37/50\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.4733 - accuracy: 0.3620\n",
            "Epoch 00037: loss did not improve from 1.47077\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.4731 - accuracy: 0.3622 - val_loss: 1.5358 - val_accuracy: 0.3216\n",
            "Epoch 38/50\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.4728 - accuracy: 0.3642\n",
            "Epoch 00038: loss did not improve from 1.47077\n",
            "595/595 [==============================] - 7s 13ms/step - loss: 1.4729 - accuracy: 0.3644 - val_loss: 1.5747 - val_accuracy: 0.3159\n",
            "Epoch 39/50\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.4717 - accuracy: 0.3657\n",
            "Epoch 00039: loss did not improve from 1.47077\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.4717 - accuracy: 0.3657 - val_loss: 1.5523 - val_accuracy: 0.3264\n",
            "Epoch 40/50\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.4688 - accuracy: 0.3667\n",
            "Epoch 00040: loss improved from 1.47077 to 1.46867, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f59c41d7fd0>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.4687 - accuracy: 0.3667 - val_loss: 1.5319 - val_accuracy: 0.3385\n",
            "Epoch 41/50\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.4670 - accuracy: 0.3681\n",
            "Epoch 00041: loss improved from 1.46867 to 1.46697, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f59c41d7fd0>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.4670 - accuracy: 0.3681 - val_loss: 1.5752 - val_accuracy: 0.3275\n",
            "Epoch 42/50\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.4677 - accuracy: 0.3696\n",
            "Epoch 00042: loss did not improve from 1.46697\n",
            "595/595 [==============================] - 7s 13ms/step - loss: 1.4678 - accuracy: 0.3693 - val_loss: 1.5681 - val_accuracy: 0.3469\n",
            "Epoch 43/50\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.4671 - accuracy: 0.3643\n",
            "Epoch 00043: loss did not improve from 1.46697\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.4671 - accuracy: 0.3646 - val_loss: 1.5685 - val_accuracy: 0.3297\n",
            "Epoch 44/50\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.4643 - accuracy: 0.3661\n",
            "Epoch 00044: loss improved from 1.46697 to 1.46454, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f59c41d7fd0>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.4645 - accuracy: 0.3659 - val_loss: 1.5665 - val_accuracy: 0.3367\n",
            "Epoch 45/50\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.4648 - accuracy: 0.3704\n",
            "Epoch 00045: loss did not improve from 1.46454\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.4648 - accuracy: 0.3700 - val_loss: 1.5553 - val_accuracy: 0.3393\n",
            "Epoch 46/50\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.4630 - accuracy: 0.3654\n",
            "Epoch 00046: loss improved from 1.46454 to 1.46255, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f59c41d7fd0>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.4626 - accuracy: 0.3657 - val_loss: 1.5682 - val_accuracy: 0.3213\n",
            "Epoch 47/50\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.4625 - accuracy: 0.3659\n",
            "Epoch 00047: loss did not improve from 1.46255\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.4626 - accuracy: 0.3656 - val_loss: 1.5984 - val_accuracy: 0.3377\n",
            "Epoch 48/50\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.4617 - accuracy: 0.3690\n",
            "Epoch 00048: loss improved from 1.46255 to 1.46157, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f59c41d7fd0>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.4616 - accuracy: 0.3692 - val_loss: 1.6402 - val_accuracy: 0.3270\n",
            "Epoch 49/50\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.4582 - accuracy: 0.3743\n",
            "Epoch 00049: loss improved from 1.46157 to 1.45834, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f59c41d7fd0>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.4583 - accuracy: 0.3743 - val_loss: 1.5950 - val_accuracy: 0.3375\n",
            "Epoch 50/50\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.4600 - accuracy: 0.3749\n",
            "Epoch 00050: loss did not improve from 1.45834\n",
            "595/595 [==============================] - 7s 13ms/step - loss: 1.4599 - accuracy: 0.3752 - val_loss: 1.5681 - val_accuracy: 0.3469\n",
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
            "Epoch 1/50\n",
            "  2/595 [..............................] - ETA: 31s - loss: 2.3347 - accuracy: 0.1200WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0151s vs `on_train_batch_end` time: 0.0912s). Check your callbacks.\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.6021 - accuracy: 0.3112\n",
            "Epoch 00001: loss improved from inf to 1.60236, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adagrad.Adagrad object at 0x7f59b9872be0>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.6024 - accuracy: 0.3110 - val_loss: 1.5213 - val_accuracy: 0.3399\n",
            "Epoch 2/50\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5546 - accuracy: 0.3298\n",
            "Epoch 00002: loss improved from 1.60236 to 1.55455, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adagrad.Adagrad object at 0x7f59b9872be0>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5545 - accuracy: 0.3297 - val_loss: 1.5087 - val_accuracy: 0.3399\n",
            "Epoch 3/50\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5539 - accuracy: 0.3283\n",
            "Epoch 00003: loss improved from 1.55455 to 1.55411, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adagrad.Adagrad object at 0x7f59b9872be0>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5541 - accuracy: 0.3282 - val_loss: 1.5104 - val_accuracy: 0.3399\n",
            "Epoch 4/50\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5501 - accuracy: 0.3276\n",
            "Epoch 00004: loss improved from 1.55411 to 1.54999, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adagrad.Adagrad object at 0x7f59b9872be0>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5500 - accuracy: 0.3276 - val_loss: 1.5069 - val_accuracy: 0.3399\n",
            "Epoch 5/50\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.5471 - accuracy: 0.3255\n",
            "Epoch 00005: loss improved from 1.54999 to 1.54706, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adagrad.Adagrad object at 0x7f59b9872be0>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5471 - accuracy: 0.3255 - val_loss: 1.5065 - val_accuracy: 0.3399\n",
            "Epoch 6/50\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5439 - accuracy: 0.3280\n",
            "Epoch 00006: loss improved from 1.54706 to 1.54387, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adagrad.Adagrad object at 0x7f59b9872be0>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5439 - accuracy: 0.3277 - val_loss: 1.5065 - val_accuracy: 0.3399\n",
            "Epoch 7/50\n",
            "589/595 [============================>.] - ETA: 0s - loss: 1.5399 - accuracy: 0.3330\n",
            "Epoch 00007: loss improved from 1.54387 to 1.54005, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adagrad.Adagrad object at 0x7f59b9872be0>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5400 - accuracy: 0.3325 - val_loss: 1.5049 - val_accuracy: 0.3399\n",
            "Epoch 8/50\n",
            "589/595 [============================>.] - ETA: 0s - loss: 1.5431 - accuracy: 0.3358\n",
            "Epoch 00008: loss did not improve from 1.54005\n",
            "595/595 [==============================] - 7s 13ms/step - loss: 1.5432 - accuracy: 0.3357 - val_loss: 1.5042 - val_accuracy: 0.3399\n",
            "Epoch 9/50\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5408 - accuracy: 0.3307\n",
            "Epoch 00009: loss did not improve from 1.54005\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5407 - accuracy: 0.3307 - val_loss: 1.5043 - val_accuracy: 0.3399\n",
            "Epoch 10/50\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.5409 - accuracy: 0.3351\n",
            "Epoch 00010: loss did not improve from 1.54005\n",
            "595/595 [==============================] - 7s 12ms/step - loss: 1.5407 - accuracy: 0.3350 - val_loss: 1.5040 - val_accuracy: 0.3399\n",
            "Epoch 11/50\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5352 - accuracy: 0.3389\n",
            "Epoch 00011: loss improved from 1.54005 to 1.53548, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adagrad.Adagrad object at 0x7f59b9872be0>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5355 - accuracy: 0.3385 - val_loss: 1.5036 - val_accuracy: 0.3399\n",
            "Epoch 12/50\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5358 - accuracy: 0.3374\n",
            "Epoch 00012: loss did not improve from 1.53548\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5361 - accuracy: 0.3370 - val_loss: 1.5042 - val_accuracy: 0.3399\n",
            "Epoch 13/50\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5358 - accuracy: 0.3363\n",
            "Epoch 00013: loss did not improve from 1.53548\n",
            "595/595 [==============================] - 7s 13ms/step - loss: 1.5358 - accuracy: 0.3363 - val_loss: 1.5036 - val_accuracy: 0.3399\n",
            "Epoch 14/50\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.5356 - accuracy: 0.3385\n",
            "Epoch 00014: loss improved from 1.53548 to 1.53489, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adagrad.Adagrad object at 0x7f59b9872be0>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5349 - accuracy: 0.3391 - val_loss: 1.5046 - val_accuracy: 0.3399\n",
            "Epoch 15/50\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.5333 - accuracy: 0.3394\n",
            "Epoch 00015: loss improved from 1.53489 to 1.53339, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adagrad.Adagrad object at 0x7f59b9872be0>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5334 - accuracy: 0.3395 - val_loss: 1.5042 - val_accuracy: 0.3399\n",
            "Epoch 16/50\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.5339 - accuracy: 0.3346\n",
            "Epoch 00016: loss did not improve from 1.53339\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5339 - accuracy: 0.3346 - val_loss: 1.5036 - val_accuracy: 0.3399\n",
            "Epoch 17/50\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5356 - accuracy: 0.3394\n",
            "Epoch 00017: loss did not improve from 1.53339\n",
            "595/595 [==============================] - 7s 12ms/step - loss: 1.5357 - accuracy: 0.3392 - val_loss: 1.5040 - val_accuracy: 0.3399\n",
            "Epoch 18/50\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5341 - accuracy: 0.3347\n",
            "Epoch 00018: loss did not improve from 1.53339\n",
            "595/595 [==============================] - 7s 12ms/step - loss: 1.5341 - accuracy: 0.3348 - val_loss: 1.5035 - val_accuracy: 0.3399\n",
            "Epoch 19/50\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5329 - accuracy: 0.3395\n",
            "Epoch 00019: loss improved from 1.53339 to 1.53306, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adagrad.Adagrad object at 0x7f59b9872be0>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5331 - accuracy: 0.3394 - val_loss: 1.5033 - val_accuracy: 0.3399\n",
            "Epoch 20/50\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5326 - accuracy: 0.3364\n",
            "Epoch 00020: loss improved from 1.53306 to 1.53253, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adagrad.Adagrad object at 0x7f59b9872be0>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5325 - accuracy: 0.3364 - val_loss: 1.5032 - val_accuracy: 0.3399\n",
            "Epoch 21/50\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5326 - accuracy: 0.3389\n",
            "Epoch 00021: loss did not improve from 1.53253\n",
            "595/595 [==============================] - 7s 12ms/step - loss: 1.5328 - accuracy: 0.3388 - val_loss: 1.5030 - val_accuracy: 0.3399\n",
            "Epoch 22/50\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5315 - accuracy: 0.3381\n",
            "Epoch 00022: loss improved from 1.53253 to 1.53179, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adagrad.Adagrad object at 0x7f59b9872be0>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5318 - accuracy: 0.3380 - val_loss: 1.5043 - val_accuracy: 0.3399\n",
            "Epoch 23/50\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5311 - accuracy: 0.3376\n",
            "Epoch 00023: loss improved from 1.53179 to 1.53145, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adagrad.Adagrad object at 0x7f59b9872be0>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5315 - accuracy: 0.3377 - val_loss: 1.5030 - val_accuracy: 0.3399\n",
            "Epoch 24/50\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5312 - accuracy: 0.3387\n",
            "Epoch 00024: loss improved from 1.53145 to 1.53125, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adagrad.Adagrad object at 0x7f59b9872be0>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5313 - accuracy: 0.3387 - val_loss: 1.5038 - val_accuracy: 0.3399\n",
            "Epoch 25/50\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5319 - accuracy: 0.3410\n",
            "Epoch 00025: loss did not improve from 1.53125\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5319 - accuracy: 0.3413 - val_loss: 1.5033 - val_accuracy: 0.3399\n",
            "Epoch 26/50\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.5325 - accuracy: 0.3401\n",
            "Epoch 00026: loss did not improve from 1.53125\n",
            "595/595 [==============================] - 7s 13ms/step - loss: 1.5325 - accuracy: 0.3401 - val_loss: 1.5032 - val_accuracy: 0.3399\n",
            "Epoch 27/50\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5306 - accuracy: 0.3423\n",
            "Epoch 00027: loss improved from 1.53125 to 1.53047, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adagrad.Adagrad object at 0x7f59b9872be0>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5305 - accuracy: 0.3423 - val_loss: 1.5033 - val_accuracy: 0.3399\n",
            "Epoch 28/50\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.5298 - accuracy: 0.3420\n",
            "Epoch 00028: loss improved from 1.53047 to 1.52983, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adagrad.Adagrad object at 0x7f59b9872be0>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5298 - accuracy: 0.3420 - val_loss: 1.5035 - val_accuracy: 0.3399\n",
            "Epoch 29/50\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.5269 - accuracy: 0.3410\n",
            "Epoch 00029: loss improved from 1.52983 to 1.52705, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adagrad.Adagrad object at 0x7f59b9872be0>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5271 - accuracy: 0.3411 - val_loss: 1.5023 - val_accuracy: 0.3399\n",
            "Epoch 30/50\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.5293 - accuracy: 0.3418\n",
            "Epoch 00030: loss did not improve from 1.52705\n",
            "595/595 [==============================] - 7s 13ms/step - loss: 1.5293 - accuracy: 0.3418 - val_loss: 1.5026 - val_accuracy: 0.3399\n",
            "Epoch 31/50\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5297 - accuracy: 0.3398\n",
            "Epoch 00031: loss did not improve from 1.52705\n",
            "595/595 [==============================] - 7s 12ms/step - loss: 1.5295 - accuracy: 0.3402 - val_loss: 1.5033 - val_accuracy: 0.3399\n",
            "Epoch 32/50\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5292 - accuracy: 0.3431\n",
            "Epoch 00032: loss did not improve from 1.52705\n",
            "595/595 [==============================] - 7s 13ms/step - loss: 1.5288 - accuracy: 0.3432 - val_loss: 1.5029 - val_accuracy: 0.3399\n",
            "Epoch 33/50\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5287 - accuracy: 0.3434\n",
            "Epoch 00033: loss did not improve from 1.52705\n",
            "595/595 [==============================] - 7s 12ms/step - loss: 1.5285 - accuracy: 0.3432 - val_loss: 1.5023 - val_accuracy: 0.3399\n",
            "Epoch 34/50\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5272 - accuracy: 0.3427\n",
            "Epoch 00034: loss did not improve from 1.52705\n",
            "595/595 [==============================] - 7s 12ms/step - loss: 1.5272 - accuracy: 0.3424 - val_loss: 1.5025 - val_accuracy: 0.3399\n",
            "Epoch 35/50\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.5280 - accuracy: 0.3401\n",
            "Epoch 00035: loss did not improve from 1.52705\n",
            "595/595 [==============================] - 7s 12ms/step - loss: 1.5284 - accuracy: 0.3400 - val_loss: 1.5022 - val_accuracy: 0.3399\n",
            "Epoch 36/50\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5302 - accuracy: 0.3397\n",
            "Epoch 00036: loss did not improve from 1.52705\n",
            "595/595 [==============================] - 7s 12ms/step - loss: 1.5301 - accuracy: 0.3396 - val_loss: 1.5025 - val_accuracy: 0.3399\n",
            "Epoch 37/50\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5285 - accuracy: 0.3422\n",
            "Epoch 00037: loss did not improve from 1.52705\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5285 - accuracy: 0.3421 - val_loss: 1.5020 - val_accuracy: 0.3399\n",
            "Epoch 38/50\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5280 - accuracy: 0.3402\n",
            "Epoch 00038: loss did not improve from 1.52705\n",
            "595/595 [==============================] - 7s 12ms/step - loss: 1.5276 - accuracy: 0.3405 - val_loss: 1.5015 - val_accuracy: 0.3399\n",
            "Epoch 39/50\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5276 - accuracy: 0.3431\n",
            "Epoch 00039: loss improved from 1.52705 to 1.52695, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adagrad.Adagrad object at 0x7f59b9872be0>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5269 - accuracy: 0.3436 - val_loss: 1.5016 - val_accuracy: 0.3399\n",
            "Epoch 40/50\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.5274 - accuracy: 0.3432\n",
            "Epoch 00040: loss did not improve from 1.52695\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5273 - accuracy: 0.3428 - val_loss: 1.5012 - val_accuracy: 0.3399\n",
            "Epoch 41/50\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.5291 - accuracy: 0.3409\n",
            "Epoch 00041: loss did not improve from 1.52695\n",
            "595/595 [==============================] - 7s 13ms/step - loss: 1.5291 - accuracy: 0.3411 - val_loss: 1.5016 - val_accuracy: 0.3399\n",
            "Epoch 42/50\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.5260 - accuracy: 0.3444\n",
            "Epoch 00042: loss improved from 1.52695 to 1.52564, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adagrad.Adagrad object at 0x7f59b9872be0>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5256 - accuracy: 0.3449 - val_loss: 1.5019 - val_accuracy: 0.3399\n",
            "Epoch 43/50\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5266 - accuracy: 0.3411\n",
            "Epoch 00043: loss did not improve from 1.52564\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5269 - accuracy: 0.3410 - val_loss: 1.5016 - val_accuracy: 0.3399\n",
            "Epoch 44/50\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5261 - accuracy: 0.3432\n",
            "Epoch 00044: loss did not improve from 1.52564\n",
            "595/595 [==============================] - 7s 13ms/step - loss: 1.5259 - accuracy: 0.3432 - val_loss: 1.5015 - val_accuracy: 0.3399\n",
            "Epoch 45/50\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.5254 - accuracy: 0.3432\n",
            "Epoch 00045: loss did not improve from 1.52564\n",
            "595/595 [==============================] - 7s 13ms/step - loss: 1.5259 - accuracy: 0.3426 - val_loss: 1.5016 - val_accuracy: 0.3399\n",
            "Epoch 46/50\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5272 - accuracy: 0.3396\n",
            "Epoch 00046: loss did not improve from 1.52564\n",
            "595/595 [==============================] - 7s 12ms/step - loss: 1.5270 - accuracy: 0.3399 - val_loss: 1.5017 - val_accuracy: 0.3399\n",
            "Epoch 47/50\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5263 - accuracy: 0.3411\n",
            "Epoch 00047: loss did not improve from 1.52564\n",
            "595/595 [==============================] - 7s 13ms/step - loss: 1.5265 - accuracy: 0.3410 - val_loss: 1.5016 - val_accuracy: 0.3399\n",
            "Epoch 48/50\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.5267 - accuracy: 0.3412\n",
            "Epoch 00048: loss did not improve from 1.52564\n",
            "595/595 [==============================] - 7s 13ms/step - loss: 1.5260 - accuracy: 0.3418 - val_loss: 1.5014 - val_accuracy: 0.3399\n",
            "Epoch 49/50\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5263 - accuracy: 0.3438\n",
            "Epoch 00049: loss did not improve from 1.52564\n",
            "595/595 [==============================] - 7s 12ms/step - loss: 1.5262 - accuracy: 0.3436 - val_loss: 1.5012 - val_accuracy: 0.3399\n",
            "Epoch 50/50\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5254 - accuracy: 0.3403\n",
            "Epoch 00050: loss improved from 1.52564 to 1.52495, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adagrad.Adagrad object at 0x7f59b9872be0>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5250 - accuracy: 0.3404 - val_loss: 1.5010 - val_accuracy: 0.3399\n",
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
            "Epoch 1/50\n",
            "  2/595 [..............................] - ETA: 33s - loss: 2.0763 - accuracy: 0.1400WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0143s vs `on_train_batch_end` time: 0.0976s). Check your callbacks.\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5553 - accuracy: 0.3291\n",
            "Epoch 00001: loss improved from inf to 1.55545, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adamax.Adamax object at 0x7f59b980be10>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5554 - accuracy: 0.3291 - val_loss: 1.5041 - val_accuracy: 0.3399\n",
            "Epoch 2/50\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.5335 - accuracy: 0.3389\n",
            "Epoch 00002: loss improved from 1.55545 to 1.53351, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adamax.Adamax object at 0x7f59b980be10>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5335 - accuracy: 0.3389 - val_loss: 1.5014 - val_accuracy: 0.3399\n",
            "Epoch 3/50\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.5284 - accuracy: 0.3379\n",
            "Epoch 00003: loss improved from 1.53351 to 1.52836, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adamax.Adamax object at 0x7f59b980be10>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5284 - accuracy: 0.3379 - val_loss: 1.5152 - val_accuracy: 0.3399\n",
            "Epoch 4/50\n",
            "589/595 [============================>.] - ETA: 0s - loss: 1.5258 - accuracy: 0.3400\n",
            "Epoch 00004: loss improved from 1.52836 to 1.52622, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adamax.Adamax object at 0x7f59b980be10>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5262 - accuracy: 0.3392 - val_loss: 1.5033 - val_accuracy: 0.3399\n",
            "Epoch 5/50\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.5199 - accuracy: 0.3424\n",
            "Epoch 00005: loss improved from 1.52622 to 1.51986, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adamax.Adamax object at 0x7f59b980be10>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5199 - accuracy: 0.3424 - val_loss: 1.4914 - val_accuracy: 0.3399\n",
            "Epoch 6/50\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5199 - accuracy: 0.3426\n",
            "Epoch 00006: loss improved from 1.51986 to 1.51983, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adamax.Adamax object at 0x7f59b980be10>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5198 - accuracy: 0.3427 - val_loss: 1.5032 - val_accuracy: 0.3399\n",
            "Epoch 7/50\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5173 - accuracy: 0.3423\n",
            "Epoch 00007: loss improved from 1.51983 to 1.51749, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adamax.Adamax object at 0x7f59b980be10>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5175 - accuracy: 0.3425 - val_loss: 1.4980 - val_accuracy: 0.3399\n",
            "Epoch 8/50\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5161 - accuracy: 0.3434\n",
            "Epoch 00008: loss improved from 1.51749 to 1.51585, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adamax.Adamax object at 0x7f59b980be10>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5159 - accuracy: 0.3432 - val_loss: 1.4876 - val_accuracy: 0.3407\n",
            "Epoch 9/50\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5131 - accuracy: 0.3417\n",
            "Epoch 00009: loss improved from 1.51585 to 1.51286, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adamax.Adamax object at 0x7f59b980be10>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5129 - accuracy: 0.3418 - val_loss: 1.4944 - val_accuracy: 0.3372\n",
            "Epoch 10/50\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5123 - accuracy: 0.3444\n",
            "Epoch 00010: loss improved from 1.51286 to 1.51260, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adamax.Adamax object at 0x7f59b980be10>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5126 - accuracy: 0.3442 - val_loss: 1.4970 - val_accuracy: 0.3372\n",
            "Epoch 11/50\n",
            "589/595 [============================>.] - ETA: 0s - loss: 1.5111 - accuracy: 0.3473\n",
            "Epoch 00011: loss improved from 1.51260 to 1.51063, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adamax.Adamax object at 0x7f59b980be10>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5106 - accuracy: 0.3476 - val_loss: 1.4914 - val_accuracy: 0.3205\n",
            "Epoch 12/50\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.5104 - accuracy: 0.3449\n",
            "Epoch 00012: loss improved from 1.51063 to 1.51050, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adamax.Adamax object at 0x7f59b980be10>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5105 - accuracy: 0.3449 - val_loss: 1.4974 - val_accuracy: 0.3372\n",
            "Epoch 13/50\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5096 - accuracy: 0.3428\n",
            "Epoch 00013: loss improved from 1.51050 to 1.50957, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adamax.Adamax object at 0x7f59b980be10>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5096 - accuracy: 0.3428 - val_loss: 1.4949 - val_accuracy: 0.3372\n",
            "Epoch 14/50\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5062 - accuracy: 0.3465\n",
            "Epoch 00014: loss improved from 1.50957 to 1.50615, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adamax.Adamax object at 0x7f59b980be10>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5061 - accuracy: 0.3464 - val_loss: 1.5026 - val_accuracy: 0.3372\n",
            "Epoch 15/50\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.5055 - accuracy: 0.3471\n",
            "Epoch 00015: loss improved from 1.50615 to 1.50546, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adamax.Adamax object at 0x7f59b980be10>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5055 - accuracy: 0.3471 - val_loss: 1.4993 - val_accuracy: 0.3372\n",
            "Epoch 16/50\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5056 - accuracy: 0.3492\n",
            "Epoch 00016: loss did not improve from 1.50546\n",
            "595/595 [==============================] - 7s 13ms/step - loss: 1.5056 - accuracy: 0.3495 - val_loss: 1.4980 - val_accuracy: 0.3399\n",
            "Epoch 17/50\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.5024 - accuracy: 0.3489\n",
            "Epoch 00017: loss improved from 1.50546 to 1.50244, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adamax.Adamax object at 0x7f59b980be10>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5024 - accuracy: 0.3489 - val_loss: 1.4937 - val_accuracy: 0.3367\n",
            "Epoch 18/50\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5023 - accuracy: 0.3465\n",
            "Epoch 00018: loss did not improve from 1.50244\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5026 - accuracy: 0.3464 - val_loss: 1.4954 - val_accuracy: 0.3205\n",
            "Epoch 19/50\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.4989 - accuracy: 0.3542\n",
            "Epoch 00019: loss improved from 1.50244 to 1.49865, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adamax.Adamax object at 0x7f59b980be10>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.4986 - accuracy: 0.3544 - val_loss: 1.5002 - val_accuracy: 0.3367\n",
            "Epoch 20/50\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.4982 - accuracy: 0.3518\n",
            "Epoch 00020: loss improved from 1.49865 to 1.49848, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adamax.Adamax object at 0x7f59b980be10>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.4985 - accuracy: 0.3519 - val_loss: 1.4951 - val_accuracy: 0.3227\n",
            "Epoch 21/50\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.4991 - accuracy: 0.3511\n",
            "Epoch 00021: loss did not improve from 1.49848\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.4988 - accuracy: 0.3510 - val_loss: 1.4979 - val_accuracy: 0.3143\n",
            "Epoch 22/50\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.4967 - accuracy: 0.3492\n",
            "Epoch 00022: loss improved from 1.49848 to 1.49667, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adamax.Adamax object at 0x7f59b980be10>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.4967 - accuracy: 0.3492 - val_loss: 1.4914 - val_accuracy: 0.3243\n",
            "Epoch 23/50\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.4951 - accuracy: 0.3516\n",
            "Epoch 00023: loss improved from 1.49667 to 1.49511, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adamax.Adamax object at 0x7f59b980be10>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.4951 - accuracy: 0.3516 - val_loss: 1.4950 - val_accuracy: 0.3065\n",
            "Epoch 24/50\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.4959 - accuracy: 0.3535\n",
            "Epoch 00024: loss did not improve from 1.49511\n",
            "595/595 [==============================] - 7s 13ms/step - loss: 1.4961 - accuracy: 0.3535 - val_loss: 1.4951 - val_accuracy: 0.3369\n",
            "Epoch 25/50\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.4945 - accuracy: 0.3521\n",
            "Epoch 00025: loss improved from 1.49511 to 1.49430, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adamax.Adamax object at 0x7f59b980be10>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.4943 - accuracy: 0.3523 - val_loss: 1.4991 - val_accuracy: 0.3358\n",
            "Epoch 26/50\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.4938 - accuracy: 0.3506\n",
            "Epoch 00026: loss improved from 1.49430 to 1.49425, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adamax.Adamax object at 0x7f59b980be10>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.4943 - accuracy: 0.3504 - val_loss: 1.4865 - val_accuracy: 0.3326\n",
            "Epoch 27/50\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.4939 - accuracy: 0.3527\n",
            "Epoch 00027: loss improved from 1.49425 to 1.49390, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adamax.Adamax object at 0x7f59b980be10>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.4939 - accuracy: 0.3527 - val_loss: 1.4908 - val_accuracy: 0.3213\n",
            "Epoch 28/50\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.4909 - accuracy: 0.3533\n",
            "Epoch 00028: loss improved from 1.49390 to 1.49083, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adamax.Adamax object at 0x7f59b980be10>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.4908 - accuracy: 0.3532 - val_loss: 1.4918 - val_accuracy: 0.3135\n",
            "Epoch 29/50\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.4891 - accuracy: 0.3523\n",
            "Epoch 00029: loss improved from 1.49083 to 1.48911, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adamax.Adamax object at 0x7f59b980be10>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.4891 - accuracy: 0.3522 - val_loss: 1.4934 - val_accuracy: 0.3175\n",
            "Epoch 30/50\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.4910 - accuracy: 0.3548\n",
            "Epoch 00030: loss did not improve from 1.48911\n",
            "595/595 [==============================] - 7s 13ms/step - loss: 1.4905 - accuracy: 0.3550 - val_loss: 1.5024 - val_accuracy: 0.3219\n",
            "Epoch 31/50\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.4887 - accuracy: 0.3515\n",
            "Epoch 00031: loss improved from 1.48911 to 1.48862, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adamax.Adamax object at 0x7f59b980be10>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.4886 - accuracy: 0.3515 - val_loss: 1.5028 - val_accuracy: 0.3165\n",
            "Epoch 32/50\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.4880 - accuracy: 0.3565\n",
            "Epoch 00032: loss improved from 1.48862 to 1.48799, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adamax.Adamax object at 0x7f59b980be10>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.4880 - accuracy: 0.3565 - val_loss: 1.4962 - val_accuracy: 0.3358\n",
            "Epoch 33/50\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.4857 - accuracy: 0.3543\n",
            "Epoch 00033: loss improved from 1.48799 to 1.48555, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adamax.Adamax object at 0x7f59b980be10>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.4856 - accuracy: 0.3542 - val_loss: 1.4972 - val_accuracy: 0.3116\n",
            "Epoch 34/50\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.4866 - accuracy: 0.3557\n",
            "Epoch 00034: loss did not improve from 1.48555\n",
            "595/595 [==============================] - 7s 13ms/step - loss: 1.4864 - accuracy: 0.3560 - val_loss: 1.4973 - val_accuracy: 0.3326\n",
            "Epoch 35/50\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.4833 - accuracy: 0.3616\n",
            "Epoch 00035: loss improved from 1.48555 to 1.48334, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adamax.Adamax object at 0x7f59b980be10>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.4833 - accuracy: 0.3617 - val_loss: 1.4953 - val_accuracy: 0.3423\n",
            "Epoch 36/50\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.4840 - accuracy: 0.3608\n",
            "Epoch 00036: loss did not improve from 1.48334\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.4839 - accuracy: 0.3613 - val_loss: 1.5000 - val_accuracy: 0.3189\n",
            "Epoch 37/50\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.4825 - accuracy: 0.3614\n",
            "Epoch 00037: loss improved from 1.48334 to 1.48232, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adamax.Adamax object at 0x7f59b980be10>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.4823 - accuracy: 0.3617 - val_loss: 1.5010 - val_accuracy: 0.3348\n",
            "Epoch 38/50\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.4834 - accuracy: 0.3639\n",
            "Epoch 00038: loss did not improve from 1.48232\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.4836 - accuracy: 0.3639 - val_loss: 1.5014 - val_accuracy: 0.3291\n",
            "Epoch 39/50\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.4818 - accuracy: 0.3583\n",
            "Epoch 00039: loss improved from 1.48232 to 1.48190, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adamax.Adamax object at 0x7f59b980be10>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.4819 - accuracy: 0.3586 - val_loss: 1.4979 - val_accuracy: 0.3079\n",
            "Epoch 40/50\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.4813 - accuracy: 0.3637\n",
            "Epoch 00040: loss improved from 1.48190 to 1.48129, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adamax.Adamax object at 0x7f59b980be10>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.4813 - accuracy: 0.3637 - val_loss: 1.5017 - val_accuracy: 0.3140\n",
            "Epoch 41/50\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.4789 - accuracy: 0.3597\n",
            "Epoch 00041: loss improved from 1.48129 to 1.47887, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adamax.Adamax object at 0x7f59b980be10>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.4789 - accuracy: 0.3597 - val_loss: 1.5018 - val_accuracy: 0.3259\n",
            "Epoch 42/50\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.4804 - accuracy: 0.3587\n",
            "Epoch 00042: loss did not improve from 1.47887\n",
            "595/595 [==============================] - 7s 13ms/step - loss: 1.4802 - accuracy: 0.3587 - val_loss: 1.4961 - val_accuracy: 0.3227\n",
            "Epoch 43/50\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.4788 - accuracy: 0.3599\n",
            "Epoch 00043: loss improved from 1.47887 to 1.47880, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adamax.Adamax object at 0x7f59b980be10>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.4788 - accuracy: 0.3600 - val_loss: 1.4985 - val_accuracy: 0.3221\n",
            "Epoch 44/50\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.4790 - accuracy: 0.3605\n",
            "Epoch 00044: loss did not improve from 1.47880\n",
            "595/595 [==============================] - 7s 12ms/step - loss: 1.4792 - accuracy: 0.3603 - val_loss: 1.5130 - val_accuracy: 0.3278\n",
            "Epoch 45/50\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.4783 - accuracy: 0.3635\n",
            "Epoch 00045: loss improved from 1.47880 to 1.47768, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adamax.Adamax object at 0x7f59b980be10>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.4777 - accuracy: 0.3638 - val_loss: 1.4963 - val_accuracy: 0.3310\n",
            "Epoch 46/50\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.4771 - accuracy: 0.3639\n",
            "Epoch 00046: loss improved from 1.47768 to 1.47717, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adamax.Adamax object at 0x7f59b980be10>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.4772 - accuracy: 0.3637 - val_loss: 1.4970 - val_accuracy: 0.3264\n",
            "Epoch 47/50\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.4787 - accuracy: 0.3628\n",
            "Epoch 00047: loss did not improve from 1.47717\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.4781 - accuracy: 0.3634 - val_loss: 1.5040 - val_accuracy: 0.3213\n",
            "Epoch 48/50\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.4762 - accuracy: 0.3661\n",
            "Epoch 00048: loss improved from 1.47717 to 1.47640, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adamax.Adamax object at 0x7f59b980be10>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.4764 - accuracy: 0.3661 - val_loss: 1.4992 - val_accuracy: 0.3208\n",
            "Epoch 49/50\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.4754 - accuracy: 0.3651\n",
            "Epoch 00049: loss improved from 1.47640 to 1.47536, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adamax.Adamax object at 0x7f59b980be10>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.4754 - accuracy: 0.3650 - val_loss: 1.5013 - val_accuracy: 0.3251\n",
            "Epoch 50/50\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.4726 - accuracy: 0.3676\n",
            "Epoch 00050: loss improved from 1.47536 to 1.47272, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adamax.Adamax object at 0x7f59b980be10>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.4727 - accuracy: 0.3675 - val_loss: 1.4998 - val_accuracy: 0.3367\n",
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
            "Epoch 1/50\n",
            "  2/595 [..............................] - ETA: 33s - loss: 2.1722 - accuracy: 0.0400   WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0142s vs `on_train_batch_end` time: 0.0968s). Check your callbacks.\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.6596 - accuracy: 0.3032\n",
            "Epoch 00001: loss improved from inf to 1.65915, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.ftrl.Ftrl object at 0x7f59b2f7e080>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.6591 - accuracy: 0.3035 - val_loss: 1.5447 - val_accuracy: 0.3399\n",
            "Epoch 2/50\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5574 - accuracy: 0.3436\n",
            "Epoch 00002: loss improved from 1.65915 to 1.55745, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.ftrl.Ftrl object at 0x7f59b2f7e080>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5574 - accuracy: 0.3435 - val_loss: 1.5183 - val_accuracy: 0.3399\n",
            "Epoch 3/50\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5435 - accuracy: 0.3435\n",
            "Epoch 00003: loss improved from 1.55745 to 1.54326, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.ftrl.Ftrl object at 0x7f59b2f7e080>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5433 - accuracy: 0.3437 - val_loss: 1.5096 - val_accuracy: 0.3399\n",
            "Epoch 4/50\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5368 - accuracy: 0.3437\n",
            "Epoch 00004: loss improved from 1.54326 to 1.53677, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.ftrl.Ftrl object at 0x7f59b2f7e080>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5368 - accuracy: 0.3437 - val_loss: 1.5056 - val_accuracy: 0.3399\n",
            "Epoch 5/50\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.5348 - accuracy: 0.3443\n",
            "Epoch 00005: loss improved from 1.53677 to 1.53525, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.ftrl.Ftrl object at 0x7f59b2f7e080>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5352 - accuracy: 0.3438 - val_loss: 1.5030 - val_accuracy: 0.3399\n",
            "Epoch 6/50\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5325 - accuracy: 0.3437\n",
            "Epoch 00006: loss improved from 1.53525 to 1.53254, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.ftrl.Ftrl object at 0x7f59b2f7e080>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5325 - accuracy: 0.3433 - val_loss: 1.5013 - val_accuracy: 0.3399\n",
            "Epoch 7/50\n",
            "589/595 [============================>.] - ETA: 0s - loss: 1.5307 - accuracy: 0.3439\n",
            "Epoch 00007: loss improved from 1.53254 to 1.53099, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.ftrl.Ftrl object at 0x7f59b2f7e080>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5310 - accuracy: 0.3436 - val_loss: 1.5011 - val_accuracy: 0.3399\n",
            "Epoch 8/50\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5310 - accuracy: 0.3438\n",
            "Epoch 00008: loss did not improve from 1.53099\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5310 - accuracy: 0.3437 - val_loss: 1.5000 - val_accuracy: 0.3399\n",
            "Epoch 9/50\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5302 - accuracy: 0.3437\n",
            "Epoch 00009: loss improved from 1.53099 to 1.53016, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.ftrl.Ftrl object at 0x7f59b2f7e080>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5302 - accuracy: 0.3436 - val_loss: 1.4995 - val_accuracy: 0.3399\n",
            "Epoch 10/50\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5298 - accuracy: 0.3436\n",
            "Epoch 00010: loss improved from 1.53016 to 1.52974, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.ftrl.Ftrl object at 0x7f59b2f7e080>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5297 - accuracy: 0.3436 - val_loss: 1.4991 - val_accuracy: 0.3399\n",
            "Epoch 11/50\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.5308 - accuracy: 0.3437\n",
            "Epoch 00011: loss did not improve from 1.52974\n",
            "595/595 [==============================] - 7s 13ms/step - loss: 1.5307 - accuracy: 0.3434 - val_loss: 1.4995 - val_accuracy: 0.3399\n",
            "Epoch 12/50\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.5300 - accuracy: 0.3431\n",
            "Epoch 00012: loss did not improve from 1.52974\n",
            "595/595 [==============================] - 7s 13ms/step - loss: 1.5301 - accuracy: 0.3434 - val_loss: 1.4995 - val_accuracy: 0.3399\n",
            "Epoch 13/50\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.5300 - accuracy: 0.3436\n",
            "Epoch 00013: loss did not improve from 1.52974\n",
            "595/595 [==============================] - 7s 12ms/step - loss: 1.5300 - accuracy: 0.3436 - val_loss: 1.4989 - val_accuracy: 0.3399\n",
            "Epoch 14/50\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5299 - accuracy: 0.3436\n",
            "Epoch 00014: loss improved from 1.52974 to 1.52948, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.ftrl.Ftrl object at 0x7f59b2f7e080>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5295 - accuracy: 0.3440 - val_loss: 1.4983 - val_accuracy: 0.3399\n",
            "Epoch 15/50\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5290 - accuracy: 0.3439\n",
            "Epoch 00015: loss improved from 1.52948 to 1.52879, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.ftrl.Ftrl object at 0x7f59b2f7e080>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5288 - accuracy: 0.3440 - val_loss: 1.4984 - val_accuracy: 0.3399\n",
            "Epoch 16/50\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5290 - accuracy: 0.3436\n",
            "Epoch 00016: loss did not improve from 1.52879\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5292 - accuracy: 0.3434 - val_loss: 1.4985 - val_accuracy: 0.3399\n",
            "Epoch 17/50\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5282 - accuracy: 0.3437\n",
            "Epoch 00017: loss improved from 1.52879 to 1.52853, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.ftrl.Ftrl object at 0x7f59b2f7e080>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5285 - accuracy: 0.3435 - val_loss: 1.4985 - val_accuracy: 0.3399\n",
            "Epoch 18/50\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5290 - accuracy: 0.3438\n",
            "Epoch 00018: loss did not improve from 1.52853\n",
            "595/595 [==============================] - 7s 13ms/step - loss: 1.5293 - accuracy: 0.3436 - val_loss: 1.4980 - val_accuracy: 0.3399\n",
            "Epoch 19/50\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5290 - accuracy: 0.3436\n",
            "Epoch 00019: loss did not improve from 1.52853\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5289 - accuracy: 0.3435 - val_loss: 1.4983 - val_accuracy: 0.3399\n",
            "Epoch 20/50\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5285 - accuracy: 0.3437\n",
            "Epoch 00020: loss did not improve from 1.52853\n",
            "595/595 [==============================] - 7s 12ms/step - loss: 1.5286 - accuracy: 0.3437 - val_loss: 1.4985 - val_accuracy: 0.3399\n",
            "Epoch 21/50\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5292 - accuracy: 0.3436\n",
            "Epoch 00021: loss did not improve from 1.52853\n",
            "595/595 [==============================] - 7s 12ms/step - loss: 1.5294 - accuracy: 0.3432 - val_loss: 1.4985 - val_accuracy: 0.3399\n",
            "Epoch 22/50\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5300 - accuracy: 0.3435\n",
            "Epoch 00022: loss did not improve from 1.52853\n",
            "595/595 [==============================] - 7s 13ms/step - loss: 1.5300 - accuracy: 0.3433 - val_loss: 1.4987 - val_accuracy: 0.3399\n",
            "Epoch 23/50\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5278 - accuracy: 0.3438\n",
            "Epoch 00023: loss improved from 1.52853 to 1.52793, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.ftrl.Ftrl object at 0x7f59b2f7e080>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5279 - accuracy: 0.3436 - val_loss: 1.4985 - val_accuracy: 0.3399\n",
            "Epoch 24/50\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.5281 - accuracy: 0.3440\n",
            "Epoch 00024: loss did not improve from 1.52793\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5281 - accuracy: 0.3440 - val_loss: 1.4986 - val_accuracy: 0.3399\n",
            "Epoch 25/50\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5285 - accuracy: 0.3434\n",
            "Epoch 00025: loss did not improve from 1.52793\n",
            "595/595 [==============================] - 7s 12ms/step - loss: 1.5284 - accuracy: 0.3434 - val_loss: 1.4983 - val_accuracy: 0.3399\n",
            "Epoch 26/50\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.5282 - accuracy: 0.3436\n",
            "Epoch 00026: loss did not improve from 1.52793\n",
            "595/595 [==============================] - 7s 12ms/step - loss: 1.5282 - accuracy: 0.3436 - val_loss: 1.4984 - val_accuracy: 0.3399\n",
            "Epoch 27/50\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5283 - accuracy: 0.3437\n",
            "Epoch 00027: loss did not improve from 1.52793\n",
            "595/595 [==============================] - 7s 12ms/step - loss: 1.5281 - accuracy: 0.3436 - val_loss: 1.4982 - val_accuracy: 0.3399\n",
            "Epoch 28/50\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.5298 - accuracy: 0.3432\n",
            "Epoch 00028: loss did not improve from 1.52793\n",
            "595/595 [==============================] - 7s 12ms/step - loss: 1.5295 - accuracy: 0.3435 - val_loss: 1.4982 - val_accuracy: 0.3399\n",
            "Epoch 29/50\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5284 - accuracy: 0.3436\n",
            "Epoch 00029: loss did not improve from 1.52793\n",
            "595/595 [==============================] - 7s 12ms/step - loss: 1.5286 - accuracy: 0.3436 - val_loss: 1.4984 - val_accuracy: 0.3399\n",
            "Epoch 30/50\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5290 - accuracy: 0.3438\n",
            "Epoch 00030: loss did not improve from 1.52793\n",
            "595/595 [==============================] - 7s 12ms/step - loss: 1.5286 - accuracy: 0.3436 - val_loss: 1.4982 - val_accuracy: 0.3399\n",
            "Epoch 31/50\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.5286 - accuracy: 0.3427\n",
            "Epoch 00031: loss did not improve from 1.52793\n",
            "595/595 [==============================] - 7s 12ms/step - loss: 1.5283 - accuracy: 0.3436 - val_loss: 1.4988 - val_accuracy: 0.3399\n",
            "Epoch 32/50\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5283 - accuracy: 0.3439\n",
            "Epoch 00032: loss did not improve from 1.52793\n",
            "595/595 [==============================] - 7s 13ms/step - loss: 1.5285 - accuracy: 0.3435 - val_loss: 1.4985 - val_accuracy: 0.3399\n",
            "Epoch 33/50\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5294 - accuracy: 0.3427\n",
            "Epoch 00033: loss did not improve from 1.52793\n",
            "595/595 [==============================] - 7s 12ms/step - loss: 1.5288 - accuracy: 0.3434 - val_loss: 1.4987 - val_accuracy: 0.3399\n",
            "Epoch 34/50\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5284 - accuracy: 0.3436\n",
            "Epoch 00034: loss did not improve from 1.52793\n",
            "595/595 [==============================] - 7s 13ms/step - loss: 1.5285 - accuracy: 0.3435 - val_loss: 1.4988 - val_accuracy: 0.3399\n",
            "Epoch 35/50\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5286 - accuracy: 0.3436\n",
            "Epoch 00035: loss did not improve from 1.52793\n",
            "595/595 [==============================] - 7s 12ms/step - loss: 1.5286 - accuracy: 0.3436 - val_loss: 1.4989 - val_accuracy: 0.3399\n",
            "Epoch 36/50\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5276 - accuracy: 0.3435\n",
            "Epoch 00036: loss improved from 1.52793 to 1.52740, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.ftrl.Ftrl object at 0x7f59b2f7e080>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5274 - accuracy: 0.3437 - val_loss: 1.4987 - val_accuracy: 0.3399\n",
            "Epoch 37/50\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5273 - accuracy: 0.3434\n",
            "Epoch 00037: loss improved from 1.52740 to 1.52715, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.ftrl.Ftrl object at 0x7f59b2f7e080>_Batch25_LR0.001_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5272 - accuracy: 0.3435 - val_loss: 1.4988 - val_accuracy: 0.3399\n",
            "Epoch 38/50\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5272 - accuracy: 0.3445\n",
            "Epoch 00038: loss did not improve from 1.52715\n",
            "595/595 [==============================] - 7s 12ms/step - loss: 1.5278 - accuracy: 0.3439 - val_loss: 1.4990 - val_accuracy: 0.3399\n",
            "Epoch 39/50\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5288 - accuracy: 0.3437\n",
            "Epoch 00039: loss did not improve from 1.52715\n",
            "595/595 [==============================] - 7s 12ms/step - loss: 1.5288 - accuracy: 0.3438 - val_loss: 1.4988 - val_accuracy: 0.3399\n",
            "Epoch 40/50\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5273 - accuracy: 0.3437\n",
            "Epoch 00040: loss did not improve from 1.52715\n",
            "595/595 [==============================] - 7s 12ms/step - loss: 1.5274 - accuracy: 0.3435 - val_loss: 1.4991 - val_accuracy: 0.3399\n",
            "Epoch 41/50\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5273 - accuracy: 0.3440\n",
            "Epoch 00041: loss did not improve from 1.52715\n",
            "595/595 [==============================] - 7s 12ms/step - loss: 1.5274 - accuracy: 0.3436 - val_loss: 1.4991 - val_accuracy: 0.3399\n",
            "Epoch 42/50\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5276 - accuracy: 0.3437\n",
            "Epoch 00042: loss did not improve from 1.52715\n",
            "595/595 [==============================] - 7s 12ms/step - loss: 1.5274 - accuracy: 0.3436 - val_loss: 1.4991 - val_accuracy: 0.3399\n",
            "Epoch 43/50\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.5283 - accuracy: 0.3434\n",
            "Epoch 00043: loss did not improve from 1.52715\n",
            "595/595 [==============================] - 7s 12ms/step - loss: 1.5280 - accuracy: 0.3437 - val_loss: 1.4989 - val_accuracy: 0.3399\n",
            "Epoch 44/50\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.5275 - accuracy: 0.3434\n",
            "Epoch 00044: loss did not improve from 1.52715\n",
            "595/595 [==============================] - 7s 13ms/step - loss: 1.5275 - accuracy: 0.3434 - val_loss: 1.4990 - val_accuracy: 0.3399\n",
            "Epoch 45/50\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5273 - accuracy: 0.3440\n",
            "Epoch 00045: loss did not improve from 1.52715\n",
            "595/595 [==============================] - 7s 13ms/step - loss: 1.5276 - accuracy: 0.3436 - val_loss: 1.4991 - val_accuracy: 0.3399\n",
            "Epoch 46/50\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.5278 - accuracy: 0.3436\n",
            "Epoch 00046: loss did not improve from 1.52715\n",
            "595/595 [==============================] - 7s 12ms/step - loss: 1.5280 - accuracy: 0.3435 - val_loss: 1.4990 - val_accuracy: 0.3399\n",
            "Epoch 47/50\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5281 - accuracy: 0.3434\n",
            "Epoch 00047: loss did not improve from 1.52715\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5278 - accuracy: 0.3436 - val_loss: 1.4992 - val_accuracy: 0.3399\n",
            "Epoch 48/50\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5279 - accuracy: 0.3436\n",
            "Epoch 00048: loss did not improve from 1.52715\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5279 - accuracy: 0.3436 - val_loss: 1.4991 - val_accuracy: 0.3399\n",
            "Epoch 49/50\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5276 - accuracy: 0.3437\n",
            "Epoch 00049: loss did not improve from 1.52715\n",
            "595/595 [==============================] - 7s 12ms/step - loss: 1.5276 - accuracy: 0.3437 - val_loss: 1.4992 - val_accuracy: 0.3399\n",
            "Epoch 50/50\n",
            "589/595 [============================>.] - ETA: 0s - loss: 1.5268 - accuracy: 0.3440\n",
            "Epoch 00050: loss did not improve from 1.52715\n",
            "595/595 [==============================] - 7s 12ms/step - loss: 1.5273 - accuracy: 0.3436 - val_loss: 1.4991 - val_accuracy: 0.3399\n",
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
            "Epoch 1/50\n",
            "  2/595 [..............................] - ETA: 31s - loss: 2.1827 - accuracy: 0.1000WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0144s vs `on_train_batch_end` time: 0.0926s). Check your callbacks.\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5557 - accuracy: 0.3273\n",
            "Epoch 00001: loss improved from inf to 1.55562, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f59b0bdc0b8>_Batch25_LR0.01_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5556 - accuracy: 0.3274 - val_loss: 1.5055 - val_accuracy: 0.3399\n",
            "Epoch 2/50\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.5370 - accuracy: 0.3372\n",
            "Epoch 00002: loss improved from 1.55562 to 1.53693, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f59b0bdc0b8>_Batch25_LR0.01_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5369 - accuracy: 0.3377 - val_loss: 1.5081 - val_accuracy: 0.3375\n",
            "Epoch 3/50\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.5326 - accuracy: 0.3397\n",
            "Epoch 00003: loss improved from 1.53693 to 1.53259, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f59b0bdc0b8>_Batch25_LR0.01_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5326 - accuracy: 0.3397 - val_loss: 1.5073 - val_accuracy: 0.3399\n",
            "Epoch 4/50\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5301 - accuracy: 0.3424\n",
            "Epoch 00004: loss improved from 1.53259 to 1.52969, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f59b0bdc0b8>_Batch25_LR0.01_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5297 - accuracy: 0.3424 - val_loss: 1.5038 - val_accuracy: 0.3399\n",
            "Epoch 5/50\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5298 - accuracy: 0.3429\n",
            "Epoch 00005: loss improved from 1.52969 to 1.52939, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f59b0bdc0b8>_Batch25_LR0.01_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5294 - accuracy: 0.3431 - val_loss: 1.5054 - val_accuracy: 0.3399\n",
            "Epoch 6/50\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5284 - accuracy: 0.3415\n",
            "Epoch 00006: loss improved from 1.52939 to 1.52830, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f59b0bdc0b8>_Batch25_LR0.01_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5283 - accuracy: 0.3413 - val_loss: 1.5034 - val_accuracy: 0.3399\n",
            "Epoch 7/50\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5273 - accuracy: 0.3422\n",
            "Epoch 00007: loss improved from 1.52830 to 1.52718, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f59b0bdc0b8>_Batch25_LR0.01_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5272 - accuracy: 0.3422 - val_loss: 1.5039 - val_accuracy: 0.3399\n",
            "Epoch 8/50\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5266 - accuracy: 0.3432\n",
            "Epoch 00008: loss improved from 1.52718 to 1.52615, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f59b0bdc0b8>_Batch25_LR0.01_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5262 - accuracy: 0.3433 - val_loss: 1.4998 - val_accuracy: 0.3399\n",
            "Epoch 9/50\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5259 - accuracy: 0.3419\n",
            "Epoch 00009: loss improved from 1.52615 to 1.52535, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f59b0bdc0b8>_Batch25_LR0.01_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5254 - accuracy: 0.3424 - val_loss: 1.4999 - val_accuracy: 0.3399\n",
            "Epoch 10/50\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.5241 - accuracy: 0.3438\n",
            "Epoch 00010: loss improved from 1.52535 to 1.52465, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f59b0bdc0b8>_Batch25_LR0.01_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5247 - accuracy: 0.3439 - val_loss: 1.5060 - val_accuracy: 0.3399\n",
            "Epoch 11/50\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5229 - accuracy: 0.3438\n",
            "Epoch 00011: loss improved from 1.52465 to 1.52328, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f59b0bdc0b8>_Batch25_LR0.01_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5233 - accuracy: 0.3433 - val_loss: 1.4973 - val_accuracy: 0.3399\n",
            "Epoch 12/50\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5247 - accuracy: 0.3429\n",
            "Epoch 00012: loss did not improve from 1.52328\n",
            "595/595 [==============================] - 7s 12ms/step - loss: 1.5243 - accuracy: 0.3432 - val_loss: 1.4952 - val_accuracy: 0.3399\n",
            "Epoch 13/50\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5233 - accuracy: 0.3427\n",
            "Epoch 00013: loss improved from 1.52328 to 1.52327, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f59b0bdc0b8>_Batch25_LR0.01_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5233 - accuracy: 0.3429 - val_loss: 1.5010 - val_accuracy: 0.3399\n",
            "Epoch 14/50\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5223 - accuracy: 0.3444\n",
            "Epoch 00014: loss improved from 1.52327 to 1.52237, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f59b0bdc0b8>_Batch25_LR0.01_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5224 - accuracy: 0.3444 - val_loss: 1.4972 - val_accuracy: 0.3399\n",
            "Epoch 15/50\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5215 - accuracy: 0.3441\n",
            "Epoch 00015: loss improved from 1.52237 to 1.52176, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f59b0bdc0b8>_Batch25_LR0.01_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5218 - accuracy: 0.3439 - val_loss: 1.4977 - val_accuracy: 0.3399\n",
            "Epoch 16/50\n",
            "589/595 [============================>.] - ETA: 0s - loss: 1.5216 - accuracy: 0.3443\n",
            "Epoch 00016: loss improved from 1.52176 to 1.52147, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f59b0bdc0b8>_Batch25_LR0.01_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5215 - accuracy: 0.3439 - val_loss: 1.4912 - val_accuracy: 0.3369\n",
            "Epoch 17/50\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5215 - accuracy: 0.3429\n",
            "Epoch 00017: loss improved from 1.52147 to 1.52134, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f59b0bdc0b8>_Batch25_LR0.01_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5213 - accuracy: 0.3428 - val_loss: 1.4945 - val_accuracy: 0.3399\n",
            "Epoch 18/50\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.5203 - accuracy: 0.3422\n",
            "Epoch 00018: loss improved from 1.52134 to 1.52010, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f59b0bdc0b8>_Batch25_LR0.01_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5201 - accuracy: 0.3424 - val_loss: 1.4996 - val_accuracy: 0.3399\n",
            "Epoch 19/50\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5197 - accuracy: 0.3429\n",
            "Epoch 00019: loss improved from 1.52010 to 1.51972, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f59b0bdc0b8>_Batch25_LR0.01_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5197 - accuracy: 0.3429 - val_loss: 1.4925 - val_accuracy: 0.3399\n",
            "Epoch 20/50\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5208 - accuracy: 0.3427\n",
            "Epoch 00020: loss did not improve from 1.51972\n",
            "595/595 [==============================] - 7s 12ms/step - loss: 1.5207 - accuracy: 0.3426 - val_loss: 1.5184 - val_accuracy: 0.3399\n",
            "Epoch 21/50\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.5184 - accuracy: 0.3430\n",
            "Epoch 00021: loss improved from 1.51972 to 1.51848, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f59b0bdc0b8>_Batch25_LR0.01_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5185 - accuracy: 0.3434 - val_loss: 1.5268 - val_accuracy: 0.3399\n",
            "Epoch 22/50\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5204 - accuracy: 0.3429\n",
            "Epoch 00022: loss did not improve from 1.51848\n",
            "595/595 [==============================] - 7s 12ms/step - loss: 1.5198 - accuracy: 0.3433 - val_loss: 1.4964 - val_accuracy: 0.3399\n",
            "Epoch 23/50\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5192 - accuracy: 0.3428\n",
            "Epoch 00023: loss did not improve from 1.51848\n",
            "595/595 [==============================] - 7s 13ms/step - loss: 1.5188 - accuracy: 0.3432 - val_loss: 1.5109 - val_accuracy: 0.3399\n",
            "Epoch 24/50\n",
            "589/595 [============================>.] - ETA: 0s - loss: 1.5180 - accuracy: 0.3442\n",
            "Epoch 00024: loss improved from 1.51848 to 1.51811, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f59b0bdc0b8>_Batch25_LR0.01_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5181 - accuracy: 0.3441 - val_loss: 1.5014 - val_accuracy: 0.3399\n",
            "Epoch 25/50\n",
            "589/595 [============================>.] - ETA: 0s - loss: 1.5170 - accuracy: 0.3432\n",
            "Epoch 00025: loss improved from 1.51811 to 1.51700, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f59b0bdc0b8>_Batch25_LR0.01_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5170 - accuracy: 0.3430 - val_loss: 1.4958 - val_accuracy: 0.3399\n",
            "Epoch 26/50\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5170 - accuracy: 0.3428\n",
            "Epoch 00026: loss improved from 1.51700 to 1.51689, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f59b0bdc0b8>_Batch25_LR0.01_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5169 - accuracy: 0.3428 - val_loss: 1.4952 - val_accuracy: 0.3399\n",
            "Epoch 27/50\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5143 - accuracy: 0.3433\n",
            "Epoch 00027: loss improved from 1.51689 to 1.51445, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f59b0bdc0b8>_Batch25_LR0.01_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5144 - accuracy: 0.3432 - val_loss: 1.5125 - val_accuracy: 0.3399\n",
            "Epoch 28/50\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5167 - accuracy: 0.3424\n",
            "Epoch 00028: loss did not improve from 1.51445\n",
            "595/595 [==============================] - 7s 13ms/step - loss: 1.5169 - accuracy: 0.3424 - val_loss: 1.5033 - val_accuracy: 0.3399\n",
            "Epoch 29/50\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5160 - accuracy: 0.3436\n",
            "Epoch 00029: loss did not improve from 1.51445\n",
            "595/595 [==============================] - 7s 13ms/step - loss: 1.5158 - accuracy: 0.3434 - val_loss: 1.5016 - val_accuracy: 0.3399\n",
            "Epoch 30/50\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5159 - accuracy: 0.3429\n",
            "Epoch 00030: loss did not improve from 1.51445\n",
            "595/595 [==============================] - 7s 13ms/step - loss: 1.5157 - accuracy: 0.3431 - val_loss: 1.4935 - val_accuracy: 0.3399\n",
            "Epoch 31/50\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.5144 - accuracy: 0.3415\n",
            "Epoch 00031: loss improved from 1.51445 to 1.51441, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f59b0bdc0b8>_Batch25_LR0.01_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5144 - accuracy: 0.3415 - val_loss: 1.5011 - val_accuracy: 0.3399\n",
            "Epoch 32/50\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5143 - accuracy: 0.3418\n",
            "Epoch 00032: loss improved from 1.51441 to 1.51427, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f59b0bdc0b8>_Batch25_LR0.01_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5143 - accuracy: 0.3418 - val_loss: 1.5067 - val_accuracy: 0.3399\n",
            "Epoch 33/50\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5144 - accuracy: 0.3447\n",
            "Epoch 00033: loss did not improve from 1.51427\n",
            "595/595 [==============================] - 7s 12ms/step - loss: 1.5144 - accuracy: 0.3447 - val_loss: 1.5019 - val_accuracy: 0.3399\n",
            "Epoch 34/50\n",
            "589/595 [============================>.] - ETA: 0s - loss: 1.5133 - accuracy: 0.3423\n",
            "Epoch 00034: loss improved from 1.51427 to 1.51376, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f59b0bdc0b8>_Batch25_LR0.01_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5138 - accuracy: 0.3422 - val_loss: 1.4979 - val_accuracy: 0.3399\n",
            "Epoch 35/50\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5135 - accuracy: 0.3448\n",
            "Epoch 00035: loss improved from 1.51376 to 1.51330, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f59b0bdc0b8>_Batch25_LR0.01_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5133 - accuracy: 0.3449 - val_loss: 1.4936 - val_accuracy: 0.3399\n",
            "Epoch 36/50\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.5127 - accuracy: 0.3447\n",
            "Epoch 00036: loss improved from 1.51330 to 1.51267, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f59b0bdc0b8>_Batch25_LR0.01_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5127 - accuracy: 0.3447 - val_loss: 1.4984 - val_accuracy: 0.3399\n",
            "Epoch 37/50\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.5128 - accuracy: 0.3458\n",
            "Epoch 00037: loss improved from 1.51267 to 1.51266, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f59b0bdc0b8>_Batch25_LR0.01_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5127 - accuracy: 0.3456 - val_loss: 1.4895 - val_accuracy: 0.3399\n",
            "Epoch 38/50\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5113 - accuracy: 0.3430\n",
            "Epoch 00038: loss improved from 1.51266 to 1.51157, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f59b0bdc0b8>_Batch25_LR0.01_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5116 - accuracy: 0.3428 - val_loss: 1.4925 - val_accuracy: 0.3393\n",
            "Epoch 39/50\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5118 - accuracy: 0.3432\n",
            "Epoch 00039: loss did not improve from 1.51157\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5117 - accuracy: 0.3436 - val_loss: 1.5276 - val_accuracy: 0.3340\n",
            "Epoch 40/50\n",
            "589/595 [============================>.] - ETA: 0s - loss: 1.5109 - accuracy: 0.3451\n",
            "Epoch 00040: loss improved from 1.51157 to 1.51091, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f59b0bdc0b8>_Batch25_LR0.01_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5109 - accuracy: 0.3450 - val_loss: 1.4986 - val_accuracy: 0.3399\n",
            "Epoch 41/50\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5118 - accuracy: 0.3463\n",
            "Epoch 00041: loss did not improve from 1.51091\n",
            "595/595 [==============================] - 7s 12ms/step - loss: 1.5117 - accuracy: 0.3464 - val_loss: 1.5011 - val_accuracy: 0.3399\n",
            "Epoch 42/50\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5109 - accuracy: 0.3439\n",
            "Epoch 00042: loss did not improve from 1.51091\n",
            "595/595 [==============================] - 7s 12ms/step - loss: 1.5110 - accuracy: 0.3437 - val_loss: 1.5008 - val_accuracy: 0.3372\n",
            "Epoch 43/50\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5107 - accuracy: 0.3448\n",
            "Epoch 00043: loss improved from 1.51091 to 1.51050, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f59b0bdc0b8>_Batch25_LR0.01_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5105 - accuracy: 0.3449 - val_loss: 1.5099 - val_accuracy: 0.3399\n",
            "Epoch 44/50\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.5113 - accuracy: 0.3439\n",
            "Epoch 00044: loss did not improve from 1.51050\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5113 - accuracy: 0.3439 - val_loss: 1.5022 - val_accuracy: 0.3372\n",
            "Epoch 45/50\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5101 - accuracy: 0.3443\n",
            "Epoch 00045: loss improved from 1.51050 to 1.51000, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f59b0bdc0b8>_Batch25_LR0.01_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5100 - accuracy: 0.3443 - val_loss: 1.5027 - val_accuracy: 0.3372\n",
            "Epoch 46/50\n",
            "589/595 [============================>.] - ETA: 0s - loss: 1.5095 - accuracy: 0.3438\n",
            "Epoch 00046: loss improved from 1.51000 to 1.50932, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f59b0bdc0b8>_Batch25_LR0.01_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5093 - accuracy: 0.3445 - val_loss: 1.5084 - val_accuracy: 0.3375\n",
            "Epoch 47/50\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5095 - accuracy: 0.3437\n",
            "Epoch 00047: loss did not improve from 1.50932\n",
            "595/595 [==============================] - 7s 12ms/step - loss: 1.5094 - accuracy: 0.3436 - val_loss: 1.4981 - val_accuracy: 0.3245\n",
            "Epoch 48/50\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5093 - accuracy: 0.3433\n",
            "Epoch 00048: loss did not improve from 1.50932\n",
            "595/595 [==============================] - 7s 13ms/step - loss: 1.5098 - accuracy: 0.3432 - val_loss: 1.5083 - val_accuracy: 0.3375\n",
            "Epoch 49/50\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5088 - accuracy: 0.3468\n",
            "Epoch 00049: loss improved from 1.50932 to 1.50881, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f59b0bdc0b8>_Batch25_LR0.01_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5088 - accuracy: 0.3468 - val_loss: 1.5093 - val_accuracy: 0.3372\n",
            "Epoch 50/50\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5091 - accuracy: 0.3438\n",
            "Epoch 00050: loss did not improve from 1.50881\n",
            "595/595 [==============================] - 7s 12ms/step - loss: 1.5090 - accuracy: 0.3439 - val_loss: 1.4919 - val_accuracy: 0.3372\n",
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
            "Epoch 1/50\n",
            "  2/595 [..............................] - ETA: 42s - loss: 3.1488 - accuracy: 0.2200   WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0150s vs `on_train_batch_end` time: 0.1315s). Check your callbacks.\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5630 - accuracy: 0.3333\n",
            "Epoch 00001: loss improved from inf to 1.56282, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.rmsprop.RMSprop object at 0x7f59b0bce5c0>_Batch25_LR0.01_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5628 - accuracy: 0.3334 - val_loss: 1.5065 - val_accuracy: 0.3399\n",
            "Epoch 2/50\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5301 - accuracy: 0.3427\n",
            "Epoch 00002: loss improved from 1.56282 to 1.53018, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.rmsprop.RMSprop object at 0x7f59b0bce5c0>_Batch25_LR0.01_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5302 - accuracy: 0.3426 - val_loss: 1.4938 - val_accuracy: 0.3399\n",
            "Epoch 3/50\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5294 - accuracy: 0.3434\n",
            "Epoch 00003: loss improved from 1.53018 to 1.52923, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.rmsprop.RMSprop object at 0x7f59b0bce5c0>_Batch25_LR0.01_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5292 - accuracy: 0.3436 - val_loss: 1.4991 - val_accuracy: 0.3399\n",
            "Epoch 4/50\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5298 - accuracy: 0.3438\n",
            "Epoch 00004: loss did not improve from 1.52923\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5298 - accuracy: 0.3436 - val_loss: 1.4925 - val_accuracy: 0.3399\n",
            "Epoch 5/50\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5290 - accuracy: 0.3438\n",
            "Epoch 00005: loss did not improve from 1.52923\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5296 - accuracy: 0.3436 - val_loss: 1.4962 - val_accuracy: 0.3399\n",
            "Epoch 6/50\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5298 - accuracy: 0.3436\n",
            "Epoch 00006: loss did not improve from 1.52923\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5298 - accuracy: 0.3436 - val_loss: 1.4986 - val_accuracy: 0.3399\n",
            "Epoch 7/50\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5300 - accuracy: 0.3433\n",
            "Epoch 00007: loss did not improve from 1.52923\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5298 - accuracy: 0.3436 - val_loss: 1.4970 - val_accuracy: 0.3399\n",
            "Epoch 8/50\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5296 - accuracy: 0.3436\n",
            "Epoch 00008: loss did not improve from 1.52923\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5297 - accuracy: 0.3436 - val_loss: 1.5001 - val_accuracy: 0.3399\n",
            "Epoch 9/50\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5298 - accuracy: 0.3439\n",
            "Epoch 00009: loss did not improve from 1.52923\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5295 - accuracy: 0.3436 - val_loss: 1.4960 - val_accuracy: 0.3399\n",
            "Epoch 10/50\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5302 - accuracy: 0.3438\n",
            "Epoch 00010: loss did not improve from 1.52923\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5303 - accuracy: 0.3436 - val_loss: 1.4957 - val_accuracy: 0.3399\n",
            "Epoch 11/50\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5304 - accuracy: 0.3437\n",
            "Epoch 00011: loss did not improve from 1.52923\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5304 - accuracy: 0.3436 - val_loss: 1.4980 - val_accuracy: 0.3399\n",
            "Epoch 12/50\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5302 - accuracy: 0.3442\n",
            "Epoch 00012: loss did not improve from 1.52923\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5304 - accuracy: 0.3436 - val_loss: 1.4941 - val_accuracy: 0.3399\n",
            "Epoch 13/50\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.5294 - accuracy: 0.3441\n",
            "Epoch 00013: loss did not improve from 1.52923\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5296 - accuracy: 0.3436 - val_loss: 1.4961 - val_accuracy: 0.3399\n",
            "Epoch 14/50\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.5298 - accuracy: 0.3433\n",
            "Epoch 00014: loss did not improve from 1.52923\n",
            "595/595 [==============================] - 7s 13ms/step - loss: 1.5297 - accuracy: 0.3436 - val_loss: 1.4979 - val_accuracy: 0.3399\n",
            "Epoch 15/50\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5294 - accuracy: 0.3438\n",
            "Epoch 00015: loss did not improve from 1.52923\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5294 - accuracy: 0.3436 - val_loss: 1.4927 - val_accuracy: 0.3399\n",
            "Epoch 16/50\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5297 - accuracy: 0.3436\n",
            "Epoch 00016: loss did not improve from 1.52923\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5296 - accuracy: 0.3436 - val_loss: 1.4921 - val_accuracy: 0.3399\n",
            "Epoch 17/50\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5303 - accuracy: 0.3434\n",
            "Epoch 00017: loss did not improve from 1.52923\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5299 - accuracy: 0.3436 - val_loss: 1.4927 - val_accuracy: 0.3399\n",
            "Epoch 18/50\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5300 - accuracy: 0.3431\n",
            "Epoch 00018: loss did not improve from 1.52923\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5296 - accuracy: 0.3436 - val_loss: 1.4993 - val_accuracy: 0.3399\n",
            "Epoch 19/50\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5305 - accuracy: 0.3434\n",
            "Epoch 00019: loss did not improve from 1.52923\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5304 - accuracy: 0.3436 - val_loss: 1.4951 - val_accuracy: 0.3399\n",
            "Epoch 20/50\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5301 - accuracy: 0.3436\n",
            "Epoch 00020: loss did not improve from 1.52923\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5301 - accuracy: 0.3436 - val_loss: 1.4947 - val_accuracy: 0.3399\n",
            "Epoch 21/50\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5299 - accuracy: 0.3438\n",
            "Epoch 00021: loss did not improve from 1.52923\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5301 - accuracy: 0.3436 - val_loss: 1.4925 - val_accuracy: 0.3399\n",
            "Epoch 22/50\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5298 - accuracy: 0.3436\n",
            "Epoch 00022: loss did not improve from 1.52923\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5298 - accuracy: 0.3436 - val_loss: 1.4903 - val_accuracy: 0.3399\n",
            "Epoch 23/50\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5303 - accuracy: 0.3435\n",
            "Epoch 00023: loss did not improve from 1.52923\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5300 - accuracy: 0.3436 - val_loss: 1.4915 - val_accuracy: 0.3399\n",
            "Epoch 24/50\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5299 - accuracy: 0.3438\n",
            "Epoch 00024: loss did not improve from 1.52923\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5300 - accuracy: 0.3436 - val_loss: 1.4985 - val_accuracy: 0.3399\n",
            "Epoch 25/50\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5293 - accuracy: 0.3438\n",
            "Epoch 00025: loss did not improve from 1.52923\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5299 - accuracy: 0.3436 - val_loss: 1.4931 - val_accuracy: 0.3399\n",
            "Epoch 26/50\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5296 - accuracy: 0.3440\n",
            "Epoch 00026: loss did not improve from 1.52923\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5299 - accuracy: 0.3436 - val_loss: 1.4970 - val_accuracy: 0.3399\n",
            "Epoch 27/50\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5297 - accuracy: 0.3436\n",
            "Epoch 00027: loss did not improve from 1.52923\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5297 - accuracy: 0.3436 - val_loss: 1.4943 - val_accuracy: 0.3399\n",
            "Epoch 28/50\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5292 - accuracy: 0.3438\n",
            "Epoch 00028: loss did not improve from 1.52923\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5295 - accuracy: 0.3436 - val_loss: 1.4958 - val_accuracy: 0.3399\n",
            "Epoch 29/50\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5293 - accuracy: 0.3437\n",
            "Epoch 00029: loss did not improve from 1.52923\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5293 - accuracy: 0.3436 - val_loss: 1.4972 - val_accuracy: 0.3399\n",
            "Epoch 30/50\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5299 - accuracy: 0.3434\n",
            "Epoch 00030: loss did not improve from 1.52923\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5301 - accuracy: 0.3436 - val_loss: 1.4938 - val_accuracy: 0.3399\n",
            "Epoch 31/50\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5301 - accuracy: 0.3437\n",
            "Epoch 00031: loss did not improve from 1.52923\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5301 - accuracy: 0.3436 - val_loss: 1.4920 - val_accuracy: 0.3399\n",
            "Epoch 32/50\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5304 - accuracy: 0.3436\n",
            "Epoch 00032: loss did not improve from 1.52923\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5303 - accuracy: 0.3436 - val_loss: 1.4928 - val_accuracy: 0.3399\n",
            "Epoch 33/50\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5304 - accuracy: 0.3435\n",
            "Epoch 00033: loss did not improve from 1.52923\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5306 - accuracy: 0.3436 - val_loss: 1.4918 - val_accuracy: 0.3399\n",
            "Epoch 34/50\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5302 - accuracy: 0.3435\n",
            "Epoch 00034: loss did not improve from 1.52923\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5300 - accuracy: 0.3436 - val_loss: 1.4975 - val_accuracy: 0.3399\n",
            "Epoch 35/50\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5301 - accuracy: 0.3438\n",
            "Epoch 00035: loss did not improve from 1.52923\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5300 - accuracy: 0.3436 - val_loss: 1.4917 - val_accuracy: 0.3399\n",
            "Epoch 36/50\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5308 - accuracy: 0.3437\n",
            "Epoch 00036: loss did not improve from 1.52923\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5308 - accuracy: 0.3436 - val_loss: 1.4920 - val_accuracy: 0.3399\n",
            "Epoch 37/50\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5305 - accuracy: 0.3432\n",
            "Epoch 00037: loss did not improve from 1.52923\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5301 - accuracy: 0.3436 - val_loss: 1.4959 - val_accuracy: 0.3399\n",
            "Epoch 38/50\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5304 - accuracy: 0.3437\n",
            "Epoch 00038: loss did not improve from 1.52923\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5303 - accuracy: 0.3436 - val_loss: 1.4928 - val_accuracy: 0.3399\n",
            "Epoch 39/50\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5304 - accuracy: 0.3436\n",
            "Epoch 00039: loss did not improve from 1.52923\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5302 - accuracy: 0.3436 - val_loss: 1.4941 - val_accuracy: 0.3399\n",
            "Epoch 40/50\n",
            "589/595 [============================>.] - ETA: 0s - loss: 1.5302 - accuracy: 0.3435\n",
            "Epoch 00040: loss did not improve from 1.52923\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5299 - accuracy: 0.3436 - val_loss: 1.4980 - val_accuracy: 0.3399\n",
            "Epoch 41/50\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5302 - accuracy: 0.3436\n",
            "Epoch 00041: loss did not improve from 1.52923\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5302 - accuracy: 0.3436 - val_loss: 1.4994 - val_accuracy: 0.3399\n",
            "Epoch 42/50\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5299 - accuracy: 0.3431\n",
            "Epoch 00042: loss did not improve from 1.52923\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5299 - accuracy: 0.3436 - val_loss: 1.5012 - val_accuracy: 0.3399\n",
            "Epoch 43/50\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.5302 - accuracy: 0.3436\n",
            "Epoch 00043: loss did not improve from 1.52923\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5302 - accuracy: 0.3436 - val_loss: 1.4931 - val_accuracy: 0.3399\n",
            "Epoch 44/50\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5298 - accuracy: 0.3436\n",
            "Epoch 00044: loss did not improve from 1.52923\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5297 - accuracy: 0.3436 - val_loss: 1.4945 - val_accuracy: 0.3399\n",
            "Epoch 45/50\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5296 - accuracy: 0.3439\n",
            "Epoch 00045: loss did not improve from 1.52923\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5298 - accuracy: 0.3436 - val_loss: 1.4929 - val_accuracy: 0.3399\n",
            "Epoch 46/50\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.5299 - accuracy: 0.3436\n",
            "Epoch 00046: loss did not improve from 1.52923\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5299 - accuracy: 0.3436 - val_loss: 1.4917 - val_accuracy: 0.3399\n",
            "Epoch 47/50\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.5293 - accuracy: 0.3439\n",
            "Epoch 00047: loss did not improve from 1.52923\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5297 - accuracy: 0.3436 - val_loss: 1.4967 - val_accuracy: 0.3399\n",
            "Epoch 48/50\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5295 - accuracy: 0.3438\n",
            "Epoch 00048: loss did not improve from 1.52923\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5296 - accuracy: 0.3436 - val_loss: 1.4945 - val_accuracy: 0.3399\n",
            "Epoch 49/50\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5293 - accuracy: 0.3439\n",
            "Epoch 00049: loss did not improve from 1.52923\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5300 - accuracy: 0.3436 - val_loss: 1.4967 - val_accuracy: 0.3399\n",
            "Epoch 50/50\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5298 - accuracy: 0.3437\n",
            "Epoch 00050: loss did not improve from 1.52923\n",
            "595/595 [==============================] - 7s 13ms/step - loss: 1.5299 - accuracy: 0.3436 - val_loss: 1.4948 - val_accuracy: 0.3399\n",
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
            "Epoch 1/50\n",
            "  2/595 [..............................] - ETA: 39s - loss: 1.9916 - accuracy: 0.2000WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0142s vs `on_train_batch_end` time: 0.1169s). Check your callbacks.\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5508 - accuracy: 0.3316\n",
            "Epoch 00001: loss improved from inf to 1.55038, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f59b0a1cf60>_Batch25_LR0.01_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5504 - accuracy: 0.3317 - val_loss: 1.4961 - val_accuracy: 0.3399\n",
            "Epoch 2/50\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.5343 - accuracy: 0.3438\n",
            "Epoch 00002: loss improved from 1.55038 to 1.53427, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f59b0a1cf60>_Batch25_LR0.01_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5343 - accuracy: 0.3438 - val_loss: 1.4989 - val_accuracy: 0.3399\n",
            "Epoch 3/50\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.5289 - accuracy: 0.3436\n",
            "Epoch 00003: loss improved from 1.53427 to 1.52891, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f59b0a1cf60>_Batch25_LR0.01_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5289 - accuracy: 0.3436 - val_loss: 1.4961 - val_accuracy: 0.3399\n",
            "Epoch 4/50\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5287 - accuracy: 0.3441\n",
            "Epoch 00004: loss improved from 1.52891 to 1.52884, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f59b0a1cf60>_Batch25_LR0.01_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5288 - accuracy: 0.3436 - val_loss: 1.5011 - val_accuracy: 0.3399\n",
            "Epoch 5/50\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.5290 - accuracy: 0.3436\n",
            "Epoch 00005: loss did not improve from 1.52884\n",
            "595/595 [==============================] - 7s 13ms/step - loss: 1.5290 - accuracy: 0.3436 - val_loss: 1.4964 - val_accuracy: 0.3399\n",
            "Epoch 6/50\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.5292 - accuracy: 0.3438\n",
            "Epoch 00006: loss did not improve from 1.52884\n",
            "595/595 [==============================] - 7s 12ms/step - loss: 1.5292 - accuracy: 0.3436 - val_loss: 1.4989 - val_accuracy: 0.3399\n",
            "Epoch 7/50\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.5288 - accuracy: 0.3436\n",
            "Epoch 00007: loss improved from 1.52884 to 1.52880, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f59b0a1cf60>_Batch25_LR0.01_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5288 - accuracy: 0.3436 - val_loss: 1.5025 - val_accuracy: 0.3399\n",
            "Epoch 8/50\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5289 - accuracy: 0.3437\n",
            "Epoch 00008: loss did not improve from 1.52880\n",
            "595/595 [==============================] - 7s 13ms/step - loss: 1.5291 - accuracy: 0.3436 - val_loss: 1.4910 - val_accuracy: 0.3399\n",
            "Epoch 9/50\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.5286 - accuracy: 0.3435\n",
            "Epoch 00009: loss did not improve from 1.52880\n",
            "595/595 [==============================] - 7s 13ms/step - loss: 1.5290 - accuracy: 0.3436 - val_loss: 1.4977 - val_accuracy: 0.3399\n",
            "Epoch 10/50\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.5291 - accuracy: 0.3437\n",
            "Epoch 00010: loss did not improve from 1.52880\n",
            "595/595 [==============================] - 7s 13ms/step - loss: 1.5289 - accuracy: 0.3436 - val_loss: 1.4922 - val_accuracy: 0.3399\n",
            "Epoch 11/50\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5292 - accuracy: 0.3436\n",
            "Epoch 00011: loss did not improve from 1.52880\n",
            "595/595 [==============================] - 7s 12ms/step - loss: 1.5292 - accuracy: 0.3436 - val_loss: 1.4986 - val_accuracy: 0.3399\n",
            "Epoch 12/50\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5293 - accuracy: 0.3437\n",
            "Epoch 00012: loss did not improve from 1.52880\n",
            "595/595 [==============================] - 7s 12ms/step - loss: 1.5291 - accuracy: 0.3436 - val_loss: 1.4996 - val_accuracy: 0.3399\n",
            "Epoch 13/50\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5291 - accuracy: 0.3437\n",
            "Epoch 00013: loss did not improve from 1.52880\n",
            "595/595 [==============================] - 7s 12ms/step - loss: 1.5290 - accuracy: 0.3436 - val_loss: 1.4966 - val_accuracy: 0.3399\n",
            "Epoch 14/50\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5293 - accuracy: 0.3434\n",
            "Epoch 00014: loss did not improve from 1.52880\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5291 - accuracy: 0.3436 - val_loss: 1.4958 - val_accuracy: 0.3399\n",
            "Epoch 15/50\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5289 - accuracy: 0.3435\n",
            "Epoch 00015: loss did not improve from 1.52880\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5288 - accuracy: 0.3436 - val_loss: 1.4971 - val_accuracy: 0.3399\n",
            "Epoch 16/50\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5291 - accuracy: 0.3436\n",
            "Epoch 00016: loss did not improve from 1.52880\n",
            "595/595 [==============================] - 7s 13ms/step - loss: 1.5290 - accuracy: 0.3436 - val_loss: 1.4943 - val_accuracy: 0.3399\n",
            "Epoch 17/50\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5289 - accuracy: 0.3441\n",
            "Epoch 00017: loss did not improve from 1.52880\n",
            "595/595 [==============================] - 7s 13ms/step - loss: 1.5289 - accuracy: 0.3436 - val_loss: 1.4955 - val_accuracy: 0.3399\n",
            "Epoch 18/50\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5292 - accuracy: 0.3436\n",
            "Epoch 00018: loss did not improve from 1.52880\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5290 - accuracy: 0.3436 - val_loss: 1.4960 - val_accuracy: 0.3399\n",
            "Epoch 19/50\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5291 - accuracy: 0.3435\n",
            "Epoch 00019: loss did not improve from 1.52880\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5290 - accuracy: 0.3436 - val_loss: 1.4981 - val_accuracy: 0.3399\n",
            "Epoch 20/50\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5292 - accuracy: 0.3436\n",
            "Epoch 00020: loss did not improve from 1.52880\n",
            "595/595 [==============================] - 7s 13ms/step - loss: 1.5290 - accuracy: 0.3436 - val_loss: 1.4983 - val_accuracy: 0.3399\n",
            "Epoch 21/50\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5293 - accuracy: 0.3436\n",
            "Epoch 00021: loss did not improve from 1.52880\n",
            "595/595 [==============================] - 7s 12ms/step - loss: 1.5293 - accuracy: 0.3436 - val_loss: 1.5002 - val_accuracy: 0.3399\n",
            "Epoch 22/50\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.5289 - accuracy: 0.3436\n",
            "Epoch 00022: loss did not improve from 1.52880\n",
            "595/595 [==============================] - 7s 12ms/step - loss: 1.5289 - accuracy: 0.3436 - val_loss: 1.4922 - val_accuracy: 0.3399\n",
            "Epoch 23/50\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5291 - accuracy: 0.3437\n",
            "Epoch 00023: loss did not improve from 1.52880\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5290 - accuracy: 0.3436 - val_loss: 1.4981 - val_accuracy: 0.3399\n",
            "Epoch 24/50\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5288 - accuracy: 0.3439\n",
            "Epoch 00024: loss did not improve from 1.52880\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5290 - accuracy: 0.3436 - val_loss: 1.4967 - val_accuracy: 0.3399\n",
            "Epoch 25/50\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5286 - accuracy: 0.3438\n",
            "Epoch 00025: loss improved from 1.52880 to 1.52876, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f59b0a1cf60>_Batch25_LR0.01_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5288 - accuracy: 0.3436 - val_loss: 1.5009 - val_accuracy: 0.3399\n",
            "Epoch 26/50\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5295 - accuracy: 0.3436\n",
            "Epoch 00026: loss did not improve from 1.52876\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5293 - accuracy: 0.3436 - val_loss: 1.4980 - val_accuracy: 0.3399\n",
            "Epoch 27/50\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5291 - accuracy: 0.3437\n",
            "Epoch 00027: loss did not improve from 1.52876\n",
            "595/595 [==============================] - 7s 13ms/step - loss: 1.5292 - accuracy: 0.3436 - val_loss: 1.4943 - val_accuracy: 0.3399\n",
            "Epoch 28/50\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5290 - accuracy: 0.3435\n",
            "Epoch 00028: loss did not improve from 1.52876\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5289 - accuracy: 0.3436 - val_loss: 1.4981 - val_accuracy: 0.3399\n",
            "Epoch 29/50\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5278 - accuracy: 0.3438\n",
            "Epoch 00029: loss improved from 1.52876 to 1.52829, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f59b0a1cf60>_Batch25_LR0.01_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5283 - accuracy: 0.3436 - val_loss: 1.5012 - val_accuracy: 0.3399\n",
            "Epoch 30/50\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5290 - accuracy: 0.3440\n",
            "Epoch 00030: loss did not improve from 1.52829\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5294 - accuracy: 0.3436 - val_loss: 1.4978 - val_accuracy: 0.3399\n",
            "Epoch 31/50\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5287 - accuracy: 0.3434\n",
            "Epoch 00031: loss did not improve from 1.52829\n",
            "595/595 [==============================] - 7s 13ms/step - loss: 1.5285 - accuracy: 0.3436 - val_loss: 1.4966 - val_accuracy: 0.3399\n",
            "Epoch 32/50\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.5291 - accuracy: 0.3437\n",
            "Epoch 00032: loss did not improve from 1.52829\n",
            "595/595 [==============================] - 7s 12ms/step - loss: 1.5290 - accuracy: 0.3436 - val_loss: 1.5004 - val_accuracy: 0.3399\n",
            "Epoch 33/50\n",
            "589/595 [============================>.] - ETA: 0s - loss: 1.5294 - accuracy: 0.3436\n",
            "Epoch 00033: loss did not improve from 1.52829\n",
            "595/595 [==============================] - 7s 12ms/step - loss: 1.5291 - accuracy: 0.3436 - val_loss: 1.4977 - val_accuracy: 0.3399\n",
            "Epoch 34/50\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5293 - accuracy: 0.3436\n",
            "Epoch 00034: loss did not improve from 1.52829\n",
            "595/595 [==============================] - 7s 12ms/step - loss: 1.5291 - accuracy: 0.3436 - val_loss: 1.4964 - val_accuracy: 0.3399\n",
            "Epoch 35/50\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.5293 - accuracy: 0.3436\n",
            "Epoch 00035: loss did not improve from 1.52829\n",
            "595/595 [==============================] - 7s 12ms/step - loss: 1.5293 - accuracy: 0.3436 - val_loss: 1.4951 - val_accuracy: 0.3399\n",
            "Epoch 36/50\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5285 - accuracy: 0.3444\n",
            "Epoch 00036: loss did not improve from 1.52829\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5290 - accuracy: 0.3436 - val_loss: 1.4942 - val_accuracy: 0.3399\n",
            "Epoch 37/50\n",
            "589/595 [============================>.] - ETA: 0s - loss: 1.5298 - accuracy: 0.3429\n",
            "Epoch 00037: loss did not improve from 1.52829\n",
            "595/595 [==============================] - 7s 12ms/step - loss: 1.5288 - accuracy: 0.3436 - val_loss: 1.5000 - val_accuracy: 0.3399\n",
            "Epoch 38/50\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5291 - accuracy: 0.3438\n",
            "Epoch 00038: loss did not improve from 1.52829\n",
            "595/595 [==============================] - 7s 12ms/step - loss: 1.5290 - accuracy: 0.3436 - val_loss: 1.4989 - val_accuracy: 0.3399\n",
            "Epoch 39/50\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5291 - accuracy: 0.3437\n",
            "Epoch 00039: loss did not improve from 1.52829\n",
            "595/595 [==============================] - 7s 12ms/step - loss: 1.5291 - accuracy: 0.3436 - val_loss: 1.4964 - val_accuracy: 0.3399\n",
            "Epoch 40/50\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5289 - accuracy: 0.3434\n",
            "Epoch 00040: loss did not improve from 1.52829\n",
            "595/595 [==============================] - 7s 13ms/step - loss: 1.5287 - accuracy: 0.3436 - val_loss: 1.4915 - val_accuracy: 0.3399\n",
            "Epoch 41/50\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5294 - accuracy: 0.3432\n",
            "Epoch 00041: loss did not improve from 1.52829\n",
            "595/595 [==============================] - 7s 13ms/step - loss: 1.5291 - accuracy: 0.3436 - val_loss: 1.4953 - val_accuracy: 0.3399\n",
            "Epoch 42/50\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5290 - accuracy: 0.3434\n",
            "Epoch 00042: loss did not improve from 1.52829\n",
            "595/595 [==============================] - 7s 13ms/step - loss: 1.5289 - accuracy: 0.3436 - val_loss: 1.4994 - val_accuracy: 0.3399\n",
            "Epoch 43/50\n",
            "595/595 [==============================] - ETA: 0s - loss: 1.5291 - accuracy: 0.3436\n",
            "Epoch 00043: loss did not improve from 1.52829\n",
            "595/595 [==============================] - 7s 12ms/step - loss: 1.5291 - accuracy: 0.3436 - val_loss: 1.4933 - val_accuracy: 0.3399\n",
            "Epoch 44/50\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5286 - accuracy: 0.3435\n",
            "Epoch 00044: loss did not improve from 1.52829\n",
            "595/595 [==============================] - 7s 13ms/step - loss: 1.5285 - accuracy: 0.3436 - val_loss: 1.4920 - val_accuracy: 0.3399\n",
            "Epoch 45/50\n",
            "590/595 [============================>.] - ETA: 0s - loss: 1.5293 - accuracy: 0.3433\n",
            "Epoch 00045: loss did not improve from 1.52829\n",
            "595/595 [==============================] - 7s 12ms/step - loss: 1.5290 - accuracy: 0.3436 - val_loss: 1.5013 - val_accuracy: 0.3399\n",
            "Epoch 46/50\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5292 - accuracy: 0.3437\n",
            "Epoch 00046: loss did not improve from 1.52829\n",
            "595/595 [==============================] - 7s 12ms/step - loss: 1.5288 - accuracy: 0.3436 - val_loss: 1.4958 - val_accuracy: 0.3399\n",
            "Epoch 47/50\n",
            "589/595 [============================>.] - ETA: 0s - loss: 1.5295 - accuracy: 0.3432\n",
            "Epoch 00047: loss did not improve from 1.52829\n",
            "595/595 [==============================] - 7s 13ms/step - loss: 1.5290 - accuracy: 0.3436 - val_loss: 1.4961 - val_accuracy: 0.3399\n",
            "Epoch 48/50\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5288 - accuracy: 0.3439\n",
            "Epoch 00048: loss did not improve from 1.52829\n",
            "595/595 [==============================] - 7s 12ms/step - loss: 1.5290 - accuracy: 0.3436 - val_loss: 1.4963 - val_accuracy: 0.3399\n",
            "Epoch 49/50\n",
            "589/595 [============================>.] - ETA: 0s - loss: 1.5292 - accuracy: 0.3432\n",
            "Epoch 00049: loss did not improve from 1.52829\n",
            "595/595 [==============================] - 7s 12ms/step - loss: 1.5291 - accuracy: 0.3436 - val_loss: 1.4929 - val_accuracy: 0.3399\n",
            "Epoch 50/50\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5294 - accuracy: 0.3435\n",
            "Epoch 00050: loss did not improve from 1.52829\n",
            "595/595 [==============================] - 7s 12ms/step - loss: 1.5292 - accuracy: 0.3436 - val_loss: 1.4950 - val_accuracy: 0.3399\n",
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
            "Epoch 1/50\n",
            "  2/595 [..............................] - ETA: 36s - loss: 2.0207 - accuracy: 0.1600WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0135s vs `on_train_batch_end` time: 0.1105s). Check your callbacks.\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5496 - accuracy: 0.3290\n",
            "Epoch 00001: loss improved from inf to 1.55009, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adagrad.Adagrad object at 0x7f59b0a265c0>_Batch25_LR0.01_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5501 - accuracy: 0.3284 - val_loss: 1.5087 - val_accuracy: 0.3399\n",
            "Epoch 2/50\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5323 - accuracy: 0.3405\n",
            "Epoch 00002: loss improved from 1.55009 to 1.53219, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adagrad.Adagrad object at 0x7f59b0a265c0>_Batch25_LR0.01_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5322 - accuracy: 0.3405 - val_loss: 1.4982 - val_accuracy: 0.3399\n",
            "Epoch 3/50\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5279 - accuracy: 0.3443\n",
            "Epoch 00003: loss improved from 1.53219 to 1.52812, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adagrad.Adagrad object at 0x7f59b0a265c0>_Batch25_LR0.01_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5281 - accuracy: 0.3443 - val_loss: 1.5020 - val_accuracy: 0.3399\n",
            "Epoch 4/50\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5245 - accuracy: 0.3429\n",
            "Epoch 00004: loss improved from 1.52812 to 1.52459, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adagrad.Adagrad object at 0x7f59b0a265c0>_Batch25_LR0.01_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5246 - accuracy: 0.3429 - val_loss: 1.4988 - val_accuracy: 0.3399\n",
            "Epoch 5/50\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5230 - accuracy: 0.3443\n",
            "Epoch 00005: loss improved from 1.52459 to 1.52275, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adagrad.Adagrad object at 0x7f59b0a265c0>_Batch25_LR0.01_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5228 - accuracy: 0.3442 - val_loss: 1.4977 - val_accuracy: 0.3399\n",
            "Epoch 6/50\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5240 - accuracy: 0.3432\n",
            "Epoch 00006: loss did not improve from 1.52275\n",
            "595/595 [==============================] - 7s 13ms/step - loss: 1.5240 - accuracy: 0.3431 - val_loss: 1.5002 - val_accuracy: 0.3399\n",
            "Epoch 7/50\n",
            "592/595 [============================>.] - ETA: 0s - loss: 1.5237 - accuracy: 0.3427\n",
            "Epoch 00007: loss did not improve from 1.52275\n",
            "595/595 [==============================] - 7s 12ms/step - loss: 1.5234 - accuracy: 0.3430 - val_loss: 1.5044 - val_accuracy: 0.3399\n",
            "Epoch 8/50\n",
            "591/595 [============================>.] - ETA: 0s - loss: 1.5201 - accuracy: 0.3441\n",
            "Epoch 00008: loss improved from 1.52275 to 1.52077, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adagrad.Adagrad object at 0x7f59b0a265c0>_Batch25_LR0.01_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 14ms/step - loss: 1.5208 - accuracy: 0.3438 - val_loss: 1.5031 - val_accuracy: 0.3399\n",
            "Epoch 9/50\n",
            "594/595 [============================>.] - ETA: 0s - loss: 1.5207 - accuracy: 0.3435\n",
            "Epoch 00009: loss improved from 1.52077 to 1.52043, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adagrad.Adagrad object at 0x7f59b0a265c0>_Batch25_LR0.01_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5204 - accuracy: 0.3437 - val_loss: 1.4988 - val_accuracy: 0.3399\n",
            "Epoch 10/50\n",
            "593/595 [============================>.] - ETA: 0s - loss: 1.5195 - accuracy: 0.3438\n",
            "Epoch 00010: loss improved from 1.52043 to 1.51951, saving model to model/best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adagrad.Adagrad object at 0x7f59b0a265c0>_Batch25_LR0.01_Epochs50.hdf5\n",
            "595/595 [==============================] - 8s 13ms/step - loss: 1.5195 - accuracy: 0.3439 - val_loss: 1.5025 - val_accuracy: 0.3399\n",
            "Epoch 11/50\n",
            "288/595 [=============>................] - ETA: 2s - loss: 1.5143 - accuracy: 0.3432"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l8lTHN_5ZpzC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "f7f916b7-daa2-43bc-bb9e-fd19fd5f3de2"
      },
      "source": [
        "import json\n",
        "\n",
        "# as requested in comment\n",
        "with open('result_training.txt', 'w') as file:\n",
        "     file.write(json.dumps(Tabres)) # use `json.loads` to do the reverse"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fUVybHIT4aA8",
        "colab_type": "text"
      },
      "source": [
        "Loss vs accuracy\n",
        "\n",
        "https://kharshit.github.io/blog/2018/12/07/loss-vs-accuracy\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fxXLslzA4V8j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "https://kharshit.github.io/blog/2018/12/07/loss-vs-accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fo3lfsYbZpcB",
        "colab_type": "text"
      },
      "source": [
        "####Improve the Model with GAN - NOT FUNCTIONAL SO FAR"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Qltq67Ua9ac",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "https://keras.io/guides/customizing_what_happens_in_fit/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d9lf5VWAKZ2m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras import layers\n",
        "\n",
        "# Create the discriminator\n",
        "discriminator = keras.Sequential(\n",
        "    [\n",
        "        keras.Input(shape=(28, 28, 1)),\n",
        "        layers.Conv2D(64, (3, 3), strides=(2, 2), padding=\"same\"),\n",
        "        layers.LeakyReLU(alpha=0.2),\n",
        "        layers.Conv2D(128, (3, 3), strides=(2, 2), padding=\"same\"),\n",
        "        layers.LeakyReLU(alpha=0.2),\n",
        "        layers.GlobalMaxPooling2D(),\n",
        "        layers.Dense(1),\n",
        "    ],\n",
        "    name=\"discriminator\",\n",
        ")\n",
        "\n",
        "# Create the generator\n",
        "latent_dim = 128\n",
        "generator = keras.Sequential(\n",
        "    [\n",
        "        keras.Input(shape=(latent_dim,)),\n",
        "        # We want to generate 128 coefficients to reshape into a 7x7x128 map\n",
        "        layers.Dense(7 * 7 * 128),\n",
        "        layers.LeakyReLU(alpha=0.2),\n",
        "        layers.Reshape((7, 7, 128)),\n",
        "        layers.Conv2DTranspose(128, (4, 4), strides=(2, 2), padding=\"same\"),\n",
        "        layers.LeakyReLU(alpha=0.2),\n",
        "        layers.Conv2DTranspose(128, (4, 4), strides=(2, 2), padding=\"same\"),\n",
        "        layers.LeakyReLU(alpha=0.2),\n",
        "        layers.Conv2D(1, (7, 7), padding=\"same\", activation=\"sigmoid\"),\n",
        "    ],\n",
        "    name=\"generator\",\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R4wpRPC5bTua",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class GAN(keras.Model):\n",
        "    def __init__(self, discriminator, generator, latent_dim):\n",
        "        super(GAN, self).__init__()\n",
        "        self.discriminator = discriminator\n",
        "        self.generator = generator\n",
        "        self.latent_dim = latent_dim\n",
        "\n",
        "    def compile(self, d_optimizer, g_optimizer, loss_fn):\n",
        "        super(GAN, self).compile()\n",
        "        self.d_optimizer = d_optimizer\n",
        "        self.g_optimizer = g_optimizer\n",
        "        self.loss_fn = loss_fn\n",
        "\n",
        "    def train_step(self, real_images):\n",
        "        if isinstance(real_images, tuple):\n",
        "            real_images = real_images[0]\n",
        "        # Sample random points in the latent space\n",
        "        batch_size = tf.shape(real_images)[0]\n",
        "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
        "\n",
        "        # Decode them to fake images\n",
        "        generated_images = self.generator(random_latent_vectors)\n",
        "\n",
        "        # Combine them with real images\n",
        "        combined_images = tf.concat([generated_images, real_images], axis=0)\n",
        "\n",
        "        # Assemble labels discriminating real from fake images\n",
        "        labels = tf.concat(\n",
        "            [tf.ones((batch_size, 1)), tf.zeros((batch_size, 1))], axis=0\n",
        "        )\n",
        "        # Add random noise to the labels - important trick!\n",
        "        labels += 0.05 * tf.random.uniform(tf.shape(labels))\n",
        "\n",
        "        # Train the discriminator\n",
        "        with tf.GradientTape() as tape:\n",
        "            predictions = self.discriminator(combined_images)\n",
        "            d_loss = self.loss_fn(labels, predictions)\n",
        "        grads = tape.gradient(d_loss, self.discriminator.trainable_weights)\n",
        "        self.d_optimizer.apply_gradients(\n",
        "            zip(grads, self.discriminator.trainable_weights)\n",
        "        )\n",
        "\n",
        "        # Sample random points in the latent space\n",
        "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
        "\n",
        "        # Assemble labels that say \"all real images\"\n",
        "        misleading_labels = tf.zeros((batch_size, 1))\n",
        "\n",
        "        # Train the generator (note that we should *not* update the weights\n",
        "        # of the discriminator)!\n",
        "        with tf.GradientTape() as tape:\n",
        "            predictions = self.discriminator(self.generator(random_latent_vectors))\n",
        "            g_loss = self.loss_fn(misleading_labels, predictions)\n",
        "        grads = tape.gradient(g_loss, self.generator.trainable_weights)\n",
        "        self.g_optimizer.apply_gradients(zip(grads, self.generator.trainable_weights))\n",
        "        return {\"d_loss\": d_loss, \"g_loss\": g_loss}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vOtZ0dD7bYhp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Prepare the dataset. We use both the training & test MNIST digits.\n",
        "batch_size = 64\n",
        "(x_train, _), (x_test, _) = keras.datasets.mnist.load_data()\n",
        "all_digits = np.concatenate([x_train, x_test])\n",
        "all_digits = all_digits.astype(\"float32\") / 255.0\n",
        "all_digits = np.reshape(all_digits, (-1, 28, 28, 1))\n",
        "dataset = tf.data.Dataset.from_tensor_slices(all_digits)\n",
        "dataset = dataset.shuffle(buffer_size=1024).batch(batch_size)\n",
        "\n",
        "gan = GAN(discriminator=discriminator, generator=generator, latent_dim=latent_dim)\n",
        "gan.compile(\n",
        "    d_optimizer=keras.optimizers.Adam(learning_rate=0.0003),\n",
        "    g_optimizer=keras.optimizers.Adam(learning_rate=0.0003),\n",
        "    loss_fn=keras.losses.BinaryCrossentropy(from_logits=True),\n",
        ")\n",
        "\n",
        "# To limit execution time, we only train on 100 batches. You can train on\n",
        "# the entire dataset. You will need about 20 epochs to get nice results.\n",
        "gan.fit(dataset.take(100), epochs=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cez5uqWb4SSc",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oyRJzUx_5DSF",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R2_xo8psY5Iy",
        "colab_type": "text"
      },
      "source": [
        "##Step 3: Evaluate the VGGsp500 model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PXXUYNbwZ4TX",
        "colab_type": "text"
      },
      "source": [
        "This part will evaluate the model with the testing dataset that we generated in first step. We show the accuracy, the confusion matrix and the classification report"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mUSUr5CdwPPz",
        "colab_type": "text"
      },
      "source": [
        "To improve learning after playing with all the parameters \n",
        "\n",
        "- get more dataset on stock and indice\n",
        "\n",
        "- work on vgg untrained \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eKbQ6tBAhlw8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 251
        },
        "outputId": "84b52850-8f5a-4e5d-eeb5-0cc8628b0cf6"
      },
      "source": [
        "print(\"name of the best model for each set of parameters\")\n",
        "bestmodel=\"best_model\"+vggsp500loss+\"_\"+vggsp500optimizer_name+\"_Batch\"+str(batch_size)+\"_LR\"+str(initial_learning_rate)+\".hdf5\"\n",
        "print(bestmodel)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "name of the best model for each set of parameters\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-379eac6dc040>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"name of the best model for each set of parameters\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mbestmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"best_model\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mvggsp500loss\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"_\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mvggsp500optimizer_name\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"_Batch\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"_LR\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minitial_learning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\".hdf5\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbestmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'vggsp500loss' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "23kG1lJE5KqF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "outputId": "7a92151c-ae13-4338-e07c-a1b8a5387089"
      },
      "source": [
        "%cd /content/drive/My Drive/A_transfertTFMVggSP500\n",
        "%cd model \n",
        "%cp best_modelcategorical_crossentropy_adam_Batch100_LR0.1_0.99.hdf5 /content/DL_Tools_For_Finance/model/\n",
        "%ls -l"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/A_transfertTFMVggSP500\n",
            "/content/drive/My Drive/A_transfertTFMVggSP500/model\n",
            "cp: cannot stat 'best_modelcategorical_crossentropy_adam_Batch100_LR0.1_0.99.hdf5': No such file or directory\n",
            "total 465803\n",
            "-rw------- 1 root root 59726904 Sep  5 06:04  best_modelcategorical_crossentropy_Adagrad_Batch32_LR0.01_0.98.hdf5\n",
            "-rw------- 1 root root 59456824 Sep  1 00:22 'best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adagrad.Adagrad object at 0x7f59b9654fd0>_Batch25_LR0.001_Epochs100vggforsp500.h5'\n",
            "-rw------- 1 root root 59726928 Sep  1 00:35 'best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adamax.Adamax object at 0x7f59b9808a20>_Batch25_LR0.001_Epochs100.hdf5'\n",
            "-rw------- 1 root root 59726928 Sep  1 00:35 'best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.adamax.Adamax object at 0x7f59b9808a20>_Batch25_LR0.001_Epochs100vggforsp500.h5'\n",
            "-rw------- 1 root root 59725496 Sep  1 00:44 'best_modelcategorical_crossentropy_<tensorflow.python.keras.optimizer_v2.ftrl.Ftrl object at 0x7f599f76e400>_Batch25_LR0.001_Epochs100.hdf5'\n",
            "-rw------- 1 root root 59726920 Aug 31 08:06  final_modelcategorical_crossentropy_adam_32.h5\n",
            "-rw------- 1 root root 59161736 Sep  5 06:10  initial_weights.h5\n",
            "-rw------- 1 root root 59726904 Sep  5 06:04  vggforsp500.h5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HAQnQZoR6rD6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "bd4d4adc-5751-4c3d-81a0-e3489531edda"
      },
      "source": [
        "%cd /content/DL_Tools_For_Finance/\n",
        "trained_model_path='/content/DL_Tools_For_Finance/model/final_modelcategorical_crossentropy_adam_32.h5'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/DL_Tools_For_Finance\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yxMUW3tCMVBM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 598
        },
        "outputId": "31913564-b148-4977-e387-9e10ea68b556"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from keras.utils import np_utils\n",
        "from keras.models import load_model\n",
        "from keras import backend as K\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "'''\n",
        "Here we have a trained model model/vggforsp500.h5 and datas for testing \n",
        "datas/X_test_image.csv\n",
        "datas/Y_test_StateClass_image.csv\n",
        "datas/Y_test_FutPredict_image.csv\n",
        "\n",
        "'''\n",
        "\n",
        "##\n",
        "'''\n",
        "UTILITY FUNCTIONS\n",
        "to put in another file\n",
        "'''\n",
        "\n",
        "def change_X_df__nparray_image(df_X_train_image_flattened ):\n",
        "  '''\n",
        "  setup_input_NN_image returns a dataframe of flaten image for x train and xtest\n",
        "  then this function will change each date into a nparray list of images with 32, 32, 3 size \n",
        "  '''\n",
        "  X_train_image=df_X_train_image_flattened\n",
        "  nb_train=len(X_train_image.index)\n",
        "  \n",
        "  x_train=np.zeros((nb_train,32,32,3))\n",
        "  for i in range(nb_train):\n",
        "    tmp=np.array(X_train_image.iloc[i])\n",
        "    tmp=tmp.reshape(32,32,3)\n",
        "    x_train[i]=tmp\n",
        "  return x_train\n",
        "##\n",
        "\n",
        "\n",
        "\n",
        "#recuperation of testing datas and organising it \n",
        "X_test_image=pd.read_csv('datas/X_test_image.csv')\n",
        "Y_test_StateClass_image=pd.read_csv('datas/Y_test_StateClass_image.csv')\n",
        "Y_test_FutPredict_image=pd.read_csv('datas/Y_test_FutPredict_image.csv')\n",
        "\n",
        "#setting up the index to Date\n",
        "X_test_image=X_test_image.set_index(\"Date\")\n",
        "Y_test_StateClass_image=Y_test_StateClass_image.set_index(\"Date\")\n",
        "Y_test_FutPredict_image=Y_test_FutPredict_image.set_index(\"Date\")\n",
        "\n",
        "#modify dataset to np array for input to NN\n",
        "x_test=change_X_df__nparray_image(X_test_image)\n",
        "y_test_state=np.array(Y_test_StateClass_image)\n",
        "y_test_value=np.array(Y_test_FutPredict_image)\n",
        "\n",
        "##Setting up xtest and ytest\n",
        "#Here we focus on predicting the future state Y_train_StateClass_image\n",
        "nb_test=len(X_test_image.index)\n",
        "x_test=np.zeros((nb_test,32,32,3))\n",
        "for i in range(nb_test):\n",
        "  tmp=np.array(X_test_image.iloc[i])\n",
        "  tmp=tmp.reshape(32,32,3)\n",
        "  x_test[i]=tmp\n",
        "\n",
        "y_test=np.array(Y_test_StateClass_image)\n",
        "#y_test=np.array(Y_test_FutPredict_image)\n",
        "\n",
        "#In our example we need to y into categorical as it has 6 categories\n",
        "nb_classes=6\n",
        "y_test_m = np_utils.to_categorical(y_test, nb_classes)\n",
        "############\n",
        "#recuperation of model\n",
        "vggsp500model = load_model(trained_model_path)\n",
        "\n",
        "#Evaluate the model on the test data\n",
        "score  = vggsp500model.evaluate(x_test, y_test_m)\n",
        "\n",
        "\n",
        "Y_pred = vggsp500model.predict(x_test)\n",
        "y_pred = np.argmax(Y_pred, axis=1)\n",
        "y= np.argmax(y_test_m,axis=1)\n",
        "\n",
        "\n",
        "print('Confusion Matrix')\n",
        "\n",
        "target_state = ['SS', 'SN', 'N','NB','BB','Error']\n",
        "\n",
        "def statetostring(x):\n",
        "  return target_state[int(x)]\n",
        "\n",
        "sY_pred=[statetostring(i) for i in y_pred]\n",
        "sY_real=[statetostring(i) for i in y]\n",
        "\n",
        "#matrice  de confusion\n",
        "mat=confusion_matrix(sY_real, sY_pred, normalize='true', labels=target_state)\n",
        "df_confmat=pd.DataFrame(mat,index=target_state, columns=target_state)\n",
        "\n",
        "#matrice  de confusion\n",
        "mat2=confusion_matrix(sY_real, sY_pred,  labels=target_state)\n",
        "df_confmat2=pd.DataFrame(mat2,index=target_state, columns=target_state)\n",
        "\n",
        "#Accuracy on test data\n",
        "print('Accuracy on the Test Images: ', score[1])\n",
        "#matrice  de confusion\n",
        "print(df_confmat)\n",
        "\n",
        "#matrice  de confusion\n",
        "print(df_confmat2)\n",
        "\n",
        "# Classification report\n",
        "print('classification report')\n",
        "print(classification_report(sY_real, sY_pred, target_names=target_state))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "146/146 [==============================] - 1s 5ms/step - loss: 1.4815 - accuracy: 0.3833\n",
            "Confusion Matrix\n",
            "Accuracy on the Test Images:  0.3833225667476654\n",
            "             SS        SN         N   NB        BB  Error\n",
            "SS     0.051780  0.000000  0.800971  0.0  0.147249    0.0\n",
            "SN     0.013834  0.023715  0.806324  0.0  0.156126    0.0\n",
            "N      0.007975  0.006135  0.712270  0.0  0.273620    0.0\n",
            "NB     0.007496  0.000000  0.659670  0.0  0.332834    0.0\n",
            "BB     0.004136  0.000000  0.521092  0.0  0.474773    0.0\n",
            "Error  0.000000  0.000000  0.454545  0.0  0.545455    0.0\n",
            "       SS  SN     N  NB   BB  Error\n",
            "SS     32   0   495   0   91      0\n",
            "SN      7  12   408   0   79      0\n",
            "N      13  10  1161   0  446      0\n",
            "NB      5   0   440   0  222      0\n",
            "BB      5   0   630   0  574      0\n",
            "Error   0   0     5   0    6      0\n",
            "classification report\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          SS       0.40      0.47      0.44      1209\n",
            "          SN       0.00      0.00      0.00        11\n",
            "           N       0.37      0.71      0.49      1630\n",
            "          NB       0.00      0.00      0.00       667\n",
            "          BB       0.55      0.02      0.05       506\n",
            "       Error       0.52      0.05      0.09       618\n",
            "\n",
            "    accuracy                           0.38      4641\n",
            "   macro avg       0.31      0.21      0.18      4641\n",
            "weighted avg       0.36      0.38      0.30      4641\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TIz78gePZY76",
        "colab_type": "text"
      },
      "source": [
        "##Step 4: Guess future market state from random image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ulkwESoxZm1z",
        "colab_type": "text"
      },
      "source": [
        "Take an image of an historical graph from a market webpage like investing.com and save it to the ImageM/ folder with name image1.PNG or you can change the value of image_path to the link you need.\n",
        "\n",
        "This execution tell us which market state in the future is the best representative."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yvqRijItZML8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "outputId": "c5063777-565a-44f7-82c8-2a53cb84637b"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from keras.utils import np_utils\n",
        "from keras.models import load_model\n",
        "from keras import backend as K\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "#path for trained model\n",
        "trained_model_path='model/vggforsp500.h5'\n",
        "\n",
        "#path for the image taken by the user\n",
        "#image_path='ImageM/image1.PNG'\n",
        "image_path =input(\"Enter the path of the image to check next state:\\n\")\n",
        "\n",
        "#Load the image and resize it to 32x32 and taking off the transparency\n",
        "load_img_rz = np.array(Image.open(image_path).resize((32,32)))\n",
        "#Image.fromarray(load_img_rz).save('/content/drive/My Drive/_sample_data/ImageM/image1.PNG')\n",
        "image=load_img_rz[:,:,:3]/255\n",
        "print(\"After resizing:\",image.shape)\n",
        "\n",
        "#petite astuce pour ne pas avoir d erreur avec les types list, tensors,  nparray et dataframe\n",
        "doubleimage=np.array([image,image])\n",
        "############\n",
        "#recuperation of the model\n",
        "vggsp500model = load_model(trained_model_path)\n",
        "Y_pred = vggsp500model.predict(doubleimage)\n",
        "y_pred = np.argmax(Y_pred, axis=1)\n",
        "\n",
        "target_state = ['SS', 'SN', 'N','NB','BB','Error']\n",
        "df_result=pd.DataFrame((Y_pred))\n",
        "\n",
        "df_result.columns=target_state\n",
        "df_result.index=[image_path, image_path+'1']\n",
        "print (\"for \",image_path, \"the best result is \", target_state[int(y_pred[0])] )\n",
        "\n",
        "df_result"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Enter the path of the image to check next state:\n",
            "/content/DL_Tools_For_Finance/ImageM/image2.PNG\n",
            "After resizing: (32, 32, 3)\n",
            "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f3286689ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "for  /content/DL_Tools_For_Finance/ImageM/image2.PNG the best result is  SS\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>SS</th>\n",
              "      <th>SN</th>\n",
              "      <th>N</th>\n",
              "      <th>NB</th>\n",
              "      <th>BB</th>\n",
              "      <th>Error</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>/content/DL_Tools_For_Finance/ImageM/image2.PNG</th>\n",
              "      <td>0.31159</td>\n",
              "      <td>0.119561</td>\n",
              "      <td>0.263224</td>\n",
              "      <td>0.073631</td>\n",
              "      <td>0.231359</td>\n",
              "      <td>0.000636</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>/content/DL_Tools_For_Finance/ImageM/image2.PNG1</th>\n",
              "      <td>0.31159</td>\n",
              "      <td>0.119561</td>\n",
              "      <td>0.263224</td>\n",
              "      <td>0.073631</td>\n",
              "      <td>0.231359</td>\n",
              "      <td>0.000636</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                       SS  ...     Error\n",
              "/content/DL_Tools_For_Finance/ImageM/image2.PNG   0.31159  ...  0.000636\n",
              "/content/DL_Tools_For_Finance/ImageM/image2.PNG1  0.31159  ...  0.000636\n",
              "\n",
              "[2 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GBOG00tVbHS7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 285
        },
        "outputId": "686ec072-2661-4b03-c1a8-d4ea42ed5233"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.imshow(image)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f007032f1d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gXmOXke1hlip",
        "colab_type": "text"
      },
      "source": [
        "Give the results for all the PNG file of a directory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z0aDBnGTh6yi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 662
        },
        "outputId": "2a0aa2d4-3708-496b-9b3a-ecfcab9b7387"
      },
      "source": [
        "import os\n",
        "\n",
        "path = '/content/DL_Tools_For_Finance/ImageM/'\n",
        "\n",
        "#files = os.listdir(path)\n",
        "#l_file=[]\n",
        "#for f in files:\n",
        "#    l_file.append(f)\n",
        "\n",
        "l_file=glob.glob('**/*.PNG', recursive=True)\n",
        "l_image_input_NN=[]\n",
        "\n",
        "for x_image in l_file:  \n",
        "  #Load the image and resize it to 32x32 and taking off the transparency\n",
        "  load_img_rz = np.array(Image.open(x_image).resize((32,32)))\n",
        "\n",
        "  #Image.fromarray(load_img_rz).save('/content/drive/My Drive/_sample_data/ImageM/image1.PNG')\n",
        "  image=load_img_rz[:,:,:3]/255\n",
        "  print(\"After resizing:\",image.shape)\n",
        "\n",
        "  l_image_input_NN.append(image)\n",
        "\n",
        "############\n",
        "#recuperation of the model\n",
        "vggsp500model = load_model(trained_model_path)\n",
        "Y_pred = vggsp500model.predict(np.array(l_image_input_NN))\n",
        "y_pred = np.argmax(Y_pred, axis=1)\n",
        "\n",
        "target_state = ['SS', 'SN', 'N','NB','BB','Error']\n",
        "df_result=pd.DataFrame((Y_pred))\n",
        "\n",
        "df_result.columns=target_state\n",
        "df_result.index=l_file\n",
        "\n",
        "df_decision=pd.DataFrame([target_state[i] for i in  y_pred],index=l_file, columns=['Decision'])\n",
        "df_result=pd.concat([df_result,df_decision],axis=1)\n",
        "(df_result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "After resizing: (32, 32, 3)\n",
            "After resizing: (32, 32, 3)\n",
            "After resizing: (32, 32, 3)\n",
            "After resizing: (32, 32, 3)\n",
            "After resizing: (32, 32, 3)\n",
            "After resizing: (32, 32, 3)\n",
            "After resizing: (32, 32, 3)\n",
            "After resizing: (32, 32, 3)\n",
            "After resizing: (32, 32, 3)\n",
            "After resizing: (32, 32, 3)\n",
            "After resizing: (32, 32, 3)\n",
            "After resizing: (32, 32, 3)\n",
            "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f32888d3488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>SS</th>\n",
              "      <th>SN</th>\n",
              "      <th>N</th>\n",
              "      <th>NB</th>\n",
              "      <th>BB</th>\n",
              "      <th>Error</th>\n",
              "      <th>Decision</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>ImageM/image3.PNG</th>\n",
              "      <td>0.301182</td>\n",
              "      <td>0.121881</td>\n",
              "      <td>0.264708</td>\n",
              "      <td>0.073682</td>\n",
              "      <td>0.237916</td>\n",
              "      <td>0.000632</td>\n",
              "      <td>SS</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ImageM/image2.PNG</th>\n",
              "      <td>0.311591</td>\n",
              "      <td>0.119561</td>\n",
              "      <td>0.263224</td>\n",
              "      <td>0.073630</td>\n",
              "      <td>0.231358</td>\n",
              "      <td>0.000636</td>\n",
              "      <td>SS</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ImageM/image1.PNG</th>\n",
              "      <td>0.209848</td>\n",
              "      <td>0.190190</td>\n",
              "      <td>0.379863</td>\n",
              "      <td>0.045567</td>\n",
              "      <td>0.174114</td>\n",
              "      <td>0.000418</td>\n",
              "      <td>N</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ImageM/image4.PNG</th>\n",
              "      <td>0.342570</td>\n",
              "      <td>0.142008</td>\n",
              "      <td>0.243726</td>\n",
              "      <td>0.072087</td>\n",
              "      <td>0.198942</td>\n",
              "      <td>0.000667</td>\n",
              "      <td>SS</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ImageM/image5.PNG</th>\n",
              "      <td>0.194147</td>\n",
              "      <td>0.321485</td>\n",
              "      <td>0.290173</td>\n",
              "      <td>0.048638</td>\n",
              "      <td>0.144945</td>\n",
              "      <td>0.000611</td>\n",
              "      <td>SN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ImageM/graphsp500tocrop.PNG</th>\n",
              "      <td>0.001264</td>\n",
              "      <td>0.057363</td>\n",
              "      <td>0.833274</td>\n",
              "      <td>0.032646</td>\n",
              "      <td>0.075410</td>\n",
              "      <td>0.000044</td>\n",
              "      <td>N</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ImageM/ImageM/image3.PNG</th>\n",
              "      <td>0.301182</td>\n",
              "      <td>0.121881</td>\n",
              "      <td>0.264708</td>\n",
              "      <td>0.073682</td>\n",
              "      <td>0.237916</td>\n",
              "      <td>0.000632</td>\n",
              "      <td>SS</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ImageM/ImageM/image2.PNG</th>\n",
              "      <td>0.311591</td>\n",
              "      <td>0.119561</td>\n",
              "      <td>0.263224</td>\n",
              "      <td>0.073630</td>\n",
              "      <td>0.231358</td>\n",
              "      <td>0.000636</td>\n",
              "      <td>SS</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ImageM/ImageM/image1.PNG</th>\n",
              "      <td>0.209848</td>\n",
              "      <td>0.190190</td>\n",
              "      <td>0.379863</td>\n",
              "      <td>0.045567</td>\n",
              "      <td>0.174114</td>\n",
              "      <td>0.000418</td>\n",
              "      <td>N</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ImageM/ImageM/image4.PNG</th>\n",
              "      <td>0.342570</td>\n",
              "      <td>0.142008</td>\n",
              "      <td>0.243726</td>\n",
              "      <td>0.072087</td>\n",
              "      <td>0.198942</td>\n",
              "      <td>0.000667</td>\n",
              "      <td>SS</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ImageM/ImageM/image5.PNG</th>\n",
              "      <td>0.194147</td>\n",
              "      <td>0.321485</td>\n",
              "      <td>0.290173</td>\n",
              "      <td>0.048638</td>\n",
              "      <td>0.144945</td>\n",
              "      <td>0.000611</td>\n",
              "      <td>SN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ImageM/ImageM/graphsp500tocrop.PNG</th>\n",
              "      <td>0.001264</td>\n",
              "      <td>0.057363</td>\n",
              "      <td>0.833274</td>\n",
              "      <td>0.032646</td>\n",
              "      <td>0.075410</td>\n",
              "      <td>0.000044</td>\n",
              "      <td>N</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                          SS        SN  ...     Error  Decision\n",
              "ImageM/image3.PNG                   0.301182  0.121881  ...  0.000632        SS\n",
              "ImageM/image2.PNG                   0.311591  0.119561  ...  0.000636        SS\n",
              "ImageM/image1.PNG                   0.209848  0.190190  ...  0.000418         N\n",
              "ImageM/image4.PNG                   0.342570  0.142008  ...  0.000667        SS\n",
              "ImageM/image5.PNG                   0.194147  0.321485  ...  0.000611        SN\n",
              "ImageM/graphsp500tocrop.PNG         0.001264  0.057363  ...  0.000044         N\n",
              "ImageM/ImageM/image3.PNG            0.301182  0.121881  ...  0.000632        SS\n",
              "ImageM/ImageM/image2.PNG            0.311591  0.119561  ...  0.000636        SS\n",
              "ImageM/ImageM/image1.PNG            0.209848  0.190190  ...  0.000418         N\n",
              "ImageM/ImageM/image4.PNG            0.342570  0.142008  ...  0.000667        SS\n",
              "ImageM/ImageM/image5.PNG            0.194147  0.321485  ...  0.000611        SN\n",
              "ImageM/ImageM/graphsp500tocrop.PNG  0.001264  0.057363  ...  0.000044         N\n",
              "\n",
              "[12 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w8Rqf47ewbmk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "539f26c0-046f-495b-b3f6-b7e630e6ba9c"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Enter the path of the image to check next state:/content/DL_Tools_For_Finance/ImageM/image2.PNG\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QeXi5PdYxJ3r",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "40bc8e51-f752-4868-d0a1-372f17cb57d9"
      },
      "source": [
        "direction"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/DL_Tools_For_Finance/ImageM/image2.PNG'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uE2TBYhJxQK7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}