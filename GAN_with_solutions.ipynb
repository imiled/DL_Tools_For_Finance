{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "GAN_with_solutions.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [
        "vO54aIg2eCAl",
        "r-HVHOvseCAo",
        "3nEsZVWFeCAq",
        "Vtjpb-xMokRk",
        "fgqNCA4leCAt",
        "QxVAzw4TeCA8",
        "5pE7wpeReCBE",
        "Cz5BGjR9taZ9",
        "51a7mPmWvGeR",
        "ft_WGAQgeCBS",
        "ahSfCoXoxJFC",
        "JnZbsfj4xsRF",
        "5btQmaL1zpw7",
        "k_gxQd4n0eSm",
        "-V3QjnlmBqyj",
        "UaKt5UlneCB1",
        "MAeXNYS3BCs4",
        "hm1Hxs6gCMIJ",
        "olZKzxvqEa-c",
        "U-lyA2hmGRSF",
        "kpeF45KzGeMe",
        "BmKkqSBsFUmi",
        "pwKDzxftqYR_",
        "T6TzWjDLqduG",
        "uKtdSWibqoY7",
        "wrpf8Edu1B7i",
        "ujtA8NKC1GVw",
        "_58b9HKW1POK",
        "ZqIW2DRO1Uhb",
        "t1P6LHqP1a7P",
        "krX-4HOI11RE",
        "FlHExOFG15XG",
        "AXh_Kl3v2G3t",
        "_hO6Z-c92MyN",
        "iTbW8q5b2W8L",
        "OWICh9Dt4Uxp",
        "jEaCEc8iaWEH",
        "vVyXizD1C6DQ",
        "5xkrUQrRBh-0",
        "XrtGXe8JCV29",
        "M2erYkOCHptU",
        "H7T1ofMVIG4h",
        "NlhyJ3Rou8g4",
        "wtds-zDwDvK6",
        "6OwQCLpexY5p",
        "KrBNBY5mLXOv"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.6"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/imiled/DL_Tools_For_Finance/blob/master/GAN_with_solutions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "vO54aIg2eCAl"
      },
      "source": [
        "# Generative Adversarial Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "1GhRTGP2eCAn"
      },
      "source": [
        "Content:\n",
        " * Intro: why do they exist, general structure, current impact\n",
        " * Simple example: 1D GAN to generate a specific function\n",
        " * Issue: evaluating the generative model\n",
        " * Extension: generating various classes of 1 variable functions\n",
        " * Issue: non-convergence and mode collapse\n",
        " * Application: GANs for semi-supervised learning (MNIST data)\n",
        " * Bonus task: feature matching\n",
        " * Bonus task: GANs vs. VAEs\n",
        " * Final remarks\n",
        " * References"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "r-HVHOvseCAo"
      },
      "source": [
        "## An incomplete but necessary introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "sDsr-1wheCAp"
      },
      "source": [
        "GANs are generative models originally introduced in [1] which don't attempt to explicitly model the underlying density of the data $p_{\\text{data}}$, but rather learn it implicitly and sample directly from it. VAEs, on the other hand, learn and sample from an explicit approximation of $p_{\\text{data}}$.\n",
        "\n",
        "One of the key characteristics of GANs is their training dynamics, with an interplay between two components: a _generator_ $G$ and a _discriminator_ $D$. These are usually implemented as deep neural networks, but in principle the adversarial dynamic governing GANs' training may be applied to non neural models.\n",
        "\n",
        "GANs offer various __advantages__ over previous generative models, such as:\n",
        " * they offer parallel sampling\n",
        " * the only real restriction on the generator function is that it be differentiable\n",
        " * neural networks are universal function approximators (in principle)\n",
        " \n",
        "However, they do also introduce new __disadvantages__:\n",
        " * the training is potentially unstable, leading to non-convergence and mode collapse\n",
        " * differentiability of the generator means their _direct_ application to discrete data, such as text, is harder\n",
        " * direct manipulation of the latent space of the generator is not easy\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "3nEsZVWFeCAq"
      },
      "source": [
        "### Structure and training of a GAN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ZWUVzSGnpODG",
        "colab": {}
      },
      "source": [
        "import IPython"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "8FKFOHbOeCAq"
      },
      "source": [
        "Let's see how a GAN works (image taken from [2]):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dX5Rx00Np4FK",
        "colab": {}
      },
      "source": [
        "IPython.display.HTML('<img src=\"https://drive.google.com/uc?export=view&id=10hF29LJBMjC4tCpNe9FTfe_TDKXVjGii\" width=\"600\">') \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "collapsed": true,
        "id": "vi-wgJdqeCAs"
      },
      "source": [
        "The training happens in two scenarios:\n",
        " 1. Real data is sampled from the training set and shown to the discriminator $D$. The objective here is to get $D$'s value as close as possible to $1$.\n",
        " 2. $G$ generates a fake data sample $x$, which is then shown to the discriminator. Here $G$'s objective is to generate $x=G(z)$ such that $D(x)\\approx 1$, while $D$'s objective is to achieve $D(x)\\approx 0$\n",
        " \n",
        "The assumption is that $D$ sees the same number of real and fake examples. Normally, these are drawn as two minibatches and the two learning steps (to optimise $G$ and $D$, respectively) are done at once, i.e. simultaneous SGD."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vtjpb-xMokRk",
        "colab_type": "text"
      },
      "source": [
        "#### **Question :)**  When can we consider the model to be trained (i.e. be at an equilibrium)?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5XWIXNE9sxMs",
        "colab_type": "text"
      },
      "source": [
        "When $D$'s classification accuracy is around .5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "fgqNCA4leCAt"
      },
      "source": [
        "## Enough with this s**t, show me the code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "nRKO30UfeCAt"
      },
      "source": [
        "We'll start off by building a toy example of a GAN which learns to generate pairs of real numbers obeying some sort of functional relationship, for example cubic $y=x^3$.\n",
        "\n",
        "In other words, we want a model that sample points which, when plotted, look, like a cubic curve."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "rGD8F6Wiq_JB",
        "colab": {}
      },
      "source": [
        "# Our code generates plenty of warnings because of GANs' weird compiling \n",
        "# requirements. This is to mute them.\n",
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "%tensorflow_version 1.x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5khadIBbeCAy",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "akxi1V3ueCAu",
        "colab": {}
      },
      "source": [
        "# define the cubic\n",
        "def f(x):\n",
        "    return x**3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_3YnaOi3eCA0",
        "colab": {}
      },
      "source": [
        "# plot a sample of points between, say, -5 and 5\n",
        "x = np.linspace(-5, 5, 100)\n",
        "y = f(x)\n",
        "\n",
        "plt.plot(x, y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "l16X0bugeCA3"
      },
      "source": [
        "__Why do we need this?!__\n",
        "\n",
        "Remember: a GAN is trained on both real and fake points. _Real_ training points will be taken from this cubic. \n",
        "\n",
        "So we need some code to generate them :)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "klfPRxhweCA4",
        "colab": {}
      },
      "source": [
        "# Let's stick to generating points between -5 and 5, shall we?\n",
        "def generate_real_samples(f, n=64):\n",
        "    x = (np.random.rand(n) - .5) * 10\n",
        "    y = f(x)\n",
        "    x = x.reshape(n, 1)\n",
        "    y = y.reshape(n, 1)\n",
        "    labels = np.ones((n, 1))\n",
        "    xy = np.hstack((x,y))\n",
        "    return (xy, labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "B57dtohQeCA6",
        "colab": {}
      },
      "source": [
        "x = np.linspace(-5, 5, 100)\n",
        "y = f(x)\n",
        "gen_xy, _ = generate_real_samples(f, 10) #we don't need the labels just yet\n",
        "plt.plot(x, y)\n",
        "plt.plot(gen_xy[:,0], gen_xy[:,1], \"ro\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "QxVAzw4TeCA8"
      },
      "source": [
        "### Let's get serious: the discriminator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EVkgwkPZqVIi",
        "colab_type": "text"
      },
      "source": [
        "We can now move to one of the two fundamental components of a GAN: the discriminator. This is the model which will try to discern whether the points it's seeing are real or fake.\n",
        "\n",
        "You can think of it as a kind of classifier."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "WtXAn929eCA9"
      },
      "source": [
        "We'll build the discriminator model with the following ingredients:\n",
        " * 1 hidden layer of 30 nodes\n",
        " * ReLU activations\n",
        " * He intialisation\n",
        " * binary crossentropy loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9opDCjHFeCA-",
        "colab": {}
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "\n",
        "def build_discriminator(n_inputs=2, nh_1=30, init=\"he_uniform\", act=\"relu\"):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(nh_1, activation=act, kernel_initializer=init, input_dim=n_inputs))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0qwlC-sieCBA",
        "colab": {}
      },
      "source": [
        "from keras.utils.vis_utils import plot_model\n",
        "\n",
        "model = build_discriminator()\n",
        "model.summary()\n",
        "plot_model(model, show_shapes=True, show_layer_names=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "5pE7wpeReCBE"
      },
      "source": [
        "### The star of the show: the generator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QYKOky2MrmEe",
        "colab_type": "text"
      },
      "source": [
        "The generator is the key piece of a GAN, being the model tasked with learning how to generate realistic samples, _realistic_ here meaning: looking as if they had been drawn from the underlying distribution of the training data.\n",
        "\n",
        "The way the generator is helped to fulfil its role is by _being trained_ with the feedback obtained from $D$'s classification performance. Other than that, it is really nothing more than a (nonlinear) function on top of some random noise."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "9j9In3g9eCBF"
      },
      "source": [
        "The ingredients for our discriminator model:\n",
        " * A $n$-dimensional latent space of Gaussian features (random noise fed as input to the next layer)\n",
        " * A dense hidden layer of 30 nodes with ReLU activations\n",
        " * He initialisation\n",
        " * linear activation on the output layer (to scale the generated $(x,y)$ pairs)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "pIiUfMYIeCBF",
        "colab": {}
      },
      "source": [
        "def build_generator(latent_dim, nh_1=30, init=\"he_uniform\", act=\"relu\", n_outputs=2):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(nh_1, activation=act, kernel_initializer=init, input_dim=latent_dim))\n",
        "    model.add(Dense(n_outputs, activation='linear'))\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dqHQwtNfeCBI",
        "colab": {}
      },
      "source": [
        "model = build_generator(10)\n",
        "model.summary()\n",
        "plot_model(model, show_shapes=True, show_layer_names=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cz5BGjR9taZ9",
        "colab_type": "text"
      },
      "source": [
        "#### __Question :)__ Why are we not compiling $G$? Shouldn't there be some SGD details in there??"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "14zEon_reCBH"
      },
      "source": [
        "The generator is _not_ compiled because it is _never directly fit_, unlike the discriminator which does go through a training scenario _on its own_ when it sees real samples.\n",
        "\n",
        "This is not to say no updates are backpropagated to the generator! As we'll see in a moment, backpropagation needs to happen _through_ the discriminator."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51a7mPmWvGeR",
        "colab_type": "text"
      },
      "source": [
        "#### A generator dry run"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zIUKmepXeCBK"
      },
      "source": [
        "Let's see how the model can be used to generate samples, although at the moment we shouldn't get our hopes too high..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dtuzqBzXeCBL",
        "colab": {}
      },
      "source": [
        "# sampling in the latent space\n",
        "def sample_latent_points(latent_dim, n):\n",
        "    # generate points in the latent space\n",
        "    x_input = np.random.randn(latent_dim * n)\n",
        "    # reshape into a batch of inputs for the network\n",
        "    x_input = x_input.reshape(n, latent_dim)\n",
        "    return x_input"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "s11P_aJieCBN",
        "colab": {}
      },
      "source": [
        "# we already know what we'll use the generator for... :P\n",
        "def generate_fake_samples(generator, latent_dim, n):\n",
        "    # sample points in latent space\n",
        "    x_input = sample_latent_points(latent_dim, n)\n",
        "    # pass through the generator\n",
        "    X = generator.predict(x_input)\n",
        "    labels = np.zeros((n, 1))\n",
        "    return X, labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qJXL44oceCBP",
        "colab": {}
      },
      "source": [
        "# let's put it to work\n",
        "latent_dim = 5\n",
        "model = build_generator(latent_dim, n_outputs=2)\n",
        "X_fake, _ = generate_fake_samples(model, latent_dim, 100)\n",
        "plt.scatter(X_fake[:,0], X_fake[:,1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "DeYTgHFLeCBR"
      },
      "source": [
        "More cubic or more cubist?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ft_WGAQgeCBS"
      },
      "source": [
        "### Building the GAN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "uWt70qABeCBS"
      },
      "source": [
        "Let's write the code to build and train our GAN. Predictably, we'll join the generator and discriminator we implemented thus far and - the most important bit - define how they play out together during training. (This is where we finally train $G$!!) \n",
        "\n",
        "Finally, we'll put in place a simple evaluation of the GAN.\n",
        "\n",
        "To build our joint model, we'll use:\n",
        "* binary crossentropy loss\n",
        "* Adam"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Xz1PZknEeCBT",
        "colab": {}
      },
      "source": [
        "def build_gan(generator, discriminator):\n",
        "    # NB: Why are we doing this?\n",
        "    discriminator.trainable = False\n",
        "    # stack the models together\n",
        "    model = Sequential()\n",
        "    # add generator\n",
        "    model.add(generator)\n",
        "    # add the discriminator\n",
        "    model.add(discriminator)\n",
        "    # We _do_ compile this model!\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam')\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "rj0dRQbfeCBW",
        "colab": {}
      },
      "source": [
        "latent_dim = 20\n",
        "discriminator = build_discriminator()\n",
        "generator = build_generator(latent_dim)\n",
        "gan_model = build_gan(generator, discriminator)\n",
        "gan_model.summary()\n",
        "plot_model(gan_model, show_shapes=True, show_layer_names=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ahSfCoXoxJFC",
        "colab_type": "text"
      },
      "source": [
        "#### __Question :)__ Why are we not training $D$ in the GAN model?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IJtjToquxQcM",
        "colab_type": "text"
      },
      "source": [
        "Me marked $D$'s weights as non trainable because $D$'s role within the GAN model is only to provide feedback on $G$ performance, i.e. on fake samples.\n",
        "\n",
        "We'll take care of training the discriminator as a standalone model on both real and fake data, but within the GAN we don't really want to update its weights to prevent overfitting on fake points."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JnZbsfj4xsRF",
        "colab_type": "text"
      },
      "source": [
        "### Defining the training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-lqTs-reeCBa"
      },
      "source": [
        "Let's now write the training code. Remember: the discriminator is trained on the same number of real and fake samples and _outside_ the joint model. Within the latter, the discriminator's role is merely to provide feedback to the generator."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "g44lVaSheCBb",
        "colab": {}
      },
      "source": [
        "# train the generator and discriminator\n",
        "def train(generator, discriminator, gan, latent_dim, n_epochs=10000, n_batch=128):\n",
        "    # determine half the size of one batch, for updating the discriminator\n",
        "    half_batch = int(n_batch / 2)\n",
        "    # manually enumerate epochs\n",
        "    for i in range(n_epochs):\n",
        "        # real samples\n",
        "        x_real, y_real = generate_real_samples(half_batch)\n",
        "        # fake examples\n",
        "        x_fake, y_fake = generate_fake_samples(generator, latent_dim, half_batch)\n",
        "        # train the discriminator\n",
        "        discriminator.train_on_batch(x_real, y_real)\n",
        "        discriminator.train_on_batch(x_fake, y_fake)\n",
        "        # sample the latent space: THIS is the input to the joint model!\n",
        "        x_gan = sample_latent_points(latent_dim, n_batch)\n",
        "        # QUESTION: Why are we switching the labels here??\n",
        "        y_gan = np.ones((n_batch, 1))\n",
        "        # train the generator using the discriminator's feedback\n",
        "        gan.train_on_batch(x_gan, y_gan)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5btQmaL1zpw7",
        "colab_type": "text"
      },
      "source": [
        "#### __Question :)__ Why are we switching labels when training the joint model?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "geVkBnZJz0YG",
        "colab_type": "text"
      },
      "source": [
        "We need the discriminator to believe that the samples it's seeing are _real_, even though they're not. The generated loss will _not_ impact $D$'s weights (they're nontrainable!), instead, it'll only help $G$. Indeed, a high loss would mean $G$'s samples are still too unrealistic, as $D$ would be marking them as 0."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k_gxQd4n0eSm",
        "colab_type": "text"
      },
      "source": [
        "### Adding some evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "eiwokhcieCBd"
      },
      "source": [
        "In principle we could just take the plunge and train our GAN, now...but how would we know how well it's working?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Rq6QV8E2eCBe",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import mean_squared_error as mse"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "lRMQ-6a4eCBf",
        "colab": {}
      },
      "source": [
        "def evaluate(discriminator, generator, latent_dim, ground_truth_f, error_function, eval_size=100):\n",
        "    generated_xy, generated_labs = generate_fake_samples(generator, latent_dim, eval_size)\n",
        "    real_xy, real_labs = generate_real_samples(ground_truth_f, eval_size)\n",
        "    _, dar = discriminator.evaluate(real_xy, real_labs, verbose=0)\n",
        "    _, daf = discriminator.evaluate(generated_xy, generated_labs, verbose=0)\n",
        "    correct_y = ground_truth_f(generated_xy[:,0])\n",
        "    return dar, daf, error_function(correct_y, generated_xy[:,1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "fC93jGsxeCBh"
      },
      "source": [
        "Let's incorporate an evaluation step within the training procedure."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "nvhocvuoeCBj",
        "colab": {}
      },
      "source": [
        "# train the generator and discriminator\n",
        "def train(generator, discriminator, gan, latent_dim, n_epochs=10000, n_batch=128, eval_interval=200):\n",
        "    # determine half the size of one batch, for updating the discriminator\n",
        "    half_batch = int(n_batch / 2)\n",
        "    # manually enumerate epochs\n",
        "    for i in range(n_epochs):\n",
        "        # real samples\n",
        "        x_real, y_real = generate_real_samples(f, half_batch)\n",
        "        # fake examples\n",
        "        x_fake, y_fake = generate_fake_samples(generator, latent_dim, half_batch)\n",
        "        # train the discriminator\n",
        "        discriminator.train_on_batch(x_real, y_real)\n",
        "        discriminator.train_on_batch(x_fake, y_fake)\n",
        "        # sample the latent space: THIS is the input to the joint model!\n",
        "        x_gan = sample_latent_points(latent_dim, n_batch)\n",
        "        # QUESTION: Why are we switching the labels here??\n",
        "        y_gan = np.ones((n_batch, 1))\n",
        "        # train the generator using the discriminator's feedback\n",
        "        gan.train_on_batch(x_gan, y_gan)\n",
        "        if (i+1) % eval_interval == 0:\n",
        "            dar, daf, ee = evaluate(discriminator, generator, latent_dim, f, mse)\n",
        "            print(\"epoch %s: discr acc real: %s, discr acc fake: %s, eval error: %s\" % (i, dar, daf, ee))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "lXRcWJ0HeCBk"
      },
      "source": [
        "For the sake of completeness, let's join all the relevant code in a single cell. It'll be useful, trust me."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Q08ndukdeCBl",
        "colab": {}
      },
      "source": [
        "def f(x):\n",
        "    return x**3\n",
        "\n",
        "def generate_real_samples(f, n=64):\n",
        "    x = (np.random.rand(n) - .5) * 10\n",
        "    y = f(x)\n",
        "    x = x.reshape(n, 1)\n",
        "    y = y.reshape(n, 1)\n",
        "    labels = np.ones((n, 1))\n",
        "    xy = np.hstack((x,y))\n",
        "    return (xy, labels)\n",
        "\n",
        "def build_discriminator(n_inputs=2, nh_1=30, init=\"he_uniform\", act=\"relu\"):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(nh_1, activation=act, kernel_initializer=init, input_dim=n_inputs))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "def build_generator(latent_dim, nh_1=30, init=\"he_uniform\", act=\"relu\", n_outputs=2):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(nh_1, activation=act, kernel_initializer=init, input_dim=latent_dim))\n",
        "    model.add(Dense(n_outputs, activation='linear'))\n",
        "    return model\n",
        "\n",
        "#sampling in the latent space\n",
        "def sample_latent_points(latent_dim, n):\n",
        "    # generate points in the latent space\n",
        "    x_input = np.random.randn(latent_dim * n)\n",
        "    # reshape into a batch of inputs for the network\n",
        "    x_input = x_input.reshape(n, latent_dim)\n",
        "    return x_input\n",
        "\n",
        "def generate_fake_samples(generator, latent_dim, n):\n",
        "    # sample points in latent space\n",
        "    x_input = sample_latent_points(latent_dim, n)\n",
        "    # pass through the generator\n",
        "    X = generator.predict(x_input)\n",
        "    labels = np.zeros((n, 1))\n",
        "    return X, labels\n",
        "\n",
        "def build_gan(generator, discriminator):\n",
        "    discriminator.trainable = False\n",
        "    # stack the models together\n",
        "    model = Sequential()\n",
        "    # add generator\n",
        "    model.add(generator)\n",
        "    # add the discriminator\n",
        "    model.add(discriminator)\n",
        "    # We _do_ compile this model!\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam')\n",
        "    return model\n",
        "\n",
        "def evaluate(discriminator, generator, latent_dim, ground_truth_f, error_function, eval_size=100):\n",
        "    generated_xy, generated_labs = generate_fake_samples(generator, latent_dim, eval_size)\n",
        "    real_xy, real_labs = generate_real_samples(ground_truth_f, eval_size)\n",
        "    _, dar = discriminator.evaluate(real_xy, real_labs, verbose=0)\n",
        "    _, daf = discriminator.evaluate(generated_xy, generated_labs, verbose=0)\n",
        "    correct_y = ground_truth_f(generated_xy[:,0])\n",
        "    return dar, daf, error_function(correct_y, generated_xy[:,1]), generated_xy[:,0], generated_xy[:,1]\n",
        "\n",
        "# train the generator and discriminator\n",
        "def train(generator, discriminator, gan, latent_dim, n_epochs=10000, n_batch=128, eval_interval=200):\n",
        "    gen_xy_for_evaluation = []\n",
        "    # determine half the size of one batch, for updating the discriminator\n",
        "    half_batch = int(n_batch / 2)\n",
        "    # manually enumerate epochs\n",
        "    for i in range(n_epochs):\n",
        "        # real samples\n",
        "        x_real, y_real = generate_real_samples(f, half_batch)\n",
        "        # fake examples\n",
        "        x_fake, y_fake = generate_fake_samples(generator, latent_dim, half_batch)\n",
        "        # train the discriminator\n",
        "        discriminator.train_on_batch(x_real, y_real)\n",
        "        discriminator.train_on_batch(x_fake, y_fake)\n",
        "        # sample the latent space: THIS is the input to the joint model!\n",
        "        x_gan = sample_latent_points(latent_dim, n_batch)\n",
        "        # QUESTION: Why are we switching the labels here??\n",
        "        y_gan = np.ones((n_batch, 1))\n",
        "        # train the generator using the discriminator's feedback\n",
        "        gan.train_on_batch(x_gan, y_gan)\n",
        "        if (i+1) % eval_interval == 0:\n",
        "            dar, daf, ee, gen_x, gen_y = evaluate(discriminator, generator, latent_dim, f, mse)\n",
        "            gen_xy_for_evaluation.append((gen_x, gen_y))\n",
        "            print(\"epoch %s: discr acc real: %s, discr acc fake: %s, eval error: %s\" % (i, dar, daf, ee))\n",
        "    return gen_xy_for_evaluation"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "HDhndGq7eCBn",
        "scrolled": true,
        "colab": {}
      },
      "source": [
        "latent_dim = 10\n",
        "discriminator = build_discriminator(nh_1=30)\n",
        "generator = build_generator(latent_dim, nh_1=50)\n",
        "gan_model = build_gan(generator, discriminator)\n",
        "gen_xy_data = train(generator, discriminator, gan_model, latent_dim, n_epochs=20000, eval_interval=5000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "e5xAX5LIeCBp",
        "colab": {}
      },
      "source": [
        "x = np.linspace(-5, 5, 100)\n",
        "y = f(x)\n",
        "plt.plot(x, y)\n",
        "plt.plot(*gen_xy_data[0], \"ro\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6f09m_2FeCBr",
        "colab": {}
      },
      "source": [
        "x = np.linspace(-5, 5, 100)\n",
        "y = f(x)\n",
        "plt.plot(x, y)\n",
        "plt.plot(*gen_xy_data[-2], \"ro\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "BKygLeWLeCBu",
        "colab": {}
      },
      "source": [
        "x = np.linspace(-5, 5, 100)\n",
        "y = f(x)\n",
        "plt.plot(x, y)\n",
        "plt.plot(*gen_xy_data[-1], \"ro\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16ZtCrFqBdUm",
        "colab_type": "text"
      },
      "source": [
        "What do we observe? Is the cubic nonlinearity being perfectly captured? What if we gave our networks some more depth?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-V3QjnlmBqyj",
        "colab_type": "text"
      },
      "source": [
        "### Adding some layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r3t-IcSBBuiZ",
        "colab_type": "text"
      },
      "source": [
        "Let's see if by adding some layers to $G$ and $D$ we're able to fit that cubic a bit better..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "n1yQXkx4eCBw",
        "colab": {}
      },
      "source": [
        "def f(x):\n",
        "    return x**3\n",
        "\n",
        "def generate_real_samples(f, n=64):\n",
        "    x = (np.random.rand(n) - .5) * 10\n",
        "    y = f(x)\n",
        "    x = x.reshape(n, 1)\n",
        "    y = y.reshape(n, 1)\n",
        "    labels = np.ones((n, 1))\n",
        "    xy = np.hstack((x,y))\n",
        "    return (xy, labels)\n",
        "\n",
        "def build_discriminator(n_inputs=2, nh=[30], init=\"he_uniform\", act=\"relu\"):\n",
        "    model = Sequential()\n",
        "    indim = n_inputs\n",
        "    for nhid in nh:\n",
        "        model.add(Dense(nhid, activation=act, kernel_initializer=init, input_dim=indim))\n",
        "        indim=nhid\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "def build_generator(latent_dim, nh=[30], init=\"he_uniform\", act=\"relu\", n_outputs=2):\n",
        "    model = Sequential()\n",
        "    indim = latent_dim\n",
        "    for nhid in nh:\n",
        "        model.add(Dense(nhid, activation=act, kernel_initializer=init, input_dim=indim))\n",
        "        indim=nhid\n",
        "    model.add(Dense(n_outputs, activation='linear'))\n",
        "    return model\n",
        "\n",
        "#sampling in the latent space\n",
        "def sample_latent_points(latent_dim, n):\n",
        "    # generate points in the latent space\n",
        "    x_input = np.random.randn(latent_dim * n)\n",
        "    # reshape into a batch of inputs for the network\n",
        "    x_input = x_input.reshape(n, latent_dim)\n",
        "    return x_input\n",
        "\n",
        "def generate_fake_samples(generator, latent_dim, n):\n",
        "    # sample points in latent space\n",
        "    x_input = sample_latent_points(latent_dim, n)\n",
        "    # pass through the generator\n",
        "    X = generator.predict(x_input)\n",
        "    labels = np.zeros((n, 1))\n",
        "    return X, labels\n",
        "\n",
        "def build_gan(generator, discriminator):\n",
        "    discriminator.trainable = False\n",
        "    # stack the models together\n",
        "    model = Sequential()\n",
        "    # add generator\n",
        "    model.add(generator)\n",
        "    # add the discriminator\n",
        "    model.add(discriminator)\n",
        "    # We _do_ compile this model!\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam')\n",
        "    return model\n",
        "\n",
        "def evaluate(discriminator, generator, latent_dim, ground_truth_f, error_function, eval_size=100):\n",
        "    generated_xy, generated_labs = generate_fake_samples(generator, latent_dim, eval_size)\n",
        "    real_xy, real_labs = generate_real_samples(ground_truth_f, eval_size)\n",
        "    _, dar = discriminator.evaluate(real_xy, real_labs, verbose=0)\n",
        "    _, daf = discriminator.evaluate(generated_xy, generated_labs, verbose=0)\n",
        "    correct_y = ground_truth_f(generated_xy[:,0])\n",
        "    return dar, daf, error_function(correct_y, generated_xy[:,1]), generated_xy[:,0], generated_xy[:,1]\n",
        "\n",
        "# train the generator and discriminator\n",
        "def train(generator, discriminator, gan, latent_dim, n_epochs=10000, n_batch=128, eval_interval=200):\n",
        "    gen_xy_for_evaluation = []\n",
        "    # determine half the size of one batch, for updating the discriminator\n",
        "    half_batch = int(n_batch / 2)\n",
        "    # manually enumerate epochs\n",
        "    for i in range(n_epochs):\n",
        "        # real samples\n",
        "        x_real, y_real = generate_real_samples(f, half_batch)\n",
        "        # fake examples\n",
        "        x_fake, y_fake = generate_fake_samples(generator, latent_dim, half_batch)\n",
        "        # train the discriminator\n",
        "        discriminator.train_on_batch(x_real, y_real)\n",
        "        discriminator.train_on_batch(x_fake, y_fake)\n",
        "        # sample the latent space: THIS is the input to the joint model!\n",
        "        x_gan = sample_latent_points(latent_dim, n_batch)\n",
        "        # QUESTION: Why are we switching the labels here??\n",
        "        y_gan = np.ones((n_batch, 1))\n",
        "        # train the generator using the discriminator's feedback\n",
        "        gan.train_on_batch(x_gan, y_gan)\n",
        "        if (i+1) % eval_interval == 0:\n",
        "            dar, daf, ee, gen_x, gen_y = evaluate(discriminator, generator, latent_dim, f, mse)\n",
        "            gen_xy_for_evaluation.append((gen_x, gen_y))\n",
        "            print(\"epoch %s: discr acc real: %s, discr acc fake: %s, eval error: %s\" % (i, dar, daf, ee))\n",
        "    return gen_xy_for_evaluation"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7z_XBKHpeCBy",
        "colab": {}
      },
      "source": [
        "latent_dim = 5\n",
        "discriminator = build_discriminator(nh=[10, 20, 30])\n",
        "generator = build_generator(latent_dim, nh=[10, 20, 30])\n",
        "gan_model = build_gan(generator, discriminator)\n",
        "gen_xy_data = train(generator, discriminator, gan_model, latent_dim, n_epochs=20000, eval_interval=5000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kE6sNUWheCBz",
        "colab": {}
      },
      "source": [
        "x = np.linspace(-5, 5, 100)\n",
        "y = f(x)\n",
        "plt.plot(x, y)\n",
        "plt.plot(*gen_xy_data[3], \"ro\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "UaKt5UlneCB1"
      },
      "source": [
        "## Learning multimodal distributions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "AzeiT-2KeCB2"
      },
      "source": [
        "Let's consider an extension of the model we've just seen. What would happen if we tried to learn _several_ possible functions, rather than just a single cubic? \n",
        "\n",
        "For example, we may want to learn a cubic and two quadrics, a parabola and a circle."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "CWTgd4YCeCB2",
        "colab": {}
      },
      "source": [
        "#The circle is not a function, so we'll need to change our (python) functions accordingly...\n",
        "\n",
        "def fcub(x):\n",
        "    return x**3\n",
        "\n",
        "def fpar(x):\n",
        "    return x**2\n",
        "\n",
        "def fcirc(x):\n",
        "    #let's fix the radius at 1, it's easier\n",
        "    r=1\n",
        "    #Split the output to have half the points on the upper circle, half on the lower\n",
        "    y_p = (r-x[:len(x)//2]**2)**.5\n",
        "    y_n = -(r-x[(len(x)//2):]**2)**.5\n",
        "    return  np.concatenate((y_p, y_n))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MAeXNYS3BCs4",
        "colab_type": "text"
      },
      "source": [
        "### Discriminator, again"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "2H_9WWWweCB5"
      },
      "source": [
        "Now let's adapt our discriminator function. In fact, it shouldn't look much different..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ZcF1yJnteCB5",
        "colab": {}
      },
      "source": [
        "def build_discriminator(n_inputs=100, nh=[200], init=\"he_uniform\", act=\"relu\"): \n",
        "    model = Sequential()\n",
        "    indim = n_inputs\n",
        "    for nhid in nh:\n",
        "        model.add(Dense(nhid, activation=act, kernel_initializer=init, input_dim=indim))\n",
        "        indim=nhid\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hm1Hxs6gCMIJ",
        "colab_type": "text"
      },
      "source": [
        "### Generator, too"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "kfhg1mSxeCB8"
      },
      "source": [
        "Now, this is where the situation becomes slightly more complex. Since we're generating various functions, if $G$ produced a _single_ fake point at a time, like before, it'd be difficult to establish how realistic that point is. Think about it: it'd be like evaluating an image generator from a single pixel.\n",
        "\n",
        "Instead, we'll ask $G$ to output an array of points."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "asC-tJ8-eCB9",
        "colab": {}
      },
      "source": [
        "def build_generator(latent_dim, nh=[200], init=\"he_uniform\", act=\"relu\", n_outputs=100):\n",
        "    model = Sequential()\n",
        "    indim = latent_dim\n",
        "    for nhid in nh:\n",
        "        model.add(Dense(nhid, activation=act, kernel_initializer=init, input_dim=indim))\n",
        "        indim=nhid\n",
        "    model.add(Dense(n_outputs, activation='linear'))\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "y0wmXC7feCB_"
      },
      "source": [
        "The generation of real samples needs a couple of changes, too. Given that we decided to restrict to radius 1 for the circle, we'll only generate points within [-1, 1] _for all functions_. \n",
        "\n",
        "Furthermore, we don't need to reshape the generated points as they're already in matrix form."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "30pCoNiBeCCA",
        "colab": {}
      },
      "source": [
        "def generate_real_samples(f, n=64, points_per_sample=50):\n",
        "    x = (np.random.rand(n, points_per_sample) - .5)*2\n",
        "    y = np.apply_along_axis(f, 1, x) #axis concept is actually superfluous here...\n",
        "    labels = np.ones((n, 1))\n",
        "    xy = np.hstack((x,y))\n",
        "    return (xy, labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "olZKzxvqEa-c",
        "colab_type": "text"
      },
      "source": [
        "### Evaluation for multimodal ground truth"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "AImTKPiReCCB"
      },
      "source": [
        "The only thing we need to do here is to remove the `ground_truth_f` from the subjective evaluation (output plot) because sampled points used for evaluation may belong to any of the modes of the distribution...hopefully."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "drAyRhhheCCC",
        "colab": {}
      },
      "source": [
        "def evaluate(discriminator, generator, ground_truth_f, latent_dim, points_per_sample, eval_size=100):\n",
        "    generated_xy, generated_labs = generate_fake_samples(generator, latent_dim, eval_size)\n",
        "    real_xy, real_labs = generate_real_samples(ground_truth_f, eval_size, points_per_sample)\n",
        "    _, dar = discriminator.evaluate(real_xy, real_labs, verbose=0)\n",
        "    _, daf = discriminator.evaluate(generated_xy, generated_labs, verbose=0)\n",
        "    return dar, daf, generated_xy[:,:points_per_sample], generated_xy[:,points_per_sample:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U-lyA2hmGRSF",
        "colab_type": "text"
      },
      "source": [
        "### Training for multimodal ground truth"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "e-qZm4I1eCCD"
      },
      "source": [
        "In the training function, we need to make sure all modes are represented in the training samples. We'll add a utility function to help with this part."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_5-NOieHeCCF",
        "colab": {}
      },
      "source": [
        "def generate_for_modes(modes, batch, points_per_sample):\n",
        "    split_batch = [batch//len(modes) for i in range(len(modes)-1)]\n",
        "    split_batch.append(batch- (batch//len(modes)) * (len(modes)-1)) #what's left, may or may not be the %\n",
        "    samples_split = [generate_real_samples(modes[i], sb, points_per_sample) for i,sb in enumerate(split_batch)]\n",
        "    zipped_samples = list(zip(*samples_split))\n",
        "    return np.concatenate(zipped_samples[0]), np.concatenate(zipped_samples[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "KM8J5h2FeCCJ",
        "colab": {}
      },
      "source": [
        "def train(generator, discriminator, gan, latent_dim, \n",
        "          n_epochs=5000, n_batch=128, points_per_sample=20, \n",
        "          eval_interval=200, eval_size=20, modes=[fpar, fcub, fcirc]):\n",
        "    \n",
        "    gen_xy_for_evaluation = []\n",
        "    \n",
        "    # determine half the size of one batch, for updating the discriminator\n",
        "    half_batch = int(n_batch / 2)\n",
        "    # manually enumerate epochs\n",
        "    for i in range(n_epochs):\n",
        "        # real samples\n",
        "        x_real, y_real = generate_for_modes(modes, half_batch, points_per_sample)\n",
        "        # fake examples\n",
        "        x_fake, y_fake = generate_fake_samples(generator, latent_dim, half_batch)\n",
        "        # train the discriminator\n",
        "        discriminator.train_on_batch(x_real, y_real)\n",
        "        discriminator.train_on_batch(x_fake, y_fake)\n",
        "        # sample the latent space\n",
        "        x_gan = sample_latent_points(latent_dim, n_batch)\n",
        "        y_gan = np.ones((n_batch, 1))\n",
        "        # train the generator using the discriminator's feedback\n",
        "        gan.train_on_batch(x_gan, y_gan)\n",
        "        if (i+1) % eval_interval == 0:\n",
        "            dar, daf, gen_x, gen_y = evaluate(discriminator, generator, f, latent_dim, \n",
        "                                              points_per_sample=20, eval_size=20)\n",
        "            gen_xy_for_evaluation.append((gen_x, gen_y))\n",
        "            print(\"epoch %s: discr acc real: %s, discr acc fake: %s\" % (i, dar, daf))\n",
        "    return gen_xy_for_evaluation"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kpeF45KzGeMe",
        "colab_type": "text"
      },
      "source": [
        "### Putting it all together"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "84YD6gIzeCCN"
      },
      "source": [
        "Let's put it all together, and in a class, for easier management..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xPdtdVWmeCCO",
        "colab": {}
      },
      "source": [
        "#The circle is not a function, so we'll need to change our (python) functions accordingly...\n",
        "\n",
        "def fcub(x):\n",
        "    return x**3\n",
        "\n",
        "def fpar(x):\n",
        "    return x**2\n",
        "\n",
        "def fcirc(x):\n",
        "    #let's fix the radius at 1, it's easier\n",
        "    r=1\n",
        "    #Split the output to have half the points on the upper circle, half on the lower\n",
        "    y_p = (r-x[:len(x)//2]**2)**.5\n",
        "    y_n = -(r-x[(len(x)//2):]**2)**.5\n",
        "    return  np.concatenate((y_p, y_n))\n",
        "\n",
        "def f5(x):\n",
        "    return x**5+x**3-x**2\n",
        "\n",
        "#I know, it's a pretty shitty class, but it's only to reduce shadowing\n",
        "class MultimodalGanExperiment:\n",
        "    \n",
        "    def __init__(self, points_per_sample=50, latent_dim = 10, eval_size=20, batch_size=128):\n",
        "        self.points_per_sample = points_per_sample\n",
        "        self.latent_dim = latent_dim\n",
        "        self.eval_size = eval_size\n",
        "        self.n_batch = batch_size\n",
        "        \n",
        "    def build_discriminator(self, nh=[200], init=\"he_uniform\", act=\"relu\"): \n",
        "        n_inputs= self.points_per_sample * 2 # *2 b/c we're accounting for x and y!\n",
        "        model = Sequential()\n",
        "        indim = n_inputs\n",
        "        for nhid in nh:\n",
        "            model.add(Dense(nhid, activation=act, kernel_initializer=init, input_dim=indim))\n",
        "            indim=nhid\n",
        "        model.add(Dense(1, activation='sigmoid'))\n",
        "        model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "        return model\n",
        "\n",
        "    def build_generator(self, nh=[200], init=\"he_uniform\", act=\"relu\"):\n",
        "        n_outputs = self.points_per_sample * 2\n",
        "        model = Sequential()\n",
        "        indim = self.latent_dim\n",
        "        for nhid in nh:\n",
        "            model.add(Dense(nhid, activation=act, kernel_initializer=init, input_dim=indim))\n",
        "            indim=nhid\n",
        "        model.add(Dense(n_outputs, activation='linear'))\n",
        "        return model\n",
        "\n",
        "    def generate_real_samples(self, f, n):\n",
        "        x = (np.random.rand(n, self.points_per_sample) - .5)*2\n",
        "        y = np.apply_along_axis(f, 1, x) #axis concept is actually superfluous here...\n",
        "        labels = np.ones((n, 1))\n",
        "        xy = np.hstack((x,y))\n",
        "        return (xy, labels)\n",
        "\n",
        "    #sampling in the latent space\n",
        "    def sample_latent_points(self, n):\n",
        "        # generate points in the latent space\n",
        "        x_input = np.random.randn(self.latent_dim * n)\n",
        "        # reshape into a batch of inputs for the network\n",
        "        x_input = x_input.reshape(n, self.latent_dim)\n",
        "        return x_input\n",
        "\n",
        "    def generate_fake_samples(self, generator, n):\n",
        "        # sample points in latent space\n",
        "        x_input = sample_latent_points(self.latent_dim, n)\n",
        "        # pass through the generator\n",
        "        X = generator.predict(x_input)\n",
        "        labels = np.zeros((n, 1))\n",
        "        return X, labels\n",
        "\n",
        "    def build_gan(self, generator, discriminator):\n",
        "        discriminator.trainable = False\n",
        "        # stack the models together\n",
        "        model = Sequential()\n",
        "        # add generator\n",
        "        model.add(generator)\n",
        "        # add the discriminator\n",
        "        model.add(discriminator)\n",
        "        # We _do_ compile this model!\n",
        "        model.compile(loss='binary_crossentropy', optimizer='adam')\n",
        "        return model\n",
        "\n",
        "    def evaluate(self, discriminator, generator, modes):\n",
        "        generated_xy, generated_labs = self.generate_fake_samples(generator, self.eval_size)\n",
        "        real_xy, real_labs = self.generate_for_modes(modes, self.eval_size)\n",
        "        _, dar = discriminator.evaluate(real_xy, real_labs, verbose=0)\n",
        "        _, daf = discriminator.evaluate(generated_xy, generated_labs, verbose=0)\n",
        "        return dar, daf, generated_xy[:,:self.points_per_sample], generated_xy[:,self.points_per_sample:]\n",
        "\n",
        "    def generate_for_modes(self, modes, batch):\n",
        "        split_batch = [batch//len(modes) for i in range(len(modes)-1)]\n",
        "        split_batch.append(batch- (batch//len(modes)) * (len(modes)-1)) #what's left, may or may not be the %\n",
        "        samples_split = [self.generate_real_samples(modes[i], sb) for i,sb in enumerate(split_batch)]\n",
        "        zipped_samples = list(zip(*samples_split))\n",
        "        return np.concatenate(zipped_samples[0]), np.concatenate(zipped_samples[1])\n",
        "#         for i,sb in enumerate(split_batch):\n",
        "#             yield self.generate_real_samples(modes[i], sb)\n",
        "\n",
        "    def train(self, generator, discriminator, gan, modes,\n",
        "              n_epochs=5000, n_batch=128, \n",
        "              eval_interval=200, eval_size=20):\n",
        "\n",
        "        gen_xy_for_evaluation = []\n",
        "\n",
        "        # determine half the size of one batch, for updating the discriminator\n",
        "        half_batch = int(self.n_batch / 2)\n",
        "        # manually enumerate epochs\n",
        "        for i in range(n_epochs):\n",
        "            # real samples\n",
        "            x_real, y_real = self.generate_for_modes(modes, half_batch)\n",
        "            # fake examples\n",
        "            x_fake, y_fake = self.generate_fake_samples(generator, half_batch)\n",
        "            # train the discriminator\n",
        "            discriminator.train_on_batch(x_real, y_real)\n",
        "            discriminator.train_on_batch(x_fake, y_fake)\n",
        "            # sample the latent space\n",
        "            x_gan = self.sample_latent_points(self.n_batch)\n",
        "            y_gan = np.ones((self.n_batch, 1))\n",
        "            # train the generator using the discriminator's feedback\n",
        "            gan.train_on_batch(x_gan, y_gan)\n",
        "            if (i+1) % eval_interval == 0:\n",
        "                dar, daf, gen_x, gen_y = self.evaluate(discriminator, generator, modes)\n",
        "                gen_xy_for_evaluation.append((gen_x, gen_y))\n",
        "                print(\"epoch %s: discr acc real: %s, discr acc fake: %s\" % (i, dar, daf))\n",
        "        return gen_xy_for_evaluation"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JGOgReI1eCCP",
        "colab": {}
      },
      "source": [
        "mge = MultimodalGanExperiment(points_per_sample=50, latent_dim = 10, eval_size=36, batch_size=3)\n",
        "modes = [fcirc, fcub, fpar]\n",
        "# fig, axs = plt.subplots(3, 6)\n",
        "x, _= mge.generate_for_modes(modes, 6)\n",
        "# for i,x in enumerate(xg):\n",
        "#     for j in range(6):\n",
        "#         axs[i,j].plot(x[0][j,:10], x[0][j,10:], \"ro\")\n",
        "fig, axs=plt.subplots(3, 2) #10, 6 even if we've got more\n",
        "fig.set_size_inches(6, 9)\n",
        "axsf = axs.flatten()\n",
        "for i,xc in enumerate(x):\n",
        "        axsf[i].plot(xc[:50], xc[50:], \"ro\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "AVD7k_IUeCCR",
        "colab": {}
      },
      "source": [
        "#ld=40, pps=20\n",
        "mge = MultimodalGanExperiment(points_per_sample=200, latent_dim = 15, eval_size=50, batch_size=128)\n",
        "\n",
        "#modes = [fcirc, fcub, fpar]\n",
        "modes = [f5, fcub, fpar]\n",
        "\n",
        "discriminator = mge.build_discriminator(nh=[80, 40, 20, 10]) #100 100 100\n",
        "generator = mge.build_generator(nh=[20, 40, 60]) #70 70 70 70\n",
        "gan_model = mge.build_gan(generator, discriminator)\n",
        "\n",
        "gen_xy_data = mge.train(generator, discriminator, gan_model, modes, n_epochs=20000, eval_interval=2000) #50000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9mfWG0htFJTZ",
        "colab_type": "text"
      },
      "source": [
        "Let's have a look at generated samples..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AU3d45duFRYI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from itertools import product\n",
        "fig, axs=plt.subplots(len(gen_xy_data), 8) \n",
        "fig.set_size_inches(15, 18)\n",
        "for i,j in product(range(len(gen_xy_data)), range(8)):\n",
        "    axs[i,j].plot(gen_xy_data[i][0][j], gen_xy_data[i][1][j], \"ro\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BmKkqSBsFUmi",
        "colab_type": "text"
      },
      "source": [
        "### Adding some steroids\n",
        "\n",
        "Our GAN, as it is, doesn't seem to be particularly effective with the learning of a multimodal distribution. Why? What makes this case so different from the unimodal, 1D function example?\n",
        "\n",
        "It's hard to give a precise answer to this question, particularly with the rather weak understanding of GANs' dynamics that most of us currently have. One reasonable guess is that the functional relationships of $(x,y)$ pairs in the multimodal case are more diverse and complex, and as such, we may need to increase the network's complexity accordingly.\n",
        "\n",
        "Indeed, we may think that learning the shape of a single 1D function is really a 1D problem, while learning the shapes of various 1D functions is closer to a 2D problem...\n",
        "\n",
        "With that in mind, what could we add to our network to make it more sensitive to the relationship among 2D points?\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Q5_tUay2Djb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.layers import Conv2D, LeakyReLU, Dropout, Flatten, Reshape, InputLayer, Conv2DTranspose, Permute\n",
        "from keras.backend import expand_dims"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BMSQb-JPNTjw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MultimodalGanExperiment:\n",
        "    \n",
        "    def __init__(self, points_per_sample=50, latent_dim = 10, eval_size=20, batch_size=128):\n",
        "        self.points_per_sample = points_per_sample\n",
        "        self.latent_dim = latent_dim\n",
        "        self.eval_size = eval_size\n",
        "        self.n_batch = batch_size\n",
        "        \n",
        "    def build_discriminator(self, init=\"he_uniform\", act=\"relu\"): \n",
        "        nh=4\n",
        "        nchan=16\n",
        "        model = Sequential()\n",
        "        in_points = InputLayer(input_shape=(2, self.points_per_sample, 1), \n",
        "                               name=\"discr_input\")\n",
        "        model.add(in_points)\n",
        "        # YOUR CODE HERE :)\n",
        "        model.add(Dense(1, activation='sigmoid'))\n",
        "        model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "        return model\n",
        "\n",
        "    def build_generator(self, init=\"he_uniform\", act=\"relu\"):\n",
        "        nh=4\n",
        "        nchan=16\n",
        "        model = Sequential()\n",
        "        # YOUR CODE HERE :)\n",
        "        return model\n",
        "\n",
        "    def generate_real_samples(self, f, n):\n",
        "        # YOUR CODE HERE\n",
        "        return (xy, labels)\n",
        "\n",
        "    #sampling in the latent space\n",
        "    def sample_latent_points(self, n):\n",
        "        # generate points in the latent space\n",
        "        x_input = np.random.randn(self.latent_dim * n)\n",
        "        # reshape into a batch of inputs for the network\n",
        "        x_input = x_input.reshape(n, self.latent_dim)\n",
        "        return x_input\n",
        "\n",
        "    def generate_fake_samples(self, generator, n):\n",
        "        # sample points in latent space\n",
        "        x_input = self.sample_latent_points(n)\n",
        "        # pass through the generator\n",
        "        X = generator.predict(x_input)\n",
        "        labels = np.zeros((n, 1))\n",
        "        return X, labels\n",
        "\n",
        "    def build_gan(self, generator, discriminator):\n",
        "        discriminator.trainable = False\n",
        "        # stack the models together\n",
        "        model = Sequential()\n",
        "        # add generator\n",
        "        model.add(generator)\n",
        "        # add the discriminator\n",
        "        model.add(discriminator)\n",
        "        # We _do_ compile this model!\n",
        "        model.compile(loss='binary_crossentropy', optimizer='adam')\n",
        "        return model\n",
        "\n",
        "    def evaluate(self, discriminator, generator, modes):\n",
        "        # YOUR CODE HERE\n",
        "        _, dar = discriminator.evaluate(real_xy, real_labs, verbose=0, steps=1)\n",
        "        _, daf = discriminator.evaluate(generated_xy, generated_labs, verbose=0, steps=1)\n",
        "        return dar, daf, generated_xy[:,0], generated_xy[:,1]\n",
        "\n",
        "    def generate_for_modes(self, modes, batch):\n",
        "        # YOUR CODE HERE\n",
        "        return np.concatenate(zipped_samples[0], axis=0), np.concatenate(zipped_samples[1])\n",
        "\n",
        "    def train(self, generator, discriminator, gan, modes,\n",
        "              n_epochs=5000, n_batch=128, \n",
        "              eval_interval=200, eval_size=20):\n",
        "\n",
        "        gen_xy_for_evaluation = []\n",
        "\n",
        "        # determine half the size of one batch, for updating the discriminator\n",
        "        half_batch = int(self.n_batch / 2)\n",
        "        # manually enumerate epochs\n",
        "        for i in range(n_epochs):\n",
        "            # real samples\n",
        "            # YOUR CODE HERE\n",
        "\n",
        "            # fake examples\n",
        "            x_fake, y_fake = self.generate_fake_samples(generator, half_batch)\n",
        "            \n",
        "            # train the discriminator\n",
        "            discriminator.train_on_batch(x_real, y_real)\n",
        "            discriminator.train_on_batch(x_fake, y_fake)\n",
        "            # sample the latent space\n",
        "            x_gan = self.sample_latent_points(self.n_batch)\n",
        "            y_gan = np.ones((self.n_batch, 1))\n",
        "            # train the generator using the discriminator's feedback\n",
        "            gan.train_on_batch(x_gan, y_gan)\n",
        "            if (i+1) % eval_interval == 0:\n",
        "                dar, daf, gen_x, gen_y = self.evaluate(discriminator, generator, modes)\n",
        "                gen_xy_for_evaluation.append((gen_x, gen_y))\n",
        "                print(\"epoch %s: discr acc real: %s, discr acc fake: %s\" % (i, dar, daf))\n",
        "        return gen_xy_for_evaluation"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TvjEKMRc0nyY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MultimodalGanExperiment:\n",
        "    \n",
        "    def __init__(self, points_per_sample=50, latent_dim = 10, eval_size=20, batch_size=128):\n",
        "        self.points_per_sample = points_per_sample\n",
        "        self.latent_dim = latent_dim\n",
        "        self.eval_size = eval_size\n",
        "        self.n_batch = batch_size\n",
        "        \n",
        "    def build_discriminator(self, init=\"he_uniform\", act=\"relu\"): \n",
        "        nh=4\n",
        "        nchan=16\n",
        "        model = Sequential()\n",
        "        in_points = InputLayer(input_shape=(2, self.points_per_sample, 1), name=\"discr_input\")\n",
        "        model.add(in_points)\n",
        "        for lyr in range(nh):\n",
        "            model.add(Conv2D(nchan, (20, 2), strides=2, padding=\"same\"))\n",
        "            model.add(Dropout(.4))\n",
        "            model.add(LeakyReLU(alpha=0.2))\n",
        "        model.add(Flatten())\n",
        "        model.add(Dropout(.4))\n",
        "        model.add(Dense(1, activation='sigmoid'))\n",
        "        model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "        return model\n",
        "\n",
        "    def build_generator(self, init=\"he_uniform\", act=\"relu\"):\n",
        "        nh=4\n",
        "        nchan=16\n",
        "        model = Sequential()\n",
        "        #model.add(Input(shape=(self.latent_dim,)))\n",
        "        starting_conv_dim = max(self.points_per_sample//nh, 1)\n",
        "        n_nodes = nchan * starting_conv_dim * 2\n",
        "        model.add(Dense(n_nodes, input_shape=(self.latent_dim,)))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        model.add(Reshape((starting_conv_dim, 2, nchan)))\n",
        "        for lyr in range(2): #it's all shitty and hardcoded\n",
        "            model.add(Conv2DTranspose(nchan, (20,2), strides=(2,1), padding='same'))\n",
        "            model.add(Dropout(.4))\n",
        "            model.add(LeakyReLU(alpha=0.2))\n",
        "        model.add(Conv2D(1, (20,2), activation='tanh', padding='same'))\n",
        "        model.add(Permute((2, 1, 3)))\n",
        "        # model.add(Flatten())\n",
        "#        model.add(Reshape((2, self.points_per_sample)))\n",
        "        # print(model.output)\n",
        "        return model\n",
        "\n",
        "    def generate_real_samples(self, f, n):\n",
        "        x = ((np.random.rand(n, self.points_per_sample) - .5)*2)\n",
        "        y = np.apply_along_axis(f, 1, x) #axis concept is actually superfluous here...\n",
        "        labels = np.ones((n, 1))\n",
        "        xy = np.stack((x.reshape(n, self.points_per_sample),\n",
        "                       y.reshape(n, self.points_per_sample)), 1)\n",
        "        return (xy, labels)\n",
        "\n",
        "    #sampling in the latent space\n",
        "    def sample_latent_points(self, n):\n",
        "        # generate points in the latent space\n",
        "        x_input = np.random.randn(self.latent_dim * n)\n",
        "        # reshape into a batch of inputs for the network\n",
        "        x_input = x_input.reshape(n, self.latent_dim)\n",
        "        return x_input\n",
        "\n",
        "    def generate_fake_samples(self, generator, n):\n",
        "        # sample points in latent space\n",
        "        x_input = self.sample_latent_points(n)\n",
        "        # pass through the generator\n",
        "        X = generator.predict(x_input)\n",
        "        labels = np.zeros((n, 1))\n",
        "        return X, labels\n",
        "\n",
        "    def build_gan(self, generator, discriminator):\n",
        "        discriminator.trainable = False\n",
        "        # stack the models together\n",
        "        model = Sequential()\n",
        "        # add generator\n",
        "        model.add(generator)\n",
        "        # add the discriminator\n",
        "        model.add(discriminator)\n",
        "        # We _do_ compile this model!\n",
        "        model.compile(loss='binary_crossentropy', optimizer='adam')\n",
        "        return model\n",
        "\n",
        "    def evaluate(self, discriminator, generator, modes):\n",
        "        generated_xy, generated_labs = self.generate_fake_samples(generator, self.eval_size)\n",
        "        real_xy, real_labs = self.generate_for_modes(modes, self.eval_size)\n",
        "        real_xy = expand_dims(real_xy)\n",
        "        _, dar = discriminator.evaluate(real_xy, real_labs, verbose=0, steps=1)\n",
        "        _, daf = discriminator.evaluate(generated_xy, generated_labs, verbose=0, steps=1)\n",
        "        return dar, daf, generated_xy[:,0], generated_xy[:,1]\n",
        "\n",
        "    def generate_for_modes(self, modes, batch):\n",
        "        split_batch = [batch//len(modes) for i in range(len(modes)-1)]\n",
        "        split_batch.append(batch- (batch//len(modes)) * (len(modes)-1)) #what's left, may or may not be the %\n",
        "        samples_split = [self.generate_real_samples(modes[i], sb) for i,sb in \n",
        "                         enumerate(split_batch)]\n",
        "        zipped_samples = list(zip(*samples_split))\n",
        "        # print(zipped_samples[0][0].shape)\n",
        "        # print(zipped_samples[0][1].shape)\n",
        "        return np.concatenate(zipped_samples[0], axis=0), np.concatenate(zipped_samples[1])\n",
        "#         for i,sb in enumerate(split_batch):\n",
        "#             yield self.generate_real_samples(modes[i], sb)\n",
        "\n",
        "    def train(self, generator, discriminator, gan, modes,\n",
        "              n_epochs=5000, n_batch=128, \n",
        "              eval_interval=200, eval_size=20):\n",
        "\n",
        "        gen_xy_for_evaluation = []\n",
        "\n",
        "        # determine half the size of one batch, for updating the discriminator\n",
        "        half_batch = int(self.n_batch / 2)\n",
        "        # manually enumerate epochs\n",
        "        for i in range(n_epochs):\n",
        "            # real samples\n",
        "            x_real, y_real = self.generate_for_modes(modes, half_batch)\n",
        "            x_real = expand_dims(x_real)\n",
        "            # print(\"xreal: \" + str(x_real.shape))\n",
        "            # fake examples\n",
        "            x_fake, y_fake = self.generate_fake_samples(generator, half_batch)\n",
        "            # print(\"xfake: \" + str(x_fake.shape))\n",
        "            # print(y_fake.shape)\n",
        "            # train the discriminator\n",
        "            discriminator.train_on_batch(x_real, y_real)\n",
        "            discriminator.train_on_batch(x_fake, y_fake)\n",
        "            # sample the latent space\n",
        "            x_gan = self.sample_latent_points(self.n_batch)\n",
        "            y_gan = np.ones((self.n_batch, 1))\n",
        "            # train the generator using the discriminator's feedback\n",
        "            gan.train_on_batch(x_gan, y_gan)\n",
        "            if (i+1) % eval_interval == 0:\n",
        "                dar, daf, gen_x, gen_y = self.evaluate(discriminator, generator, modes)\n",
        "                gen_xy_for_evaluation.append((gen_x, gen_y))\n",
        "                print(\"epoch %s: discr acc real: %s, discr acc fake: %s\" % (i, dar, daf))\n",
        "        return gen_xy_for_evaluation"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "B8jlYgDrGmEd",
        "colab": {}
      },
      "source": [
        "#ld=40, pps=20\n",
        "mge = MultimodalGanExperiment(points_per_sample=200, latent_dim = 30, \n",
        "                              eval_size=50, batch_size=128)\n",
        "\n",
        "#modes = [fcirc, fcub, fpar]\n",
        "modes = [f5, fcub, fpar]\n",
        "\n",
        "discriminator = mge.build_discriminator() #100 100 100\n",
        "generator = mge.build_generator() #70 70 70 70\n",
        "gan_model = mge.build_gan(generator, discriminator)\n",
        "\n",
        "gen_xy_data = mge.train(generator, discriminator, gan_model, modes, n_epochs=500, eval_interval=50) #50000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "td_m3z0ZLEMW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig, axs=plt.subplots(len(gen_xy_data), 8) \n",
        "fig.set_size_inches(15, 18)\n",
        "for i,j in product(range(len(gen_xy_data)), range(8)):\n",
        "    axs[i,j].plot(gen_xy_data[i][0][-j,:,0], gen_xy_data[i][1][-j,:,0], \"ro\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "mWOCnt7KqShW"
      },
      "source": [
        "It's clear that the network, without further optimisation or heuristic, needs longer times to converge to something plausible and, quite possibly, more architectural complexity to capture the 3 modes of our data. Even so, we can start seeing the first signs of _mode collapse_, since most generated graphs show a diagonally looking upward trend crossing (0,0), very reminiscent of our cubic, but much less so of a parabola or circle."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "pwKDzxftqYR_"
      },
      "source": [
        "#### A note on mode collapse"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SHoWlu4AKTtA",
        "colab_type": "text"
      },
      "source": [
        "Mode collapse is one of the fundamental problems affecting GANs. As the name suggests, it's the behaviour associated with the generator only focussing on one, or some, of the possible modes.\n",
        "\n",
        "It is an issue that needs to be specifically accounted and corrected for, insofar a simple loss function doesn't normally penalise it: if $G$ learnt to generate perfect parabolas and never even attempted to generate circles, the discriminator loss on those samples would still be low.\n",
        "\n",
        "This said, _mode collapse is not necessarily an evil_. It may be acceptable in scenarios where even generating a small subset of all the possible data modes is a viable solution.\n",
        "\n",
        "In [2], the authors suggest the use of _minibatch features_ to correct mode collapse. Intuitively, it consists in allowing the discriminator to compare a generated sample to a minibatch of fake and one of real data, in order to determine whether the sample is _too much_ alike other generated points."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "T6TzWjDLqduG"
      },
      "source": [
        "## Semi-supervised Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "jB5BDgZcqjdF"
      },
      "source": [
        "One of the most exciting, _real_ applications of GANs is pushing the envelope of semi-supervised learning. SSL is quite possibly the most common and realistic scenario faced by ML practitioners, where only a small fraction of the available data has been labeled: the challenge is to make the most of it. \n",
        "\n",
        "The idea behind GANs for SSL is simple: learn the distribution of the labelled data in the hope of being able to generate realistic enough samples to make up for the shortage labelled points.\n",
        "\n",
        "The implementation of it is, of course, less straightforward and rife with subtleties, such as\n",
        "\n",
        "* What happens if there are classes for which I have no labelled samples?\n",
        "* How do I distinguish between generated and unlabelled?\n",
        "* What happens with mode collapse?\n",
        "* To what extent does the quality of the generated samples impact the classifier I want to train?\n",
        "\n",
        "Let's get to work and we'll address some of these points (among other things).\n",
        "\n",
        "__How__:\n",
        "* We'll work on MNIST data\n",
        "* We'll discuss how to turn a discriminator into a classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "uKtdSWibqoY7"
      },
      "source": [
        "### The structure of our SGAN\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "NdPMNBEdzuSI"
      },
      "source": [
        "Let's start by giving the general skeleton of how our SGAN class will look like. It's quite similar to what we've coded so far, hence we can save up some work."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gP6gyBdHqehF",
        "colab": {}
      },
      "source": [
        "class SGAN:\n",
        "    \n",
        "    def __init__(self, latent_dim):\n",
        "        pass\n",
        "\n",
        "    def build_discriminator(self, in_shape=(28,28,1)):\n",
        "        pass\n",
        "\n",
        "    # define the standalone generator model\n",
        "    def build_generator(self):\n",
        "        pass\n",
        "\n",
        "    # define the combined generator and discriminator model, for updating the generator\n",
        "    def build_gan(self):\n",
        "        self.discriminator.trainable = False\n",
        "        # connect image output from generator as input to discriminator\n",
        "        gan_output = self.discriminator(self.generator.output)\n",
        "        # define gan model as taking noise and outputting a classification\n",
        "        model = Model(self.generator.input, gan_output)\n",
        "        # compile model\n",
        "        opt = Adam(lr=0.0002, beta_1=0.5)\n",
        "        model.compile(loss='binary_crossentropy', optimizer=opt)\n",
        "        return model\n",
        "\n",
        "    @staticmethod\n",
        "    def prepare_real_data():\n",
        "        # load dataset\n",
        "        (X_train, y_train), (_, _) = mnist.load_data()\n",
        "        X = np.expand_dims(X_train, axis=-1).astype('float32')\n",
        "        # rescale from [-255, 255] to [-1,1]\n",
        "        X = (X - 127.5) / 127.5\n",
        "        return [X, y_train]\n",
        "\n",
        "    # select a balanced, supervised subdataset\n",
        "    def prepare_supervised_samples(self, n_samples=100):\n",
        "        X, y = self.dataset\n",
        "        X_list, y_list = list(), list()\n",
        "        n_per_class = int(n_samples / self.n_classes)\n",
        "        for i in range(self.n_classes):\n",
        "            # choose random instances of images of a given class\n",
        "            Xi_all = X[y == i]\n",
        "            xi = np.random.randint(0, len(Xi_all), n_per_class)\n",
        "            # add to list\n",
        "            for j in xi:\n",
        "                X_list.append(Xi_all[j])\n",
        "                y_list.append(i)\n",
        "        return np.asarray(X_list), np.asarray(y_list)\n",
        "\n",
        "    #generate real points for the discriminator from a sample of labelled data\n",
        "    @staticmethod\n",
        "    def generate_real_samples(labelled_data, n_samples):\n",
        "        X, y = labelled_data\n",
        "        xi = np.random.randint(0, X.shape[0], n_samples)\n",
        "        X_samp, y_samp = X[xi], y[xi]\n",
        "        # generate class labels for discriminator\n",
        "        ones = np.ones((n_samples, 1))\n",
        "        return [X_samp, y_samp], ones\n",
        "\n",
        "    # sample the latent space for the generator\n",
        "    def sample_latent_points(self, n_samples):\n",
        "        lat_pnts = np.random.randn(self.latent_dim * n_samples)\n",
        "        lat_pnts = lat_pnts.reshape(n_samples, self.latent_dim)\n",
        "        return lat_pnts\n",
        "\n",
        "    # generate fake samples for the discriminator\n",
        "    def generate_fake_samples(self, n_samples):\n",
        "        latent_points = self.sample_latent_points(n_samples)\n",
        "        generated_images = self.generator.predict(latent_points)\n",
        "        y = np.zeros((n_samples, 1))\n",
        "        return generated_images, y\n",
        "\n",
        "    # generate samples, save as a plot and save the model\n",
        "    def summarize_performance(self, step, n_samples=100):\n",
        "        X, _ = self.generate_fake_samples(n_samples)\n",
        "        # rescale for simplicity\n",
        "        X = (X + 1) / 2.0\n",
        "        # plot\n",
        "        for i in range(100):\n",
        "            pyplot.subplot(10, 10, 1 + i)\n",
        "            pyplot.axis('off')\n",
        "            pyplot.imshow(X[i, :, :, 0], cmap='gray_r')\n",
        "        fl = 'generated_images_%04d.png' % (step+1)\n",
        "        pyplot.savefig(fl)\n",
        "        pyplot.close()\n",
        "        # evaluate the classifier\n",
        "        # WARNING!! DON'T TRY THIS AT HOME!! (why? :)\n",
        "        X, y = self.dataset\n",
        "        _, acc = self.classifier.evaluate(X, y, verbose=0)\n",
        "        print('===> Classifier Accuracy: %.3f%%' % (acc * 100))\n",
        "        # save the generator\n",
        "        gen_file = 'generator_%04d.h5' % (step+1)\n",
        "        self.generator.save(gen_file)\n",
        "        # save the classifier\n",
        "        class_file = 'classifier_%04d.h5' % (step+1)\n",
        "        self.classifier.save(class_file)\n",
        "\n",
        "    # train the model!\n",
        "    def train(self, n_epochs=20, batch_size=100):\n",
        "        #Prepare the training data...\n",
        "        #Your code here\n",
        "        \n",
        "        # Now train\n",
        "        for i in range(training_steps):\n",
        "            #Your code here\n",
        "            \n",
        "            # summarize losses \n",
        "            if (i+1) % 500 == 0:\n",
        "                print('-> step=%d \\n'\\\n",
        "                    '   classifier -> loss = %.3f, accuracy = %.0f \\n'\\\n",
        "                    '   discriminator -> loss on real = %.3f, loss on fake = %.3f \\n'\\\n",
        "                    '   generator -> loss = %.3f' % \n",
        "                    (i+1, c_loss, c_acc*100, d_loss1, d_loss2, g_loss))\n",
        "            # evaluate model performance...and save!\n",
        "            if (i+1) % bat_per_epo == 0:\n",
        "                self.summarize_performance(i)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "wrpf8Edu1B7i"
      },
      "source": [
        "### The discriminator of a SGAN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "p0MrDKZi1DB_"
      },
      "source": [
        "Unsurprisingly, this is the most delicate aspect concerning the architecture of the network. Remember: our aim is to train a classifier with very little labelled data. The natural place for a classifier within a GAN is, of course, the discriminator. But how do we fit it in?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ujtA8NKC1GVw"
      },
      "source": [
        "#### First things first: feature extraction\n",
        "\n",
        "Let's turn those $28\\times28$ images into something we can work with. Any ideas? :)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c1C1S4ft_F2R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "advcsbuu_U4m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.layers import LeakyReLU"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c9u8aWJr_Zl_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "LeakyReLU()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4rrh-Q0s1FjG",
        "colab": {}
      },
      "source": [
        "def build_discriminator(self, in_shape=(28,28,1)):\n",
        "    # image input\n",
        "    in_image = Input(shape=in_shape)\n",
        "    # downsample\n",
        "    fe = Conv2D(128, (3,3), strides=(2,2), padding='same')(in_image)\n",
        "    fe = LeakyReLU(alpha=0.2)(fe)\n",
        "    # downsample\n",
        "    fe = Conv2D(128, (3,3), strides=(2,2), padding='same')(fe)\n",
        "    fe = LeakyReLU(alpha=0.2)(fe)\n",
        "    # downsample\n",
        "    fe = Conv2D(128, (3,3), strides=(2,2), padding='same')(fe)\n",
        "    fe = LeakyReLU(alpha=0.2)(fe)\n",
        "    # flatten feature maps\n",
        "    fe = Flatten()(fe)\n",
        "    # dropout\n",
        "    fe = Dropout(0.4)(fe)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_58b9HKW1POK"
      },
      "source": [
        "#### Making it an unsupervised discriminator\n",
        "\n",
        "Unsurprisingly: pass the vector through a dense layer with sigmoid activation, optimise against binary crossentropy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "MCzAEJr_1R28",
        "colab": {}
      },
      "source": [
        "d_out_layer = Dense(1, activation='sigmoid')(fe)\n",
        "# define and compile unsupervised discriminator model\n",
        "d_model = Model(in_image, d_out_layer)\n",
        "d_model.compile(loss='binary_crossentropy', optimizer=Adam(lr=0.0002, beta_1=0.5))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ZqIW2DRO1Uhb"
      },
      "source": [
        "#### Using the same features to classify\n",
        "\n",
        "Strategy: collapse the layer into 10 classes, pick one (which activation do you need?), optimise against categorical crossentropy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "h2Z2AlCi1XAp",
        "colab": {}
      },
      "source": [
        "# supervised output\n",
        "c_out_layer = Dense(n_classes, activation=\"softmax\")(fe)\n",
        "# define and compile supervised discriminator model\n",
        "c_model = Model(in_image, c_out_layer)\n",
        "c_model.compile(loss='sparse_categorical_crossentropy', \n",
        "                optimizer=Adam(lr=0.0002, beta_1=0.5), \n",
        "                metrics=['accuracy']) #we've got evaluation metrics now!"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "t1P6LHqP1a7P"
      },
      "source": [
        "#### What we have now\n",
        "\n",
        "With this approach we have two models, an unsupervised discriminator and a classifier, that share the feature extraction weights and have two separate outputs. Let's have a look:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "uEHx7iT11ezN",
        "colab": {}
      },
      "source": [
        "import keras.datasets.mnist #import load_data\n",
        "from keras.optimizers import Adam\n",
        "from keras.models import Model\n",
        "from keras.layers import Input\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Reshape\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Conv2D\n",
        "from keras.layers import Conv2DTranspose\n",
        "from keras.layers import LeakyReLU\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import Lambda\n",
        "from keras.layers import Activation\n",
        "from keras.utils.vis_utils import plot_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "nUzVTXBT1pF7",
        "colab": {}
      },
      "source": [
        "def build_discriminator(n_classes, in_shape=(28,28,1)):\n",
        "    # image input\n",
        "    in_image = Input(shape=in_shape)\n",
        "    # downsample\n",
        "    fe = Conv2D(128, (3,3), strides=(2,2), padding='same')(in_image)\n",
        "    fe = LeakyReLU(alpha=0.2)(fe)\n",
        "    # downsample\n",
        "    fe = Conv2D(128, (3,3), strides=(2,2), padding='same')(fe)\n",
        "    fe = LeakyReLU(alpha=0.2)(fe)\n",
        "    # downsample\n",
        "    fe = Conv2D(128, (3,3), strides=(2,2), padding='same')(fe)\n",
        "    fe = LeakyReLU(alpha=0.2)(fe)\n",
        "    # flatten feature maps\n",
        "    fe = Flatten()(fe)\n",
        "    # dropout\n",
        "    fe = Dropout(0.4)(fe)\n",
        "    d_out_layer = Dense(1, activation='sigmoid')(fe)\n",
        "    # define and compile unsupervised discriminator model\n",
        "    d_model = Model(in_image, d_out_layer)\n",
        "    d_model.compile(loss='binary_crossentropy', optimizer=Adam(lr=0.0002, beta_1=0.5))\n",
        "    # supervised output\n",
        "    c_out_layer = Dense(n_classes, activation='softmax')(fe)\n",
        "    # define and compile supervised discriminator model\n",
        "    c_model = Model(in_image, c_out_layer)\n",
        "    c_model.compile(loss='sparse_categorical_crossentropy', \n",
        "                    optimizer=Adam(lr=0.0002, beta_1=0.5), \n",
        "                    metrics=['accuracy']) #we've got evaluation metrics now!\n",
        "    return d_model, c_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jB0r2U-71py6",
        "colab": {}
      },
      "source": [
        "d_model, c_model = build_discriminator(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "UKHWm5kX1u6Z"
      },
      "source": [
        "Here is what the unsupervised discriminator looks like:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ShR5eQYFvRK7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_model(d_model, show_shapes=True, show_layer_names=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "nXK0P_1B1yP0"
      },
      "source": [
        "...and the classifier:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lP4ZAc5QvlC0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_model(c_model, show_shapes=True, show_layer_names=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "krX-4HOI11RE"
      },
      "source": [
        "#### The twist: feeding the classifier into the discriminator\n",
        "\n",
        "The classifier, in its last layer and before the softmax activation, contains the information about which of the $n$ output classes those features are more likely associated with. In [2] Salimans et al. suggest that if the classifier were passed a generated but unrealistic input, then its last layer should be about flat, and with fairly small values. That is: it wouldn't know how to classify the sample because it doesn't look like any real data point.\n",
        "\n",
        "__SHOULD WE SEPARATE THE ACTIVATION AS A SINGLE LAYER IN ORDER TO AVOID DISTORTION BY THE FINAL LAYER'S WEIGHTS?__\n",
        "\n",
        "If that's the case, one might pass the last classification layer as input to the discriminator, which would then take advantage of the additional discriminative power of the classifier. In order to make this approach work they suggest the following tweak to the discriminator's activation function:\n",
        "\n",
        "$$D(\\mathbf x) = \\frac{L(\\mathbf x)}{L(\\mathbf x)+1},\\ \\text{with } L(\\mathbf x) = \\sum e^{x_i}$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "uBFqiAdP18wl",
        "colab": {}
      },
      "source": [
        "import keras.backend as Kb\n",
        "def new_activation(output):\n",
        "    logexpsum = Kb.sum(Kb.exp(output), axis=-1, keepdims=True)\n",
        "    result = logexpsum / (logexpsum + 1.0)\n",
        "    return result\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "QF_zwiea1_Z8",
        "colab": {}
      },
      "source": [
        "#Using the classifier for discrimination\n",
        "def build_discriminator(n_classes, in_shape=(28,28,1)):\n",
        "    # image input\n",
        "    in_image = Input(shape=in_shape)\n",
        "    # downsample\n",
        "    fe = Conv2D(128, (3,3), strides=(2,2), padding='same')(in_image)\n",
        "    fe = LeakyReLU(alpha=0.2)(fe)\n",
        "    # downsample\n",
        "    fe = Conv2D(128, (3,3), strides=(2,2), padding='same')(fe)\n",
        "    fe = LeakyReLU(alpha=0.2)(fe)\n",
        "    # downsample\n",
        "    fe = Conv2D(128, (3,3), strides=(2,2), padding='same')(fe)\n",
        "    fe = LeakyReLU(alpha=0.2)(fe)\n",
        "    # flatten feature maps\n",
        "    fe = Flatten()(fe)\n",
        "    # dropout\n",
        "    fe = Dropout(0.4)(fe)\n",
        "    # separate the last dense layer\n",
        "    fe = Dense(self.n_classes)(fe)\n",
        "    # supervised output\n",
        "    c_out_layer = Activation('softmax')(fe)\n",
        "    # define and compile supervised discriminator model\n",
        "    c_model = Model(in_image, c_out_layer)\n",
        "    c_model.compile(loss='sparse_categorical_crossentropy', \n",
        "                    optimizer=Adam(lr=0.0002, beta_1=0.5), \n",
        "                    metrics=['accuracy'])\n",
        "    # unsupervised output: the input is the classifier's last dense layer!\n",
        "    d_out_layer = Lambda(new_activation)(fe)\n",
        "    # define and compile unsupervised discriminator model\n",
        "    d_model = Model(in_image, d_out_layer)\n",
        "    d_model.compile(loss='binary_crossentropy', \n",
        "                    optimizer=Adam(lr=0.0002, beta_1=0.5))\n",
        "    return d_model, c_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "FlHExOFG15XG"
      },
      "source": [
        "\n",
        "#### Diversion: is this all we've got?\n",
        "\n",
        "Are there other ways to train a classifier and a discriminator together? Which? Who dares to try one??\n",
        "\n",
        "_(Alternatives: single model with [discriminator, classifier] output; two models with shared weights, as before.)_\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "AXh_Kl3v2G3t"
      },
      "source": [
        "### Building the generator\n",
        "\n",
        "Remember: we downsampled the image for the discriminator. Let's do the opposite for the generator, upsampling. We can be perfectly symmetrical, start from an intermediate dimension (7? 14?), from an arbitrary one, use a different stride and window or do something different altogether.\n",
        "\n",
        "Anyone wants to try some alternatives?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ef2esyuS2Jcu",
        "colab": {}
      },
      "source": [
        "def build_generator(self):\n",
        "    # image generator input\n",
        "    in_lat = Input(shape=(self.latent_dim,))\n",
        "    # foundation for 7x7 image\n",
        "    n_nodes = 128 * 7 * 7\n",
        "    gen = Dense(n_nodes)(in_lat)\n",
        "    gen = LeakyReLU(alpha=0.2)(gen)\n",
        "    gen = Reshape((7, 7, 128))(gen)\n",
        "    # upsample to 14x14\n",
        "    gen = Conv2DTranspose(128, (4,4), strides=(2,2), padding='same')(gen)\n",
        "    gen = LeakyReLU(alpha=0.2)(gen)\n",
        "    # upsample to 28x28\n",
        "    gen = Conv2DTranspose(128, (4,4), strides=(2,2), padding='same')(gen)\n",
        "    gen = LeakyReLU(alpha=0.2)(gen)\n",
        "    # output\n",
        "    out_layer = Conv2D(1, (7,7), activation='tanh', padding='same')(gen)\n",
        "    # define model\n",
        "    model = Model(in_lat, out_layer)\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_hO6Z-c92MyN"
      },
      "source": [
        "### Writing the training code\n",
        "\n",
        "We're not done yet! We need to prepare the training data and write the training code :P"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yA-x4L5X2MPl",
        "colab": {}
      },
      "source": [
        "#a few guidelines...\n",
        "def train(self, n_epochs=20, batch_size=100):\n",
        "    # prepare the supervised data\n",
        "    X_sup, y_sup = \n",
        "    # compute the number of training batches per epoch and the training iterations\n",
        "    \n",
        "    # also! don't forget that every batch must be split in half: real and fake\n",
        "    half_batch = batch_size // 2\n",
        "    # this will help us keep the same nomenclature :P\n",
        "    print('n_epochs=%d, batch_size=%d, 1/2 batch=%d, batch per epoch=%d, total steps=%d' % \n",
        "        (n_epochs, batch_size, half_batch, batches_per_epoch, training_steps))\n",
        "\n",
        "    # Now train\n",
        "    for i in range(training_steps):\n",
        "        # update the supervised discriminator a.k.a. classifier\n",
        "        [X_sup_sample, y_sup_sample], _ = \n",
        "        # update the standard, unsupervised discriminator\n",
        "        # compute the loss on real and fake data separately\n",
        "        [X_real, _], y_real = \n",
        "        d_loss1 = \n",
        "        X_fake, y_fake = \n",
        "        d_loss2 = \n",
        "        # update the generator\n",
        "        X_lat, y_lat = \n",
        "        g_loss = \n",
        "        # summarize losses \n",
        "        if (i+1) % 500 == 0:\n",
        "            print('-> step=%d \\n'\\\n",
        "                '   classifier -> loss = %.3f, accuracy = %.0f \\n'\\\n",
        "                '   discriminator -> loss on real = %.3f, loss on fake = %.3f \\n'\\\n",
        "                '   generator -> loss = %.3f' % \n",
        "                (i+1, c_loss, c_acc*100, d_loss1, d_loss2, g_loss))\n",
        "        # evaluate model performance...and save!\n",
        "        if (i+1) % bat_per_epo == 0:\n",
        "            self.summarize_performance(i)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mzHSKvmg2Q8x",
        "colab": {}
      },
      "source": [
        "def train(self, n_epochs=20, batch_size=100):\n",
        "    # supervised data\n",
        "    X_sup, y_sup = self.prepare_supervised_samples()\n",
        "    # each epoch we'll train a certain number of batches...\n",
        "    batches_per_epoch = int(self.dataset[0].shape[0] / batch_size)\n",
        "    # calculate the number of training iterations\n",
        "    training_steps = batches_per_epoch * n_epochs\n",
        "    # this'll come in handy...\n",
        "    half_batch = batch_size // 2\n",
        "    print('n_epochs=%d, batch_size=%d, 1/2 batch=%d, batch per epoch=%d, total steps=%d' % \n",
        "        (n_epochs, batch_size, half_batch, batches_per_epoch, training_steps))\n",
        "\n",
        "    # Now train\n",
        "    for i in range(training_steps):\n",
        "        # update the supervised discriminator a.k.a. classifier\n",
        "        [X_sup_sample, y_sup_sample], _ = self.generate_real_samples([X_sup, y_sup], half_batch)\n",
        "        c_loss, c_acc = self.classifier.train_on_batch_size(X_sup_sample, y_sup_sample)\n",
        "        # update the standard, unsupervised discriminator\n",
        "        # compute the loss on real and fake data separately\n",
        "        [X_real, _], y_real = self.generate_real_samples(self.dataset, half_batch)\n",
        "        d_loss1 = self.discriminator.train_on_batch_size(X_real, y_real)\n",
        "        X_fake, y_fake = self.generate_fake_samples(half_batch)\n",
        "        d_loss2 = self.discriminator.train_on_batch_size(X_fake, y_fake)\n",
        "        # update the generator\n",
        "        X_lat, y_lat = self.sample_latent_points(batch_size), np.ones((batch_size, 1))\n",
        "        g_loss = self.gan.train_on_batch_size(X_lat, y_lat)\n",
        "        # summarize losses \n",
        "        if (i+1) % 500 == 0:\n",
        "            print('-> step=%d \\n'\\\n",
        "                '   classifier -> loss = %.3f, accuracy = %.0f \\n'\\\n",
        "                '   discriminator -> loss on real = %.3f, loss on fake = %.3f \\n'\\\n",
        "                '   generator -> loss = %.3f' % \n",
        "                (i+1, c_loss, c_acc*100, d_loss1, d_loss2, g_loss))\n",
        "        # evaluate model performance...and save!\n",
        "        if (i+1) % bat_per_epo == 0:\n",
        "            self.summarize_performance(i)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "iTbW8q5b2W8L"
      },
      "source": [
        "### Putting it all together"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Td-r4IB52YLY",
        "colab": {}
      },
      "source": [
        "# SGAN on MNIST data\n",
        "import keras.datasets.mnist as mnist\n",
        "from keras.optimizers import Adam\n",
        "from keras.models import Model\n",
        "from keras.layers import Input\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Reshape\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Conv2D\n",
        "from keras.layers import Conv2DTranspose\n",
        "from keras.layers import LeakyReLU\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import Lambda\n",
        "from keras.layers import Activation\n",
        "from matplotlib import pyplot\n",
        "from keras import backend as Kb\n",
        "\n",
        "class SGAN:\n",
        "    \n",
        "    def __init__(self, latent_dim):\n",
        "        self.dataset = self.prepare_real_data()\n",
        "        self.n_classes = 10\n",
        "        self.latent_dim = latent_dim\n",
        "        self.generator = self.build_generator()\n",
        "        self.discriminator, self.classifier = self.build_discriminator()\n",
        "        self.gan = self.build_gan()\n",
        "\n",
        "    # custom activation function\n",
        "    @staticmethod\n",
        "    def custom_activation(output):\n",
        "        logexpsum = Kb.sum(Kb.exp(output), axis=-1, keepdims=True)\n",
        "        result = logexpsum / (logexpsum + 1.0)\n",
        "        return result\n",
        "\n",
        "    # define the standalone supervised and unsupervised discriminator models\n",
        "    def build_discriminator(self, in_shape=(28,28,1)):\n",
        "        # image input\n",
        "        in_image = Input(shape=in_shape)\n",
        "        # downsample\n",
        "        fe = Conv2D(128, (3,3), strides=(2,2), padding='same')(in_image)\n",
        "        fe = LeakyReLU(alpha=0.2)(fe)\n",
        "        # downsample\n",
        "        fe = Conv2D(128, (3,3), strides=(2,2), padding='same')(fe)\n",
        "        fe = LeakyReLU(alpha=0.2)(fe)\n",
        "        # downsample\n",
        "        fe = Conv2D(128, (3,3), strides=(2,2), padding='same')(fe)\n",
        "        fe = LeakyReLU(alpha=0.2)(fe)\n",
        "        # flatten feature maps\n",
        "        fe = Flatten()(fe)\n",
        "        # dropout\n",
        "        fe = Dropout(0.4)(fe)\n",
        "        # output layer nodes\n",
        "        fe = Dense(self.n_classes)(fe)\n",
        "        # supervised output\n",
        "        c_out_layer = Activation('softmax')(fe)\n",
        "        # define and compile supervised discriminator model\n",
        "        c_model = Model(in_image, c_out_layer)\n",
        "        c_model.compile(loss='sparse_categorical_crossentropy', \n",
        "                        optimizer=Adam(lr=0.0002, beta_1=0.5), \n",
        "                        metrics=['accuracy'])\n",
        "        # unsupervised output\n",
        "        d_out_layer = Lambda(self.custom_activation)(fe)\n",
        "        # define and compile unsupervised discriminator model\n",
        "        d_model = Model(in_image, d_out_layer)\n",
        "        d_model.compile(loss='binary_crossentropy', \n",
        "                        optimizer=Adam(lr=0.0002, beta_1=0.5))\n",
        "        return d_model, c_model\n",
        "\n",
        "    # define the standalone generator model\n",
        "    def build_generator(self):\n",
        "        # image generator input\n",
        "        in_lat = Input(shape=(self.latent_dim,))\n",
        "        # foundation for 7x7 image\n",
        "        n_nodes = 128 * 7 * 7\n",
        "        gen = Dense(n_nodes)(in_lat)\n",
        "        gen = LeakyReLU(alpha=0.2)(gen)\n",
        "        gen = Reshape((7, 7, 128))(gen)\n",
        "        # upsample to 14x14\n",
        "        gen = Conv2DTranspose(128, (4,4), strides=(2,2), padding='same')(gen)\n",
        "        gen = LeakyReLU(alpha=0.2)(gen)\n",
        "        # upsample to 28x28\n",
        "        gen = Conv2DTranspose(128, (4,4), strides=(2,2), padding='same')(gen)\n",
        "        gen = LeakyReLU(alpha=0.2)(gen)\n",
        "        # output\n",
        "        out_layer = Conv2D(1, (7,7), activation='tanh', padding='same')(gen)\n",
        "        # define model\n",
        "        model = Model(in_lat, out_layer)\n",
        "        return model\n",
        "\n",
        "    # define the combined generator and discriminator model, for updating the generator\n",
        "    def build_gan(self):\n",
        "        # make weights in the discriminator not trainable\n",
        "        self.discriminator.trainable = False\n",
        "        # connect image output from generator as input to discriminator\n",
        "        gan_output = self.discriminator(self.generator.output)\n",
        "        # define gan model as taking noise and outputting a classification\n",
        "        model = Model(self.generator.input, gan_output)\n",
        "        # compile model\n",
        "        opt = Adam(lr=0.0002, beta_1=0.5)\n",
        "        model.compile(loss='binary_crossentropy', optimizer=opt)\n",
        "        return model\n",
        "\n",
        "    @staticmethod\n",
        "    def prepare_real_data():\n",
        "        # load dataset\n",
        "        (X_train, y_train), (_, _) = mnist.load_data()\n",
        "        X = np.expand_dims(X_train, axis=-1).astype('float32')\n",
        "        # rescale from [-255, 255] to [-1,1]\n",
        "        X = (X - 127.5) / 127.5\n",
        "        return [X, y_train]\n",
        "\n",
        "    # select a balanced, supervised subdataset\n",
        "    def prepare_supervised_samples(self, n_samples=100):\n",
        "        X, y = self.dataset\n",
        "        X_list, y_list = list(), list()\n",
        "        n_per_class = int(n_samples / self.n_classes)\n",
        "        for i in range(self.n_classes):\n",
        "            # choose random instances of images of a given class\n",
        "            Xi_all = X[y == i]\n",
        "            xi = np.random.randint(0, len(Xi_all), n_per_class)\n",
        "            # add to list\n",
        "            for j in xi:\n",
        "                X_list.append(Xi_all[j])\n",
        "                y_list.append(i)\n",
        "        return np.asarray(X_list), np.asarray(y_list)\n",
        "\n",
        "    #generate real points for the discriminator from a sample of labelled data\n",
        "    @staticmethod\n",
        "    def generate_real_samples(labelled_data, n_samples):\n",
        "        X, y = labelled_data\n",
        "        xi = np.random.randint(0, X.shape[0], n_samples)\n",
        "        X_samp, y_samp = X[xi], y[xi]\n",
        "        # generate class labels for discriminator\n",
        "        ones = np.ones((n_samples, 1))\n",
        "        return [X_samp, y_samp], ones\n",
        "\n",
        "    # sample the latent space for the generator\n",
        "    def sample_latent_points(self, n_samples):\n",
        "        lat_pnts = np.random.randn(self.latent_dim * n_samples)\n",
        "        lat_pnts = lat_pnts.reshape(n_samples, self.latent_dim)\n",
        "        return lat_pnts\n",
        "\n",
        "    # generate fake samples for the discriminator\n",
        "    def generate_fake_samples(self, n_samples):\n",
        "        latent_points = self.sample_latent_points(n_samples)\n",
        "        generated_images = self.generator.predict(latent_points)\n",
        "        y = np.zeros((n_samples, 1))\n",
        "        return generated_images, y\n",
        "\n",
        "    # generate samples, save as a plot and save the model\n",
        "    def summarize_performance(self, step, n_samples=100):\n",
        "        X, _ = self.generate_fake_samples(n_samples)\n",
        "        # rescale for simplicity\n",
        "        X = (X + 1) / 2.0\n",
        "        # plot\n",
        "        for i in range(100):\n",
        "            pyplot.subplot(10, 10, 1 + i)\n",
        "            pyplot.axis('off')\n",
        "            pyplot.imshow(X[i, :, :, 0], cmap='gray_r')\n",
        "        fl = 'generated_images_%04d.png' % (step+1)\n",
        "        pyplot.savefig(fl)\n",
        "        pyplot.close()\n",
        "        # evaluate the classifier\n",
        "        # WARNING!! DON'T TRY THIS AT HOME!! (why? :)\n",
        "        X, y = self.dataset\n",
        "        _, acc = self.classifier.evaluate(X, y, verbose=0)\n",
        "        print('===> Classifier Accuracy: %.3f%%' % (acc * 100))\n",
        "        # save the generator\n",
        "        gen_file = 'generator_%04d.h5' % (step+1)\n",
        "        self.generator.save(gen_file)\n",
        "        # save the classifier\n",
        "        class_file = 'classifier_%04d.h5' % (step+1)\n",
        "        self.classifier.save(class_file)\n",
        "\n",
        "    # train the model!\n",
        "    def train(self, n_epochs=20, batch_size=100):\n",
        "        # supervised data\n",
        "        X_sup, y_sup = self.prepare_supervised_samples()\n",
        "        # each epoch we'll train a certain number of batches...\n",
        "        batches_per_epoch = int(self.dataset[0].shape[0] / batch_size)\n",
        "        # calculate the number of training iterations\n",
        "        training_steps = batches_per_epoch * n_epochs\n",
        "        # this'll come in handy...\n",
        "        half_batch = batch_size // 2\n",
        "        print('n_epochs=%d, batch_size=%d, 1/2 batch=%d, batch per epoch=%d, total steps=%d' % \n",
        "            (n_epochs, batch_size, half_batch, batches_per_epoch, training_steps))\n",
        "        \n",
        "        # Now train\n",
        "        for i in range(training_steps):\n",
        "            # update the supervised discriminator a.k.a. classifier\n",
        "            [X_sup_sample, y_sup_sample], _ = self.generate_real_samples([X_sup, y_sup], half_batch)\n",
        "            c_loss, c_acc = self.classifier.train_on_batch(X_sup_sample, y_sup_sample)\n",
        "            # update the standard, unsupervised discriminator\n",
        "            # compute the loss on real and fake data separately\n",
        "            [X_real, _], y_real = self.generate_real_samples(self.dataset, half_batch)\n",
        "            d_loss1 = self.discriminator.train_on_batch(X_real, y_real)\n",
        "            X_fake, y_fake = self.generate_fake_samples(half_batch)\n",
        "            d_loss2 = self.discriminator.train_on_batch(X_fake, y_fake)\n",
        "            # update the generator\n",
        "            X_lat, y_lat = self.sample_latent_points(batch_size), np.ones((batch_size, 1))\n",
        "            g_loss = self.gan.train_on_batch(X_lat, y_lat)\n",
        "            # summarize losses \n",
        "            if (i+1) % 500 == 0:\n",
        "                print('-> step=%d \\n'\\\n",
        "                    '   classifier -> loss = %.3f, accuracy = %.0f \\n'\\\n",
        "                    '   discriminator -> loss on real = %.3f, loss on fake = %.3f \\n'\\\n",
        "                    '   generator -> loss = %.3f' % \n",
        "                    (i+1, c_loss, c_acc*100, d_loss1, d_loss2, g_loss))\n",
        "            # evaluate model performance...and save!\n",
        "            if (i+1) % batches_per_epoch == 0:\n",
        "                self.summarize_performance(i)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "YpbmFtxT4Q-G"
      },
      "source": [
        "Let's run it!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QJZgvbZvemyO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "sgan = SGAN(30)\n",
        "\n",
        "sgan.train(n_epochs=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "OWICh9Dt4Uxp"
      },
      "source": [
        "### Simple Bonus Task: Feature matching\n",
        "\n",
        "What we did by piping the classifier into the discriminator moves along the lines of another interesting strategy suggested in [2]: feature matching. It consists of asking the generator to generate data whose statistics match those used by the discriminator.\n",
        "\n",
        "Think about it: $G$ is not only trying to fool $D$, it is doing so by _explicitly_ using $D$'s criteria against it. Salimans et al. claim that this change greatly helps in reducing the instability of GAN training, in particular the performance oscillation between $G$ and $D$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "V4gaBXcs4Ry8",
        "colab": {}
      },
      "source": [
        "class SGAN_FM(SGAN):\n",
        "    \n",
        "    def __init__(self, latent_dim):\n",
        "        self.dataset = self.prepare_real_data()\n",
        "        self.n_classes = 10\n",
        "        self.unlab_imgs = Input(shape=(28, 28, 1))\n",
        "        self.latent_dim = latent_dim\n",
        "        self.generator = self.build_generator()\n",
        "        self.discriminator, self.classifier, self.fe_model = self.build_discriminator()\n",
        "        self.fm_layer = self.fe_model.output\n",
        "        self.gan = self.build_gan()\n",
        "\n",
        "    # custom activation function\n",
        "    @staticmethod\n",
        "    def custom_activation(output):\n",
        "        logexpsum = Kb.sum(Kb.exp(output), axis=-1, keepdims=True)\n",
        "        result = logexpsum / (logexpsum + 1.0)\n",
        "        return result\n",
        "\n",
        "    def build_discriminator(self, in_shape=(28,28,1)):\n",
        "        # image input\n",
        "        in_image = Input(shape=in_shape)\n",
        "        # downsample\n",
        "        fe = Conv2D(128, (3,3), strides=(2,2), padding='same')(in_image)\n",
        "        fe = LeakyReLU(alpha=0.2)(fe)\n",
        "        # downsample\n",
        "        fe = Conv2D(128, (3,3), strides=(2,2), padding='same')(fe)\n",
        "        fe = LeakyReLU(alpha=0.2)(fe)\n",
        "        # downsample\n",
        "        fe = Conv2D(128, (3,3), strides=(2,2), padding='same')(fe)\n",
        "        fe = LeakyReLU(alpha=0.2)(fe)\n",
        "        # flatten feature maps\n",
        "        fe = Flatten()(fe)\n",
        "        # dropout\n",
        "        fe = Dropout(0.4)(fe)\n",
        "        \n",
        "        # output layer nodes\n",
        "        fe = Dense(self.n_classes)(fe)\n",
        "        \n",
        "        fe_model = Model(in_image, fe)\n",
        "        \n",
        "        # supervised output\n",
        "        c_out_layer = Activation('softmax')(fe)\n",
        "        # define and compile supervised discriminator model\n",
        "        c_model = Model(in_image, c_out_layer)\n",
        "        c_model.compile(loss='sparse_categorical_crossentropy', \n",
        "                        optimizer=Adam(lr=0.0002, beta_1=0.5), \n",
        "                        metrics=['accuracy'])\n",
        "        # unsupervised output\n",
        "        d_out_layer = Lambda(self.custom_activation)(fe)\n",
        "        # define and compile unsupervised discriminator model\n",
        "        d_model = Model(in_image, d_out_layer)\n",
        "        d_model.compile(loss='binary_crossentropy', \n",
        "                        optimizer=Adam(lr=0.0002, beta_1=0.5))\n",
        "        return d_model, c_model, fe_model\n",
        "\n",
        "    # define the standalone generator model\n",
        "    def build_generator(self):\n",
        "        # image generator input\n",
        "        in_lat = Input(shape=(self.latent_dim,))\n",
        "        # foundation for 7x7 image\n",
        "        n_nodes = 128 * 7 * 7\n",
        "        gen = Dense(n_nodes)(in_lat)\n",
        "        gen = LeakyReLU(alpha=0.2)(gen)\n",
        "        gen = Reshape((7, 7, 128))(gen)\n",
        "        # upsample to 14x14\n",
        "        gen = Conv2DTranspose(128, (4,4), strides=(2,2), padding='same')(gen)\n",
        "        gen = LeakyReLU(alpha=0.2)(gen)\n",
        "        # upsample to 28x28\n",
        "        gen = Conv2DTranspose(128, (4,4), strides=(2,2), padding='same')(gen)\n",
        "        gen = LeakyReLU(alpha=0.2)(gen)\n",
        "        # output\n",
        "        out_layer = Conv2D(1, (7,7), activation='tanh', padding='same')(gen)\n",
        "        # define model\n",
        "        model = Model(in_lat, out_layer)\n",
        "        return model\n",
        "    \n",
        "    def new_gen_loss(self):\n",
        "        def out_loss(y_pred, y_true):\n",
        "            #need to do a new forward pass here\n",
        "            output_unlabeled = self.fe_model.get_output_at(-1)\n",
        "            m1 = Kb.mean(output_unlabeled, axis=0)\n",
        "            m2 = Kb.mean(y_pred, axis=0)\n",
        "            loss = Kb.mean(Kb.abs(m1-m2))\n",
        "            return loss\n",
        "        return out_loss\n",
        "    \n",
        "    def build_gan(self):\n",
        "        # make weights in the discriminator not trainable\n",
        "        self.discriminator.trainable = False\n",
        "        # connect image output from generator as input to discriminator\n",
        "        gan_output = self.fe_model(self.generator.output)\n",
        "        model = Model(self.generator.input, gan_output)\n",
        "        # compile model\n",
        "        opt = Adam(lr=0.0002, beta_1=0.5)\n",
        "        model.compile(loss=self.new_gen_loss(), optimizer=opt)\n",
        "        return model\n",
        "    \n",
        "    # train the model!\n",
        "    def train(self, n_epochs=20, batch_size=100):\n",
        "        # supervised data\n",
        "        X_sup, y_sup = self.prepare_supervised_samples()\n",
        "        # each epoch we'll train a certain number of batches...\n",
        "        batches_per_epoch = int(self.dataset[0].shape[0] / batch_size)\n",
        "        # calculate the number of training iterations\n",
        "        training_steps = batches_per_epoch * n_epochs\n",
        "        # this'll come in handy...\n",
        "        half_batch = batch_size // 2\n",
        "        print('n_epochs=%d, batch_size=%d, 1/2 batch=%d, batch per epoch=%d, total steps=%d' % \n",
        "            (n_epochs, batch_size, half_batch, batches_per_epoch, training_steps))\n",
        "        \n",
        "        # Now train\n",
        "        for i in range(training_steps):\n",
        "            # update the supervised discriminator a.k.a. classifier\n",
        "            [X_sup_sample, y_sup_sample], _ = self.generate_real_samples([X_sup, y_sup], half_batch)\n",
        "            c_loss, c_acc = self.classifier.train_on_batch(X_sup_sample, y_sup_sample)\n",
        "            # update the standard, unsupervised discriminator\n",
        "            # compute the loss on real and fake data separately\n",
        "            [X_real, _], y_real = self.generate_real_samples(self.dataset, half_batch)\n",
        "            d_loss1 = self.discriminator.train_on_batch(X_real, y_real)\n",
        "            X_fake, y_fake = self.generate_fake_samples(half_batch)\n",
        "            d_loss2 = self.discriminator.train_on_batch(X_fake, y_fake)\n",
        "            \n",
        "#            self.fm_layer = self.fe_model.predict(X_real)\n",
        "\n",
        "            # update the generator\n",
        "            X_lat, y_lat = self.sample_latent_points(batch_size), np.ones((batch_size, 1))\n",
        "            g_loss = self.gan.train_on_batch(X_lat, y_lat)\n",
        "            # summarize losses \n",
        "            if (i+1) % 500 == 0:\n",
        "                print('-> step=%d \\n'\\\n",
        "                    '   classifier -> loss = %.3f, accuracy = %.0f \\n'\\\n",
        "                    '   discriminator -> loss on real = %.3f, loss on fake = %.3f \\n'\\\n",
        "                    '   generator -> loss = %.3f' % \n",
        "                    (i+1, c_loss, c_acc*100, d_loss1, d_loss2, g_loss))\n",
        "            # evaluate model performance...and save!\n",
        "            if (i+1) % batches_per_epoch == 0:\n",
        "                self.summarize_performance(i)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jEaCEc8iaWEH",
        "colab_type": "text"
      },
      "source": [
        "## Real Bonus Tasks\n",
        "\n",
        "If you've reached this point still lucid and unharmed, it means you possess a remarkable mastery of generative models and neural architectures. It's highly commendable.\n",
        "\n",
        "But it also means it's time to take the wheels off. So let's think about the following problems :)) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vVyXizD1C6DQ",
        "colab_type": "text"
      },
      "source": [
        "### GANs vs. VAEs\n",
        "\n",
        "Both VAEs and GANs tackle a similar challenge, namely that of generating realistic looking data. VAEs explicitly model the underlying distribution, GANs somehow learn it implicitly. \n",
        "\n",
        "1.  How does a VAE perform on our unimodal and multimodal 1D function learning task?\n",
        "2.  Could we use a VAE for semisupervised learning? Would it train a better classifier than a GAN?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5xkrUQrRBh-0",
        "colab_type": "text"
      },
      "source": [
        "### Class-targeted GAN\n",
        "\n",
        "From the MNIST SGAN code extrapolate a simple GAN to generate MNIST digits. Whenever we call the generator, it'll produce whatever sample suits its fancy. This is not always ideal.\n",
        "\n",
        "Modify the GAN so that we can specify the class of the samples to generate."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XrtGXe8JCV29",
        "colab_type": "text"
      },
      "source": [
        "### Latent space exploration\n",
        "\n",
        "Can the generator's latent space tell us something about the classes? What if we tried to explore it with different forms of clustering? (distance-based, density-based...)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M2erYkOCHptU",
        "colab_type": "text"
      },
      "source": [
        "### Parametrisation of the latent space\n",
        "\n",
        "In VAEs we have two parameters that control the generation of samples in a smooth way. Can we obtain something similar with GANs?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H7T1ofMVIG4h",
        "colab_type": "text"
      },
      "source": [
        "### Make mode-collapse happen\n",
        "\n",
        "Build a GAN that is supposed to generate a circle of bidimentional standard Gaussians. Mode collapse should appear in the form of undersampling/absence of some of the Gaussians."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NlhyJ3Rou8g4",
        "colab_type": "text"
      },
      "source": [
        "###GAAs: Generative Adversarial Anything\n",
        "\n",
        "Do they _really_ have to be networks? Could we apply the GAN training framework to any kind of generator/discriminator models? How would we optimise the parameters? \n",
        "\n",
        "_(This kind of lies outside the scope of a course on DL, but it certainly is a worthy conceptual exercise.)_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wtds-zDwDvK6",
        "colab_type": "text"
      },
      "source": [
        "### Tabular GAN\n",
        "\n",
        "How can we adapt a GAN to generate tabular data? I recommend approaching this problem by steps: first, think about purely continuous data, then think about how you would handle categorical variables. This is a non trivial task and there is published literature on models tackling this issue.\n",
        "\n",
        "Some details to keep in mind:\n",
        " * Features have ranges that make some values plausible, others implausible.\n",
        " * Features could be generated by separated channels, but their correlation must be taken into account (e.g. on medical data, you won't be able to generate data for a male with a high risk of cervical cancer)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6OwQCLpexY5p",
        "colab_type": "text"
      },
      "source": [
        "## Conclusions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T650IO_exeUu",
        "colab_type": "text"
      },
      "source": [
        "GANs are powerful generative models that implicitly learn the distribution lying under the training data. To do so, they leverate, in an unsupervised context, an adversarial training where a generator and a discriminator compete against each other.\n",
        "\n",
        "The two models usually being neural networks, intuitively means GANs are even more (over)parametrised and data-hungry than standard NNs, which in GANs often end up being but a part of a much larger architecture. Furthermore, the adversarial training adds additional instabilities and hurdles to convergence to the usual gradient-descent based methods, such as the numerous oscillations needed to reach an equilibrium in the generator/discriminator optimisation game, and the possibility of the game getting stuck in a local optimum as in the case of mode collapse.\n",
        "\n",
        "The many technical pitfalls and the still not thorough understanding we have of these models motivate much of the ongoing research on GANs, which despite these drawbacks keep showing promising results in a vast array of applications, including the scenario most commonly faced by practitioners of training models in a semi-supervised fashion, a case in which GANs can help generate non-trivial, non-distorting labelled examples."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KrBNBY5mLXOv",
        "colab_type": "text"
      },
      "source": [
        "## References"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HqDcLxHrLbJH",
        "colab_type": "text"
      },
      "source": [
        "[1] Goodfellow Ian, Generative Adversarial Networks, NIPS 2016 Tutorial, https://arxiv.org/pdf/1701.00160.pdf\n",
        "\n",
        "[2] Salimans T. et al., Improved Techniques for Training GANs, https://arxiv.org/pdf/1606.03498.pdf\n",
        "\n",
        "[3] Chinthala S. et al., How to train a GAN? Tips and tricks to make GANs work., https://github.com/soumith/ganhacks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c5uHpO6bS4k_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# %tensorflow_version 2.x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ir_NzTGqRdrw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import tensorflow as tf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s6SMVT9bSVFR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from datetime import datetime\n",
        "# logdir = \"logs/scalars/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "# file_writer = tf.summary.create_file_writer(logdir + \"/metrics\")\n",
        "# file_writer.set_as_default()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R6zSfjOGTQFw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# %reload_ext tensorboard"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ikZoNe5BTE0j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# %tensorboard --inspect --logdir logs/scalars/20200110-124802/metrics"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "stLVTntPPuG8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# SGAN on MNIST data\n",
        "import keras.datasets.mnist as mnist\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense, Reshape, Flatten, Conv2D\n",
        "from tensorflow.keras.layers import Conv2DTranspose, LeakyReLU, Dropout, Lambda, Activation\n",
        "from matplotlib import pyplot\n",
        "from tensorflow.keras import backend as Kb\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir)\n",
        "\n",
        "class SGAN:\n",
        "    \n",
        "    def __init__(self, latent_dim):\n",
        "        self.dataset = self.prepare_real_data()\n",
        "        self.n_classes = 10\n",
        "        self.latent_dim = latent_dim\n",
        "        self.generator = self.build_generator()\n",
        "        self.discriminator, self.classifier = self.build_discriminator()\n",
        "        self.gan = self.build_gan()\n",
        "\n",
        "    # custom activation function\n",
        "    @staticmethod\n",
        "    def custom_activation(output):\n",
        "        logexpsum = Kb.sum(Kb.exp(output), axis=-1, keepdims=True)\n",
        "        result = logexpsum / (logexpsum + 1.0)\n",
        "        return result\n",
        "\n",
        "    # define the standalone supervised and unsupervised discriminator models\n",
        "    def build_discriminator(self, in_shape=(28,28,1)):\n",
        "        # image input\n",
        "        in_image = Input(shape=in_shape)\n",
        "        # downsample\n",
        "        fe = Conv2D(128, (3,3), strides=(2,2), padding='same')(in_image)\n",
        "        fe = LeakyReLU(alpha=0.2)(fe)\n",
        "        # downsample\n",
        "        fe = Conv2D(128, (3,3), strides=(2,2), padding='same')(fe)\n",
        "        fe = LeakyReLU(alpha=0.2)(fe)\n",
        "        # downsample\n",
        "        fe = Conv2D(128, (3,3), strides=(2,2), padding='same')(fe)\n",
        "        fe = LeakyReLU(alpha=0.2)(fe)\n",
        "        # flatten feature maps\n",
        "        fe = Flatten()(fe)\n",
        "        # dropout\n",
        "        fe = Dropout(0.4)(fe)\n",
        "        # output layer nodes\n",
        "        fe = Dense(self.n_classes)(fe)\n",
        "        # supervised output\n",
        "        c_out_layer = Activation('softmax')(fe)\n",
        "        # define and compile supervised discriminator model\n",
        "        c_model = Model(in_image, c_out_layer)\n",
        "        c_model.compile(loss='sparse_categorical_crossentropy', \n",
        "                        optimizer=Adam(lr=0.0002, beta_1=0.5), \n",
        "                        metrics=['accuracy'])\n",
        "        # unsupervised output\n",
        "        d_out_layer = Lambda(self.custom_activation)(fe)\n",
        "        # define and compile unsupervised discriminator model\n",
        "        d_model = Model(in_image, d_out_layer)\n",
        "        d_model.compile(loss='binary_crossentropy', \n",
        "                        optimizer=Adam(lr=0.0002, beta_1=0.5))\n",
        "        return d_model, c_model\n",
        "\n",
        "    # define the standalone generator model\n",
        "    def build_generator(self):\n",
        "        # image generator input\n",
        "        in_lat = Input(shape=(self.latent_dim,))\n",
        "        # foundation for 7x7 image\n",
        "        n_nodes = 128 * 7 * 7\n",
        "        gen = Dense(n_nodes)(in_lat)\n",
        "        gen = LeakyReLU(alpha=0.2)(gen)\n",
        "        gen = Reshape((7, 7, 128))(gen)\n",
        "        # upsample to 14x14\n",
        "        gen = Conv2DTranspose(128, (4,4), strides=(2,2), padding='same')(gen)\n",
        "        gen = LeakyReLU(alpha=0.2)(gen)\n",
        "        # upsample to 28x28\n",
        "        gen = Conv2DTranspose(128, (4,4), strides=(2,2), padding='same')(gen)\n",
        "        gen = LeakyReLU(alpha=0.2)(gen)\n",
        "        # output\n",
        "        out_layer = Conv2D(1, (7,7), activation='tanh', padding='same')(gen)\n",
        "        # define model\n",
        "        model = Model(in_lat, out_layer)\n",
        "        return model\n",
        "\n",
        "    # define the combined generator and discriminator model, for updating the generator\n",
        "    def build_gan(self):\n",
        "        # make weights in the discriminator not trainable\n",
        "        self.discriminator.trainable = False\n",
        "        # connect image output from generator as input to discriminator\n",
        "        gan_output = self.discriminator(self.generator.output)\n",
        "        # define gan model as taking noise and outputting a classification\n",
        "        model = Model(self.generator.input, gan_output)\n",
        "        # compile model\n",
        "        opt = Adam(lr=0.0002, beta_1=0.5)\n",
        "        model.compile(loss='binary_crossentropy', optimizer=opt)\n",
        "        return model\n",
        "\n",
        "    @staticmethod\n",
        "    def prepare_real_data():\n",
        "        # load dataset\n",
        "        (X_train, y_train), (_, _) = mnist.load_data()\n",
        "        X = np.expand_dims(X_train, axis=-1).astype('float32')\n",
        "        # rescale from [-255, 255] to [-1,1]\n",
        "        X = (X - 127.5) / 127.5\n",
        "        return [X, y_train]\n",
        "\n",
        "    # select a balanced, supervised subdataset\n",
        "    def prepare_supervised_samples(self, n_samples=100):\n",
        "        X, y = self.dataset\n",
        "        X_list, y_list = list(), list()\n",
        "        n_per_class = int(n_samples / self.n_classes)\n",
        "        for i in range(self.n_classes):\n",
        "            # choose random instances of images of a given class\n",
        "            Xi_all = X[y == i]\n",
        "            xi = np.random.randint(0, len(Xi_all), n_per_class)\n",
        "            # add to list\n",
        "            for j in xi:\n",
        "                X_list.append(Xi_all[j])\n",
        "                y_list.append(i)\n",
        "        return np.asarray(X_list), np.asarray(y_list)\n",
        "\n",
        "    #generate real points for the discriminator from a sample of labelled data\n",
        "    @staticmethod\n",
        "    def generate_real_samples(labelled_data, n_samples):\n",
        "        X, y = labelled_data\n",
        "        xi = np.random.randint(0, X.shape[0], n_samples)\n",
        "        X_samp, y_samp = X[xi], y[xi]\n",
        "        # generate class labels for discriminator\n",
        "        ones = np.ones((n_samples, 1))\n",
        "        return [X_samp, y_samp], ones\n",
        "\n",
        "    # sample the latent space for the generator\n",
        "    def sample_latent_points(self, n_samples):\n",
        "        lat_pnts = np.random.randn(self.latent_dim * n_samples)\n",
        "        lat_pnts = lat_pnts.reshape(n_samples, self.latent_dim)\n",
        "        return lat_pnts\n",
        "\n",
        "    # generate fake samples for the discriminator\n",
        "    def generate_fake_samples(self, n_samples):\n",
        "        latent_points = self.sample_latent_points(n_samples)\n",
        "        generated_images = self.generator.predict(latent_points)\n",
        "        y = np.zeros((n_samples, 1))\n",
        "        return generated_images, y\n",
        "\n",
        "    # generate samples, save as a plot and save the model\n",
        "    def summarize_performance(self, step, n_samples=100):\n",
        "        X, _ = self.generate_fake_samples(n_samples)\n",
        "        # rescale for simplicity\n",
        "        X = (X + 1) / 2.0\n",
        "        # plot\n",
        "        for i in range(100):\n",
        "            pyplot.subplot(10, 10, 1 + i)\n",
        "            pyplot.axis('off')\n",
        "            pyplot.imshow(X[i, :, :, 0], cmap='gray_r')\n",
        "        fl = 'generated_images_%04d.png' % (step+1)\n",
        "        pyplot.savefig(fl)\n",
        "        pyplot.close()\n",
        "        # evaluate the classifier\n",
        "        # WARNING!! DON'T TRY THIS AT HOME!! (why? :)\n",
        "        X, y = self.dataset\n",
        "        _, acc = self.classifier.evaluate(X, y, verbose=0)\n",
        "        print('===> Classifier Accuracy: %.3f%%' % (acc * 100))\n",
        "        # save the generator\n",
        "        gen_file = 'generator_%04d.h5' % (step+1)\n",
        "        self.generator.save(gen_file)\n",
        "        # save the classifier\n",
        "        class_file = 'classifier_%04d.h5' % (step+1)\n",
        "        self.classifier.save(class_file)\n",
        "\n",
        "    # train the model!\n",
        "    def train(self, n_epochs=20, batch_size=100):\n",
        "        # supervised data\n",
        "        X_sup, y_sup = self.prepare_supervised_samples()\n",
        "        # each epoch we'll train a certain number of batches...\n",
        "        batches_per_epoch = int(self.dataset[0].shape[0] / batch_size)\n",
        "        # calculate the number of training iterations\n",
        "        training_steps = batches_per_epoch * n_epochs\n",
        "        # this'll come in handy...\n",
        "        half_batch = batch_size // 2\n",
        "        print('n_epochs=%d, batch_size=%d, 1/2 batch=%d, batch per epoch=%d, total steps=%d' % \n",
        "            (n_epochs, batch_size, half_batch, batches_per_epoch, training_steps))\n",
        "        \n",
        "        # Now train\n",
        "        for i in range(training_steps):\n",
        "            # update the supervised discriminator a.k.a. classifier\n",
        "            [X_sup_sample, y_sup_sample], _ = self.generate_real_samples([X_sup, y_sup], half_batch)\n",
        "            c_loss, c_acc = self.classifier.train_on_batch(X_sup_sample, y_sup_sample)\n",
        "            # update the standard, unsupervised discriminator\n",
        "            # compute the loss on real and fake data separately\n",
        "            [X_real, _], y_real = self.generate_real_samples(self.dataset, half_batch)\n",
        "            d_loss1 = self.discriminator.train_on_batch(X_real, y_real)\n",
        "            X_fake, y_fake = self.generate_fake_samples(half_batch)\n",
        "            d_loss2 = self.discriminator.train_on_batch(X_fake, y_fake)\n",
        "            # update the generator\n",
        "            X_lat, y_lat = self.sample_latent_points(batch_size), np.ones((batch_size, 1))\n",
        "            g_loss = self.gan.train_on_batch(X_lat, y_lat)\n",
        "            # summarize losses \n",
        "            if (i+1) % 500 == 0:\n",
        "                print('-> step=%d \\n'\\\n",
        "                    '   classifier -> loss = %.3f, accuracy = %.0f \\n'\\\n",
        "                    '   discriminator -> loss on real = %.3f, loss on fake = %.3f \\n'\\\n",
        "                    '   generator -> loss = %.3f' % \n",
        "                    (i+1, c_loss, c_acc*100, d_loss1, d_loss2, g_loss))\n",
        "            # evaluate model performance...and save!\n",
        "            if (i+1) % batches_per_epoch == 0:\n",
        "                self.summarize_performance(i)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}