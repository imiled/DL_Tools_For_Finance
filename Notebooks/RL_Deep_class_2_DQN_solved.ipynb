{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "name": "MAIN DRL_class_2_DQN_solved.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/imiled/DL_Tools_For_Finance/blob/master/MAIN_DRL_class_2_DQN_solved.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5K7FlYfUjKL-",
        "colab_type": "text"
      },
      "source": [
        "#### Install dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4woswnRsjKMB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n",
        "!apt-get update > /dev/null 2>&1\n",
        "!apt-get install cmake > /dev/null 2>&1\n",
        "!pip install --upgrade setuptools > /dev/null 2>&1\n",
        "!pip install ez_setup > /dev/null 2>&1\n",
        "!pip install gym > /dev/null 2>&1\n",
        "!pip install gym[atari] > /dev/null 2>&1\n",
        "!pip install gym-super-mario-bros > /dev/null 2>&1\n",
        "!pip install git+https://github.com/JKCooper2/gym-bandits#egg=gym-bandits > /dev/null 2>&1\n",
        "!pip install tensorflow-gpu==2.0.0 > /dev/null 2>&1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j0OYmmQijKMF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "e9e7fcb9-9096-4f2d-ec40-1450a05f0c94"
      },
      "source": [
        "%matplotlib inline\n",
        "import gym_bandits\n",
        "import gym\n",
        "from gym import logger as gymlogger\n",
        "from gym.wrappers import Monitor\n",
        "gymlogger.set_level(40) #error only\n",
        "from nes_py.wrappers import JoypadSpace\n",
        "import gym_super_mario_bros\n",
        "from gym_super_mario_bros.actions import SIMPLE_MOVEMENT\n",
        "\n",
        "try:\n",
        "    # %tensorflow_version solo existe in Colab.\n",
        "    %tensorflow_version 2.x\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "import tensorflow as tf\n",
        "from collections import deque\n",
        "import progressbar\n",
        "import numpy as np\n",
        "import skimage\n",
        "import random\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "import glob\n",
        "import io\n",
        "import base64\n",
        "from IPython.display import HTML\n",
        "\n",
        "from IPython import display as ipythondisplay\n",
        "from pyvirtualdisplay import Display\n",
        "display = Display(visible=0, size=(1400, 900))\n",
        "display.start()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "xdpyinfo was not found, X start can not be checked! Please install xdpyinfo!\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Display cmd_param=['Xvfb', '-br', '-nolisten', 'tcp', '-screen', '0', '1400x900x24', ':1001'] cmd=['Xvfb', '-br', '-nolisten', 'tcp', '-screen', '0', '1400x900x24', ':1001'] oserror=None return_code=None stdout=\"None\" stderr=\"None\" timeout_happened=False>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qxbY8XFEjKMH",
        "colab_type": "text"
      },
      "source": [
        "### OpenAI Gym"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yhecKF8RjKMH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class environment(object):\n",
        "    def __init__(self, env_name):\n",
        "        self.name = env_name\n",
        "        self.env = self.wrap_env(gym.make(self.name))\n",
        "        \n",
        "    @staticmethod\n",
        "    def show_video():\n",
        "        mp4list = glob.glob('video/*.mp4')\n",
        "        if len(mp4list) > 0:\n",
        "            mp4 = mp4list[0]\n",
        "            video = io.open(mp4, 'r+b').read()\n",
        "            encoded = base64.b64encode(video)\n",
        "            ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
        "                        loop controls style=\"height: 400px;\">\n",
        "                        <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "                     </video>'''.format(encoded.decode('ascii'))))\n",
        "        else: \n",
        "            print(\"Could not find video\")\n",
        "\n",
        "    @staticmethod\n",
        "    def wrap_env(env):\n",
        "        \"\"\"\n",
        "        Utility function to enable video recording of gym environment and displaying it\n",
        "        To enable video, just do \"env = wrap_env(env)\"\"\n",
        "        \"\"\"\n",
        "        env = Monitor(env, './video', force=True)\n",
        "        return env"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8bI4Q8QdUwNT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class utils_class(object):\n",
        "    @staticmethod\n",
        "    def preprocess(state):\n",
        "        output = skimage.color.rgb2gray(state)\n",
        "        output = skimage.util.crop(output, (34, 16), (0, 0))\n",
        "        output = skimage.transform.resize(output,(84,84)) / 255.0\n",
        "        return output\n",
        "\n",
        "    def stack_states(self, state, next_state):\n",
        "        next_state = self.preprocess(next_state)\n",
        "        next_state = np.append(state[:,:,1:], np.expand_dims(next_state, 2), axis=2)\n",
        "        return next_state\n",
        "\n",
        "    def initial_state(self, state):\n",
        "        state = self.preprocess(state)\n",
        "        state = np.stack([state] * 4, axis=2)\n",
        "        state = np.expand_dims(state, axis=0)\n",
        "        return state\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ehZ1J_PzAXzq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "utils = utils_class()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oDhzcthSjKMJ",
        "colab_type": "text"
      },
      "source": [
        "# Deep Q Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A_iJP2HScJBA",
        "colab_type": "text"
      },
      "source": [
        "In this notebook we will learn about the popular Deep Q-Network (DQN) algorithm.\n",
        "In essence, DQN uses neural networks to approximate the Q function. We will apply the DQN agent to solve any Atari game environment, and learn about the basic tricks to let DQN work in practice.\n",
        "\n",
        "Learning goals:\n",
        "- Deep Q Network\n",
        "- Experience Replay\n",
        "- Target Network\n",
        "- Double DQN\n",
        "- Dueling architecture"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xLRpTGO6f-V4",
        "colab_type": "text"
      },
      "source": [
        "## Deep Q Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wIsypmlwf-Y6",
        "colab_type": "text"
      },
      "source": [
        "Deep Q-network is a seminal piece of work to make the training of Q-learning more data-efficient, when the Q value is approximated with a nonlinear function $Q(s, a, \\theta) \\sim Q^*(s,a)$. The Q network can be a multi-layer dense neural network, a convolutional network (CNN), or a recurrent network, depending on the problem.\n",
        "In this notebook, we will focus on CNN, as the problem at hand concerns with the atari games universe.\n",
        "\n",
        "In particular,  we deal with an architecture consisting on three\n",
        "hidden convolutional layers, followed by one fully connected hidden layer, followed by the output layer.\n",
        "The three successive hidden convolutional layers of DQN produce 32 8×8 feature maps, 64 4×4 feature maps, and 64 3×3 feature maps. The activation function of the units of each feature map is a\n",
        "rectifier nonlinearity. \n",
        "As an input we simply pass the game screen alone and get the Q values for all possible actions in the state in the output layer.\n",
        "\n",
        "\n",
        "We update the weights and minimize the loss through gradient descent. The loss is given by the expression:\n",
        "\n",
        "$$ l = \\left( r + \\gamma max_{a'} Q(s', a', \\theta) - Q(s, a, \\theta) \\right) ^2 = (y_i - Q(s, a, \\theta))^2$$\n",
        "\n",
        "However, training a non-linear Deep Neural Network use to be unstable if we apply it naively. Two main ingredients are necessary to stabilize training, namely experience replay and a separately updated target network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FwT97TWjKKgg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DQModel(tf.keras.Model):\n",
        "    def __init__(self, action_size):\n",
        "        super(DQModel, self).__init__()\n",
        "        self.action_size = action_size\n",
        "        self.conv_ft_32_kn_8_str_4_relu = tf.keras.layers.Convolution2D(32, (8, 8), strides=4, padding='same', activation='relu')\n",
        "        self.conv_ft_64_kn_4_str_2_relu = tf.keras.layers.Convolution2D(64, (4, 4), strides=2, padding='same', activation='relu')\n",
        "        self.conv_ft_64_kn_3_str_1_relu = tf.keras.layers.Convolution2D(64, (3, 3), strides=1, padding='same', activation='relu')\n",
        "        self.flatten = tf.keras.layers.Flatten()\n",
        "        self.dense_512_relu = tf.keras.layers.Dense(512, activation='relu')\n",
        "        self.dense_actions_linear = tf.keras.layers.Dense(self.action_size)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x = self.conv_ft_32_kn_8_str_4_relu(inputs)\n",
        "        x = self.conv_ft_64_kn_4_str_2_relu(x)\n",
        "        x = self.conv_ft_64_kn_3_str_1_relu(x)\n",
        "        x = self.flatten(x)\n",
        "        x = self.dense_512_relu(x)\n",
        "        x = self.dense_actions_linear(x)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gwi3YG6Qf-bn",
        "colab_type": "text"
      },
      "source": [
        "### Experince Replay"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "38m57N_Hf-eH",
        "colab_type": "text"
      },
      "source": [
        "This method stores the agent’s experience $(S, A, R, S_{next})$ at each time\n",
        "step in a replay memory that is accessed in minibatches to perform the weight updates. Experience Replay decorrelates the data and leads to better data efficiency (in the end, the agent learns from a wide range of experiences!). \n",
        "Also, neural networks will overfit with correlated experience, so by selecting a random batch of experiences from reply buffer we will reduce the overfitting.\n",
        "\n",
        "At the beginning, the replay buffer is filled with random experience.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xHdl2KP6Ixse",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class experience_replay(object):\n",
        "    def __init__(self, maxlen=2000):\n",
        "        self.buffer = deque(maxlen=maxlen)\n",
        "\n",
        "    def store(self, state, action, reward, next_state, done):\n",
        "        self.buffer.append((state, action, reward, next_state, done))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gmFfx06lhdwY",
        "colab_type": "text"
      },
      "source": [
        "### Target Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E1vslZ9rhhFp",
        "colab_type": "text"
      },
      "source": [
        "A second way to improve DQN stability is by using a separate network to estimate the TD target. \n",
        "\n",
        "Previously, we were using the same $Q$ function for calculating both the target value and the predicted value, which could give raise to strong divergences.\n",
        "To avoid this problem, we use a separate network called a target network for just calculating the target value. \n",
        "\n",
        "This target network has the same architecture as the function approximator but with frozen parameters $\\theta '$. Every T steps (a hyperparameter) the parameters from the Q network are copied to the target network. \n",
        "\n",
        "Under these hypothesis the loss function becomes:\n",
        "\n",
        "$$ l = \\left( r + \\gamma max_{a'} Q(s', a', \\theta') - Q(s, a, \\theta) \\right) ^2$$\n",
        "\n",
        "Notice the $\\theta'$ instead $\\theta$ in the $max$ term of the formula.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pXICU6P_JTv9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class target_network(object):\n",
        "    def __init__(self, model):\n",
        "        self.target_model = model\n",
        "\n",
        "    def copy_model_parameters(self, model):\n",
        "        self.target_model.set_weights(model.get_weights())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eyDYtAjGCSMN",
        "colab_type": "text"
      },
      "source": [
        "### The algorithm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-FpZ4WAljqPj",
        "colab_type": "text"
      },
      "source": [
        "In light of all of above considerations, we can enumerate the steps involved in DQN algorithms as follows:\n",
        "1. First, we preprocess and feed the game screen (state s) to our DQN, which will\n",
        "return the Q values of all possible actions in the state.\n",
        "2. Select an action using the epsilon-greedy policy: with probability\n",
        "$\\epsilon$ select a random action, otherwise, with probability $1-\\epsilon$, select the action which has a maximum Q\n",
        "3. Perform the action in a state s and move to a new state s', receiving a reward. \n",
        "4. Store the transition in the replay buffer as $<s,a,r,s'>$\n",
        "5. Sample some random batches of transitions from the replay buffer and\n",
        "calculate the loss.\n",
        "6. Perform gradient descent with respect to our actual network parameters in\n",
        "order to minimize this loss.\n",
        "8. After every k steps, copy the actual network weights to the target network\n",
        "weights .\n",
        "9. Repeat these steps for M number of episodes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i1Njc4uumwAp",
        "colab_type": "text"
      },
      "source": [
        "__Ex.__ Implement DQN and test your implementation in the MsPacman-v0 env"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mYgErB0pTBkk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DQL(target_network, experience_replay):\n",
        "    def __init__(self, agent, env, maxlen=2000, gamma=0.6, epsilon=0.1, learning_rate=0.01):\n",
        "        target_network.__init__(self, agent)\n",
        "        experience_replay.__init__(self, maxlen)\n",
        "\n",
        "        self.env = env\n",
        "        self.n_actions = self.env.action_space.n\n",
        "\n",
        "        # Initialize parameters\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon\n",
        "\n",
        "        # Optimizer\n",
        "        self.learning_rate = learning_rate\n",
        "        self.opt = tf.keras.optimizers.Adam(lr=learning_rate)\n",
        "\n",
        "        # base and target networks\n",
        "        self.agent = agent\n",
        "        #self.agent(utils.initial_state(state))\n",
        "        self.copy_model_parameters(self.agent)\n",
        "\n",
        "\n",
        "    def initialize_episode(self):\n",
        "        state = self.env.reset()\n",
        "        state = utils.preprocess(state)\n",
        "        state = np.stack([state] * 4, axis=2)\n",
        "        epoch, episodic_reward, episodic_loss, done = 0, 0, [], False\n",
        "        return done, state, epoch, episodic_reward, episodic_loss\n",
        "\n",
        "    def policy(self, state):\n",
        "        if np.random.rand() <= self.epsilon:\n",
        "            return self.env.action_space.sample() \n",
        "        q_values = self.agent(state)\n",
        "        return np.argmax(q_values[0])\n",
        "    \n",
        "    def optimize(self, experiences):\n",
        "        states = np.array(list(x[0] for x in experiences))\n",
        "        actions = np.array(list(x[1] for x in experiences))\n",
        "        rewards = np.array(list(x[2] for x in experiences))\n",
        "        next_states = np.array(list(x[3] for x in experiences))\n",
        "        done = np.array(list(x[4] for x in experiences))\n",
        "        with tf.GradientTape() as tape:\n",
        "            target = self.agent(states)\n",
        "            action_one_hot = tf.one_hot(actions, self.n_actions, 1.0, 0.0)\n",
        "            pred = tf.reduce_sum(target * action_one_hot, axis=-1)\n",
        "            t = self.target_model(next_states)\n",
        "            y = rewards + (1. - done) * self.gamma * tf.reduce_max(t, axis=-1)\n",
        "            loss = tf.keras.losses.MSE(y, pred)\n",
        "\n",
        "        grads = tape.gradient(loss, self.agent.trainable_weights)\n",
        "        self.opt.apply_gradients(zip(grads,\n",
        "                                      self.agent.trainable_weights)) \n",
        "        return loss\n",
        "        \n",
        "    \n",
        "    def run(self, num_episodes=800, batch_size=50, copy_steps=100):\n",
        "        step = 0\n",
        "        for i in range(num_episodes):\n",
        "            done, state, epoch, episodic_reward, episodic_loss = self.initialize_episode()\n",
        "            while not done:\n",
        "                action = self.policy(np.expand_dims(state, axis=0))\n",
        "                next_state, reward, done, _ = self.env.step(action)\n",
        "\n",
        "                # Store this transistion as an experience in the replay buffer\n",
        "                next_state = utils.preprocess(next_state)\n",
        "                next_state = utils.stack_states(state, next_state)\n",
        "                self.buffer.append([state, action, reward, next_state, done])\n",
        "                \n",
        "                # After certain steps, we train our Q network with samples from the experience replay buffer\n",
        "                if step > batch_size:\n",
        "                    minibatch = random.sample(self.buffer, batch_size)\n",
        "                    loss = self.optimize(minibatch)\n",
        "                    episodic_loss.append(loss)\n",
        "\n",
        "                # after some interval we copy our main Q network weights to target Q network\n",
        "                if step % copy_steps == 0 and step > batch_size:\n",
        "                    self.copy_model_parameters(self.agent)\n",
        "\n",
        "                state = next_state\n",
        "                step += 1\n",
        "                episodic_reward += reward\n",
        "            print(i, episodic_reward)\n",
        "      \n",
        "         "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oV8GCVcjZr37",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def runparralel(myDQN, num_episodes=800, batch_size=50, copy_steps=100):\n",
        "        \n",
        "                if num_episodes==1:\n",
        "                    done, state, epoch, episodic_reward, episodic_loss = myDQN.initialize_episode()\n",
        "                    while not done:\n",
        "                        action = myDQN.policy(np.expand_dims(state, axis=0))\n",
        "                        next_state, reward, done, _ = myDQN.env.step(action)\n",
        "\n",
        "                        # Store this transistion as an experience in the replay buffer\n",
        "                        next_state = utils.preprocess(next_state)\n",
        "                        next_state = utils.stack_states(state, next_state)\n",
        "                        myDQN.buffer.append([state, action, reward, next_state, done])\n",
        "                        \n",
        "                        # After certain steps, we train our Q network with samples from the experience replay buffer\n",
        "                        if myDQN.step > batch_size:\n",
        "                            minibatch = random.sample(myDQN.buffer, batch_size)\n",
        "                            loss = myDQN.optimize(minibatch)\n",
        "                            episodic_loss.append(loss)\n",
        "\n",
        "                        # after some interval we copy our main Q network weights to target Q network\n",
        "                        if myDQN.step % copy_steps == 0 and myDQN.step > batch_size:\n",
        "                            myDQN.copy_model_parameters(myDQN.agent)\n",
        "\n",
        "                        state = next_state\n",
        "                        myDQN.step += 1\n",
        "                        episodic_reward += reward\n",
        "                    print(myDQN.step, episodic_reward)\n",
        "                else :\n",
        "                  b=num_episodes//2\n",
        "                  runparralel(myDQN, num_episodes=b, batch_size=50, copy_steps=100)\n",
        "                  runparralel(myDQN, num_episodes=b, batch_size=50, copy_steps=100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gdf1ydU70HFx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "env = environment(\"MsPacman-v0\").env\n",
        "dqnAgent = DQModel(env.action_space.n)\n",
        "dqlearning = DQL(dqnAgent, env)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H2wtV1XkW0Ry",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "fb9a7fb0-b421-4221-d3af-d410865d6357"
      },
      "source": [
        "import _thread\n",
        "import time\n",
        "global step\n",
        "step =0\n",
        "runparralel(myDQN=dqlearning, num_episodes=800)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "Error",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mError\u001b[0m                                     Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-48-eadc15a666c6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mglobal\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mrunparralel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmyDQN\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdqlearning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_episodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m800\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-10-db7c621985de>\u001b[0m in \u001b[0;36mrunparralel\u001b[0;34m(myDQN, num_episodes, batch_size, copy_steps)\u001b[0m\n\u001b[1;32m     31\u001b[0m                 \u001b[0;32melse\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m                   \u001b[0mb\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_episodes\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m                   \u001b[0mrunparralel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmyDQN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_episodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m                   \u001b[0mrunparralel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmyDQN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_episodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-db7c621985de>\u001b[0m in \u001b[0;36mrunparralel\u001b[0;34m(myDQN, num_episodes, batch_size, copy_steps)\u001b[0m\n\u001b[1;32m     31\u001b[0m                 \u001b[0;32melse\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m                   \u001b[0mb\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_episodes\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m                   \u001b[0mrunparralel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmyDQN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_episodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m                   \u001b[0mrunparralel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmyDQN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_episodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-db7c621985de>\u001b[0m in \u001b[0;36mrunparralel\u001b[0;34m(myDQN, num_episodes, batch_size, copy_steps)\u001b[0m\n\u001b[1;32m     31\u001b[0m                 \u001b[0;32melse\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m                   \u001b[0mb\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_episodes\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m                   \u001b[0mrunparralel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmyDQN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_episodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m                   \u001b[0mrunparralel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmyDQN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_episodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-db7c621985de>\u001b[0m in \u001b[0;36mrunparralel\u001b[0;34m(myDQN, num_episodes, batch_size, copy_steps)\u001b[0m\n\u001b[1;32m     31\u001b[0m                 \u001b[0;32melse\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m                   \u001b[0mb\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_episodes\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m                   \u001b[0mrunparralel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmyDQN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_episodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m                   \u001b[0mrunparralel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmyDQN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_episodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-db7c621985de>\u001b[0m in \u001b[0;36mrunparralel\u001b[0;34m(myDQN, num_episodes, batch_size, copy_steps)\u001b[0m\n\u001b[1;32m     31\u001b[0m                 \u001b[0;32melse\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m                   \u001b[0mb\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_episodes\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m                   \u001b[0mrunparralel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmyDQN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_episodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m                   \u001b[0mrunparralel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmyDQN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_episodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-db7c621985de>\u001b[0m in \u001b[0;36mrunparralel\u001b[0;34m(myDQN, num_episodes, batch_size, copy_steps)\u001b[0m\n\u001b[1;32m     31\u001b[0m                 \u001b[0;32melse\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m                   \u001b[0mb\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_episodes\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m                   \u001b[0mrunparralel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmyDQN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_episodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m                   \u001b[0mrunparralel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmyDQN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_episodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-db7c621985de>\u001b[0m in \u001b[0;36mrunparralel\u001b[0;34m(myDQN, num_episodes, batch_size, copy_steps)\u001b[0m\n\u001b[1;32m     31\u001b[0m                 \u001b[0;32melse\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m                   \u001b[0mb\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_episodes\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m                   \u001b[0mrunparralel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmyDQN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_episodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m                   \u001b[0mrunparralel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmyDQN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_episodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-db7c621985de>\u001b[0m in \u001b[0;36mrunparralel\u001b[0;34m(myDQN, num_episodes, batch_size, copy_steps)\u001b[0m\n\u001b[1;32m     31\u001b[0m                 \u001b[0;32melse\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m                   \u001b[0mb\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_episodes\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m                   \u001b[0mrunparralel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmyDQN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_episodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m                   \u001b[0mrunparralel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmyDQN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_episodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-db7c621985de>\u001b[0m in \u001b[0;36mrunparralel\u001b[0;34m(myDQN, num_episodes, batch_size, copy_steps)\u001b[0m\n\u001b[1;32m     31\u001b[0m                 \u001b[0;32melse\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m                   \u001b[0mb\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_episodes\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m                   \u001b[0mrunparralel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmyDQN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_episodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m                   \u001b[0mrunparralel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmyDQN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_episodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-db7c621985de>\u001b[0m in \u001b[0;36mrunparralel\u001b[0;34m(myDQN, num_episodes, batch_size, copy_steps)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mnum_episodes\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m                     \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepisodic_reward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepisodic_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmyDQN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitialize_episode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m                     \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m                         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmyDQN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-cf4146c6fed2>\u001b[0m in \u001b[0;36minitialize_episode\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minitialize_episode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gym/wrappers/monitor.py\u001b[0m in \u001b[0;36mreset\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_before_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m         \u001b[0mobservation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_after_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gym/wrappers/monitor.py\u001b[0m in \u001b[0;36m_before_reset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_before_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menabled\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstats_recorder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbefore_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_after_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gym/wrappers/monitoring/stats_recorder.py\u001b[0m in \u001b[0;36mbefore_reset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdone\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Tried to reset environment which is not done. While the monitor is active for {}, you cannot call reset() unless the episode is over.\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mError\u001b[0m: Tried to reset environment which is not done. While the monitor is active for MsPacman-v0, you cannot call reset() unless the episode is over."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KGlJJUguYo67",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def factorial_recursive(n):\n",
        "    # Base case: 1! = 1\n",
        "    if n == 1:\n",
        "        return 1\n",
        "\n",
        "    # Recursive case: n! = n * (n-1)!\n",
        "    else:\n",
        "        return n * factorial_recursive(n-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZG07gVxzYqWR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "factorial_recursive(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w9JsANeVc8TB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import _thread\n",
        "import time\n",
        "\n",
        "# Define a function for the thread\n",
        "def print_time( threadName, delay):\n",
        "   count = 0\n",
        "   while count < 5:\n",
        "      time.sleep(delay)\n",
        "      count += 1\n",
        "      print(\"{}: {} \".format(threadName, time.ctime(time.time())))\n",
        "\n",
        "global val\n",
        "def sum_rec_inter(i, n):\n",
        "    # Base case: 1! = 1\n",
        "    global val \n",
        "    if i==n:\n",
        "        val=val+n\n",
        "        #print(val,\"\\n\")\n",
        "    #\n",
        "    else:\n",
        "        try:\n",
        "          print(\"*\",i, n,\"*\\n\")\n",
        "          _thread.start_new_thread( sum_rec_inter, ( i, i+(n-i)//2 ) )\n",
        "          _thread.start_new_thread( sum_rec_inter, (1+i+(n-i)//2 , n ) )\n",
        "        except:\n",
        "          print(\"Error: unable to start thread\")\n",
        "\n",
        "# Create two threads as follows\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-mY4K5zpcD5v",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 167
        },
        "outputId": "e5adbe22-07f2-40ad-ede4-19f8de209910"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-98b3a00c4554>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhelp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthreading\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'threading' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NWsJN-F7qWis",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        },
        "outputId": "01fdff1b-21f9-497f-dd27-d469ae1fba3a"
      },
      "source": [
        "val=0\n",
        "sum_rec_inter(0,10)\n",
        "print(\"le resultat est {}\".format(val))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "* 0 10 *\n",
            "\n",
            "le resultat est 0\n",
            "* **0  9 6 5 10 10 *\n",
            " *\n",
            "\n",
            "*\n",
            "\n",
            "\n",
            "* 3 5 *\n",
            "\n",
            "* 6 8 **\n",
            " \n",
            "3*  46  *\n",
            "7\n",
            " *\n",
            "\n",
            "* 0 2 *\n",
            "\n",
            "* 0 1 *\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "saRgJRhpddYr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d2f983e6-dbba-45fc-e428-3a38b7e76174"
      },
      "source": [
        "val"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "55"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MhlYIRkW7odY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d097a612-7561-4c93-ac4a-67742371dd00"
      },
      "source": [
        "val"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "15"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cEXbsWEz4xph",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = \"somevalue\"\n",
        "\n",
        "def func_A ():\n",
        "   global x\n",
        "   # Do things to x\n",
        "   return x\n",
        "\n",
        "def func_B():\n",
        "   x=func_A()\n",
        "   # Do things\n",
        "   return x\n",
        "\n",
        "#func_A()\n",
        "func_B()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UnRCHaibjMSr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import _thread\n",
        "import time\n",
        "\n",
        "# Define a function for the thread\n",
        "def print_time( threadName, delay):\n",
        "   count = 0\n",
        "   while count < 5:\n",
        "      time.sleep(delay)\n",
        "      count += 1\n",
        "      print(\"{}: {} \".format(threadName, time.ctime(time.time())))\n",
        "\n",
        "# Create two threads as follows\n",
        "try:\n",
        "   _thread.start_new_thread( print_time, (\"Thread-1\", 2, ) )\n",
        "   _thread.start_new_thread( print_time, (\"Thread-2\", 4, ) )\n",
        "except:\n",
        "   print(\"Error: unable to start thread\")\n",
        "\n",
        "while 1:\n",
        "   pass\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JVm3WvSNqr7q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dqnAgent.save_weights('/content/drive/My Drive/weightmodelpacman')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-9mwyj39sG0W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dqnAgent.save('my_model.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NYreEqBwrSpG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FFfzaEO0hQOO",
        "colab_type": "text"
      },
      "source": [
        "### Double Q Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T-dR_sQnhWYv",
        "colab_type": "text"
      },
      "source": [
        " DQN tends to overestimate $Q$ values due to its max operation applied to both selecting and estimating actions. \n",
        "\n",
        "To get around this problem, we can use the Q network for selection and the target network for estimation when making updates. \n",
        "\n",
        "In practice, we modify our target function:\n",
        "\n",
        "$$y_i ^{DQN} = r + \\gamma max_{a'} Q(s', a', \\theta') $$\n",
        "\n",
        "as follows:\n",
        "\n",
        "$$y_i ^{DoubleDQN} = r + \\gamma Q(s, argmax Q(s', a', \\hat{\\theta}), \\theta') $$\n",
        "\n",
        "Notice we have two Q functions each with different weights $\\theta'$ and $\\hat{\\theta}$. While one is used to select the best action, the other one is used to evaluate the action, and vice versa."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rlUMOoTJj3j2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DoubleDQL(DQL):\n",
        "    def __init__(self, agent, env, maxlen=2000, gamma=0.6, epsilon=0.1, learning_rate=0.01):\n",
        "        DQL.__init__(self, agent, env, maxlen, gamma, epsilon, learning_rate)\n",
        "\n",
        "    def optimize(self, experiences):\n",
        "        states = np.array(list(x[0] for x in experiences))\n",
        "        actions = np.array(list(x[1] for x in experiences))\n",
        "        rewards = np.array(list(x[2] for x in experiences))\n",
        "        next_states = np.array(list(x[3] for x in experiences))\n",
        "        done = np.array(list(x[4] for x in experiences))\n",
        "        with tf.GradientTape() as tape:\n",
        "            target = self.agent(states)\n",
        "            action_one_hot = tf.one_hot(actions, self.n_actions, 1.0, 0.0)\n",
        "            pred = tf.reduce_sum(target * action_one_hot, axis=-1)\n",
        "\n",
        "            actions_next = tf.argmax(self.agent(next_states), axis=-1)\n",
        "            t = self.target_model(next_states)\n",
        "            q_next = np.array(list(t[i, j] for i, j in enumerate(actions_next)))\n",
        "            y = rewards + (1. - done) * self.gamma * q_next\n",
        "            loss = tf.keras.losses.MSE(y, pred)\n",
        "\n",
        "        grads = tape.gradient(loss, self.agent.trainable_weights)\n",
        "        self.opt.apply_gradients(zip(grads, self.agent.trainable_weights)) \n",
        "        return loss\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lNmcdKKr0LOU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "env = environment(\"MsPacman-v0\").env\n",
        "dqnAgent = DQModel(env.action_space.n)\n",
        "dqlearning = DoubleDQL(dqnAgent, env)\n",
        "dqlearning.run()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P7A7hsIYhWb_",
        "colab_type": "text"
      },
      "source": [
        "### Dueling architecture"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ny7gyhMNhWpK",
        "colab_type": "text"
      },
      "source": [
        "Dueling Q-network architecture is charecterizes by dividing the fully connected layer at the end of DQN into two branches, one for predicting the state value, V, and the other for predicting the advantage, A. Remember that the advantage function specifies how good it is for an agent to perform an action a compared to other actions.\n",
        "\n",
        "![](https://lilianweng.github.io/lil-log/assets/images/dueling-q-network.png)\n",
        "\n",
        "The Q-value is then reconstructed as \n",
        "\n",
        "$$Q(s,a)=V(s)+A(s,a)$$\n",
        "\n",
        "To make sure the estimated advantage values sum up to zero, $\\sum_a A(s,a) \\pi(a|s)=0$, we deduct the mean value from the prediction.\n",
        "\n",
        "$$Q(s,a)=V(s)+\\left(A(s,a)−\\frac{1}{|A|}\\sum_a A(s,a)\\right) $$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3anBy6XnMvyI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DuelingDQModel(tf.keras.Model):\n",
        "    def __init__(self, action_size):\n",
        "        super(DuelingDQModel, self).__init__()\n",
        "        self.action_size = action_size\n",
        "        self.conv_32_8_8_str4_relu = tf.keras.layers.Convolution2D(32, (8, 8), strides=4, padding='same', activation='relu')\n",
        "        self.conv_64_4_4_str2_relu = tf.keras.layers.Convolution2D(64, (4, 4), strides=2, padding='same', activation='relu')\n",
        "        self.conv_64_3_3_str1_relu = tf.keras.layers.Convolution2D(64, (3, 3), strides=1, padding='same', activation='relu')\n",
        "        self.flatten = tf.keras.layers.Flatten()\n",
        "        self.dense_512_relu_state_value = tf.keras.layers.Dense(512, activation='relu')\n",
        "        self.dense_512_relu_advantage = tf.keras.layers.Dense(512, activation='relu')\n",
        "        self.dense_1_linear_state_value = tf.keras.layers.Dense(1)\n",
        "        self.dense_actions_linear_advantage = tf.keras.layers.Dense(self.action_size)\n",
        "        self.lambda_mean_advantage = tf.keras.layers.Lambda(lambda x: tf.reduce_mean(x, axis=1, keepdims=True))\n",
        "        self.merge_subs_advantage = tf.keras.layers.Subtract()\n",
        "        self.dense_actions_q = tf.keras.layers.Add()\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x = self.conv_32_8_8_str4_relu(inputs)\n",
        "        x = self.conv_64_4_4_str2_relu(x)\n",
        "        x = self.conv_64_3_3_str1_relu(x)\n",
        "        x = self.flatten(x)\n",
        "        state_value = self.dense_512_relu_state_value(x)\n",
        "        state_value = self.dense_1_linear_state_value(state_value)\n",
        "        advantage = self.dense_512_relu_advantage(x)\n",
        "        advantage = self.dense_actions_linear_advantage(advantage)\n",
        "        mean_advantage = self.lambda_mean_advantage(advantage)\n",
        "        adv = self.merge_subs_advantage([advantage, mean_advantage])\n",
        "        q = self.dense_actions_q([state_value, adv])\n",
        "        return q"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YfDSXYjE0Oi3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "env = environment(\"MsPacman-v0\").env\n",
        "dqnAgent = DuelingDQModel(env.action_space.n)\n",
        "dqlearning = DQL(dqnAgent, env)\n",
        "dqlearning.run()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
