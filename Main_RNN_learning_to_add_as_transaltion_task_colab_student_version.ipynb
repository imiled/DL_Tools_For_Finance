{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "name": "Main_RNN_learning_to_add_as_transaltion_task_colab_student_version.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/imiled/DL_Tools_For_Finance/blob/master/Main_RNN_learning_to_add_as_transaltion_task_colab_student_version.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KEHaOY2MpCzJ",
        "colab_type": "text"
      },
      "source": [
        "# An implementation of sequence to sequence learning to perform addition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hSw07AnWn45K",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "outputId": "6e1288bd-5bfd-451c-a71c-e0deda889e54"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3tmD7QSqcyf8",
        "colab_type": "text"
      },
      "source": [
        "## Sequence to sequence models\n",
        "\n",
        "RNNs can be used for all sort of tasks, as illustrated in the image bellow:\n",
        "\n",
        "![](http://cntk.ai/jup/paradigms.jpg)\n",
        "\n",
        "Among these, one of the most prominent uses of RNNs are the Sequence to Sequence models, where one *translates* a sequence into another.\n",
        "\n",
        "![](https://jeddy92.github.io/images/ts_intro/seq2seq_ts.png)\n",
        "\n",
        "It can be used for time series multi-step prediction, as above, or for several #NLProc tasks:\n",
        "\n",
        "* Question answering:\n",
        "\n",
        "![](https://static.wixstatic.com/media/bede95_222a9958cbc94b8a800ee13486dadd7b~mv2.png)\n",
        "\n",
        "* Language translation:\n",
        "\n",
        "![](http://cntk.ai/jup/s2s.png)\n",
        "\n",
        "* Image captioning:\n",
        "\n",
        "![](https://i.pinimg.com/originals/0f/72/68/0f7268c8e7a59adca8c0cba848891a76.png)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tpcCP1SppOfh",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "## Encoder-Decoder LSTM\n",
        "\n",
        "An encoder-decoder LSTM is a model comprised of two sub-models: one called the encoder that reads the input sequences and compresses it to a fixed-length internal representation, and an output model called the decoder that interprets the internal representation and uses it to predict the output sequence.\n",
        "\n",
        "\n",
        "**Example:**\n",
        "<br></br>\n",
        "Given the input: \"535+61\", the network should predict the ouput: \"596\" character by character.\n",
        "Padding is handled by using a repeated sentinel character (space).\n",
        "\n",
        "Input may optionally be reversed, shown to increase performance in many tasks in\n",
        "[Learning to Execute](http://arxiv.org/abs/1410.4615)\n",
        "and\n",
        "[Sequence to Sequence Learning with Neural Networks](http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf). Theoretically it introduces shorter term dependencies between source and target.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I7wk4v6yg4l5",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "Below you can find some expected results.\n",
        "\n",
        "Two digits reversed:\n",
        "+ One layer LSTM (128 HN), 5k training examples = 99% train/test accuracy in 55 epochs\n",
        "\n",
        "Three digits reversed:\n",
        "+ One layer LSTM (128 HN), 50k training examples = 99% train/test accuracy in 100 epochs\n",
        "\n",
        "Four digits reversed:\n",
        "+ One layer LSTM (128 HN), 400k training examples = 99% train/test accuracy in 20 epochs\n",
        "\n",
        "Five digits reversed:\n",
        "+ One layer LSTM (128 HN), 550k training examples = 99% train/test accuracy in 30 epochs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JR3IxZNLspSf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f497163a-36cc-4985-a4c0-5631544ea630"
      },
      "source": [
        "%tensorflow_version 2.x"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W-OVc181srbF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "try:\n",
        "    from tensorflow.python.util import module_wrapper as deprecation\n",
        "except ImportError:\n",
        "    from tensorflow.python.util import deprecation_wrapper as deprecation\n",
        "deprecation._PER_MODULE_WARNING_LIMIT = 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RsXcEw16cyf-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from six.moves import range\n",
        "import sys\n",
        "import tensorflow as tf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QItHN0DocygC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras import layers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s0rBfIoVwsYR",
        "colab_type": "text"
      },
      "source": [
        "## 1-hot character encoder for sequences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cTGlss4RcygF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CharacterTable(object):\n",
        "    \"\"\"Given a set of characters:\n",
        "    + Encode them to a one-hot integer representation\n",
        "    + Decode the one-hot or integer representation to their character output\n",
        "    + Decode a vector of probabilities to their character output\n",
        "    \"\"\"\n",
        "    def __init__(self, chars):\n",
        "        \"\"\"Initialize character table.\n",
        "\n",
        "        # Arguments\n",
        "            chars: Characters that can appear in the input.\n",
        "        \"\"\"\n",
        "        self.chars = sorted(set(chars))\n",
        "        self.char_indices = dict((c, i) for i, c in enumerate(self.chars))\n",
        "        self.indices_char = dict((i, c) for i, c in enumerate(self.chars))\n",
        "\n",
        "    def encode(self, C, num_rows):\n",
        "        \"\"\"One-hot encode given string C.\n",
        "\n",
        "        # Arguments\n",
        "            C: string, to be encoded.\n",
        "            num_rows: Number of rows in the returned one-hot encoding. This is\n",
        "                used to keep the # of rows for each data the same.\n",
        "        \"\"\"\n",
        "        x = np.zeros((num_rows, len(self.chars)))\n",
        "        for i, c in enumerate(C):\n",
        "            x[i, self.char_indices[c]] = 1\n",
        "        return x\n",
        "\n",
        "    def decode(self, x, calc_argmax=True):\n",
        "        \"\"\"Decode the given vector or 2D array to their character output.\n",
        "\n",
        "        # Arguments\n",
        "            x: A vector or a 2D array of probabilities or one-hot representations;\n",
        "                or a vector of character indices (used with `calc_argmax=False`).\n",
        "            calc_argmax: Whether to find the character index with maximum\n",
        "                probability, defaults to `True`.\n",
        "        \"\"\"\n",
        "        if calc_argmax:\n",
        "            x = x.argmax(axis=-1)\n",
        "        return ''.join(self.indices_char[x] for x in x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tbwhviINcygH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# All the numbers, plus sign and space for padding.\n",
        "chars = '0123456789+ '\n",
        "ctable = CharacterTable(chars)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EhG3171qcygK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# try encoding something\n",
        "str_ = '0 +'\n",
        "encoded=ctable.encode(str_,num_rows=6)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zyc2o4aFcygM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5a2da920-9754-4f68-cfdd-4f2580336615"
      },
      "source": [
        "# now, decode it!\n",
        "ctable.decode(encoded)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'0 +   '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yQCylGjkcygP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define colors to pretty print our results\n",
        "class colors:\n",
        "    ok = '\\033[92m'\n",
        "    fail = '\\033[91m'\n",
        "    close = '\\033[0m'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6MH8a3rFw3S8",
        "colab_type": "text"
      },
      "source": [
        "## Generate training sequences\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P83FhHPYcygR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_training(TRAINING_SIZE = 50000, DIGITS = 3, REVERSE = True):\n",
        "\n",
        "    # Maximum length of input is 'int + int' (e.g., '345+678'). Maximum length of\n",
        "    # int is DIGITS.\n",
        "    MAXLEN = DIGITS + 1 + DIGITS\n",
        "\n",
        "    questions = []\n",
        "    answers = []\n",
        "    seen = set()    #enleve les dupliquer\n",
        "    print('Generating data...')\n",
        "    while len(questions) < TRAINING_SIZE:\n",
        "        f = lambda: int(''.join(np.random.choice(list('0123456789'))\n",
        "                        for i in range(np.random.randint(1, DIGITS + 1))))\n",
        "        a, b = f(), f()\n",
        "        # Skip any addition questions we've already seen\n",
        "        # Also skip any such that x+Y == Y+x (hence the sorting).\n",
        "        key = tuple(sorted((a, b))) #ordenar los datos\n",
        "        if key in seen:\n",
        "            continue\n",
        "        seen.add(key)\n",
        "        # Pad the data with spaces such that it is always MAXLEN.\n",
        "        #padding=pour que les sequences ont la meme taille\n",
        "        q = '{}+{}'.format(a, b)\n",
        "        query = q + ' ' * (MAXLEN - len(q))\n",
        "        ans = str(a + b)\n",
        "        # Answers can be of maximum size DIGITS + 1.\n",
        "        ans += ' ' * (DIGITS + 1 - len(ans))\n",
        "        if REVERSE:\n",
        "            # Reverse the query, e.g., '12+345  ' becomes '  543+21'. (Note the\n",
        "            # space used for padding.)\n",
        "            query = query[::-1]\n",
        "        questions.append(query)\n",
        "        answers.append(ans)\n",
        "    print('Total addition questions:', len(questions))\n",
        "    out = np.vstack((np.array(questions), np.array(answers)))\n",
        "    return out.T"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TaKT6tUzcygT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "c7709647-8805-42dc-f7ee-815d40889399"
      },
      "source": [
        "train3char = generate_training(TRAINING_SIZE=50000, DIGITS=3, REVERSE=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Generating data...\n",
            "Total addition questions: 50000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yoExko9IcygX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6a58e483-5033-4fa7-be6d-2c00c6d77f78"
      },
      "source": [
        "train3char.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(50000, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HqVjqGxzcygb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "99316d98-96d6-462c-dabd-ce604335a3b7"
      },
      "source": [
        "# let's see some samples\n",
        "r = np.random.randint(low=0, high=train3char.shape[0], size=4)\n",
        "train3char[r, :]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([['501+21 ', '522 '],\n",
              "       ['563+908', '1471'],\n",
              "       ['393+57 ', '450 '],\n",
              "       ['680+10 ', '690 ']], dtype='<U7')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sqKodpyHLGyh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "598b631d-405e-4b12-f6bb-d5600714421c"
      },
      "source": [
        "ctable.encode('99',2).shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2, 12)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DBMem8bvg_wH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "829d7c01-ae9e-4698-8c2a-db66370f3033"
      },
      "source": [
        "ctable"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[' ', '+', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5KMEukqcLGTE",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MYX2lXkjw98-",
        "colab_type": "text"
      },
      "source": [
        "### Vectorize training sample with the character encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aKNLAXb3cyge",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def vectorization(train, DIGITS, voc_len, encoder):\n",
        "    \"\"\"\n",
        "    In this method we use the encoding table to convert characters to vectors.\n",
        "    \"\"\"\n",
        "    MAXLEN = 2*DIGITS+1\n",
        "    print('Vectorization...')\n",
        "    x = np.zeros(shape=(train.shape[0], MAXLEN, voc_len), dtype=np.float)\n",
        "    y = np.zeros(shape=(train.shape[0],DIGITS+1,  voc_len), dtype=np.float)\n",
        "    # encode questions\n",
        "    for i, sentence in enumerate(train[:, 0]):\n",
        "        x[i] = encoder.encode(sentence, MAXLEN)\n",
        "    # encode answers\n",
        "    for i, sentence in enumerate(train[:, 1]):\n",
        "        y[i] =  encoder.encode(sentence, DIGITS+1)\n",
        "\n",
        "    # Shuffle (x, y) in unison as the later parts of x will almost all be larger\n",
        "    # digits.\n",
        "    indices = np.arange(len(y))\n",
        "    np.random.shuffle(indices)\n",
        "    x = x[indices]\n",
        "    y = y[indices]\n",
        "\n",
        "    # Explicitly set apart 10% for validation data that we never train over.\n",
        "    split_at = len(x) - len(x) // 10\n",
        "    (x_train, x_val) = x[:split_at], x[split_at:]\n",
        "    (y_train, y_val) = y[:split_at], y[split_at:]\n",
        "\n",
        "    print('Shapes in training Data:')\n",
        "    print(x_train.shape)\n",
        "    print(y_train.shape)\n",
        "\n",
        "    print('Shapes in validation Data:')\n",
        "    print(x_val.shape)\n",
        "    print(y_val.shape)\n",
        "    return x_train, y_train, x_val, y_val"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pnrm42Iycygg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "outputId": "01d56031-3973-4d1a-be0b-901bb5d4c8af"
      },
      "source": [
        "x_train_3char, y_train_3char, x_val_3char, y_val_3char = vectorization(train3char, DIGITS=3, voc_len=len(chars), encoder=ctable)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vectorization...\n",
            "Shapes in training Data:\n",
            "(45000, 7, 12)\n",
            "(45000, 4, 12)\n",
            "Shapes in validation Data:\n",
            "(5000, 7, 12)\n",
            "(5000, 4, 12)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CYEG6FK_MTaK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZffS9MEC1Fbp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "outputId": "d4f402b2-bae4-4157-cb02-d88de026431e"
      },
      "source": [
        "x_train_3char[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
              "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
              "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_xQ1dBodNJST",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aaEJUt31xDvI",
        "colab_type": "text"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dksg0YlyxHGm",
        "colab_type": "text"
      },
      "source": [
        "### Model definition\n",
        "\n",
        "The encoder consists of a single RNN that compress the input sequence into a single vector.\n",
        "\n",
        "Then, we repeat this vector to feed it to the the decoding RNN.\n",
        "Finally, we use a Dense layer (an embedding, shared across all output timestamps) to produce the output characters. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8b8Ex62oOdQC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.layers import GRU, LSTM, RNN,RepeatVector, TimeDistributed, Dense\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.optimizers import RMSprop"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D-7AUNWZcygi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_model(chars, rnn_type='gru', DIGITS=3, HIDDEN_SIZE=10,  DECODER_LAYERS=1):\n",
        "    tf.keras.backend.clear_session()\n",
        "    MAXLEN = 2*DIGITS + 1\n",
        "    if rnn_type.lower() == 'gru':\n",
        "        rnn = GRU\n",
        "    elif rnn_type.lower() == 'lstm':\n",
        "        rnn = LSTM\n",
        "    elif rnn_type.lower() == 'rnn':\n",
        "        rnn = RNN\n",
        "    else:\n",
        "        print('{rnn_type} RNN type not covered'.format(rnn_type=rnn_type))\n",
        "        sys.exit(0)\n",
        "        \n",
        "    print('Build model...')\n",
        "    model = Sequential()\n",
        "    model.add(rnn(units=HIDDEN_SIZE, input_shape=(MAXLEN, len(chars))))\n",
        "    model.add(RepeatVector(DIGITS+1))\n",
        "\n",
        "    # \"Encode\" the input sequence using an RNN, producing an output of HIDDEN_SIZE.\n",
        "    # Note: In a situation where your input sequences have a variable length,\n",
        "    # use input_shape=(None, num_feature).\n",
        "    \n",
        "    # As the decoder RNN's input, repeatedly provide with the last output of\n",
        "    # RNN for each time step. Repeat 'DIGITS + 1' times as that's the maximum\n",
        "    # length of output, e.g., when DIGITS=3, max output is 999+999=1998.\n",
        "    # for this, use layers.RepeatVector()\n",
        "  \n",
        "    \n",
        "    # The decoder RNN could be multiple layers stacked or a single layer.\n",
        "    for _ in range(DECODER_LAYERS):\n",
        "        # By setting return_sequences to True, return not only the last output but\n",
        "        # all the outputs so far in the form of (num_samples, timesteps,\n",
        "        # output_dim). This is necessary as TimeDistributed in the below expects\n",
        "        # the first dimension to be the timesteps.\n",
        "          model.add(rnn(HIDDEN_SIZE, return_sequences=True))\n",
        " \n",
        "\n",
        "    # Apply a dense layer to the every temporal slice of an input. For each of step\n",
        "    # of the output sequence, decide which character should be chosen.\n",
        "    dense=Dense(units=len(chars), activation='softmax')\n",
        "    model.add(TimeDistributed(dense))\n",
        "    model.compile(loss='categorical_crossentropy',\n",
        "                  optimizer=RMSprop(learning_rate=0.1, clip=1),\n",
        "                  metrics=['categorical_accuracy'])\n",
        "    model.summary()\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ENx_6M4Icygk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e092eb98-593d-489b-bb37-2c29fdd85c65"
      },
      "source": [
        "model3char_gru_10 = build_model(chars, rnn_type='gru', DIGITS=3, HIDDEN_SIZE=10, DECODER_LAYERS=1)\n",
        "model3char_gru_20 = build_model(chars, rnn_type='gru', DIGITS=3, HIDDEN_SIZE=20, DECODER_LAYERS=1)\n",
        "model3char_lstm_10 = build_model(chars, rnn_type='lstm', DIGITS=3, HIDDEN_SIZE=10, DECODER_LAYERS=1)\n",
        "model3char_lstm_20 = build_model(chars, rnn_type='lstm', DIGITS=3, HIDDEN_SIZE=20, DECODER_LAYERS=1)\n",
        "\n",
        "model3char_gru_10_dec10 = build_model(chars, rnn_type='gru', DIGITS=3, HIDDEN_SIZE=10, DECODER_LAYERS=10)\n",
        "model3char_gru_40_dec10 = build_model(chars, rnn_type='gru', DIGITS=3, HIDDEN_SIZE=40, DECODER_LAYERS=10)\n",
        "model3char_lstm_10_dec4 = build_model(chars, rnn_type='lstm', DIGITS=3, HIDDEN_SIZE=10, DECODER_LAYERS=4)\n",
        "model3char_lstm_20_dec4 = build_model(chars, rnn_type='lstm', DIGITS=3, HIDDEN_SIZE=20, DECODER_LAYERS=4)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Build model...\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "gru (GRU)                    (None, 10)                720       \n",
            "_________________________________________________________________\n",
            "repeat_vector (RepeatVector) (None, 4, 10)             0         \n",
            "_________________________________________________________________\n",
            "gru_1 (GRU)                  (None, 4, 10)             660       \n",
            "_________________________________________________________________\n",
            "time_distributed (TimeDistri (None, 4, 12)             132       \n",
            "=================================================================\n",
            "Total params: 1,512\n",
            "Trainable params: 1,512\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Build model...\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "gru (GRU)                    (None, 20)                2040      \n",
            "_________________________________________________________________\n",
            "repeat_vector (RepeatVector) (None, 4, 20)             0         \n",
            "_________________________________________________________________\n",
            "gru_1 (GRU)                  (None, 4, 20)             2520      \n",
            "_________________________________________________________________\n",
            "time_distributed (TimeDistri (None, 4, 12)             252       \n",
            "=================================================================\n",
            "Total params: 4,812\n",
            "Trainable params: 4,812\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Build model...\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm (LSTM)                  (None, 10)                920       \n",
            "_________________________________________________________________\n",
            "repeat_vector (RepeatVector) (None, 4, 10)             0         \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 4, 10)             840       \n",
            "_________________________________________________________________\n",
            "time_distributed (TimeDistri (None, 4, 12)             132       \n",
            "=================================================================\n",
            "Total params: 1,892\n",
            "Trainable params: 1,892\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Build model...\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm (LSTM)                  (None, 20)                2640      \n",
            "_________________________________________________________________\n",
            "repeat_vector (RepeatVector) (None, 4, 20)             0         \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 4, 20)             3280      \n",
            "_________________________________________________________________\n",
            "time_distributed (TimeDistri (None, 4, 12)             252       \n",
            "=================================================================\n",
            "Total params: 6,172\n",
            "Trainable params: 6,172\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Build model...\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "gru (GRU)                    (None, 10)                720       \n",
            "_________________________________________________________________\n",
            "repeat_vector (RepeatVector) (None, 4, 10)             0         \n",
            "_________________________________________________________________\n",
            "gru_1 (GRU)                  (None, 4, 10)             660       \n",
            "_________________________________________________________________\n",
            "gru_2 (GRU)                  (None, 4, 10)             660       \n",
            "_________________________________________________________________\n",
            "gru_3 (GRU)                  (None, 4, 10)             660       \n",
            "_________________________________________________________________\n",
            "gru_4 (GRU)                  (None, 4, 10)             660       \n",
            "_________________________________________________________________\n",
            "gru_5 (GRU)                  (None, 4, 10)             660       \n",
            "_________________________________________________________________\n",
            "gru_6 (GRU)                  (None, 4, 10)             660       \n",
            "_________________________________________________________________\n",
            "gru_7 (GRU)                  (None, 4, 10)             660       \n",
            "_________________________________________________________________\n",
            "gru_8 (GRU)                  (None, 4, 10)             660       \n",
            "_________________________________________________________________\n",
            "gru_9 (GRU)                  (None, 4, 10)             660       \n",
            "_________________________________________________________________\n",
            "gru_10 (GRU)                 (None, 4, 10)             660       \n",
            "_________________________________________________________________\n",
            "time_distributed (TimeDistri (None, 4, 12)             132       \n",
            "=================================================================\n",
            "Total params: 7,452\n",
            "Trainable params: 7,452\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Build model...\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "gru (GRU)                    (None, 40)                6480      \n",
            "_________________________________________________________________\n",
            "repeat_vector (RepeatVector) (None, 4, 40)             0         \n",
            "_________________________________________________________________\n",
            "gru_1 (GRU)                  (None, 4, 40)             9840      \n",
            "_________________________________________________________________\n",
            "gru_2 (GRU)                  (None, 4, 40)             9840      \n",
            "_________________________________________________________________\n",
            "gru_3 (GRU)                  (None, 4, 40)             9840      \n",
            "_________________________________________________________________\n",
            "gru_4 (GRU)                  (None, 4, 40)             9840      \n",
            "_________________________________________________________________\n",
            "gru_5 (GRU)                  (None, 4, 40)             9840      \n",
            "_________________________________________________________________\n",
            "gru_6 (GRU)                  (None, 4, 40)             9840      \n",
            "_________________________________________________________________\n",
            "gru_7 (GRU)                  (None, 4, 40)             9840      \n",
            "_________________________________________________________________\n",
            "gru_8 (GRU)                  (None, 4, 40)             9840      \n",
            "_________________________________________________________________\n",
            "gru_9 (GRU)                  (None, 4, 40)             9840      \n",
            "_________________________________________________________________\n",
            "gru_10 (GRU)                 (None, 4, 40)             9840      \n",
            "_________________________________________________________________\n",
            "time_distributed (TimeDistri (None, 4, 12)             492       \n",
            "=================================================================\n",
            "Total params: 105,372\n",
            "Trainable params: 105,372\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Build model...\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm (LSTM)                  (None, 10)                920       \n",
            "_________________________________________________________________\n",
            "repeat_vector (RepeatVector) (None, 4, 10)             0         \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 4, 10)             840       \n",
            "_________________________________________________________________\n",
            "lstm_2 (LSTM)                (None, 4, 10)             840       \n",
            "_________________________________________________________________\n",
            "lstm_3 (LSTM)                (None, 4, 10)             840       \n",
            "_________________________________________________________________\n",
            "lstm_4 (LSTM)                (None, 4, 10)             840       \n",
            "_________________________________________________________________\n",
            "time_distributed (TimeDistri (None, 4, 12)             132       \n",
            "=================================================================\n",
            "Total params: 4,412\n",
            "Trainable params: 4,412\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Build model...\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm (LSTM)                  (None, 20)                2640      \n",
            "_________________________________________________________________\n",
            "repeat_vector (RepeatVector) (None, 4, 20)             0         \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 4, 20)             3280      \n",
            "_________________________________________________________________\n",
            "lstm_2 (LSTM)                (None, 4, 20)             3280      \n",
            "_________________________________________________________________\n",
            "lstm_3 (LSTM)                (None, 4, 20)             3280      \n",
            "_________________________________________________________________\n",
            "lstm_4 (LSTM)                (None, 4, 20)             3280      \n",
            "_________________________________________________________________\n",
            "time_distributed (TimeDistri (None, 4, 12)             252       \n",
            "=================================================================\n",
            "Total params: 16,012\n",
            "Trainable params: 16,012\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bBQoVmucXOc2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#comparar con el time sereie"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZdpKBiANzmqM",
        "colab_type": "text"
      },
      "source": [
        "### Model training\n",
        "\n",
        "In each epoch, print how many characters are we guessing correctly"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xQquFw3Ncygn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model, x_train, y_train, x_val, y_val,  n_epochs=20, BATCH_SIZE=128, REVERSE=False):\n",
        "    # Train the model each generation and show predictions against the validation\n",
        "    # dataset.\n",
        "    for iteration in range(1, n_epochs+1):\n",
        "        print()\n",
        "        print('-' * 50)\n",
        "        print('Iteration', iteration)\n",
        "        # fit, providing the validation data that we have created earlier\n",
        "        model.fit(x_train,y_train,epochs=n_epochs, batch_size=BATCH_SIZE, validation_data=[x_val, y_val])\n",
        "        \n",
        "        # Select N samples from the validation set at random so we can visualize\n",
        "        # errors.\n",
        "        N=5\n",
        "        for i in range(N):\n",
        "            ind = np.random.randint(0, len(x_val))\n",
        "            rowx, rowy = x_val[np.array([ind])], y_val[np.array([ind])]\n",
        "            q = ctable.decode(rowx[0])\n",
        "            correct = ctable.decode(rowy[0])\n",
        "\n",
        "            preds = model.predict_classes(rowx, batch_size=1, verbose=1)\n",
        "            guess = ctable.decode(preds[0], calc_argmax=False)\n",
        "            \n",
        "            print('Question: ', q[::-1] if REVERSE else q, end=' ')\n",
        "            print('Answer: ', correct, end=' ')\n",
        "            if correct == guess:\n",
        "                print(colors.ok + '☑' + colors.close, end=' ')\n",
        "            else:\n",
        "                print(colors.fail + '☒' + colors.close, end=' ')\n",
        "            print('Guess: ', guess, end='\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2j3W6S1lku3P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "  model3char.fit(x_train_3char,y_train_3char,epochs=1, batch_size=128, validation_data=[x_val_3char, y_val_3char])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "RjXg-uNWcygp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "2b5d3836-c9c6-4db1-bd33-60210ebaa2ad"
      },
      "source": [
        "# train \n",
        "x=train(model3char_lstm_10_dec4, x_train_3char, y_train_3char, x_val_3char, y_val_3char,  n_epochs=15, BATCH_SIZE=128, REVERSE=False)\n",
        "    "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "--------------------------------------------------\n",
            "Iteration 1\n",
            "Train on 45000 samples, validate on 5000 samples\n",
            "Epoch 1/15\n",
            "45000/45000 [==============================] - 6s 125us/sample - loss: 1.0533 - categorical_accuracy: 0.5857 - val_loss: 1.1027 - val_categorical_accuracy: 0.5616\n",
            "Epoch 2/15\n",
            "45000/45000 [==============================] - 6s 125us/sample - loss: 1.0501 - categorical_accuracy: 0.5863 - val_loss: 1.0039 - val_categorical_accuracy: 0.6054\n",
            "Epoch 3/15\n",
            "45000/45000 [==============================] - 6s 124us/sample - loss: 1.0439 - categorical_accuracy: 0.5886 - val_loss: 1.0514 - val_categorical_accuracy: 0.5840\n",
            "Epoch 4/15\n",
            "45000/45000 [==============================] - 6s 123us/sample - loss: 1.0411 - categorical_accuracy: 0.5892 - val_loss: 1.0175 - val_categorical_accuracy: 0.5993\n",
            "Epoch 5/15\n",
            "45000/45000 [==============================] - 6s 126us/sample - loss: 1.0379 - categorical_accuracy: 0.5913 - val_loss: 1.1190 - val_categorical_accuracy: 0.5728\n",
            "Epoch 6/15\n",
            "45000/45000 [==============================] - 6s 126us/sample - loss: 1.0322 - categorical_accuracy: 0.5931 - val_loss: 1.0232 - val_categorical_accuracy: 0.5972\n",
            "Epoch 7/15\n",
            "45000/45000 [==============================] - 6s 125us/sample - loss: 1.0285 - categorical_accuracy: 0.5949 - val_loss: 1.0715 - val_categorical_accuracy: 0.5820\n",
            "Epoch 8/15\n",
            "45000/45000 [==============================] - 6s 125us/sample - loss: 1.0264 - categorical_accuracy: 0.5945 - val_loss: 1.0097 - val_categorical_accuracy: 0.6079\n",
            "Epoch 9/15\n",
            "45000/45000 [==============================] - 6s 125us/sample - loss: 1.0225 - categorical_accuracy: 0.5962 - val_loss: 1.0021 - val_categorical_accuracy: 0.6061\n",
            "Epoch 10/15\n",
            "45000/45000 [==============================] - 6s 124us/sample - loss: 1.0190 - categorical_accuracy: 0.5980 - val_loss: 0.9722 - val_categorical_accuracy: 0.6126\n",
            "Epoch 11/15\n",
            "45000/45000 [==============================] - 6s 125us/sample - loss: 1.0131 - categorical_accuracy: 0.5996 - val_loss: 1.0600 - val_categorical_accuracy: 0.5788\n",
            "Epoch 12/15\n",
            "45000/45000 [==============================] - 6s 124us/sample - loss: 1.0117 - categorical_accuracy: 0.5998 - val_loss: 1.0492 - val_categorical_accuracy: 0.5840\n",
            "Epoch 13/15\n",
            "45000/45000 [==============================] - 6s 125us/sample - loss: 1.0091 - categorical_accuracy: 0.6019 - val_loss: 1.0497 - val_categorical_accuracy: 0.5914\n",
            "Epoch 14/15\n",
            "45000/45000 [==============================] - 6s 124us/sample - loss: 1.0056 - categorical_accuracy: 0.6033 - val_loss: 1.0016 - val_categorical_accuracy: 0.6002\n",
            "Epoch 15/15\n",
            "45000/45000 [==============================] - 6s 125us/sample - loss: 1.0032 - categorical_accuracy: 0.6032 - val_loss: 1.0436 - val_categorical_accuracy: 0.5853\n",
            "1/1 [==============================] - 0s 9ms/sample\n",
            "Question:  51+135  Answer:  186  \u001b[91m☒\u001b[0m Guess:  180 \n",
            "1/1 [==============================] - 0s 7ms/sample\n",
            "Question:  528+34  Answer:  562  \u001b[91m☒\u001b[0m Guess:  576 \n",
            "1/1 [==============================] - 0s 7ms/sample\n",
            "Question:  504+83  Answer:  587  \u001b[91m☒\u001b[0m Guess:  606 \n",
            "1/1 [==============================] - 0s 7ms/sample\n",
            "Question:  6+75    Answer:  81   \u001b[91m☒\u001b[0m Guess:  86  \n",
            "1/1 [==============================] - 0s 7ms/sample\n",
            "Question:  941+295 Answer:  1236 \u001b[91m☒\u001b[0m Guess:  1260\n",
            "\n",
            "--------------------------------------------------\n",
            "Iteration 2\n",
            "Train on 45000 samples, validate on 5000 samples\n",
            "Epoch 1/15\n",
            "45000/45000 [==============================] - 6s 124us/sample - loss: 0.9986 - categorical_accuracy: 0.6049 - val_loss: 1.0338 - val_categorical_accuracy: 0.5907\n",
            "Epoch 2/15\n",
            "45000/45000 [==============================] - 6s 124us/sample - loss: 0.9971 - categorical_accuracy: 0.6059 - val_loss: 1.0154 - val_categorical_accuracy: 0.6012\n",
            "Epoch 3/15\n",
            "45000/45000 [==============================] - 6s 124us/sample - loss: 0.9918 - categorical_accuracy: 0.6074 - val_loss: 0.9597 - val_categorical_accuracy: 0.6166\n",
            "Epoch 4/15\n",
            "45000/45000 [==============================] - 6s 124us/sample - loss: 0.9916 - categorical_accuracy: 0.6081 - val_loss: 1.0128 - val_categorical_accuracy: 0.5957\n",
            "Epoch 5/15\n",
            "45000/45000 [==============================] - 6s 125us/sample - loss: 0.9864 - categorical_accuracy: 0.6086 - val_loss: 0.9317 - val_categorical_accuracy: 0.6284\n",
            "Epoch 6/15\n",
            "45000/45000 [==============================] - 6s 125us/sample - loss: 0.9883 - categorical_accuracy: 0.6087 - val_loss: 0.9793 - val_categorical_accuracy: 0.6123\n",
            "Epoch 7/15\n",
            "45000/45000 [==============================] - 6s 123us/sample - loss: 0.9842 - categorical_accuracy: 0.6099 - val_loss: 1.0153 - val_categorical_accuracy: 0.6043\n",
            "Epoch 8/15\n",
            "45000/45000 [==============================] - 6s 124us/sample - loss: 0.9812 - categorical_accuracy: 0.6117 - val_loss: 0.9945 - val_categorical_accuracy: 0.6086\n",
            "Epoch 9/15\n",
            "45000/45000 [==============================] - 6s 123us/sample - loss: 0.9787 - categorical_accuracy: 0.6115 - val_loss: 0.9685 - val_categorical_accuracy: 0.6170\n",
            "Epoch 10/15\n",
            "45000/45000 [==============================] - 6s 124us/sample - loss: 0.9761 - categorical_accuracy: 0.6122 - val_loss: 0.9552 - val_categorical_accuracy: 0.6151\n",
            "Epoch 11/15\n",
            "45000/45000 [==============================] - 6s 124us/sample - loss: 0.9757 - categorical_accuracy: 0.6122 - val_loss: 1.0060 - val_categorical_accuracy: 0.6036\n",
            "Epoch 12/15\n",
            "45000/45000 [==============================] - 6s 125us/sample - loss: 0.9720 - categorical_accuracy: 0.6150 - val_loss: 0.9455 - val_categorical_accuracy: 0.6216\n",
            "Epoch 13/15\n",
            "45000/45000 [==============================] - 6s 125us/sample - loss: 0.9728 - categorical_accuracy: 0.6143 - val_loss: 0.9223 - val_categorical_accuracy: 0.6344\n",
            "Epoch 14/15\n",
            "45000/45000 [==============================] - 6s 124us/sample - loss: 0.9697 - categorical_accuracy: 0.6150 - val_loss: 0.9742 - val_categorical_accuracy: 0.6115\n",
            "Epoch 15/15\n",
            "45000/45000 [==============================] - 6s 124us/sample - loss: 0.9673 - categorical_accuracy: 0.6162 - val_loss: 0.9620 - val_categorical_accuracy: 0.6115\n",
            "1/1 [==============================] - 0s 6ms/sample\n",
            "Question:  33+342  Answer:  375  \u001b[91m☒\u001b[0m Guess:  378 \n",
            "1/1 [==============================] - 0s 7ms/sample\n",
            "Question:  526+546 Answer:  1072 \u001b[91m☒\u001b[0m Guess:  1062\n",
            "1/1 [==============================] - 0s 6ms/sample\n",
            "Question:  944+697 Answer:  1641 \u001b[91m☒\u001b[0m Guess:  1618\n",
            "1/1 [==============================] - 0s 6ms/sample\n",
            "Question:  23+705  Answer:  728  \u001b[91m☒\u001b[0m Guess:  729 \n",
            "1/1 [==============================] - 0s 7ms/sample\n",
            "Question:  862+62  Answer:  924  \u001b[91m☒\u001b[0m Guess:  929 \n",
            "\n",
            "--------------------------------------------------\n",
            "Iteration 3\n",
            "Train on 45000 samples, validate on 5000 samples\n",
            "Epoch 1/15\n",
            "45000/45000 [==============================] - 6s 125us/sample - loss: 0.9656 - categorical_accuracy: 0.6159 - val_loss: 0.9770 - val_categorical_accuracy: 0.6110\n",
            "Epoch 2/15\n",
            "45000/45000 [==============================] - 6s 123us/sample - loss: 0.9628 - categorical_accuracy: 0.6187 - val_loss: 1.0123 - val_categorical_accuracy: 0.5993\n",
            "Epoch 3/15\n",
            "45000/45000 [==============================] - 6s 124us/sample - loss: 0.9618 - categorical_accuracy: 0.6183 - val_loss: 1.0949 - val_categorical_accuracy: 0.5763\n",
            "Epoch 4/15\n",
            "45000/45000 [==============================] - 6s 125us/sample - loss: 0.9607 - categorical_accuracy: 0.6175 - val_loss: 1.0170 - val_categorical_accuracy: 0.6006\n",
            "Epoch 5/15\n",
            "45000/45000 [==============================] - 6s 125us/sample - loss: 0.9577 - categorical_accuracy: 0.6188 - val_loss: 0.9718 - val_categorical_accuracy: 0.6117\n",
            "Epoch 6/15\n",
            "45000/45000 [==============================] - 6s 125us/sample - loss: 0.9571 - categorical_accuracy: 0.6197 - val_loss: 0.9673 - val_categorical_accuracy: 0.6108\n",
            "Epoch 7/15\n",
            " 2688/45000 [>.............................] - ETA: 4s - loss: 0.9730 - categorical_accuracy: 0.6137"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "isOKq8i-ocMx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "28169b39-ebce-403f-bab3-d891f59d58b9"
      },
      "source": [
        "ctable.decode(x_val_3char[10]),ctable.decode(y_val_3char[10])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('475+503', '978 ')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fyS7mslcooa-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4XCEHTiwzeNb",
        "colab_type": "text"
      },
      "source": [
        "## Do the same with 5 digits sums.\n",
        "\n",
        "* Generate train and validation sets\n",
        "* Vectorize the datasets\n",
        "* Build the model for handling 5 digit sequences\n",
        "* Train it"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GChcjfJ_cygr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# generate train\n",
        "train5char = ?"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gd95n1Ydcygv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "r = np.random.randint(low=0, high=train5char.shape[0], size=4)\n",
        "train5char[r, :]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1iA2f9Blcygy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# vectorize\n",
        "x_train_5char, y_train_5char, x_val_5char, y_val_5char = ?"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oodvTNrzcyg1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# build the model\n",
        "model5char = ?"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "dAx2UYBscyg4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train\n",
        "?"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qXxDeYDucyg6",
        "colab_type": "text"
      },
      "source": [
        "### Check Five digits reversed:\n",
        "+ With one LSTM layer (128 HN) and 550k training examples, you should be able to reach 99% train/test accuracy within 30 epochs\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SWDAuxdCcyg6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "?"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p3TVcRqy5-Mb",
        "colab_type": "text"
      },
      "source": [
        "## Next steps\n",
        "\n",
        "Wanna here more about Seq2Seq models? There are several resources that might interest you:\n",
        "\n",
        "* [Keras intro](https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html) to Seq2Seq with the functional API. Here, you will learn how to feed the state vectors and 1-char target sequence to the decoder to produce predictions for the next character. I recommend you to try this approach in the weather time-series dataset =)\n",
        "\n",
        "* Checkout this [excellent post](https://machinelearningmastery.com/how-to-develop-lstm-models-for-multi-step-time-series-forecasting-of-household-power-consumption/) where Seq2Seq models are used for time-series forecasting of household power consumption. \n",
        "\n",
        "  There, you will not only use LSTM encoders/decorders, but also combine one-dimensional CNN with LSTMs. In the latter case, the CNN is used as an encoder to learn features from sub-sequences of input data which are provided as time steps to a decoding LSTM. This architecture is called a [CNN-LSTM](https://arxiv.org/abs/1411.4389).\n",
        "\n",
        "  We can go even further and use the CNN in combination with the LSTM as our encoder. This approach is called [ConvLSTM](https://arxiv.org/abs/1506.04214v1) and is expected to capture better spatiotemporal correlations. \n",
        "\n",
        "* An important improvement to RNNs is the use of the [Attention mechanism](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html), that helps memorizing long source sentences. Attention models are not extensively covered in the master (a couple of hours at the very end of the DL module), so that's the kind of material you could explore in your Master's Thesis ;) The cenit of the attention mechanisms is the so called [*self-attention*](http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf), which has given rise to the Transformer arquitecture. Based upon it, OpenAI has been capable of generating text of [very high quality](https://talktotransformer.com/)!\n",
        "\n",
        "* In the 2nd module, you will see how to apply these networks to NLP problems. Meanwhile, you might want to read about [image captioning](https://www.tensorflow.org/tutorials/text/image_captioning), [neural translation with attention](https://www.tensorflow.org/tutorials/text/nmt_with_attention), [text generation](https://www.tensorflow.org/tutorials/text/text_generation) or [question-answer generation]()\n"
      ]
    }
  ]
}